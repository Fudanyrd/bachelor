@Article{Li2025,
author="Li, Yingling
and Cai, Muxin
and Chen, Junjie
and Xu, Yang
and Huang, Lei
and Li, Jianping",
title="Context-aware prompting for LLM-based program repair",
journal="Automated Software Engineering",
year="2025",
month="Apr",
day="18",
volume="32",
number="2",
pages="42",
abstract="Automated program repair (APR) plays a crucial role in ensuring the quality of software code, as manual bug-fixing is extremely time-consuming and labor-intensive. Traditional APR tools (e.g., template-based approaches) face the challenge of generalizing to different bug patterns, while deep learning (DL)-based methods heavily rely on training datasets and struggle to fix unseen bugs. Recently, large language models (LLMs) have shown great potential in APR due to their ability to generate patches, having achieved promising results. However, their effectiveness is still constrained by the casually-determined context (e.g., being unable to adaptively select the specific context according to the situation of each defect). Therefore, a more effective APR approach is highly needed, which provides more precise and comprehensive context for the given defect to enhance the robustness of LLM-based APRs. In this paper, we propose a context-aware APR approach named CodeCorrector, which designs a Chain-of-Thought (CoT) approach to follow developers' program repair behaviors. Given a failing test and its buggy file, CodeCorrector first analyzes why the test fails based on the failure message to infer repair direction; then selects the relevant context information to this repair direction; finally builds the context-aware repair prompt to guide LLMs for patch generation. Our motivation is to offer a novel perspective for enhancing LLM-based program repair through context-aware prompting, which adaptively selects specific context for a given defect. The evaluation on the widely-used Defects4J (i.e., v1.2 and v2.0) benchmark shows that overall, by executing a small number of repairs (i.e., as few as ten rounds), CodeCorrector outperforms all the state-of-the-art baselines on the more complex defects in Defects4J v2.0 and the defects without fine-grained defect localization information in Defects4J v1.2. Specifically, a total of 38 defects are fixed by only CodeCorrector. We further analyze the contributions of two core components (i.e., repair directions, global context selection) to the performance of CodeCorrector, especially repair directions, which improve CodeCorrector by 112{\%} in correct patches and 78{\%} in plausible patches on Defects4J v1.2. Moreover, CodeCorrector generates more valid and correct patches, achieving a 377{\%} improvement over the base LLM GPT-3.5 and a 268{\%} improvement over GPT-4.",
issn="1573-7535",
doi="10.1007/s10515-025-00512-w",
url="https://doi.org/10.1007/s10515-025-00512-w"
}


@Article{Liu2025,
author="Liu, Zixin
and Du, Xiaozhi
and Liu, Hairui",
title="ReAPR: Automatic program repair via retrieval-augmented large language models",
journal="Software Quality Journal",
year="2025",
month="Jul",
day="22",
volume="33",
number="3",
pages="30",
abstract="Automatic Program Repair (APR) aims to automatically fix software defects, significantly reducing the efforts of manual debugging. Recent studies have demonstrated impressive results in utilizing Large Language Models (LLMs) for software bug fixing. Current LLM-based approaches depend solely on the pre-trained knowledge of LLMs, overlooking the prior knowledge contained in historical bug repair records, which increases the likelihood of hallucinations. To address this challenge, this paper proposes ReAPR, a retrieval-augmented framework for APR. We first curate a high-quality retrieval database by carefully compiling and filtering the existing datasets for APR. Subsequently, ReAPR leverages a retriever to fetch bug-fix pairs similar to the target bug from a retrieval database, providing contextual hints to guide the LLMs in the repair process. We then investigate two techniques to retrieve bug-fix pairs associated with the function to be fixed: BM25 and Dense Passage Retrieval (DPR). After retrieving the relevant bug-fix pair, we construct a prompt and integrate the retrieved pair into it. Besides, we also compare the proposed RAG-based approach with the parameter-efficient fine-tuning (PEFT) approaches on repair performance. To validate the effectiveness of ReAPR, we conduct extensive experiments based on the widely-used benchmark dataset Defects4j 2.0 as well as the latest benchmark GitBug-Java. The results show that ReAPR, based on the CodeLlama(7B) backbone, successfully fixes 68 and 59 bugs in the DPR and BM25 settings, respectively, in Defects4j 2.0, outperforming the best baseline approach by 18 and 9 bugs under the same repair settings.",
issn="1573-1367",
doi="10.1007/s11219-025-09728-1",
url="https://doi.org/10.1007/s11219-025-09728-1"
}


@Article{Wang2025,
author="Wang, Shaosheng
and Lu, Lu
and Qiu, Shaojian
and Tian, Qingyan
and Lin, Haishan",
title="DALO-APR: LLM-based automatic program repair with data augmentation and loss function optimization",
journal="The Journal of Supercomputing",
year="2025",
month="Mar",
day="21",
volume="81",
number="5",
pages="640",
abstract="Automatic program repair (APR) has made significant strides with the advent of large language models (LLMs) such as T5 and CodeT5. However, LLM-based APR models may rely on repetitive repair patterns due to limited training data diversity, resulting in suboptimal performance. Additionally, common loss functions, such as cross-entropy, may not fully prioritize repair locations or optimize the model's output probability distribution to favor more accurate repair candidates. To address these challenges, this paper proposes a method for LLM-Based APR with Data Augmentation and Loss Function Optimization (DALO-APR). The data augmentation strategy expands the variety of repair patterns by randomly deleting, inserting, swapping tokens, and injecting errors. The optimized loss function helps the model rank more accurate repair candidates higher. Experimental results on Java, JavaScript, Python, and C datasets demonstrate that DALO-APR improves both error localization and bug fixing. Compared to baseline models, DALO-APR shows improvements across multiple metrics, especially with a 105.65{\%} increase in 100{\%} accuracy.",
issn="1573-0484",
doi="10.1007/s11227-025-07102-3",
url="https://doi.org/10.1007/s11227-025-07102-3"
}


@inproceedings{10.1007/978-981-96-4509-1_28,
 abstract = {Automated Program Repair (APR) is currently receiving increasing attention in the field of software engineering because more and more bugs are emerging with the continuous expansion of program size. Recently, Large Language Models (LLMs) have got an outstanding progress in automated debugging field. Some researches utilized Language Models that pre-trained on source code, such as UnixCoder, Incoder, etc., to generate patches for bugs, and achieved impressive performance. Noteworthy, large language models are proved to benefit from in-context learning (ICL). By receiving well designed prompt, models can understand user needs better and provide a more accurate response. However, we find that existing prompted LLM-based APR methods either used too simple prompts or too complex ones. Therefore, this study explores and validates the proper structure of prompt that helps LLM fix more bugs. In this paper, we present a medium length prompt format to enhance CodeLlama's repairing capability. Specifically, we design prompt that include three part: role-profile, bug informations and repair instruction. For each bug, we extract their corresponding informations such as function description and bug location, fill in the prompt frame, and finally send it to CodeLlama to generate a patch. Our experiments show that the proper length of prompt can help CodeLlama model fix more bugs correctly.},
 address = {Singapore},
 author = {Liang, HanZhao
and Gao, Cuiyun
and Jia, Yan},
 booktitle = {Cyberspace Simulation and Evaluation},
 editor = {Xu, Guangxia
and Zhou, Wanlei
and Zhang, Jiawei
and Zhang, Yanchun
and Jia, Yan},
 isbn = {978-981-96-4509-1},
 pages = {413--427},
 publisher = {Springer Nature Singapore},
 title = {Exploration Study About LLM with Proper Prompt in Automated Program Repair},
 url = {https://link.springer.com/chapter/10.1007/978-981-96-4509-1_28},
 year = {2025}
}

@inproceedings{10.1007/978-3-031-78386-9_28,
 abstract = {Large Language Model-based Automated Program Repair (LLM-APR) has recently received significant attention as a debugging assistance. Our objective is to improve the performance of LLM-APR. In this study, we focus on semantic information contained in the source code. Semantic information refers to elements used by the programmer to understand the source code, which does not contribute to compilation or execution. We picked out specification, method names and variable names as semantic information. In the investigation, we prepared eight prompts, each consisting of all combinations of three types of semantic information. The experimental results showed that all semantic information improves the performance of LLM-APR, and variable names are particularly significant.},
 address = {Cham},
 author = {Hori, Shota
and Matsumoto, Shinsuke
and Higo, Yoshiki
and Kusumoto, Shinji
and Yasuda, Kazuya
and Ito, Shinji
and Huyen, Phan Thi Thanh},
 booktitle = {Product-Focused Software Process Improvement},
 editor = {Pfahl, Dietmar
and Gonzalez Huerta, Javier
and Kl{\"u}nder, Jil
and Anwar, Hina},
 isbn = {978-3-031-78386-9},
 pages = {377--385},
 publisher = {Springer Nature Switzerland},
 title = {The Effects of Semantic Information on LLM-Based Program Repair},
 url = {https://link.springer.com/chapter/10.1007/978-3-031-78386-9_28},
 year = {2025}
}

@Article{Du2024,
author="Du, Xiaoting
and Liu, Zhihao
and Li, Chenglong
and Ma, Xiangyue
and Li, Yingzhuo
and Wang, Xinyu",
title="LLM-BRC: A large language model-based bug report classification framework",
journal="Software Quality Journal",
year="2024",
month="Sep",
day="01",
volume="32",
number="3",
pages="985--1005",
abstract="Deep learning frameworks serve as the cornerstone for constructing robust deep learning systems. However, bugs within these frameworks can have severe consequences, negatively affecting various applications. Accurately classifying and understanding these bugs is essential to ensure framework reliability. By doing so, developers can proactively take appropriate measures to mitigate potential risks associated with specific bug types in both current and future software releases. Despite the significance of bug report classification, existing methods fall short in terms of performance, rendering them impractical for real-world applications. To address this limitation, we propose a bug report classification framework for deep learning frameworks, called LLM--BRC, leveraging OpenAI's latest embedding model, text-embedding-ada-002. Our LLM--BRC framework achieves an impressive accuracy range of 92{\%} to 98.75{\%} in bug report classification for three deep learning frameworks: TensorFlow, MXNET, and PaddlePaddle. This represents a substantial improvement of 17.21{\%} to 69.15{\%} compared to existing methods. Furthermore, we conduct a comprehensive investigation into the impact of different bug report components and different models.",
issn="1573-1367",
doi="10.1007/s11219-024-09675-3",
url="https://doi.org/10.1007/s11219-024-09675-3"
}


@Article{Lin2025,
author="Lin, Renze
and Wang, Ran
and Hu, Guanghuan
and Xu, Xianghua",
title="LMFuzz: Program repair fuzzing based on large language models",
journal="Automated Software Engineering",
year="2025",
month="Oct",
day="28",
volume="33",
number="1",
pages="25",
abstract="Generating programs using large language models (LLMs) for fuzz testing has emerged as a significant testing methodology. While traditional fuzzers can produce correct programs, their effectiveness is limited by excessive constraints and restricted API combinations, resulting in insufficient coverage of the target system's code and impacting testing efficiency. Unlike traditional methods, large language model based fuzzers can generate more diverse code, effectively addressing key issues of conventional fuzzers. However, the lack of constraints on API combinations during the generation process often leads to reduced program validity. Therefore, a crucial challenge is to enhance the validity of generated code while maintaining its diversity. To address this issue, we propose a novel and universal fuzzer, LMFuzz. To ensure the fuzzer's generation capability, we utilize a large language model as the primary generator and model the operator selection problem within the fuzzing loop as a multi-armed bandit problem. We introduce the Thompson Sampling algorithm to enhance both the diversity and validity of program generation. To improve the validity of the generated code, we incorporate a program repair loop that iteratively corrects the generated programs, thereby reducing errors caused by the lack of API combination constraints. Experimental results demonstrate that LMFuzz significantly surpasses existing state-of-the-art large language model based fuzzers in terms of coverage and validity, and also exhibits notable advantages in generating diverse programs. Furthermore, LMFuzz has identified 24 bugs across five popular programming languages and their corresponding systems.",
issn="1573-7535",
doi="10.1007/s10515-025-00568-8",
url="https://doi.org/10.1007/s10515-025-00568-8"
}


@Article{Dikici2025,
author="Dikici, Sena
and Bilgin, Turgay Tugay",
title="Advancements in automated program repair: a comprehensive review",
journal="Knowledge and Information Systems",
year="2025",
month="Jun",
day="01",
volume="67",
number="6",
pages="4737--4783",
abstract="This review paper presents a comprehensive examination of automated program repair (APR) and its significant contribution to the field of modern software engineering. It elucidates how APR methodologies markedly mitigate manual debugging needs by automating the detection and resolution of software glitches. The study encompasses an in-depth exploration of three primary categories of APR techniques: template-based, machine learning, and deep learning approaches, drawing from an exhaustive evaluation of 41 APR tools. Each category showcases distinct strategies for managing diverse software errors, underscoring the breadth and effectiveness of current APR methodologies. Template-based APR solutions utilize pre-established patterns to efficiently tackle common coding issues, while machine learning-driven approaches dynamically devise repair strategies from historical bug-fix datasets. Deep learning methods extend error rectification boundaries by delving into the semantic context of code, yielding more precise adjustments. The ongoing advancement of APR technologies necessitates researchers to address critical challenges, including the integration of semantic-syntactic analyses, mitigation of data scarcity, optimization of cross-platform tools, development of context-aware approaches, enhancement of fault localization and patch validation processes, and establishment of standardized performance evaluation metrics. This comprehensive analysis underscores the pivotal role of APR in enhancing software efficiency and reliability, representing significant progress in software development and maintenance practices.",
issn="0219-3116",
doi="10.1007/s10115-025-02383-9",
url="https://doi.org/10.1007/s10115-025-02383-9"
}


@Article{Kang2024,
author="Kang, Sungmin
and Chen, Bei
and Yoo, Shin
and Lou, Jian-Guang",
title="Explainable automated debugging via large language model-driven scientific debugging",
journal="Empirical Software Engineering",
year="2024",
month="Dec",
day="18",
volume="30",
number="2",
pages="45",
abstract="Automated debugging techniques have the potential to reduce developer effort in debugging. However, while developers want rationales for the provided automatic debugging results, existing techniques are ill-suited to provide them, as their deduction process differs significantly froof human developers. Inspired by the way developers interact with code when debugging, we propose Automated Scientific Debugging (AutoSD), a technique that prompts large language models to automatically generate hypotheses, uses debuggers to interact with buggy code, and thus automatically reach conclusions prior to patch generation. In doing so, we aim to produce explanations of how a specific patch has been generated, with the hope that these explanations will lead to enhanced developer decision-making. Our empirical analysis on three program repair benchmarks shows that AutoSDperforms competitively with other program repair baselines, and that it can indicate when it is confident in its results. Furthermore, we perform a human study with 20 participants to evaluate AutoSD-generated explanations. Participants with access to explanations judged patch correctness more accurately in five out of six real-world bugs studied. Furthermore, 70{\%} of participants answered that they wanted explanations when using repair tools, and 55{\%} answered that they were satisfied with the Scientific Debugging presentation.",
issn="1573-7616",
doi="10.1007/s10664-024-10594-x",
url="https://doi.org/10.1007/s10664-024-10594-x"
}


@inproceedings{10.1007/978-981-95-4499-8_33,
 abstract = {The versatility of large language models (LLMs) leads to an increasing penetration rate and market share of LLMs. Many new LLMs have launched, challenging the selection and evaluation of an appropriate LLM for target applications. In this research, attention is drawn to education as one of the most remarkable areas. Evaluating LLMs using different evaluation metrics is both subjective and objective. Using the advanced search tool in Scopus, 231 relevant studies have been shortlisted for the review of LLM evaluation metrics in education. Some basic analyses, including the number of publications by year, a frequency table of the subject areas, distribution of document types, distribution of source titles, a word cloud summary of the top 50 keywords, and distribution of regions, were provided. It was followed by an in-depth discussion of the top 10 highly cited publications. Then, five challenges in LLM evaluation were analyzed.},
 address = {Singapore},
 author = {Paoprasert, Naraphorn
and Chui, Kwok Tai
and Yip, Tik Yung
and Liu, Jiaqi
and Ng, Kwan Keung},
 booktitle = {Technology in Education. Smart and Innovative Learning},
 editor = {Cheung, Simon K. S.
and Liu, Xiaojun
and Xu, Guoai
and Kwok, Lam-For},
 isbn = {978-981-95-4499-8},
 pages = {402--415},
 publisher = {Springer Nature Singapore},
 title = {A Review of Large Language Model Evaluation Metrics in Education},
 url = {https://link.springer.com/chapter/10.1007/978-981-95-4499-8_33},
 year = {2026}
}

@Article{Yang2025,
author="Yang, Ruofan
and Xu, Xianghua
and Wang, Ran",
title="LLM-enhanced evolutionary test generation for untyped languages",
journal="Automated Software Engineering",
year="2025",
month="Feb",
day="17",
volume="32",
number="1",
pages="20",
abstract="Dynamic programming languages, such as Python, are widely used for their flexibility and support for rapid development. However, the absence of explicit parameter type declarations poses significant challenges in generating automated test cases. This often leads to random assignment of parameter types, increasing the search space and reducing testing efficiency. Current evolutionary algorithms, which rely heavily on random mutations, struggle to handle specific data types and frequently fall into local optima, making it difficult to generate high-quality test cases. Moreover, the resulting test suites often contain errors, preventing immediate usage in real-world applications. To address these challenges, this paper proposes the use of large language models to enhance test case generation for dynamic programming languages. Our method involves three key steps: analyzing parameter types to narrow the search space, introducing meaningful data during mutations to increase test case relevance, and using large language models to automatically repair errors in the generated test suites. Experimental results demonstrate a 16{\%} improvement in test coverage, faster evolutionary cycles, and an increase in the number of executable test suites. These findings highlight the potential of large language models in improving both the efficiency and reliability of test case generation for dynamic programming languages.",
issn="1573-7535",
doi="10.1007/s10515-025-00496-7",
url="https://doi.org/10.1007/s10515-025-00496-7"
}


@Article{Zhang2026,
author="Zhang, Chengming
and Wang, Haoye
and Xu, Chuyang
and Liu, Jiakun
and Liu, Kui
and Liu, Zhongxin",
title="Can test cases generated by large language models facilitate automated program repair?",
journal="Empirical Software Engineering",
year="2026",
month="Feb",
day="07",
volume="31",
number="3",
pages="68",
abstract="Automated program repair (APR) is proposed to reduce manual debugging efforts by automatically fixing buggy programs. Traditional APR techniques rely heavily on test cases, categorized into trigger-based and trigger-free approaches. While trigger-based methods achieve higher accuracy, their dependence on well-established test suites limits real-world applicability. Trigger-free methods, though more flexible, suffer from inferior fault localization performance due to the absence of trigger tests. Recent advances leverage large language models (LLMs) to generate bug-reproducing test cases, yet their systematic integration across the APR pipeline remains unexplored. This paper presents the first comprehensive study on leveraging LLM-generated tests throughout the APR workflow. We conduct experiments on 374 single-function bugs from Defects4J, systematically evaluating the impact of LLM-generated tests on fault localization, patch generation, and patch validation. Key findings reveal that: (1) Even LLM-generated tests with incorrect assertions can enhance fault localization for trigger-free APR by providing supplementary execution traces, improving Top@1 bug detection by 61{\%} ; (2) Incorporating these tests into patch ranking boosts repair effectiveness by 12.5{\%}--24.6{\%} across Top@1--Top@5 metrics. Then we propose a novel APR framework GT-Repair which incorporates LLM-generated tests into different repair stages. Compared to the trigger-free APR pipeline without LLM-generated tests, GT-Repair achieves a 38.8{\%} improvement in Top@1 repair performance. Besides, GT-Repair achieves state-of-the-art performance on Defects4J in trigger-free scenarios, compared to state-of-the-art test-based approaches. This work demonstrates the feasibility of LLM-generated tests in APR, and provides actionable insights for future works.",
issn="1573-7616",
doi="10.1007/s10664-026-10802-w",
url="https://doi.org/10.1007/s10664-026-10802-w"
}


@inproceedings{10.1007/978-981-95-2961-2_22,
 abstract = {The last 18 months have seen an explosion of activity in both industry and research in the Generative AI space, specifically Large Language Models (LLMs). Smart contract vulnerability detection is no exception; as smart contracts exist on public chains and can have billions of dollars transacted daily, continuous improvement in vulnerability detection is crucial. This has led to many researchers investigating the usage of generative large language models (LLMs) to aid in detecting vulnerabilities in smart contracts. This paper presents a systemic review of the current LLM-based smart contract vulnerability detection tools, comparing them against traditional static and dynamic analysis tools like Slither and Mythril. Our analysis highlights key areas where each performs better and shows that while these tools show promise, the LLM-based tools available for testing are not ready to replace more traditional tools. We conclude with recommendations on how LLMs are best used in the vulnerability detection process and offer insights for improving on the state-of-the-art via hybrid approaches and targeted pre-training of much smaller models.},
 address = {Singapore},
 author = {Ince, Peter
and Yu, Jiangshan
and Liu, Joseph K.
and Du, Xiaoning
and Luo, Xiapu},
 booktitle = {Provable and Practical Security},
 editor = {Yang, Guomin
and Liu, Shengli
and Su, Chunhua
and Otsuka, Akira
and Lian, Zhuotao},
 isbn = {978-981-95-2961-2},
 pages = {426--445},
 publisher = {Springer Nature Singapore},
 title = {GenDetect: Generative Large Language Model Usage in Smart Contract Vulnerability Detection},
 url = {https://link.springer.com/chapter/10.1007/978-981-95-2961-2_22},
 year = {2026}
}

@Article{Wang2025,
author="Wang, Bo
and Deng, Ming
and Chen, Mingda
and Lin, Youfang
and Zhou, Jianyi
and Zhang, Jie M.",
title="Assessing the effectiveness of recent closed-source large language models in fault localization and automated program repair",
journal="Automated Software Engineering",
year="2025",
month="Oct",
day="28",
volume="33",
number="1",
pages="26",
abstract="Large Language Models (LLMs) have made significant advancements in code-related tasks. In the field of automated debugging, fault localization (FL) and automated program repair (APR) are two prevalent topics attracting significant research effort. Recently, in the field of FL and APR, many novel LLM-based approaches have emerged. However, most existing LLM-based studies primarily focus on the GPT models from OpenAI or open-source LLMs. With the rapid development of LLMs, various internet giants have introduced new closed-source models. In addition, due to policy restrictions, some regions can only access the commercial LLMs provided by specified companies. Despite the LLMs of OpenAI, the effectiveness of the other closed-source LLMs in FL and APR remains unknown. To better understand the effectiveness of contemporary closed-source models, we conduct a large-scale empirical study on their performance with respect to FL and APR. Specifically, our study involves 4 recent commercial closed-source LLMs (i.e., GPT-4o-Mini, Ernie-3.5, Qwen-turbo, and Doubao-pro) and 1 open-source LLM (i.e., DeepSeek-V3-chat). Note that only the GPT models have region restrictions among all LLMs we studied. We designed a total of 12 distinct prompt templates, 6 each for FL and APR, incorporating various formats and information sources. We conducted experiments to evaluate the effectiveness of FL and APR on 1036 real Java bugs from two datasets, Defects4J 2.0 and ConDefects. The key findings of the experiments indicate that (1) different LLMs tend to succeed on different sets of bugs in both FL and APR, with relatively little overlap among successful cases, implying the models possess distinct strengths in handling specific kinds of bugs, (2) the effectiveness of prompt templates varies across different models, and (3) the effectiveness of FL and APR capabilities of the studied models is significantly correlated with the bug type. We summarized all 14 findings obtained into 3 implications, which could help researchers further improve the performance of LLMs on FL and APR.",
issn="1573-7535",
doi="10.1007/s10515-025-00549-x",
url="https://doi.org/10.1007/s10515-025-00549-x"
}


@Article{Bistarelli2025,
author="Bistarelli, Stefano
and Fiore, Marco
and Mercanti, Ivan
and Mongiello, Marina",
title="Usage of Large Language Model for Code Generation Tasks: A Review",
journal="SN Computer Science",
year="2025",
month="Jul",
day="23",
volume="6",
number="6",
pages="673",
abstract="Large Language Models have received a lot of attention in recent years due to their outstanding performance on various Natural Language Processing tasks. They can be used for lots of applications, including assistance in code generation tasks. Actual literature lacks an exhaustive analysis of the benefits and drawbacks of using a Large Language Model for the generation of simple and complex code. This paper aims to overcome the issue: we perform a Literature Review to explore the state-of-the-art of the proposed topic, answering 4 Research Questions. Using the PRISMA methodology, we reviewed 66 papers published between 2021 and 2023. Our analysis reveals Python's dominance as the preferred language and identifies a significant research gap in addressing ethical constraints. Additionally, we provide insights into the performance of models such as GPT-4 and CodeLlama, and their comparative utility in tasks ranging from debugging to multi-turn program synthesis. The findings offer a foundation for future research aimed at optimizing LLMs for code generation.",
issn="2661-8907",
doi="10.1007/s42979-025-04241-5",
url="https://doi.org/10.1007/s42979-025-04241-5"
}


@Article{Zhao2025,
author="Zhao, Qianhui
and Zhang, Li
and Liu, Fang
and Liu, Yang
and Yan, Zhen
and Chen, Zhenghao
and Zhou, Yufei
and Jiang, Jing
and Li, Ge
and Sun, Zian
and Li, Zhongqi
and Ma, Yuchi",
title="Peer-aided repairer: empowering large language models to repair advanced student assignments",
journal="Empirical Software Engineering",
year="2025",
month="Dec",
day="05",
volume="31",
number="2",
pages="33",
abstract="Automated generation of feedback on programming assignments holds significant benefits for programming education, especially when it comes to advanced assignments. Automated Program Repair techniques, especially Large Language Model-based approaches, have gained notable recognition for their potential in fixing introductory assignments. However, the programs used for evaluation are relatively simple. It remains unclear how existing approaches perform in repairing programs from higher-level programming courses. To address these limitations, we curate a new advanced student assignment dataset named Defects4DS from a higher-level programming course. Subsequently, we identify the challenges related to fixing bugs in advanced assignments. Based on the analysis, we develop a framework called PaR that is powered by the Large Language Models. PaR works in three phases: Peer Solution Selection, Multi-Source Prompt Generation, and Program Repair. Peer Solution Selection identifies the closely related peer programs based on lexical, semantic, and syntactic criteria. Then Multi-Source Prompt Generation adeptly combines multiple sources of information to create a comprehensive and informative prompt for the last Program Repair stage. Evaluation reveals that PaR achieves state-of-the-art performance on Defects4DS compared to baseline approaches, with the impressive improvement of 16.13{\%} in repair rate. And experimental results on several introductory programming assignment datasets further demonstrate the effectiveness of PaR, achieving state-of-the-art results on ITSP and IntroClass datasets.",
issn="1573-7616",
doi="10.1007/s10664-025-10716-z",
url="https://doi.org/10.1007/s10664-025-10716-z"
}


@Article{Rong2025,
author="Rong, Yi
and Du, Tianfeng
and Li, Roubing
and Bao, Wenting",
title="Integrating LLM-based code optimization with human-like exclusionary reasoning for computational education",
journal="Journal of King Saud University Computer and Information Sciences ",
year="2025",
month="Jun",
day="23",
volume="37",
number="5",
pages="87",
abstract="Large Language Models (LLMs) are increasingly deployed as intelligent tutors that not only generate but also refine source code for educational purposes. Yet existing end-to-end fine-tuning strategies compel models to transform every input, often introducing superfluous or even detrimental edits that undermine both software quality and pedagogical clarity. We address this limitation by formulating exclusionary reasoning-the human practice of asking ``Should I optimize?'' before acting-as an explicit decision layer in the code-optimization pipeline. Concretely, we devise a two-stage framework in which an LLM first diagnoses whether a code segment merits modification and proceeds with optimization only when necessary, otherwise returning the original snippet verbatim. Implemented on a suite of open-source models and trained with publicly available Python corpora, our method proves model-agnostic and lightweight. Experiments on three standard benchmarks show consistent gains in functional correctness (pass@1/3/5) over conventional fine-tuning, yielding feedback that is both more accurate and easier for students to interpret. By aligning automated optimization with human selective judgment, the proposed framework transforms LLMs from indiscriminate code generators into credible virtual teaching assistants that intervene sparingly, explain clearly, and foster deeper learning of principled programming practices.",
issn="2213-1248",
doi="10.1007/s44443-025-00074-7",
url="https://doi.org/10.1007/s44443-025-00074-7"
}


@Article{Brownlee2025,
author="Brownlee, Alexander E. I.
and Callan, James
and Even-Mendoza, Karine
and Geiger, Alina
and Hanna, Carol
and Petke, Justyna
and Sarro, Federica
and Sobania, Dominik",
title="Large language model based mutations in genetic improvement",
journal="Automated Software Engineering",
year="2025",
month="Jan",
day="21",
volume="32",
number="1",
pages="15",
abstract="Ever since the first large language models (LLMs) have become available, both academics and practitioners have used them to aid software engineering tasks. However, little research as yet has been done in combining search-based software engineering (SBSE) and LLMs. In this paper, we evaluate the use of LLMs as mutation operators for genetic improvement (GI), an SBSE approach, to improve the GI search process. In a preliminary work, we explored the feasibility of combining the Gin Java GI toolkit with OpenAI LLMs in order to generate an edit for the JCodec tool. Here we extend this investigation involving three LLMs and three types of prompt, and five real-world software projects. We sample the edits at random, as well as using local search. We also conducted a qualitative analysis to understand why LLM-generated code edits break as part of our evaluation. Our results show that, compared with conventional statement GI edits, LLMs produce fewer unique edits, but these compile and pass tests more often, with the OpenAI model finding test-passing edits 77{\%} of the time. The OpenAI and Mistral LLMs are roughly equal in finding the best run-time improvements. Simpler prompts are more successful than those providing more context and examples. The qualitative analysis reveals a wide variety of areas where LLMs typically fail to produce valid edits commonly including inconsistent formatting, generating non-Java syntax, or refusing to provide a solution.",
issn="1573-7535",
doi="10.1007/s10515-024-00473-6",
url="https://doi.org/10.1007/s10515-024-00473-6"
}


@inproceedings{10.1007/978-981-97-8540-7_17,
 abstract = {Path Traversal Vulnerability is a significant security flaw that allows attackers to exploit the file system structure of web applications by manipulating user input to access files outside the intended directory structure. This vulnerability can lead to unauthorized access to sensitive files and directories, resulting in severe consequences such as information disclosure, data manipulation, and system compromise. Despite its high likelihood of exploitation, as ranked eighth in the 2023 CWE Top 25 Most Dangerous Software Weaknesses, automated repair methods for this vulnerability, particularly in Java, remain underdeveloped. This paper introduces a methodology, named PTFix, which is a rule-based and LLM technique for repairing Java Path Traversal vulnerability. PTFix includes two stages: 1) analyze and patch Java path traversal vulnerability based on pre-defined rules; 2) integrate with LLMs to patch Java codes that do not match the rule. A comparative study was conducted using four large language models: Meta Llama2 7B, Codellama Instruct 34B, Claude3, and ChatGPT-4. The results revealed that while Meta Llama2 7B and Codellama Instruct 34B failed to correctly fix any examples, Claude3 successfully repaired two instances, and ChatGPT-4 outperformed the other models by correctly repairing four examples. These findings highlight the potential of combining static rule-based methods with LLMs to improve the automated repair of path traversal vulnerabilities in Java applications.},
 address = {Singapore},
 author = {Zhang, Xiaowei
and Liu, Shigang
and Zhang, Jun
and Xiang, Yang},
 booktitle = {Data Security and Privacy Protection},
 editor = {Chen, Xiaofeng
and Huang, Xinyi
and Yung, Moti},
 isbn = {978-981-97-8540-7},
 pages = {276--293},
 publisher = {Springer Nature Singapore},
 title = {PTFix: Rule-Based and LLM Techniques for Java Path Traversal Vulnerability},
 url = {https://link.springer.com/chapter/10.1007/978-981-97-8540-7_17},
 year = {2025}
}

@Article{Hong2024,
author="Hong, Jaemin
and Ryu, Sukyoung",
title="Type-migrating C-to-Rust translation using a large language model",
journal="Empirical Software Engineering",
year="2024",
month="Oct",
day="17",
volume="30",
number="1",
pages="3",
abstract="Rust, a modern system programming language, introduces new types that prevent memory bugs and data races. This makes translating legacy system programs from C to Rust a promising approach to enhance their reliability. Since manual code translation is time-consuming, it is desirable to automate the translation. To yield satisfactory results, the translator should have the ability to perform type migration, i.e., removing C types and introducing Rust types in the code. In this work, we aim to automatically port an entire C program to Rust by translating each C function to a Rust function with a signature containing proper Rust types through type migration. This goal is challenging because (1) type migration cannot be achieved through syntactic mappings between type names, and (2) after type migration, function bodies should be correctly restructured based on the precise understanding of the functions' behavior. To address these difficulties, we leverage large language models (LLMs), which possess knowledge of program semantics and programming idioms. However, na{\"i}vely instructing LLMs to translate each function produces unsatisfactory Rust code, containing unmigrated or improperly migrated types and a huge number of type errors. To resolve these issues, we propose three techniques: (1) generating candidate signatures, (2) providing translated callees' signatures to LLMs, and (3) iteratively fixing type errors using compiler feedback. Our evaluation shows that the proposed approach yields a 63.5{\%} increase in migrated types and a 71.5{\%} decrease in type errors compared to the baseline (the na{\"i}ve LLM-based translation) with modest performance overhead.",
issn="1573-7616",
doi="10.1007/s10664-024-10573-2",
url="https://doi.org/10.1007/s10664-024-10573-2"
}


@Article{Busch2025,
author="Busch, Daniel
and Bainczyk, Alexander
and Smyth, Steven
and Steffen, Bernhard",
title="LLM-based code generation and system migration in language-driven engineering",
journal="International Journal on Software Tools for Technology Transfer",
year="2025",
month="Feb",
day="01",
volume="27",
number="1",
pages="137--147",
abstract="This paper illustrates the power of extending Language Driven Engineering (LDE) with Domain-Specific Natural Languages (DSNLs) through a case study on two levels. Both cases benefit from the characteristic decomposition feature of LDE, resulting in tasks tailored to the application of domain-specific languages, here with a focus on the application of DSNLs supported by LLM-based code generation. In the first case study, we show how DSNL-supported LDE facilitates the development of point-and-click adventures, whereas the second case study focuses on migration: We demonstrate how the entire LDE scenario for point-and-click adventure games can be migrated to output TypeScript instead of JavaScript using LLM-based code generation exclusively, without manually writing any code. This migration not only infers the required types, but also preserves an important property of the original LDE scenario: generated web applications can be automatically validated by design via automata learning and subsequent model checking. Even better, this property can be exploited to automatically validate the correctness of the migration by learning so-called difference automata that characterize the behavioral differences between the generated JavaScript-based and Type-Script-based applications.",
issn="1433-2787",
doi="10.1007/s10009-025-00798-x",
url="https://doi.org/10.1007/s10009-025-00798-x"
}


@Article{Zheng2024,
author="Zheng, Zibin
and Ning, Kaiwen
and Zhong, Qingyuan
and Chen, Jiachi
and Chen, Wenqing
and Guo, Lianghong
and Wang, Weicheng
and Wang, Yanlin",
title="Towards an understanding of large language models in software engineering tasks",
journal="Empirical Software Engineering",
year="2024",
month="Dec",
day="26",
volume="30",
number="2",
pages="50",
abstract="Large Language Models (LLMs) have drawn widespread attention and research due to their astounding performance in text generation and reasoning tasks. Derivative products, like ChatGPT, have been extensively deployed and highly sought after. Meanwhile, the evaluation and optimization of LLMs in software engineering tasks, such as code generation, have become a research focus. However, there is still a lack of systematic research on applying and evaluating LLMs in software engineering. Therefore, this paper comprehensively investigate and collate the research and products combining LLMs with software engineering, aiming to answer two questions: (1) What are the current integrations of LLMs with software engineering? (2) Can LLMs effectively handle software engineering tasks? To find the answers, we have collected related literature as extensively as possible from seven mainstream databases and selected 123 timely papers published starting from 2022 for analysis. We have categorized these papers in detail and reviewed the current research status of LLMs from the perspective of seven major software engineering tasks, hoping this will help researchers better grasp the research trends and address the issues when applying LLMs. Meanwhile, we have also organized and presented papers with evaluation content to reveal the performance and effectiveness of LLMs in various software engineering tasks, guiding researchers and developers to optimize.",
issn="1573-7616",
doi="10.1007/s10664-024-10602-0",
url="https://doi.org/10.1007/s10664-024-10602-0"
}


@Article{Deng2025,
author="Deng, Liangjun
and Zhong, Qi
and Qiu, Yao
and Chen, Jingxue
and Lei, Hang
and Yang, Shunkun
and Zhou, Liming
and Cheng, Hongyuan",
title="LLM-based program analysis for source codes, abstract syntax trees and webassembly instructions",
journal="Cluster Computing",
year="2025",
month="Sep",
day="29",
volume="28",
number="14",
pages="892",
abstract="The advancement of Web3.0 technology has brought about a urgent need for ensuring the safety and reliability of the software systems. Program analysis, a crucial aspect of software security research need a unified solution for various cross-language program. Moreover, the previous studies regard the necessity of capturing structured features from the ASTs, commonly holding a conception that plain text and compiled binary instructions are challenging to represent and identify. This paper proposes three methods for extracting structural information to demonstrate that the underlying principles of source code and instructions align with those of an abstract syntax tree. These methods include (1) embedding program instruction files directly into natural language, (2) embedding formatted source code into natural language, and (3) embedding misformatted, non-compilable source code into natural language. We train large-scale language models (LLMs) to identify defect and non-defect of WebAssembly. Experiment results demonstrate that program instruction analysis surpasses traditional techniques, achieving state-of-the-art accuracy exceeding 98.1{\%}. Our study also suggests a practical approach of plain text embedding using a 7.65 billion LLM. Interestingly, misformatted source codes are readable to humans but un-compilable, and the accuracy remains above 98.63{\%}. This paper not only introduces novel instruction and plain text embedding approach for future program security analysis, but also provides new insights for subsequent research about the three program analysis forms of plain text, ASTs, and instructions.",
issn="1573-7543",
doi="10.1007/s10586-025-05557-w",
url="https://doi.org/10.1007/s10586-025-05557-w"
}


@inproceedings{10.1007/978-981-95-3495-1_21,
 abstract = {The rapid growth in software development has led to difficulty maintaining high-quality and secure code. The reason for this is the increasing complexity of modern software systems. Traditional methods like static analysis and manual code review and refinement are time-consuming and prone to human errors which states the necessity of finding advanced automated solutions. Our study uses buggy and fixed Java code snippets to fine-tune various pre-trained Large Language Models (LLMs) and compare them based on their performance on code refinement tasks. Models like CodeT5, CodeGen2, and PolyCoder were compared using BLEU, ROUGE, and Exact Match metrics. CodeGen2 and PolyCoder achieved the highest BLEU and ROUGE scores. Hence, this paper highlights the potential of LLMs in automating code refinement tasks and provides insights for future improvements in this field.},
 address = {Singapore},
 author = {Magar, Abhay
and Joshi, Krisha
and Shah, Pooja
and Mishra, Shakti},
 booktitle = {Proceedings of International Conference on Communication and Computational Technologies},
 editor = {Kumar, Sandeep
and Hiranwal, Saroj
and Chouhan, Lokesh
and Chaudhary, Naveen Kumar},
 isbn = {978-981-95-3495-1},
 pages = {291--302},
 publisher = {Springer Nature Singapore},
 title = {Enhancing Code Quality Using Pre-trained Language Models: A Fine-Tuning Approach for Code Refinement},
 url = {https://link.springer.com/chapter/10.1007/978-981-95-3495-1_21},
 year = {2026}
}

@Article{Omari2024,
author="Omari, Safwan
and Basnet, Kshitiz
and Wardat, Mohammad",
title="Investigating large language models capabilities for automatic code repair in Python",
journal="Cluster Computing",
year="2024",
month="Nov",
day="01",
volume="27",
number="8",
pages="10717--10731",
abstract="Developers often encounter challenges with their introductory programming tasks as part of the development process. Unfortunately, rectifying these mistakes manually can be time-consuming and demanding. Automated program repair (APR) techniques offer a potential solution by synthesizing fixes for such errors. Previous research has investigated the utilization of both symbolic and neural techniques within the APR domain. However, these approaches typically demand significant engineering efforts or extensive datasets and training. In this paper, we explore the potential of using a large language model trained on code, specifically, we assess ChatGPT's capability to detect and repair bugs in simple Python programs. The experimental evaluation encompasses two benchmarks: QuixBugs and Textbook. Each benchmark consists of simple Python functions that implement well-known algorithms and each function contains a single bug. To gauge repair performance in various settings, several benchmark variations were introduced including addition of plain English documentation and code obfuscation. Based on thorough experiments, we found that ChatGPT was able to correctly detect and fix about 50{\%} of the methods, when code is documented. Repair performance drops to 25{\%} when code is obfuscated, and 15{\%} when documentation is removed and code is obfuscated. Furthermore, when compared to existing APR systems, ChatGPT considerably outperformed them.",
issn="1573-7543",
doi="10.1007/s10586-024-04490-8",
url="https://doi.org/10.1007/s10586-024-04490-8"
}


@Article{Zhang2025,
author="Zhang, Jie
and Bu, Haoyu
and Wen, Hui
and Liu, Yongji
and Fei, Haiqiang
and Xi, Rongrong
and Li, Lun
and Yang, Yun
and Zhu, Hongsong
and Meng, Dan",
title="When LLMs meet cybersecurity: a systematic literature review",
journal="Cybersecurity",
year="2025",
month="Feb",
day="05",
volume="8",
number="1",
pages="55",
abstract="The rapid development of large language models (LLMs) has opened new avenues across various fields, including cybersecurity, which faces an evolving threat landscape and demand for innovative technologies. Despite initial explorations into the application of LLMs in cybersecurity, there is a lack of a comprehensive overview of this research area. This paper addresses this gap by providing a systematic literature review, covering the analysis of over 300 works, encompassing 25 LLMs and more than 10 downstream scenarios. Our comprehensive overview addresses three key research questions: the construction of cybersecurity-oriented LLMs, the application of LLMs to various cybersecurity tasks, the challenges and further research in this area. This study aims to shed light on the extensive potential of LLMs in enhancing cybersecurity practices and serve as a valuable resource for applying LLMs in this field. We also maintain and regularly update a list of practical guides on LLMs for cybersecurity at https://github.com/tmylla/Awesome-LLM4Cybersecurity.",
issn="2523-3246",
doi="10.1186/s42400-025-00361-w",
url="https://doi.org/10.1186/s42400-025-00361-w"
}


@inproceedings{10.1007/978-3-032-10444-1_16,
 abstract = {Debugging and repairing faults when programs fail to formally verify can be complex and time-consuming. Automated Program Repair (APR) can ease this burden by automatically identifying and fixing faults. However, traditional APR techniques often rely on test suites for validation, but these may not capture all possible scenarios. In contrast, formal specifications provide strong correctness criteria, enabling more effective automated repair.},
 address = {Cham},
 author = {Wu, Valentina
and Mendes, Alexandra
and Abreu, Alexandre},
 booktitle = {Software Engineering and Formal Methods},
 editor = {Bianculli, Domenico
and G{\'o}mez-Mart{\'i}nez, Elena},
 isbn = {978-3-032-10444-1},
 pages = {261--278},
 publisher = {Springer Nature Switzerland},
 title = {Specification-Guided Repair of Arithmetic Errors in Dafny Programs Using LLMs},
 url = {https://link.springer.com/chapter/10.1007/978-3-032-10444-1_16},
 year = {2026}
}

@Article{Alsofyani2025,
author="Alsofyani, May
and Wang, Liqiang",
title="Evaluating ChatGPT's strengths and limitations for data race detection in parallel programming via prompt engineering",
journal="The Journal of Supercomputing",
year="2025",
month="Apr",
day="23",
volume="81",
number="6",
pages="776",
abstract="Large Language Models have significantly advanced software engineering, enabling tasks like code comprehension and fault detection. However, their ability to detect complex bugs, such as data races in parallel programming, remains uncertain. Fault detection in parallel programming (Pthreads) requires a deep understanding of thread-based logic, as data races occur when threads access shared data concurrently without proper synchronization. This paper explores ChatGPT's potential in Pthreads fault detection by addressing three questions: (1) Can ChatGPT effectively debug parallel programming threads? (2) How can dialogue assist with the detection of faults? (3) How can prompt engineering help to improve ChatGPT's fault detection performance?. We examine advanced prompt engineering techniques, such as Zero-Shot, Few-Shot, Chain-of-Thought, and Retrieval-Augmented Generation prompts. Additionally, we introduce three hybrid prompting techniques to enhance performance, including Chain-of-Thought with Few-Shot Prompting, Retrieval-Augmented Generation with Few-Shot Prompting, and Prompt Chaining with Few-Shot Prompting, while evaluating ChatGPT's strengths and limitations for data race detection.",
issn="1573-0484",
doi="10.1007/s11227-025-07237-3",
url="https://doi.org/10.1007/s11227-025-07237-3"
}


@inproceedings{10.1007/978-981-97-3442-9_34,
 abstract = {Fixing bugs in the wild is a challenging task. Automating it would help software development immensely. Recently, Deep Learning-based approaches, including Large Language Models, have made great progress in this direction. Unfortunately, not many of these models have been open sourced, and proprietary models often outperform open-sourced ones. In this paper we investigate whether the recently open-sourced model, CodeLlama, can be improved through fine-tuning to be competitive on the bugfixing task. We investigate two kinds of tasks: singular bugfixing and whole program bugfixing. For fine-tuning, we use code from GitHub for the former and Project CodeNet published by IBM Research for the latter. The base and fine-tuned models are evaluated on the QuixBugs dataset for singular bugfixing and a collection of incorrect assignment submissions coded by students for whole program bugfixing. We find that our fine-tuned models outperform ChatGPT (GPT-3.5) on singular bugfixing. On whole program bugixing, they come close in the number of incorrect programs fixed while corrupting only half as many.},
 address = {Singapore},
 author = {Szalontai, Bal{\'a}zs
and Vad{\'a}sz, Andr{\'a}s
and M{\'a}rton, Tam{\'a}s
and Pint{\'e}r, Bal{\'a}zs
and Gregorics, Tibor},
 booktitle = {Proceedings of International Conference on Recent Innovations in Computing},
 editor = {Ill{\'e}s, Zolt{\'a}n
and Verma, Chaman
and Gon{\c{c}}alves, Paulo J. Sequeira
and Singh, Pradeep Kumar},
 isbn = {978-981-97-3442-9},
 pages = {497--509},
 publisher = {Springer Nature Singapore},
 title = {Fine-Tuning CodeLlama to Fix Bugs},
 url = {https://link.springer.com/chapter/10.1007/978-981-97-3442-9_34},
 year = {2024}
}

@inproceedings{10.1007/978-3-031-72344-5_27,
 abstract = {Large Language Models trained on Code (LLMCs) have been shown to be effective in Automated Program Repair (APR) tasks, introducing new innovations to the field. Typically, LLMCs do not engage in error localization for APR tasks, instead treating APR more as a code refinement task. This approach often results in larger edit distances, altering the program's original structure. The principle of making minimal edits is crucial in certain scenarios, such as when correcting student programming assignments or software group development, where it's better to preserve the original intent of the code with as few changes as possible. To address these challenges, we introduce a hybrid framework for automated program repair that combines graph neural networks and large language models, which we refer to as HFRepair. HFRepair leverages the precise error localization capability of DrRepair for C programs, combining it with LLMCs to perform the line-level APR task based on the code context, aiming for minimal edits. Our experimental results demonstrate that HFRepair significantly outperforms previous state-of-the-art methods in benchmark tests. For instance, on the DeepFix dataset, HFRepair improves the full repair rate from 67.9{\%} and 71.4{\%} (achieved by DrRepair and BIFI, respectively) to 82.2{\%}, while reducing average edit distance from 33.4 and 27.7 to 11.6.},
 address = {Cham},
 author = {Xu, Zhenyu
and Sheng, Victor S.},
 booktitle = {Artificial Neural Networks and Machine Learning -- ICANN 2024},
 editor = {Wand, Michael
and Malinovsk{\'a}, Krist{\'i}na
and Schmidhuber, J{\"u}rgen
and Tetko, Igor V.},
 isbn = {978-3-031-72344-5},
 pages = {402--416},
 publisher = {Springer Nature Switzerland},
 title = {Towards Minimal Edits in Automated Program Repair: A Hybrid Framework Integrating Graph Neural Networks and Large Language Models},
 url = {https://link.springer.com/chapter/10.1007/978-3-031-72344-5_27},
 year = {2024}
}

@Article{Xu2025,
author="Xu, Caixu
and Guo, Hui
and Cen, Caicun
and Chen, Minglang
and Tao, Xiongjie
and He, Jie",
title="Efficient program optimization through knowledge-enhanced LoRA fine-tuning of large language models",
journal="The Journal of Supercomputing",
year="2025",
month="Jun",
day="10",
volume="81",
number="8",
pages="1006",
abstract="Source code optimization enables developers to enhance programs at the human--computer interaction level, thereby improving development efficiency and product quality. With the rise of large language models (LLMs), fine-tuning and prompting have become mainstream solutions for this task. However, both approaches present challenges: fine-tuning is resource-intensive due to the exponential growth in the scale of LLMs, whereas prompting, although resource-efficient, struggles to generate high-quality optimized programs. In this paper, we present CodeOPT, a LoRA-driven approach for fine-tuning LLMs to optimize C/C++ code. Instead of fine-tuning all LLM parameters, CodeOPT leverages LoRA to fine-tune only an optimization adapter, significantly reducing the number of trainable parameters. Additionally, we incorporate prior optimization knowledge during fine-tuning and introduce optimization-based instruction fine-tuning, enabling LLMs to effectively learn from external knowledge sources to improve program optimization. To evaluate the effectiveness of CodeOPT, we benchmarked it against several baselines on challenging programming tasks from different code completion platforms. Experimental results demonstrate that CodeOPT outperforms all baselines, including the state of the art, while keeping modifications to the original program minimal.",
issn="1573-0484",
doi="10.1007/s11227-025-07378-5",
url="https://doi.org/10.1007/s11227-025-07378-5"
}


@Article{TaghaviFar2025,
author="Taghavi Far, Seyed Mohammad
and Feyzi, Farid",
title="Large language models for software vulnerability detection: a guide for researchers on models, methods, techniques, datasets, and metrics",
journal="International Journal of Information Security",
year="2025",
month="Feb",
day="14",
volume="24",
number="2",
pages="78",
abstract="Large language models (LLMs) have emerged as transformative tools in the domain of software vulnerability detection and management, offering sophisticated capabilities in identifying, analyzing, and mitigating security risks. This article delves into the utilization of LLMs, examining their role in revolutionizing traditional approaches to software vulnerability detection. We explore the various categories of LLMs, such as bidirectional encoder representations from transformers (BERT) and generative pre-trained transformer (GPT), and how these models are being leveraged to improve the accuracy and efficiency of vulnerability detection. This article reviews how LLMs are being integrated into existing software security frameworks, synthesizing research findings on their performance in various contexts. It includes insights into how LLM-based methods complement traditional techniques like static analysis and fuzz testing, without engaging in a direct comparative analysis of these approaches. The comparison highlights the strengths of LLMs, such as their ability to generalize across diverse codebases and programming languages, while also addressing their limitations, such as susceptibility to biases from training data and the hallucination. The article synthesizes findings from recent research, showcasing how LLMs have been successfully employed to detect a range of vulnerabilities, from buffer overflows to SQL injections, and outlines how these models enhance productivity by automating the detection and reporting of security flaws. Additionally, we discuss the inherent challenges in applying LLMs to software vulnerability detection, such as the need for high-quality datasets, and the ethical implications related to the deployment of LLM-based systems in security-critical applications. Addressing these challenges is crucial for the future advancement of LLM technologies in the cybersecurity domain. A comprehensive introduction to foundational and specialized datasets is provided, including datasets such as CVEfixes, Big-Vul, and LineVul, which are tailored for software vulnerability detection. These datasets serve as crucial resources for training and benchmarking LLMs. Moreover, we introduce evaluation metrics such as F1-score, precision, recall, and AUC-ROC that are used to assess the performance of models in detecting and mitigating vulnerabilities, offering a structured way to gauge the success and limitations of LLMs. In addition, the article explores fine-tuning techniques such as full fine-tuning, feature extraction, adapter-based fine-tuning, and LoRA (low-rank adaptation), highlighting how each method can enhance LLM performance in vulnerability detection. By focusing on parameter-efficient fine-tuning approaches, such as adapter layers and prefix-tuning, and LoRa, we outline ways to optimize model performance while reducing computational overhead. By providing a comprehensive review of the literature and practical insights into LLM integration, this article aims to fill the gap in existing research and serve as a foundational guide for future investigations. Researchers and practitioners in the field of software security will benefit from the comparative analyses, detailed case studies, and strategic recommendations provided herein, which collectively highlight the potential of LLMs to complement and enhance traditional software vulnerability detection techniques.",
issn="1615-5270",
doi="10.1007/s10207-025-00992-7",
url="https://doi.org/10.1007/s10207-025-00992-7"
}


@inproceedings{10.1007/978-3-032-09044-7_9,
 abstract = {Automated program repair (APR) research predominantly focuses on Python environments, creating significant infrastructure gaps for compiled languages like C, C++, and Java that dominate production systems. We present the first systematic pipeline addressing multi-language APR infrastructure limitations through compiler-assisted dataset curation and paradigm-aware evaluation frameworks. Our approach combines a DFA-based code classification system achieving 92.4{\%} accuracy in programming paradigm detection with systematic dataset filtering that processes over 3 million samples to extract 30,000 high-quality object-oriented examples. Initial evaluation on Qwen3-14B using LoRA fine-tuning reveals critical adaptation thresholds: effective multi-language adaptation requires modification of approximately 1.2{\%} or more model parameters, with lighter fine-tuning underperforming baseline models. Our open-source pipeline provides end-to-end infrastructure from compiler-assisted dataset curation to cloud deployment, enabling systematic research advancement in multi-language automated program repair and establishing methodological foundations for compiler-assisted machine learning across diverse programming environments.},
 address = {Cham},
 author = {Pineda, Moises
and Luna, Diego
and Esquivel, Mariana
and Bours, Jes{\'u}s
and Salazar, Juan
and Flores-Araiza, Dainel
and Hinojosa, Salvador},
 booktitle = {Advances in Soft Computing},
 editor = {Mart{\'i}nez-Villase{\~{n}}or, Lourdes
and V{\'a}zquez, Roberto A.
and Ochoa-Ruiz, Gilberto},
 isbn = {978-3-032-09044-7},
 pages = {115--127},
 publisher = {Springer Nature Switzerland},
 title = {Beyond SWE-Bench: A Compiler-Assisted Pipeline for Multi-language Automated Program Repair},
 url = {https://link.springer.com/chapter/10.1007/978-3-032-09044-7_9},
 year = {2026}
}

@inproceedings{10.1007/978-3-031-97207-2_31,
 abstract = {In this paper, we address the problem of manual debugging, which nowadays remains resource-intensive and in some parts archaic. This problem is especially evident in increasingly complex and distributed software systems. Therefore, our objective of this work is to introduce an approach that can possibly be applied to any system, at both the macro- and micro-level, to ease this debugging process. This approach utilizes a system's process data, in conjunction with generative AI, to generate natural-language explanations. These explanations are generated from the actual process data, interface information, and documentation to guide the developers more efficiently to understand the behavior and possible errors of a process and its sub-processes. Here, we present a demonstrator that employs this approach on a component-based Java system. However, our approach is language-agnostic. Ideally, the generated explanations will provide a good understanding of the process, even if developers are not familiar with all the details of the considered system. Our demonstrator is provided as an open-source web application that is freely accessible to all users.},
 address = {Cham},
 author = {Schiese, Dennis
and Both, Andreas},
 booktitle = {Web Engineering},
 editor = {Verma, Himanshu
and Bozzon, Alessandro
and Mauri, Andrea
and Yang, Jie},
 isbn = {978-3-031-97207-2},
 pages = {379--382},
 publisher = {Springer Nature Switzerland},
 title = {Post-hoc LLM-Supported Debugging of Distributed Processes},
 url = {https://link.springer.com/chapter/10.1007/978-3-031-97207-2_31},
 year = {2026}
}

@Article{Cao2025,
author="Cao, Jialun
and Li, Meiziniu
and Wen, Ming
and Cheung, Shing-Chi",
title="A study on prompt design, advantages and limitations of ChatGPT for deep learning program repair",
journal="Automated Software Engineering",
year="2025",
month="Mar",
day="07",
volume="32",
number="1",
pages="30",
abstract="The emergence of large language models (LLMs) such as ChatGPT has revolutionized many fields. In particular, recent advances in LLMs have triggered various studies examining the use of these models for software development tasks, such as program repair, code understanding, and code generation. Prior studies have shown the capability of ChatGPT in repairing conventional programs. However, debugging deep learning (DL) programs poses unique challenges since the decision logic is not directly encoded in the source code. This requires LLMs to not only parse the source code syntactically but also understand the intention of DL programs. Therefore, ChatGPT's capability in repairing DL programs remains unknown. To fill this gap, our study aims to answer three research questions: (1) Can ChatGPT debug DL programs effectively? (2) How can ChatGPT's repair performance be improved by prompting? (3) In which way can dialogue help facilitate the repair? Our study analyzes the typical information that is useful for prompt design and suggests enhanced prompt templates that are more efficient for repairing DL programs. On top of them, we summarize the dual perspectives (i.e., advantages and disadvantages) of ChatGPT's ability, such as its handling of API misuse and recommendation, and its shortcomings in identifying default parameters. Our findings indicate that ChatGPT has the potential to repair DL programs effectively and that prompt engineering and dialogue can further improve its performance by providing more code intention. We also identified the key intentions that can enhance ChatGPT's program repairing capability.",
issn="1573-7535",
doi="10.1007/s10515-025-00492-x",
url="https://doi.org/10.1007/s10515-025-00492-x"
}


@Article{Chen2025,
author="Chen, Kezhou
and Wang, Tao
and Ni, Mingzhe
and Cheng, Lianglun
and Wang, Zhuowei
and Chen, Chong",
title="Collaborative LLM agents for flexible software development of intelligent industrial robot control systems",
journal="Complex {\&} Intelligent Systems",
year="2025",
month="Oct",
day="24",
volume="11",
number="12",
pages="476",
abstract="Software plays a crucial role in robot control systems, and its efficient, flexible development is essential for production. Such software must be customized to specific production processes, requiring developers with specialized expertise in these areas---the high development threshold results in reduced efficiency and increased costs. Recently, significant progress has been made in automated problem-solving through societies of agents based on large language models (LLMs). To automate software development for industrial robot control systems, this paper introduces an Industrial Robot Control Software Auto-Development (IRCSAD) framework with multi-agent collaboration, and a Low-code Industrial Software Platform (LISP) for validating IRCSAD-developed software. IRCSAD automates software development and iterative optimization of prompts through the collaboration of multiple LLM-based agents. This work also proposes a software testing methodology for robot control systems based on reliability assessment. An experimental study that develops software for the robot control system for assembly, sorting, and inspection tasks is implemented. The results show that collaboration with LISP enhances IRCSAD's ability to solve complex problems in the development process, saves development costs, and improves development efficiency.",
issn="2198-6053",
doi="10.1007/s40747-025-02051-z",
url="https://doi.org/10.1007/s40747-025-02051-z"
}


@inproceedings{10.1007/978-3-032-07992-3_19,
 abstract = {Large Language Models (LLMs) have demonstrated strong capabilities in code generation, yet studies reveal that their outputs may contain critical security vulnerabilities. This study investigates the effectiveness of Parameter-Efficient Fine-Tuning (PEFT) techniques in enhancing the secure code generation capabilities of LLMs. Using the CodeGen-Multi 2B model as the base, applied two state-of-the-art PEFT methods--LoRA and IA3--to fine-tune the model on a curated dataset of C and C++ code. The objective is to evaluate whether fine-tuned models can reduce vulnerability incidence and to compare the relative performance of LoRA and IA3. Security evaluations are conducted using prompts designed to elicit eight common vulnerabilities from the MITRE CWE Top 25 list. Findings show that PEFT fine-tuning significantly improves the model's ability to generate secure code, with both LoRA and IA3 outperforming the base model. Among the two, LoRA exhibits a slight advantage in reducing vulnerabilities. This research highlights the potential of lightweight fine-tuning methods to mitigate security risks in LLM-generated code and offers practical insights for future advancements in secure AI-assisted software development.},
 address = {Cham},
 author = {Hasan, Md Anwarul},
 booktitle = {Proceedings of the Future Technologies Conference (FTC) 2025, Volume 4},
 editor = {Arai, Kohei},
 isbn = {978-3-032-07992-3},
 pages = {290--309},
 publisher = {Springer Nature Switzerland},
 title = {Secure Code Generation with Parameter-Efficient Fine-Tuning of LLMs},
 url = {https://link.springer.com/chapter/10.1007/978-3-032-07992-3_19},
 year = {2026}
}

@inproceedings{10.1007/978-981-96-6537-2_2,
 abstract = {Subjective manual evaluation of hallucinations in large language models (LLMs) remains a challenging task due to its time-consuming and labored character. It expects human expertise to analyze each LLL response critically to assess cases where the model produces wrong or misleading data. This manual approach is usually small-scale and logical in extent, bringing elements of judgment into the assessment equation. To solve these problems, we have proposed a novel hybrid test automation framework that integrates rules-based and model-based grading methods. The rule-based evaluation, done at the per-commit granularity during the continuous integration (CI) cycle, offers quick feedback on the exact hallucination patterns, while the model-based grading method is carried out during a increment release and is evaluated using a critique LLM. Since there is already known behaviors concerning hallucinations, the framework can readily encode these behaviors in a set of rules and thus be able to alert on potential issues. A more in-depth evaluation is carried out on the LLM by using model-graded evaluation, which is done after the release stage. The first approach focuses on training machine learning models to estimate the probability of hallucination from different characteristics of the LLM's responses. Thus, using both of these methods in the context of our framework allows for the constant review of the potential hallucination risks during each stage of the LLM expansion. This makes it possible to identify problems that might cause problems in the use of LLM-based applications early enough and take necessary measures to address them before their manifestations, thus enabling LLM-based applications to be deployed with higher chances of working as expected.},
 address = {Singapore},
 author = {Chakraborty, Amit
and Mallick, Chirantana
and Chakraborty, Rajdeep
and Das, Saptarshi},
 booktitle = {Data Management, Analytics and Innovation},
 editor = {Goswami, Saptarsi
and Saha, Sajal
and Beed, Romit S.
and Basu, Kanadpriya},
 isbn = {978-981-96-6537-2},
 pages = {13--31},
 publisher = {Springer Nature Singapore},
 title = {HyGen---A Hybrid Automation Testing Approach for Reducing Hallucination in LLM-Based Applications},
 url = {https://link.springer.com/chapter/10.1007/978-981-96-6537-2_2},
 year = {2026}
}

@inproceedings{10.1007/978-981-96-2376-1_33,
 abstract = {GitHub is one of the most popular collaborative development tools used by many institutions and enterprises. The GitHub issue community serves as the primary tool for project developers to gather program bug information, with GitHub users posting issues based on encountered program bugs. However, a considerable portion of these issues ultimately remain unfixed by developers. We term the issues fixed by developers as `dev-fixed' issues, and conversely, we call them `wontfix' issues. Through manual analysis of 2500 bug-related issues collected from five GitHub projects, we summarize the unresolved issues from four aspects and propose a multi-agent approach based on large language model (LLM) to identify dev-fixed issues. We design the previously summarized four aspects into different agents, independently analyzed by the LLM, and ultimately parallelly provide information to a summary agent for the final determination. Compared to the baseline, our approach shows the best performance in the metric of F1 score, recall rate and precision rate, with the value of 0.852, 0.862 and 0.857 respectively. Ablation experiments demonstrate the critical importance of each agent to our approach.},
 address = {Singapore},
 author = {Han, Zhengru
and Jiang, Bo
and Xue, Weihao
and Dai, Chaoqun
and Huang, Qiao
and Wang, Ye},
 booktitle = {Computer Supported Cooperative Work and Social Computing},
 editor = {Sun, Hailong
and Fan, Hongfei
and Gao, Yongqiang
and Wang, Xiaokang
and Liu, Dongning
and Du, Bowen
and Lu, Tun},
 isbn = {978-981-96-2376-1},
 pages = {459--470},
 publisher = {Springer Nature Singapore},
 title = {A Multi-agent Collaboration Approach for Identifying Developer-Fixed Issues in GitHub Projects},
 url = {https://link.springer.com/chapter/10.1007/978-981-96-2376-1_33},
 year = {2025}
}

@inproceedings{10.1007/978-3-031-36272-9_74,
 abstract = {In educational settings, automated program repair techniques serve as a feedback mechanism to guide students working on their programming assignments. Recent work has investigated using large language models (LLMs) for program repair. In this area, most of the attention has been focused on using proprietary systems accessible through APIs. However, the limited access and control over these systems remain a block to their adoption and usage in education. The present work studies the repairing capabilities of open large language models. In particular, we focus on a recent family of generative models, which, on top of standard left-to-right program synthesis, can also predict missing spans of code at any position in a program. We experiment with one of these models on four programming datasets and show that we can obtain good repair performance even without additional training.},
 address = {Cham},
 author = {Koutcheme, Charles
and Sarsa, Sami
and Leinonen, Juho
and Hellas, Arto
and Denny, Paul},
 booktitle = {Artificial Intelligence in Education},
 editor = {Wang, Ning
and Rebolledo-Mendez, Genaro
and Matsuda, Noboru
and Santos, Olga C.
and Dimitrova, Vania},
 isbn = {978-3-031-36272-9},
 pages = {798--803},
 publisher = {Springer Nature Switzerland},
 title = {Automated Program Repair Using Generative Models for Code Infilling},
 url = {https://link.springer.com/chapter/10.1007/978-3-031-36272-9_74},
 year = {2023}
}

@inproceedings{10.1007/978-3-031-63028-6_8,
 abstract = {LLMs trained in the understanding of programming syntax are now providing effective assistance to developers and are being used in programming education such as in generation of coding problem examples or providing code explanations. A key aspect of programming education is understanding and dealing with error message. However, `logical errors' in which the program operates against the programmer's intentions do not receive error messages from the compiler. In this study, building on existing research on programming errors, we first define the types of logical errors that can occur in programming in general. Based on the definition, we propose an effective approach for detecting logical errors with LLMs that makes use of relations among error types in the Chain-of-Thought and Tree-of-Thought prompts. The experimental results indicate that when such logical error descriptions in the prompt are used, the average classification performance is about 21{\%} higher than the ones without them. We also conducted an experiment for exploiting the relations among errors in generating a new logical error dataset using LLMs. As there is very limited dataset for logical errors such benchmark dataset can be very useful for various programming related applications. We expect that our work can assist novice programmers in identifying the causes of code errors and correct them more effectively.},
 address = {Cham},
 author = {Lee, Yanggyu
and Jeong, Suchae
and Kim, Jihie},
 booktitle = {Generative Intelligence and Intelligent Tutoring Systems},
 editor = {Sifaleras, Angelo
and Lin, Fuhua},
 isbn = {978-3-031-63028-6},
 pages = {91--103},
 publisher = {Springer Nature Switzerland},
 title = {Improving LLM Classification of Logical Errors by Integrating Error Relationship into Prompts},
 url = {https://link.springer.com/chapter/10.1007/978-3-031-63028-6_8},
 year = {2024}
}

@Article{Tambon2025,
author="Tambon, Florian
and Moradi-Dakhel, Arghavan
and Nikanjam, Amin
and Khomh, Foutse
and Desmarais, Michel C.
and Antoniol, Giuliano",
title="Bugs in large language models generated code: an empirical study",
journal="Empirical Software Engineering",
year="2025",
month="Feb",
day="13",
volume="30",
number="3",
pages="65",
abstract="Large Language Models (LLMs) for code have gained significant attention recently. They can generate code in different programming languages based on provided prompts, fulfilling a long-lasting dream in Software Engineering (SE), i.e., automatic code generation. Similar to human-written code, LLM-generated code is prone to bugs, and these bugs have not yet been thoroughly examined by the community. Given the increasing adoption of LLM-based code generation tools (e.g., GitHub Copilot) in SE activities, it is critical to understand the characteristics of bugs contained in code generated by LLMs. This paper examines samples of 333 bugs collected from code generated using three leading LLMs (i.e., CodeGen, PanGu-Coder, and Codex) and identifies the following 10 distinctive bug patterns: Misinterpretations, Syntax Error, Silly Mistake, Prompt-biased code, Missing Corner Case, Wrong Input Type, Hallucinated Object, Wrong Attribute, Incomplete Generation, and Non-Prompted Consideration. The bug patterns are presented in the form of a taxonomy. The identified bug patterns are validated using online surveys with over 50 LLM practitioners and researchers. The surveyed participants generally asserted the significance and prevalence of the bug patterns. Researchers and practitioners can leverage these findings to develop effective quality assurance techniques for LLM-generated code. This study sheds light on the distinctive characteristics of LLM-generated code.",
issn="1573-7616",
doi="10.1007/s10664-025-10614-4",
url="https://doi.org/10.1007/s10664-025-10614-4"
}


@Article{Yang2025,
author="Yang, Haiyang
and Xie, Qi
and Cai, Shixiang
and Wang, Zhaowen
and Kuang, Li
and Xia, Yingjie",
title="LLMUpdater: Automatic comment synchronization via edit model guided LLMs",
journal="Empirical Software Engineering",
year="2025",
month="Nov",
day="08",
volume="31",
number="1",
pages="12",
abstract="Automatic comment synchronization is essential especially in the collaborative development environment, since inconsistent comments may cause code misinterpretation, leading to bugs and higher maintenance costs. Existing studies employ neural machine translation models or rule-based heuristics. However, these methods typically demand extensive training or the creation of numerous rules to cover various scenarios, making them impractical for applications. To address the problems above, this paper proposes an automatic comment synchronization method with an edit model guided Large Language Models(LLMs). For the first time, we conduct empirical research to explore the capabilities of LLMs on comment synchronization, and we find: (1) LLMs have insufficient capability on comment synchronization; (2) providing old comments as guidance can significantly improve effectiveness; (3) better results can be achieved by prompting the LLMs with edit tags at different positions to indicate what modifications are needed. Inspired by the findings, we propose a new framework LLMUpdater, which models the comment synchronization task as a fill-in-the-blank problem. LLMUpdater first utilizes an edit model to mark the positions for edits in old comments and assign corresponding tags, leaving blank spaces between them. Then LLMUpdater guide LLMs to fill in the blanks for more accurate comment synchronization, by introducing additional edit knowledge, i.e., the old comments with editing tags, as new prompts. To verify the effectiveness of the LLMUpdater, we have implemented an editing model as an example and validated it on two widely used public datasets for the comment synchronization task. Moreover, a human-in-the-loop evaluator is proposed to evaluate the updated comments. The experimental results show that our proposed approach significantly improves without fine-tuning the LLMs. Our code is available online ``(https://anonymous.4open.science/r/LLMUpdater-1F61)''.",
issn="1573-7616",
doi="10.1007/s10664-025-10752-9",
url="https://doi.org/10.1007/s10664-025-10752-9"
}


@inproceedings{10.1007/978-981-97-7707-5_31,
 abstract = {Large language models (LLMs) have been applied to help programming education on aspects such as question answering and program repair. While they make students learn more efficiently, how to use LLMs to help increase teaching efficiency is rarely explored. In this paper, we focus on harnessing LLMs to automatically generate programming exercises with the goal of alleviating teachers' workload and enhancing teaching efficiency. We first evaluate the performance of seven open-source LLMs using prompts, and then fine-tune two winning LLMs using instructions constructed with the Evol-Instruct and the ACES algorithms, respectively. Experimental results demonstrate the improved performance on the two LLMs after the instruction tuning. Additionally, our contribution encompasses the formulation of evaluation metrics and the exploration of various prompt methods.},
 address = {Singapore},
 author = {Zeng, Guolong
and Xue, Qinchen
and Lu, Xuesong},
 booktitle = {Web Information Systems and Applications},
 editor = {Jin, Cheqing
and Yang, Shiyu
and Shang, Xuequn
and Wang, Haofen
and Zhang, Yong},
 isbn = {978-981-97-7707-5},
 pages = {377--385},
 publisher = {Springer Nature Singapore},
 title = {Instruction Tuning with LLMs for Programming Exercise Generation},
 url = {https://link.springer.com/chapter/10.1007/978-981-97-7707-5_31},
 year = {2024}
}

@Article{Jiao2025,
author="Jiao, Yutong
and Han, Jiaxuan
and Huang, Cheng",
title="DeepVulHunter: enhancing the code vulnerability detection capability of LLMs through multi-round analysis",
journal="Journal of Intelligent Information Systems",
year="2025",
month="Dec",
day="01",
volume="63",
number="6",
pages="2237--2264",
abstract="As the economic losses caused by software vulnerabilities continue to escalate, automated vulnerability detection has emerged as a crucial demand in software engineering. While current Large Language Model (LLM)-based approaches demonstrate promising capabilities for vulnerability detection, they still face significant challenges including susceptibility to non-vulnerability factors like code length, severe hallucination issues, and unsatisfactory detection accuracy and balance. To overcome these limitations, we propose DeepVulHunter, a novel multi-round detection framework that utilizes Retrieval Augmented Generation (RAG) technique to provide code snippets semantically similar to the target code and their associated vulnerability information. Extensive experiments conducted across five representative models from the Llama and Deepseek series confirm that our method effectively mitigates these challenges while enhancing both accuracy and balance in vulnerability detection tasks for general large models. The best-performing Llama-405B model achieves a detection accuracy of up to 75.3{\%}, surpassing the current state-of-the-art approach that utilizes GPT-4 with Chain-of-Thought (CoT) prompting.",
issn="1573-7675",
doi="10.1007/s10844-025-00982-0",
url="https://doi.org/10.1007/s10844-025-00982-0"
}


@Article{Tang2025,
author="Tang, Yijia
and Yu, YaoShen
and Huang, Zhiqiu
and Xia, Bowei
and Cao, Yukun",
title="P2N2S: Bridging the gap between natural and programming languages for code summarization via large language models",
journal="World Wide Web",
year="2025",
month="Dec",
day="16",
volume="29",
number="1",
pages="7",
abstract="Code summarization aims to generate natural language descriptions of source code to support program comprehension and software maintenance. Traditional programming-language-processing (PLP) pipelines rely on syntax parsing and structural representations such as abstract syntax trees (ASTs). However, their strict separation of programming language (PL) and natural language (NL) often results in brittle generalization and limited adaptability. We introduce P2N2S, a fully NL-grounded framework that departs from the PLP paradigm. It operates in two stages: (1) PL2NL Conversion, where large language models (LLMs) actively translate code into line-level natural language annotations, producing semantically faithful and interpretable intermediate representations; and (2) NL2Summary Generation, where pre-trained natural language processing (NLP) summarizers refine these annotations into concise and coherent summaries. By integrating the LLM's capability to capture detailed semantics with the NLP summarizer's strengths in selective abstraction and stylistic coherence, P2N2S overcomes the rigid, structure-dependent limitations of PLP methods and produces summaries that remain concise, fluent, and accurate without relying on fragile program analyses. Experiments on two Java benchmarks show that P2N2S consistently outperforms state-of-the-art PLP and LLM baselines across four evaluation metrics. The framework also demonstrates strong robustness in automatic and human assessments, and offers practical deployability by relying solely on readily available LLM and NLP tools. Overall, P2N2S provides a stable, flexible, and accessible solution for advancing code summarization in real-world software engineering workflows.",
issn="1573-1413",
doi="10.1007/s11280-025-01395-3",
url="https://doi.org/10.1007/s11280-025-01395-3"
}


@inproceedings{10.1007/978-3-032-04549-2_24,
 abstract = {Source code summarization automates the generation of natural comments, enhancing efficiency in software development and maintenance. With the emergence of large language models (LLMs), significant progress has been made in this domain. However, current approaches either rely on manually crafted prompts or standard fine-tuning that fail to fully leverage the power of continuous embeddings. To address this gap, we propose StageCS, a novel framework integrating prompt learning and model fine-tuning for code summarization, featuring a strategic two-stage training process with a multi-branch transformer architecture. StageCS generates specialized continuous embeddings that synergistically guide LLMs to produce high-quality summaries while maximizing the benefits of targeted fine-tuning. Evaluations on the CodeSearchNet Java dataset show StageCS outperforms baselines in representative LLM architectures like PolyCoder and CodeGen. More importantly, our ablation studies demonstrate that both the multi-branch transformer and the strategic two-stage process contribute significantly. StageCS enables LLMs to excel in code summarization without extensive manual prompt engineering, delivering superior quality through specialized training.},
 address = {Cham},
 author = {Sun, Xiaoshu
and Lv, Siqi
and Wan, Wei
and Qin, Yiming
and Hu, Gang},
 booktitle = {Artificial Neural Networks and Machine Learning -- ICANN 2025},
 editor = {Senn, Walter
and Sanguineti, Marcello
and Saudargiene, Ausra
and Tetko, Igor V.
and Villa, Alessandro E. P.
and Jirsa, Viktor
and Bengio, Yoshua},
 isbn = {978-3-032-04549-2},
 pages = {294--306},
 publisher = {Springer Nature Switzerland},
 title = {A Two-Stage Framework Integrating Prompt Learning and Fine-Tuning for Code Summarization},
 url = {https://link.springer.com/chapter/10.1007/978-3-032-04549-2_24},
 year = {2026}
}

@inproceedings{10.1007/978-3-032-04190-6_2,
 abstract = {Refactoring is a crucial software engineering practice aimed at improving code quality. However, detecting and predicting refactoring activities automatically remains a challenging task due to the limited availability of labeled datasets. This study investigates the role of data augmentation techniques in enhancing refactoring detection models. We apply various augmentation strategies to expand training data and assess their impact on model performance. We also discuss validation and balancing of the resulting dataset in order to provide meaningful data for further applied ML techniques. Our findings highlight the importance of data diversity in automated refactoring detection and provide insights into optimizing augmentation strategies for software engineering applications. Experimental results demonstrate that data augmentation improves the robustness and accuracy of refactoring detection models by mitigating overfitting and enhancing generalization.},
 address = {Cham},
 author = {Moldovan, Vasilica
and Patcas, Rares
and Motogna, Simona},
 booktitle = {Software Engineering and Advanced Applications},
 editor = {Taibi, Davide
and Smite, Darja},
 isbn = {978-3-032-04190-6},
 pages = {20--36},
 publisher = {Springer Nature Switzerland},
 title = {LLMs Based Data Augmentation Techniques for Python Code Refactoring},
 url = {https://link.springer.com/chapter/10.1007/978-3-032-04190-6_2},
 year = {2026}
}

@inproceedings{10.1007/978-3-031-95127-5_16,
 abstract = {This study explores the potential of Large Language Models (LLMs) in generating automated test cases to enhance software testing and bug detection. By evaluating 100 GitHub issues, the research investigates whether LLM-generated tests could identify bugs prior to their resolution. Findings reveal that LLMs can be effective in certain cases, particularly with simpler code that has fewer dependencies and lower complexity metrics. However, the success of these tests diminishes as code complexity, imports, and function calls increase. The study highlights the potential of LLMs in software testing, while also noting the challenges associated with testing complex code structures.},
 address = {Cham},
 author = {Pehlke, Marcel
and Eudenbach, Cindy
and Graw, Maximilian
and Jansen, Marc},
 booktitle = {Computational Science and Computational Intelligence},
 editor = {Arabnia, Hamid R.
and Deligiannidis, Leonidas
and Shenavarmasouleh, Farzan
and Amirian, Soheyla
and Ghareh Mohammadi, Farid},
 isbn = {978-3-031-95127-5},
 pages = {205--217},
 publisher = {Springer Nature Switzerland},
 title = {Towards Automated Software Testing: Evaluating LLMs in Generating Effective Test Cases},
 url = {https://link.springer.com/chapter/10.1007/978-3-031-95127-5_16},
 year = {2025}
}

@Article{Wang2025,
author="Wang, Jiayi
and Yu, Ping
and Qin, Yi
and Jiang, Yanyan
and Yao, Yuan
and Ma, Xiaoxing",
title="NexuSym: Marrying symbolic path finders with large language models",
journal="Automated Software Engineering",
year="2025",
month="Jun",
day="07",
volume="32",
number="2",
pages="59",
abstract="Symbolic execution is a powerful technique for automated test case generation, ensuring comprehensive coverage of potential scenarios. However, it often struggles with complex, deep paths due to path explosion. Conversely, large language models (LLMs) utilize vast training data to generate test cases that can uncover intricate program behaviors that symbolic execution might miss. Despite their complementary strengths, integrating the systematic nature of symbolic execution with the creative capabilities of LLMs presents a significant challenge. We introduce NexuSym, an innovative tool that integrates symbolic execution with LLMs to facilitate the automatic generation of test cases. To effectively bridge the gap between these two approaches, we have developed a test case reducer, which normalizes the LLM-generated test cases to make them compatible with symbolic execution. Additionally, we propose a search space summarizer, which abstracts and condenses the search space explored by symbolic execution, enabling the LLM to focus on the most promising areas for further exploration. We instantiated NexuSym on KLEE and ChatGPT. Our evaluation of NexuSym involved 99 coreutils programs and 9 large GNU programs. The experimental results demonstrate that NexuSym significantly enhances program test coverage, with improvements of up to 20{\%} in certain cases. Furthermore, we conducted an analysis of the monetary costs associated with using the LLM API, revealing that NexuSym is a highly cost-effective solution.",
issn="1573-7535",
doi="10.1007/s10515-025-00529-1",
url="https://doi.org/10.1007/s10515-025-00529-1"
}


@Article{Alhanahnah2025,
author="Alhanahnah, Mohannad
and Rashedul Hasan, Md
and Xu, Lisong
and Bagheri, Hamid",
title="An empirical evaluation of pre-trained large language models for repairing declarative formal specifications",
journal="Empirical Software Engineering",
year="2025",
month="Jul",
day="25",
volume="30",
number="5",
pages="149",
abstract="Automatic Program Repair (APR) has garnered significant attention as a practical research domain focused on automatically fixing bugs in programs. While existing APR techniques primarily target imperative programming languages like C and Java, there is a growing need for effective solutions applicable to declarative software specification languages. This paper systematically investigates the capacity of Large Language Models (LLMs) to repair declarative specifications in Alloy, a declarative formal language used for software specification. We designed six different repair settings, encompassing single-agent and dual-agent paradigms, utilizing various LLMs. These configurations also incorporate different levels of feedback, including an auto-prompting mechanism for generating prompts autonomously using LLMs. Our study reveals that dual-agent with auto-prompting setup outperforms the other settings, albeit with a marginal increase in the number of iterations and token usage. This dual-agent setup demonstrated superior effectiveness compared to state-of-the-art Alloy APR techniques when evaluated on a comprehensive set of benchmarks. This work is the first to empirically evaluate LLM capabilities to repair declarative specifications, while taking into account recent trending LLM concepts such as LLM-based agents, feedback, auto-prompting, and tools, thus paving the way for future agent-based techniques in software engineering.",
issn="1573-7616",
doi="10.1007/s10664-025-10687-1",
url="https://doi.org/10.1007/s10664-025-10687-1"
}


@Article{Tissaoui2026,
author="Tissaoui, Anis",
title="From prompt to persona: a literature review on LLMs as single cognitive agents",
journal="Journal of Ambient Intelligence and Humanized Computing",
year="2026",
month="Jan",
day="01",
volume="17",
number="1",
pages="205--221",
abstract="Recent advancements in large language models (LLMs) have sparked growing interest in their ability to simulate not just linguistic competence, but agent like behavior. Moving beyond chatbots and task based assistants, researchers are now exploring whether LLMs can act as ``single cognitive agents'' entities endowed with memory, persona, planning capabilities, and situatedness in dynamic environments. This article presents the first structured literature review focused specifically on LLMs as single agent simulators. We examine over 70 peer reviewed publications and preprints (2018-2025), organizing them across five core dimensions: memory mechanisms, identity and persona modeling, planning and autonomy, interaction environments, and evaluation protocols. For each dimension, we analyze existing architectures, synthesize taxonomies, and identify recurring limitations. Our findings reveal a fragmented research landscape, where cognitive coherence, temporal memory, and personality persistence remain open challenges. We conclude by proposing a set of key research questions to guide future efforts, including the simulation of theory of mind, self consistent goal formation, and integrated memory persona frameworks. This review lays the groundwork for a unified research agenda aimed at developing cognitively coherent, memory augmented LLM based agents.",
issn="1868-5145",
doi="10.1007/s12652-025-05029-4",
url="https://doi.org/10.1007/s12652-025-05029-4"
}


@inproceedings{10.1007/978-3-032-00972-2_15,
 abstract = {The Software Development Life Cycle (SDLC) encompasses distinct phases, each demanding specialized expertise to ensure high-quality deliverables. Traditionally, the success of these phases has relied heavily on the availability of Subject Matter Experts (SMEs) with phase-specific skills. Recent advancements in Generative AI, particularly Large Language Models (LLMs) such as OpenAI's GPT and Anthropic's Claude, have introduced transformative possibilities in software engineering. These models, trained on extensive text corpora, show significant potential to augment various stages of the SDLC. However, the effectiveness of LLMs hinges on the quality of the prompts provided, necessitating systematic and context-aware interactions. This paper presents a novel multi-agent system leveraging systematic prompting strategies grounded in meta-model concepts to address phase-specific challenges in the SDLC. The proposed approach was validated using GPT-o1 in the development of a small yet complex business application. We detail the methodology, highlight the benefits realized, and discuss the challenges encountered during its implementation. Our findings underscore the potential of Generative AI to lower skill barriers, enhance collaboration, and accelerate software development processes, marking a significant step forward in the integration of AI into software engineering practices.},
 address = {Cham},
 author = {Pham, Vu-Thu-Nguyet
and Nguyen, Quang-Vu},
 booktitle = {The 14th Conference on Information Technology and its Applications},
 editor = {Nguyen, Ngoc Thanh
and Huynh, Cong-Phap
and Nguyen, Thanh Thuy
and Le-Khac, Nhien-An
and Seng, Sopheap
and Nguyen, Quang-Vu},
 isbn = {978-3-032-00972-2},
 pages = {187--202},
 publisher = {Springer Nature Switzerland},
 title = {Accelerating Software Development Cycle with a Multi-agent Generative AI Approach: A Case Study with OpenAI's GPT},
 url = {https://link.springer.com/chapter/10.1007/978-3-032-00972-2_15},
 year = {2026}
}

@inproceedings{10.1007/978-3-031-75110-3_3,
 abstract = {In edge-cloud systems, the quality of infrastructure deployment is crucial for delivering high-quality services, especially when using popular Infrastructure as Code (IaC) tools like Ansible. Ensuring the reliability of such large-scale code systems poses a significant challenge due to the limited testing resources. Software defect prediction (SDP) addresses this limitation by identifying defect-prone software modules, allowing developers to prioritize testing resources effectively. This paper introduces a Large Language Model (LLM)-based approach for SDP in Ansible scripts with Code-Smell-guided Prompting (CSP). CSP leverages code smell indicators extracted from Ansible scripts to refine prompts given to LLMs, enhancing their understanding of code structure concerning defects. Our experimental results demonstrate that CSP variants, particularly the Chain of Thought CSP (CoT-CSP), outperform traditional prompting strategies, as evidenced by improved F1-scores and Recall. To the best of our knowledge, this is the first attempt to employ LLMs for SDP in Ansible scripts. By employing a code smell-guided prompting strategy tailored for Ansible, we anticipate that the proposed method will enhance software quality assurance and reliability, thereby increasing the overall reliability of edge-cloud systems.},
 address = {Cham},
 author = {Hong, Hyunsun
and Lee, Sungu
and Ryu, Duksan
and Baik, Jongmoon},
 booktitle = {Current Trends in Web Engineering},
 editor = {Pautasso, Cesare
and Marcel, Patrick},
 isbn = {978-3-031-75110-3},
 pages = {30--42},
 publisher = {Springer Nature Switzerland},
 title = {Enhancing Software Defect Prediction in Ansible Scripts Using Code-Smell-Guided Prompting with Large Language Models in Edge-Cloud Infrastructures},
 url = {https://link.springer.com/chapter/10.1007/978-3-031-75110-3_3},
 year = {2025}
}

@Article{Liu2025,
author="Liu, Jingqiang
and Liang, Ruigang
and Zhu, Xiaoxi
and Zhang, Yue
and Liu, Yuling
and Liu, Qixu",
title="LLM4TDG: test-driven generation of large language models based on enhanced constraint reasoning",
journal="Cybersecurity",
year="2025",
month="May",
day="15",
volume="8",
number="1",
pages="32",
abstract="With the evolution of modern software development paradigms, component reuse, and low-code approaches have emerged as mainstream in software development. However, developers often lack an in-depth understanding of reused code. The inability of components to operate autonomously leads to insufficient testing of software functionalities and security, further exacerbating the contradiction between the increasing complexity of software architectures and the demand for accurate and efficient software automation testing. This, in turn, increases the frequency of software supply chain security incidents. This paper proposes a test-driven generation framework, LLM4TDG, based on large language models (LLMs). By formally defining the constraint dependency graph and converting it into context constraints, LLMs' ability to understand natural language descriptions such as test requirements and documents is enhanced. Constraint reasoning and backtracking mechanisms are then used to generate test drivers that satisfy the defined constraints automatically. Using the EvalPlus dataset, we evaluate the comprehensive capabilities of LLM4TDG in test case generation using four general-domain LLMs and five code-generation-domain LLMs. The experimental results indicate that our approach significantly enhances LLMs' ability to comprehend constraints in testing objectives, achieving a 47.62{\%} increase in constraint understanding across 147 testing tasks. Employing LLM4TDG significantly improves the average pass@k metric of all LLMs by 10.41{\%}. The pass@k metric for CodeQwen-chat has improved by up to 18.66{\%}. The metric surpasses the state-of-the-art GPT-4, with a performance of 92.16{\%} on HUMANEVAL and 87.14{\%} on HUMANEVAL+, which enhances the error correction and functional correctness in test-driven code generation. Meanwhile, Our experiments were conducted on a dataset of Python third-party libraries containing malicious behavior in the context of security testing tasks, validating the effectiveness of our method in real-world applications and its generalization capabilities.",
issn="2523-3246",
doi="10.1186/s42400-024-00335-4",
url="https://doi.org/10.1186/s42400-024-00335-4"
}


@Article{Dou2026,
author="Dou, Shihan
and Jia, Haoxiang
and Wu, Shenxi
and Zheng, Huiyuan
and Wu, Muling
and Tao, Yunbo
and Zhang, Ming
and Chai, Mingxu
and Fan, Jessica
and Xi, Zhiheng
and Zheng, Rui
and Wu, Yueming
and Wen, Ming
and Gui, Tao
and Zhang, Qi
and Qiu, Xipeng
and Huang, Xuanjing",
title="What is wrong with your code generated by large language models? An extensive study",
journal="Science China Information Sciences",
year="2026",
month="Jan",
day="04",
volume="69",
number="1",
pages="112107",
abstract="The increasing development of large language models (LLMs) in code generation has drawn significant attention among researchers. To enhance LLM-based code generation ability, current efforts are predominantly directed towards collecting high-quality datasets and leveraging diverse training technologies. However, there is a notable lack of comprehensive studies examining the limitations and boundaries of existing methods. To bridge this gap, we conducted an extensive empirical study evaluating the performance of three leading closed-source LLMs and six popular open-source LLMs on three commonly used benchmarks. Our investigation, which evaluated the length, cyclomatic complexity and API number of the generated code, revealed that these LLMs face challenges in generating successful code for more complex problems, and tend to produce code that is shorter yet more complicated as compared to canonical solutions. Additionally, we developed a taxonomy of bugs for incorrect codes that includes three categories and ten sub-categories, and analyzed the root cause for common bug types. To better understand the performance of LLMs in real-world projects, we also manually created a real-world benchmark RWPB. We analyzed bugs on RWPB to highlight distinct differences in bug distributions between actual scenarios and existing benchmarks. Finally, we propose a novel training-free iterative method that introduces self-critique, enabling LLMs to critique and correct their generated code based on bug types and compiler feedback. Experimental results demonstrate that our approach can significantly mitigate bugs and achieve a repair success rate of 29.2{\%} after two iterations, indicating substantial potential for LLMs to handle more complex problems. Our comprehensive and extensive study provides insights into the current limitations of LLM-based code generation and opportunities for enhancing the accuracy and quality of the generated code.",
issn="1869-1919",
doi="10.1007/s11432-025-4632-8",
url="https://doi.org/10.1007/s11432-025-4632-8"
}


@Article{Tihanyi2024,
author="Tihanyi, Norbert
and Bisztray, Tamas
and Ferrag, Mohamed Amine
and Jain, Ridhi
and Cordeiro, Lucas C.",
title="How secure is AI-generated code: a large-scale comparison of large language models",
journal="Empirical Software Engineering",
year="2024",
month="Dec",
day="21",
volume="30",
number="2",
pages="47",
abstract="This study compares state-of-the-art Large Language Models (LLMs) on their tendency to generate vulnerabilities when writing C programs using a neutral zero-shot prompt. Tihanyi et al. introduced the FormAI dataset at PROMISE '23, featuring 112,000 C programs generated by GPT-3.5-turbo, with over 51.24{\%} identified as vulnerable. We extended that research with a large-scale study involving 9 state-of-the-art models such as OpenAI's GPT-4o-mini, Google's Gemini Pro 1.0, TII's 180 billion-parameter Falcon, Meta's 13 billion-parameter Code Llama, and several other compact models. Additionally, we introduce the FormAI-v2 dataset, which comprises 331 000 compilable C programs generated by these LLMs. Each program in the dataset is labeled based on the vulnerabilities detected in its source code through formal verification, using the Efficient SMT-based Context-Bounded Model Checker (ESBMC). This technique minimizes false positives by providing a counterexample for the specific vulnerability and reduces false negatives by thoroughly completing the verification process. Our study reveals that at least 62.07{\%} of the generated programs are vulnerable. The differences between the models are minor, as they all show similar coding errors with slight variations. Our research highlights that while LLMs offer promising capabilities for code generation, deploying their output in a production environment requires proper risk assessment and validation.",
issn="1573-7616",
doi="10.1007/s10664-024-10590-1",
url="https://doi.org/10.1007/s10664-024-10590-1"
}


@Article{Zhang2026,
author="Zhang, Shuoming
and Zhao, Jiacheng
and Yu, Qiuchu
and Xia, Chunwei
and Wang, Zheng
and Feng, Xiaobing
and Cui, Huimin",
title="The new compiler stack: a survey on the synergy of LLMs and compilers",
journal="CCF Transactions on High Performance Computing",
year="2026",
month="Jan",
day="09",
abstract="This survey has provided a systematic overview of the emerging field of LLM-enabled compilation by addressing several key research questions. We first answered how LLMs are being integrated by proposing a comprehensive, multi-dimensional taxonomy that categorizes works based on their Design Philosophy (Selector, Translator, Generator), LLM Methodology, their operational Level of Code Abstraction, and the specific Task Type they address. In answering what advancements these approaches offer, we identified three primary benefits: the democratization of compiler development, the discovery of novel optimization strategies, and the broadening of the compiler's traditional scope. Finally, in addressing the field's challenges and opportunities, we highlighted the critical hurdles of ensuring correctness and achieving scalability, while identifying the development of hybrid systems as the most promising path forward. By providing these answers, this survey serves as a foundational roadmap for researchers and practitioners, charting the course for a new generation of LLM-powered, intelligent, adaptive and synergistic compilation tools.",
issn="2524-4930",
doi="10.1007/s42514-025-00270-x",
url="https://doi.org/10.1007/s42514-025-00270-x"
}


@inproceedings{10.1007/978-3-031-97576-9_6,
 abstract = {Bug localization and semantic code search within large software repositories is a significant and time-consuming challenge for developers, particularly when dealing with bug reports from end-users who lack technical expertise. Traditional similarity-based code search methods struggle with the inherent domain and vocabulary mismatch between end-user reports and codebase semantics, while directly applying Large Language Models (LLMs) is hampered by their limited context windows and lack of repository-level understanding. To address these limitations, this paper introduces a novel, structure-aware methodology for creating repository-aware LLMs using hierarchical summarization. Our approach comprises a pre-processing phase that constructs an abstract repository tree, creates a context-aware LLM primed with project knowledge, and generates hierarchical summaries at project, directory, and file levels. The inference phase employs a top-down search strategy, guiding the LLM to progressively narrow down the search space from directory-level to file-level, effectively localizing bug-relevant code. This method mitigates the context window bottleneck and leverages LLMs' semantic understanding to overcome domain gap issues. Evaluated on a real-world dataset of Jira issues from a large-scale industrial project, our approach significantly outperforms both Flat Retrieval baselines and state-of-the-art LLM + Retrieval-Augmented Generation (RAG) systems, achieving a Pass@10 of 0.89 and Recall@10 of 0.33. The results demonstrate the efficacy of hierarchical summarization in enabling scalable, task-agnostic, and structure-aware repository-level code comprehension for improved bug localization and code search, particularly in scenarios involving non-technical end-user bug reports.},
 address = {Cham},
 author = {Oskooei, Amirkia Rafiei
and Yukcu, Selcan
and Bozoglan, Mehmet Cevheri
and Aktas, Mehmet S.},
 booktitle = {Computational Science and Its Applications -- ICCSA 2025 Workshops},
 editor = {Gervasi, Osvaldo
and Murgante, Beniamino
and Garau, Chiara
and Karaca, Yeliz
and Faginas Lago, Maria Noelia
and Scorza, Francesco
and Braga, Ana Cristina},
 isbn = {978-3-031-97576-9},
 pages = {88--105},
 publisher = {Springer Nature Switzerland},
 title = {Repository-Level Code Understanding by LLMs via Hierarchical Summarization: Improving Code Search and Bug Localization},
 url = {https://link.springer.com/chapter/10.1007/978-3-031-97576-9_6},
 year = {2026}
}

@Article{Trummer2025,
author="Trummer, Immanuel",
title="Generating highly customizable python code for data processing with large language models",
journal="The VLDB Journal",
year="2025",
month="Jan",
day="31",
volume="34",
number="2",
pages="21",
abstract="CARD (Coding Assistant for Relational Data analysis) generates Python code that processes relational queries on raw data. Users can customize generated code via natural language instructions, e.g., by instructing the system to use specific libraries or produce certain output. Internally, CARD uses large language models such as GPT-4o to synthesize code. CARD automatically constructs prompts describing code generation tasks to the language models. Those prompts contain information on data format, customization requirements, as well as processing plans, generated by CARD's scenario-specific query planner. CARD automatically tests generated code by comparing its output to the output of a reference SQL engine. In case of inconsistencies, CARD re-generates code with a certain degree of randomization. Furthermore, CARD can automatically generate libraries of code samples for specific customization scenarios in a pre-processing step, leveraging those samples at run time for few-shot learning. The experiments show that CARD generates accurate code in the vast majority of scenarios. Furthermore, current trends in language models are likely to benefit CARD's performance in the future.",
issn="0949-877X",
doi="10.1007/s00778-025-00900-4",
url="https://doi.org/10.1007/s00778-025-00900-4"
}


@inproceedings{10.1007/978-3-031-81573-7_6,
 abstract = {This work addresses a research challenge in automating the translation of natural language inputs into programming language specifications. We consider the case of bug reports, which are informally written by users, and that must be specifying into executable test cases for reproducing the bug on the target software. Software bugs are indeed largely reported in natural language by users. Yet, we lack reliable tools to automatically address reported bugs (i.e., enabling their analysis, reproduction, and bug fixing). We therefore build on the recent promises brought by ChatGPT for various tasks, including in software engineering, and establish the following research question: What if Conversational Artificial Intelligence (AI) models could be used to explore the semantics of bug reports as well as to automate their reproduction? We evaluate the capabilities of ChatGPT, a state-of-the-art conversational AI, i.e., chatbot, using the popular Defects4J benchmark with its associated bug reports. The results reveal that ChatGPT can generate executable test cases that could trigger 50{\%} of the bugs reported in natural language. These results are promising not only for the research community, but also for practitioners.},
 address = {Cham},
 author = {Saban{\'e}, Aminata
and Plein, Laura
and Bissyand{\'e}, Tegawend{\'e} F.},
 booktitle = {Towards new e-Infrastructure and e-Services for Developing Countries},
 editor = {Sere, Abdoulaye
and Sie, Oumarou
and Saeed, Rashid A.},
 isbn = {978-3-031-81573-7},
 pages = {81--88},
 publisher = {Springer Nature Switzerland},
 title = {Leveraging Conversational AI for Accelerating User-Driven Software Testing},
 url = {https://link.springer.com/chapter/10.1007/978-3-031-81573-7_6},
 year = {2025}
}

@Article{Liu2025,
author="Liu, Bo
and Jiang, Yanjie
and Zhang, Yuxia
and Niu, Nan
and Li, Guangjie
and Liu, Hui",
title="Exploring the potential of general purpose LLMs in automated software refactoring: an empirical study",
journal="Automated Software Engineering",
year="2025",
month="Mar",
day="01",
volume="32",
number="1",
pages="26",
abstract="Software refactoring is an essential activity for improving the readability, maintainability, and reusability of software projects. To this end, a large number of automated or semi-automated approaches/tools have been proposed to locate poorly designed code, recommend refactoring solutions, and conduct specified refactorings. However, even equipped with such tools, it remains challenging for developers to decide where and what kind of refactorings should be applied. Recent advances in deep learning techniques, especially in large language models (LLMs), make it potentially feasible to automatically refactor source code with LLMs. However, it remains unclear how well LLMs perform compared to human experts in conducting refactorings automatically and accurately. To fill this gap, in this paper, we conduct an empirical study to investigate the potential of LLMs in automated software refactoring, focusing on the identification of refactoring opportunities and the recommendation of refactoring solutions. We first construct a high-quality refactoring dataset comprising 180 real-world refactorings from 20 projects, and conduct the empirical study on the dataset. With the to-be-refactored Java documents as input, ChatGPT and Gemini identified only 28 and 7 respectively out of the 180 refactoring opportunities. The evaluation results suggested that the performance of LLMs in identifying refactoring opportunities is generally low and remains an open problem. However, explaining the expected refactoring subcategories and narrowing the search space in the prompts substantially increased the success rate of ChatGPT from 15.6 to 86.7{\%}. Concerning the recommendation of refactoring solutions, ChatGPT recommended 176 refactoring solutions for the 180 refactorings, and 63.6{\%} of the recommended solutions were comparable to (even better than) those constructed by human experts. However, 13 out of the 176 solutions suggested by ChatGPT and 9 out of the 137 solutions suggested by Gemini were unsafe in that they either changed the functionality of the source code or introduced syntax errors, which indicate the risk of LLM-based refactoring.",
issn="1573-7535",
doi="10.1007/s10515-025-00500-0",
url="https://doi.org/10.1007/s10515-025-00500-0"
}


@Article{Wang2025,
author="Wang, Baoxin
and Luo, Yumeng
and Wang, Yixuan
and Wu, Dayong
and Che, Wanxiang
and Wang, Shijin",
title="RE2: improving Chinese grammatical error correction via retrieving appropriate examples with explanation",
journal="Frontiers of Computer Science",
year="2025",
month="Nov",
day="08",
volume="19",
number="12",
pages="1912381",
abstract="The primary objective of Chinese grammatical error correction (CGEC) is to detect and correct errors in Chinese sentences. Recent research shows that large language models (LLMs) have been applied to CGEC with significant results. For LLMs, selecting appropriate reference examples can help improve their performance. However, existing methods predominantly rely on text similarity for example retrieval, a strategy that frequently mismatches actual error patterns and retrieves lexically similar yet grammatically irrelevant sentences. To address this problem, we propose a method named RE2, which retrieves appropriate examples with explanations of grammatical errors. Instead of using text similarity of the input sentence, we use explanations of grammatical errors to select reference examples, which are used by LLMs to improve the performance of CGEC. We conduct experiments on two CGEC datasets and create a high-quality grammatical error explanation (GEE) dataset, which is not only used in our research but also serves as a valuable resource for future studies in both CGEC and GEE. The experimental results on the two datasets indicate that our proposed method effectively improves the performance of CGEC.",
issn="2095-2236",
doi="10.1007/s11704-025-41399-w",
url="https://doi.org/10.1007/s11704-025-41399-w"
}


@inproceedings{10.1007/978-981-96-5693-6_31,
 abstract = {Ensuring timely and accurate security patches is crucial for maintaining software integrity in the face of evolving vulnerabilities. This paper presents a comprehensive study on developing and applying advanced AI and deep learning models for improving security patch management. It examines the challenges in automated program repair for complex security vulnerabilities and explores the use of large language models (LLMs) to focus repair efforts on relevant code sections. Deep learning methods for vulnerability detection are also analyzed, including a new dataset of over 18,000 vulnerable functions from security-related commits. The study also reviews machine learning and deep learning applications for detecting vulnerabilities in Internet of Things (IoT) devices, addressing current limitations such as high false-positive rates and generalization difficulties. Promising research directions, such as source code-specific pre-training models, are identified to enhance the future performance of AI-driven vulnerability detection systems.},
 address = {Singapore},
 author = {Shahbazi, Zeinab
and Mesbah, Meshkat},
 booktitle = {Advances in Computer Science and Ubiquitous Computing},
 editor = {Park, Ji Su
and Camacho, David
and Gritzalis, Stefanos
and Park, James J.},
 isbn = {978-981-96-5693-6},
 pages = {199--205},
 publisher = {Springer Nature Singapore},
 title = {Deep Learning Techniques for Enhancing the Efficiency of Security Patch Development},
 url = {https://link.springer.com/chapter/10.1007/978-981-96-5693-6_31},
 year = {2025}
}

@Article{Li2025,
author="Li, Yuning
and Zhong, Wenkang
and Shen, Zongwen
and Li, Chuanyi
and Chen, Xiang
and Ge, Jidong
and Luo, Bin",
title="An empirical study on the code naturalness modeling capability for LLMs in automated patch correctness assessment",
journal="Automated Software Engineering",
year="2025",
month="Apr",
day="02",
volume="32",
number="2",
pages="35",
abstract="Just like natural language, code can exhibit naturalness. This property manifests in highly repetitive patterns within specific contexts. Code naturalness can be captured by language models and then applied to various software engineering tasks (such as fault localization and program repair). Recently, Large Language Models (LLMs) based on Transformers have become advantageous tools for modeling code naturalness. However, existing work lacks systematic studies on the code naturalness modeling capability for LLMs. To bridge this gap, this paper explores the code naturalness modeling capability for LLMs, starting with the task of automated patch correctness assessment. Specifically, we investigate whether LLMs with different architectures and scales, under varying context window sizes, (1) can identify buggy code from common code based on naturalness and consider fixed code more natural than buggy code, and (2) can distinguish different degrees of repairs (i.e., complete repairs and incomplete repairs) from automated tools. Then, we propose metrics to assess the above two capabilities of the models. Experimental results indicate that models with different architectures and scales have the code naturalness modeling capability, even models not specifically pre-trained on code. Additionally, smaller models do not necessarily exhibit weaker modeling capability compared to larger models. We also find more contextual information only provides limited benefits. Based on experimental findings, we select the best performing model that has 220 M parameters to develop an Entropy-based Automated Patch Correctness Assessment (E-APCA) approach by calculating code naturalness. On the large-scale dataset PraPatch, E-APCA surpasses traditional methods by over 20{\%} across various evaluation metrics. Compared to the latest APCA method Entropy-delta based on a 6.7B LLM, E-APCA achieves a 17.32{\%} higher correct patch recall and a 6.83{\%} higher F1 score, while the reasoning time is less than 7{\%} of that required by Entropy-delta.",
issn="1573-7535",
doi="10.1007/s10515-025-00502-y",
url="https://doi.org/10.1007/s10515-025-00502-y"
}


@Article{Yousofvand2025,
author="Yousofvand, Leila
and Soleimani, Seyfollah
and Rafe, Vahid
and Nikanjam, Amin",
title="Graph neural networks for precise bug localization through structural program analysis",
journal="Automated Software Engineering",
year="2025",
month="Oct",
day="18",
volume="33",
number="1",
pages="17",
abstract="Bug localization (BL) is known as one of the major steps in the program repair process, which generally seeks to find a set of commands causing a program to crash or fail. At the present time, locating bugs and their sources quickly seems to be impossible as the complexity of modern software development and scaling is soaring. Accordingly, there is a huge demand for BL techniques with minimal human intervention. A graph representing source code typically encodes valuable information about both the syntactic and semantic structures of programs. Many software bugs are associated with these structures, making graphs particularly suitable for bug localization (BL). Therefore, the key contributions of this work involve labeling graph nodes, classifying these nodes, and addressing imbalanced classifications within the graph data structure to effectively locate bugs in code. A graph-based bug classifier is initially introduced in the method proposed in this paper. For this purpose, the program source codes are mapped to a graph representation. Since the graph nodes do not have labels, the Gumtree algorithm is then exploited to label them by comparing the buggy graphs and the corresponding bug-free ones. Afterward, a trained, supervised node classifier, developed based on a graph neural network (GNN), is applied to classify the nodes into buggy or bug-free ones. Given the imbalance in the data, accuracy, precision, recall, and F1-score metrics are used for evaluation. Experimental results on identical datasets show that the proposed method outperforms other related approaches. The proposed approach effectively localizes a broader spectrum of bug types, such as undefined properties, functional bugs, variable naming errors, and variable misuse issues.",
issn="1573-7535",
doi="10.1007/s10515-025-00556-y",
url="https://doi.org/10.1007/s10515-025-00556-y"
}


@inproceedings{10.1007/978-3-031-80419-9_9,
 abstract = {Capturing the logical structure of programming languages poses a significant challenge for program analysis. Given the complex syntax rules, subjective code vulnerabilities, irrelevant statements, code annotations, and intricate structural information, related studies have explored various semantic comprehension and intermediate representation approaches to extract precise information for program analysis. However, most research in the generic domain ignores defective and non-defective program code, putting them in the same category. In this paper, we introduce a new program analysis method that combines WebAssembly (Wasm) instructions with a 20-billion-parameter transformer model and natural language processing. This approach aims to advance the capabilities of program analysis tools in computer science by jointly embedding Wasm instructions and natural language for more effective program analysis. Our experiments demonstrate that this fused embedding approach achieves state-of-the-art performance, and the accuracy reaches approximately 98 percent, better than traditional small-scale weight models based on intricate conversion tasks such as abstract syntax trees(AST). Moreover, it is more valuable to classify potential vulnerable and non-vulnerable programs in the formal verification special field. Our exploration enhances traditional program classification methods in software security and introduces the application of GPT to offer a more straightforward, convenient, and high-performance approach.},
 address = {Cham},
 author = {Deng, Liangjun
and Zhong, Qi
and Lei, Hang
and Qiu, Yao
and Chen, Jingxue},
 booktitle = {Emerging Information Security and Applications},
 editor = {Li, Wenjuan
and Chen, Liqun
and Lopez, Javier},
 isbn = {978-3-031-80419-9},
 pages = {118--136},
 publisher = {Springer Nature Switzerland},
 title = {GPT-Based Wasm Instruction Analysis for Program Language Processing},
 url = {https://link.springer.com/chapter/10.1007/978-3-031-80419-9_9},
 year = {2025}
}

@Article{Wang2025,
author="Wang, Luping
and Chen, Sheng
and Jiang, Linnan
and Pan, Shu
and Cai, Runze
and Yang, Sen
and Yang, Fei",
title="Parameter-efficient fine-tuning in large language models: a survey of methodologies",
journal="Artificial Intelligence Review",
year="2025",
month="May",
day="03",
volume="58",
number="8",
pages="227",
abstract="The large language models, as predicted by scaling law forecasts, have made groundbreaking progress in many fields, particularly in natural language generation tasks, where they have approached or even surpassed human levels. However, the unprecedented scale of their parameters brings significant computational and storage costs. These large language models require substantial computational resources and GPU memory to operate. When adapting large language models to specific downstream tasks, their massive parameter scale poses a significant challenge in fine-tuning on hardware platforms with limited computational power and GPU memory. To address this issue, parameter-efficient fine-tuning (PEFT) offers a practical solution by efficiently adjusting the parameters of large pre-trained models to suit various downstream tasks. Specifically, PEFT adjusts the parameters of pre-trained large language models to adapt to specific tasks or domains, minimizing the introduction of additional parameters and the computational resources required. This review mainly introduces the preliminary knowledge of PEFT, the core ideas and principles of various PEFT algorithms, the applications of PEFT, and potential future research directions. By reading this review, we believe that interested parties can quickly grasp the PEFT methodology, thereby accelerating its development and innovation.",
issn="1573-7462",
doi="10.1007/s10462-025-11236-4",
url="https://doi.org/10.1007/s10462-025-11236-4"
}


@inproceedings{10.1007/978-3-032-03705-3_11,
 abstract = {We propose an automated vulnerability detection system that synergizes static analysis and fuzzing target identification through LLMs and AI agents. Building on the Dante system, our solution integrates various reasoning models and leverages a dynamic dataset from student code contributions. The system employs reinforcement learning, prompt crafting, and test-time computing techniques to refine the detection of critical vulnerabilities in C codebases. A multi-step automated workflow performs detailed static code analysis, extracts fuzzing targets, and iteratively compiles and tests code snippets. Log outputs are summarized using reasoning models, highlighting only the most relevant errors.},
 address = {Cham},
 author = {Kapusta, Pawe{\l}
and Duch, Piotr
and Majchrowicz, Micha{\l}
and Kr{\'o}lik, Adrian},
 booktitle = {Artificial Intelligence and Soft Computing},
 editor = {Rutkowski, Leszek
and Scherer, Rafa{\l}
and Korytkowski, Marcin
and Pedrycz, Witold
and Tadeusiewicz, Ryszard
and Zurada, Jacek M.},
 isbn = {978-3-032-03705-3},
 pages = {114--125},
 publisher = {Springer Nature Switzerland},
 title = {System for Automatic Bug Detection in Code and Programs Using LLMs and AI Agents},
 url = {https://link.springer.com/chapter/10.1007/978-3-032-03705-3_11},
 year = {2026}
}

@inproceedings{10.1007/978-3-031-48796-5_13,
 abstract = {Large language models (LLMs) have been successfully applied to software engineering tasks, including program repair. However, their application in search-based techniques such as Genetic Improvement (GI) is still largely unexplored. In this paper, we evaluate the use of LLMs as mutation operators for GI to improve the search process. We expand the Gin Java GI toolkit to call OpenAI's API to generate edits for the JCodec tool. We randomly sample the space of edits using 5 different edit types. We find that the number of patches passing unit tests is up to {\$}{\$}75{\backslash}{\%}{\$}{\$}75{\%}higher with LLM-based edits than with standard Insert edits. Further, we observe that the patches found with LLMs are generally less diverse compared to standard edits. We ran GI with local search to find runtime improvements. Although many improving patches are found by LLM-enhanced GI, the best improving patch was found by standard GI.},
 address = {Cham},
 author = {Brownlee, Alexander E. I.
and Callan, James
and Even-Mendoza, Karine
and Geiger, Alina
and Hanna, Carol
and Petke, Justyna
and Sarro, Federica
and Sobania, Dominik},
 booktitle = {Search-Based Software Engineering},
 editor = {Arcaini, Paolo
and Yue, Tao
and Fredericks, Erik M.},
 isbn = {978-3-031-48796-5},
 pages = {153--159},
 publisher = {Springer Nature Switzerland},
 title = {Enhancing Genetic Improvement Mutations Using Large Language Models},
 url = {https://link.springer.com/chapter/10.1007/978-3-031-48796-5_13},
 year = {2024}
}

@inproceedings{10.1007/978-981-96-7238-7_10,
 abstract = {The inflexibility of monolithic architectures results in a low software systems scalability and agility, compared to the modular systems based on microservices that can be independently developed and deployed. To facilitate the migration from monolithic to microservices architecture (MSA), researchers and practitioners have proposed various approaches. However, most of these works focus on automating the identification of microservices, and few have addressed the practical aspects of generating and deploying them. This gap leaves the migration process costly and complex, relying on expert intervention and manual labour to transform monolith systems into microservices. In this paper, we suggest that LLMs constitute an exciting but ill-studied means to automate key steps of the generation of microservices, including source code restructuring, API implementation, and integration of microservice patterns. We introduce (1) a systematic approach for generating LLM prompts to guide the migration process and (2) insights drawn from applying this approach to a real-world system. Our position challenges the current reliance on manual labour for MSA migration and supports LLM-assisted automation, encouraging discussions on the feasibility, benefits and wider impact of incorporating LLMs into the migration process. We introduce (1) a systematic approach for generating LLM prompts to guide the migration process and (2) insights drawn from applying this approach to a real-world system. Our position challenges the current reliance on manual labour for MSA migration and supports LLM-assisted automation, driving the discussion on the practicality, advantages and impact of including LLMs in the migration process.},
 address = {Singapore},
 author = {Trabelsi, Imen
and Naouel, Moha
and Yann-Ga{\"e}l, Gu{\'e}h{\'e}neuc},
 booktitle = {Service-Oriented Computing -- ICSOC 2024 Workshops},
 editor = {Kallel, Slim
and Raibulet, Claudia
and Bouassida Rodriguez, Ismael
and Faci, Noura
and Bennaceur, Amel
and Cheikhrouhou, Saoussen
and Ben Ayed, Leila
and Sellami, Mohamed
and Nakagawa, Elisa Yumi
and Ben Halima, Riadh},
 isbn = {978-981-96-7238-7},
 pages = {121--128},
 publisher = {Springer Nature Singapore},
 title = {Exploring the Systematic Use of LLMs for Microservices Generation},
 url = {https://link.springer.com/chapter/10.1007/978-981-96-7238-7_10},
 year = {2026}
}

@Article{Cui2025,
author="Cui, Tianyu
and Ma, Shiyu
and Chen, Ziang
and Xiao, Tong
and Zhao, Chenyu
and Tao, Shimin
and Liu, Yilun
and Zhang, Shenglin
and Lin, Duoming
and Liu, Changchang
and Cai, Yuzhe
and Meng, Weibin
and Sun, Yongqian
and Pei, Dan",
title="LogEval: A comprehensive benchmark suite for LLMs in log analysis",
journal="Empirical Software Engineering",
year="2025",
month="Oct",
day="10",
volume="30",
number="6",
pages="173",
abstract="Log analysis is vital in Artificial Intelligence for IT Operations (AIOps) and plays a crucial role in ensuring software reliability and system stability. However, challenges such as the absence of comprehensive evaluation standards, inconsistencies in benchmarking practices, and limited exploration of Large Language Models (LLMs) in log-related tasks persist. To address these issues, we introduce LogEval, a comprehensive benchmark designed to systematically evaluate LLMs' performance across four key log analysis tasks: log parsing, log anomaly detection, log fault diagnosis, and log summarization. LogEval systematically tackles these challenges through the following aspects: (i) it incorporates 4,000 publicly available log entries, spanning diverse tasks and providing a strong foundation for evaluating LLM performance; (ii) it utilizes standardized prompts in both English and Chinese to ensure consistent and objective evaluations, this benchmark covers two experimental paradigms: Naive question-answering (Q{\&}A) and self-consistency (SC) Q{\&}A, under both zero-shot and few-shot settings, while also considering inference efficiency and average token usage; (iii) it features an open-source, continuously updated platform (https://nkcs.iops.ai/LogEval/) that integrates new LLMs and user-uploaded production data, fostering reproducibility and adaptability in performance comparisons. The experimental results provide valuable insights into the varying strengths of LLMs across different tasks, highlighting opportunities for further optimization and innovation for LLMs in log analysis. Our code repository is available at https://github.com/LinDuoming/LogEval.",
issn="1573-7616",
doi="10.1007/s10664-025-10701-6",
url="https://doi.org/10.1007/s10664-025-10701-6"
}


@inproceedings{10.1007/978-3-031-97620-9_15,
 abstract = {Code obfuscation alters software code to conceal its logic while retaining functionality, aiding intellectual property protection but hindering security audits and malware analysis. To address this, automated deobfuscation techniques have been developed, though existing approaches remain constrained by limited scope and specificity. Motivated by these challenges, this paper explores a novel approach for code deobfuscation based on Large Language Models (LLMs).  First, we investigate the general capabilities of LLMs in reducing code complexity by choosing five different source-to-source obfuscation methods. Despite challenges regarding semantical correctness, our findings indicate that LLMs can be very effective in this task. Building on this, we fine-tune two versatile models capable of simplifying code obfuscated through up to seven different chained obfuscation transformations while consistently outperforming deobfuscation based on compiler optimizations and general-purpose LLMs. Our best model demonstrates an average Halstead metric program length reduction of 89.21{\%} for our most challenging scenario. Finally, we conduct a memorization test to assess if performance stems from memorized code rather than true deobfuscation capabilities, which our models pass.},
 address = {Cham},
 author = {Beste, David
and Menguy, Gr{\'e}goire
and Hajipour, Hossein
and Fritz, Mario
and Cin{\`a}, Antonio Emanuele
and Bardin, S{\'e}bastien
and Holz, Thorsten
and Eisenhofer, Thorsten
and Sch{\"o}nherr, Lea},
 booktitle = {Detection of Intrusions and Malware, and Vulnerability Assessment},
 editor = {Egele, Manuel
and Moonsamy, Veelasha
and Gruss, Daniel
and Carminati, Michele},
 isbn = {978-3-031-97620-9},
 pages = {267--286},
 publisher = {Springer Nature Switzerland},
 title = {Exploring the Potential of LLMs for Code Deobfuscation},
 url = {https://link.springer.com/chapter/10.1007/978-3-031-97620-9_15},
 year = {2025}
}

@inproceedings{10.1007/978-3-031-64302-6_19,
 abstract = {Large Language Models (LLMs) now excel at generative skills and can create content at impeccable speeds. However, they are imperfect and still make various mistakes. In a Computer Science education context, as these models are widely recognized as ``AI pair programmers,'' it becomes increasingly important to train students on evaluating and debugging the LLM-generated code. In this work, we introduce HypoCompass, a novel system to facilitate deliberate practice on debugging, where human novices play the role of Teaching Assistants and help LLM-powered teachable agents debug code. We enable effective task delegation between students and LLMs in this learning-by-teaching environment: students focus on hypothesizing the cause of code errors, while adjacent skills like code completion are offloaded to LLM-agents. Our evaluations demonstrate that HypoCompass generates high-quality training materials (e.g., bugs and fixes), outperforming human counterparts fourfold in efficiency, and significantly improves student performance on debugging by 12{\%} in the pre-to-post test.},
 address = {Cham},
 author = {Ma, Qianou
and Shen, Hua
and Koedinger, Kenneth
and Wu, Sherry Tongshuang},
 booktitle = {Artificial Intelligence in Education},
 editor = {Olney, Andrew M.
and Chounta, Irene-Angelica
and Liu, Zitao
and Santos, Olga C.
and Bittencourt, Ig Ibert},
 isbn = {978-3-031-64302-6},
 pages = {265--279},
 publisher = {Springer Nature Switzerland},
 title = {How to Teach Programming in the AI Era? Using LLMs as a Teachable Agent for Debugging},
 url = {https://link.springer.com/chapter/10.1007/978-3-031-64302-6_19},
 year = {2024}
}

@inproceedings{10.1007/978-3-032-07884-1_9,
 abstract = {Software supply chain vulnerabilities arise when attackers exploit weaknesses by injecting vulnerable code into widely used packages or libraries within software repositories. While most existing approaches focus on identifying vulnerable packages or libraries, they often overlook the specific functions responsible for these vulnerabilities. Pinpointing vulnerable functions within packages or libraries is critical, as it can significantly reduce the risks associated with using open-source software. Identifying vulnerable patches is challenging because developers often submit code changes that are unrelated to vulnerability fixes. To address this issue, this paper introduces FuncVul, an innovative code chunk-based model for function-level vulnerability detection in C/C++ and Python, designed to identify multiple vulnerabilities within a function by focusing on smaller, critical code segments. To assess the model's effectiveness, we construct six code and generic code chunk based datasets using two approaches: (1) integrating patch information with large language models to label vulnerable samples and (2) leveraging large language models alone to detect vulnerabilities in function-level code. To design FuncVul vulnerability model, we utilise GraphCodeBERT fine tune model that captures both the syntactic and semantic aspects of code. Experimental results show that FuncVul outperforms existing state-of-the-art models, achieving an average accuracy of 87--92{\%} and an F1 score of 86--92{\%} across all datasets. Furthermore, we have demonstrated that our code-chunk-based FuncVul model improves 53.9{\%} accuracy and 42.0{\%} F1-score than the full function-based vulnerability prediction. The model code and datasets are publicly available on GitHub (https://github.com/sajalhalder/FuncVul).},
 address = {Cham},
 author = {Halder, Sajal
and Ahmed, Muhammad Ejaz
and Camtepe, Seyit},
 booktitle = {Computer Security -- ESORICS 2025},
 editor = {Nicomette, Vincent
and Benzekri, Abdelmalek
and Boulahia-Cuppens, Nora
and Vaidya, Jaideep},
 isbn = {978-3-032-07884-1},
 pages = {166--185},
 publisher = {Springer Nature Switzerland},
 title = {FuncVul: An Effective Function Level Vulnerability Detection Model Using LLM and Code Chunk},
 url = {https://link.springer.com/chapter/10.1007/978-3-032-07884-1_9},
 year = {2026}
}

@Article{Huang2025,
author="Huang, Yu-Huan
and Liu, Chen-Yan
and Lin, Yun
and Cai, Yu-Fan
and Jiang, Bo
and Yang, Ping
and Huang, Zhiyong
and Dong, Jin Song",
title="CoEdPilot: Interactively Recommending Project-Wise Code Edits",
journal="Journal of Computer Science and Technology",
year="2025",
month="Jul",
day="01",
volume="40",
number="4",
pages="980--992",
abstract="Incremental code editing, as a fundamental task in software development, requires developers to iteratively identify edit locations and modify code. However, existing language model-driven approaches primarily focus on generating edit solutions for a single location, failing to provide comprehensive end-to-end solutions. To address this limitation and support real-world editing scenarios, we propose CoEdPilot, a project-wide interactive code editing recommendation tool. CoEdPilot utilizes edit descriptions and edit history, and recommends the next edit location with solutions across the entire project. It further refines its recommendations based on user editing feedback, enabling an end-to-end, iterative, and interactive editing process. We implement CoEdPilot as a visual studio code extension that monitors user actions, identifies subsequent editing locations, and generates edits throughout the project. Its functionality is powered by a set of backend language models, which are trained on 180k high-quality commits from 471 open-source repositories. Extensive experiments demonstrate CoEdPilot's capabilities in accurately identifying editing locations (i.e., edit location predicted with an accuracy of 85.03{\%}--88.99{\%}) and generating high-quality edit solutions (i.e., generated edit content with a top-1 exact match rate (EMR) of 33.48{\%}--48.94{\%}). Our case study and user study of 18 participants further validate CoEdPilot's practicability.",
issn="1860-4749",
doi="10.1007/s11390-025-5139-z",
url="https://doi.org/10.1007/s11390-025-5139-z"
}


@inproceedings{10.1007/978-981-97-8669-5_21,
 abstract = {With the rise of e-commerce and web applications, Recommender Systems (RecSys) have become integral to our daily lives, offering personalized suggestions tailored to individual user preferences. Large Language Models (LLMs), with their unprecedented training scale and a vast number of model parameters, have significantly enhanced capabilities, achieving human-like proficiency in understanding, synthesizing language, and reasoning with common sense and thereby reshaping the landscape of web personalization methods. Unlike traditional Deep Neural Networks (DNNs) approaches, which face challenges in effectively understanding user interests, incorporating textual side information, generalizing to diverse recommendation scenarios, and reasoning on predictions, the advent of Large Language Models (LLMs) such as ChatGPT and GPT4 enables proactive exploration of user requests and the delivery of required information in a natural, interactive, and understandable manner. Moreover, LLMs allow web personalization systems to translate user requests into actionable plans, invoke external tools' functionalities, such as search engines, calculators, service APIs, etc. The output of these tools can be further amalgamated by LLMs to accomplish end-to-end personalization tasks. In the present era, there is a pressing need to systematically study the challenges in web personalization and explore the potential of leveraging large language models to address them. In this regard, we first summarize the diverse existing LLM-empowered recommender systems spanning multiple web applications. This includes conversational recommender systems (CRS) and rating prediction, top-k predictions, top-k recommendation in e-commerce and various online services, automated ML, online personalized content creator etc. Then, we discuss the future directions in the emerging personalized recommendation field.},
 address = {Singapore},
 author = {Bansal, Nipun
and Bala, Manju
and Sharma, Kapil},
 booktitle = {Proceedings of International Conference on Paradigms of Communication, Computing and Data Analytics},
 editor = {Mittal, Himanshu
and Nanda, Satyasai Jagannath
and Lim, Meng-Hiot},
 isbn = {978-981-97-8669-5},
 pages = {269--283},
 publisher = {Springer Nature Singapore},
 title = {Web Personalization with Large Language Models: Challenges and Future Trends},
 url = {https://link.springer.com/chapter/10.1007/978-981-97-8669-5_21},
 year = {2025}
}

@Article{Yousef2025,
author="Yousef, Mina
and Mohamed, Kareem
and Medhat, Walaa
and Mohamed, Ensaf Hussein
and Khoriba, Ghada
and Arafa, Tamer",
title="BeGrading: large language models for enhanced feedback in programming education",
journal="Neural Computing and Applications",
year="2025",
month="Jan",
day="01",
volume="37",
number="2",
pages="1027--1040",
abstract="In recent years, large language models (LLMs) have gained significant traction across various domains, including education. This paper explores the application of LLMs in grading programming assignments. By leveraging data collected from existing programming assignments and their corresponding grades, we aim to develop a robust LLM-based grading system. We also incorporate augmented data representing various grading scenarios to enhance the model's performance and ensure comprehensive coverage across all grading levels. Our approach involves training the LLM on this combined dataset to enable accurate and consistent evaluation of programming assignments. The proposed model, BeGrading, aims to reduce the grading burden on educators and provide timely and objective feedback to students. Compared to the Codestral model, our proposed model demonstrates an absolute difference rate of 19{\%}, equivalent to {\$}{\$}{\backslash}pm 0.95{\$}{\$}out of 5. This is acceptable for using a small, fine-tuned model with optimized data. Additionally, the Codestral model compared to the dataset optimized score shows a difference of 15{\%} equivalent to a margin of {\$}{\$}{\backslash}pm 0.75{\$}{\$}out of 5. Preliminary results demonstrate the potential of LLMs to perform grading tasks with a high degree of reliability, opening avenues for further research and practical applications in automated education systems.",
issn="1433-3058",
doi="10.1007/s00521-024-10449-y",
url="https://doi.org/10.1007/s00521-024-10449-y"
}


@Article{Minna2025,
author="Minna, Francesco
and Massacci, Fabio
and Tuma, Katja",
title="Analyzing and mitigating (with LLMs) the security misconfigurations of Helm charts from Artifact Hub",
journal="Empirical Software Engineering",
year="2025",
month="Jul",
day="04",
volume="30",
number="5",
pages="132",
abstract="Helm is a package manager that allows defining, installing, and upgrading applications with Kubernetes (K8s), a popular container orchestration platform. A Helm chart is a collection of files describing all dependencies, resources, and parameters required for deploying an application within a K8s cluster. This study aimed to mine and empirically evaluate the security of Helm charts, comparing the performance of existing tools in terms of misconfigurations reported by policies available by default, and measuring to what extent LLMs could be used for removing misconfigurations. For these reasons, we proposed a pipeline to mine Helm charts from Artifact Hub, a popular centralized repository, and analyze them using state-of-the-art open-source tools like Checkov and KICS. First, the pipeline runs several chart analyzers and identifies the common and unique misconfigurations reported by each tool. Secondly, it uses LLMs to suggest a mitigation for each misconfiguration. Finally, the LLM refactored chart previously generated is analyzed again by the same tools to see whether it satisfies the tool's policies. We also performed a manual analysis on a subset of charts to evaluate whether there are false positive misconfigurations from the tool's reporting and in the LLM refactoring. We found that (i) there is a significant difference between LLMs, (ii) providing a snippet of the YAML template as input might be insufficient compared to all resources, and (iii) even though LLMs can generate correct fixes, they may also delete other irrelevant configurations that break the application.",
issn="1573-7616",
doi="10.1007/s10664-025-10688-0",
url="https://doi.org/10.1007/s10664-025-10688-0"
}


@Article{Pandey2025,
author="Pandey, Sushant Kumar
and Chand, Sivajeet
and Horkoff, Jennifer
and Staron, Miroslaw
and Ochodek, Miroslaw
and Durisic, Darko",
title="Design pattern recognition: a study of large language models",
journal="Empirical Software Engineering",
year="2025",
month="Feb",
day="18",
volume="30",
number="3",
pages="69",
abstract="As Software Engineering (SE) practices evolve due to extensive increases in software size and complexity, the importance of tools to analyze and understand source code grows significantly.",
issn="1573-7616",
doi="10.1007/s10664-025-10625-1",
url="https://doi.org/10.1007/s10664-025-10625-1"
}


@Article{Zhao2026,
author="Zhao, Penghao
and Zhang, Hailin
and Yu, Qinhan
and Wang, Zhengren
and Geng, Yunteng
and Fu, Fangcheng
and Yang, Ling
and Zhang, Wentao
and Jiang, Jie
and Cui, Bin",
title="Retrieval-Augmented Generation for AI-Generated Content: A Survey",
journal="Data Science and Engineering",
year="2026",
month="Jan",
day="02",
abstract="Advancements in model algorithms, the growth of foundational models, and access to high-quality datasets have propelled the evolution of Artificial Intelligence Generated Content (AIGC). Despite its notable successes, AIGC still faces hurdles such as updating knowledge, handling long-tail data, mitigating data leakage, and managing high training and inference costs. Retrieval-augmented generation (RAG) has recently emerged as a paradigm to address such challenges. In particular, RAG introduces the information retrieval process, which enhances the generation process by retrieving relevant objects from available data stores, leading to higher accuracy and better robustness. In this paper, we comprehensively review existing efforts that integrate RAG techniques into AIGC scenarios. We first classify RAG foundations according to how the retriever augments the generator, distilling the fundamental abstractions of the augmentation methodologies for various retrievers and generators. This unified perspective encompasses all RAG scenarios, illuminating advancements and pivotal technologies that help with potential future progress. We also summarize additional enhancement methods for RAG, facilitating effective engineering and implementation of RAG systems. Then from another view, we survey practical applications of RAG across different modalities and tasks, offering valuable references for researchers and practitioners. Furthermore, we introduce the benchmarks for RAG, discuss the limitations of current RAG systems, and suggest potential directions for future research.",
issn="2364-1541",
doi="10.1007/s41019-025-00335-5",
url="https://doi.org/10.1007/s41019-025-00335-5"
}


@Article{Panchal2026,
author="Panchal, Mihir
and Deo, Arnav
and Prabhu, Varad
and Doshi, Prinkal
and Bhadane, Chetashri
and Bari, Pranit",
title="LEDGE : Leveraging dependency graphs for enhanced context aware documentation generation",
journal="Automated Software Engineering",
year="2026",
month="Feb",
day="03",
volume="33",
number="2",
pages="51",
abstract="In software engineering, effective documentation is crucial for understanding complex codebases, yet it often remains incomplete or outdated, hindering developer productivity. This paper introduces LEDGE (Leveraging Dependency Graphs for Enhanced Context Aware Documentation Generation), a novel framework that integrates dependency graphs with large language models (LLMs) to automate the generation of structured, context aware software documentation. By leveraging GraphRAG, LEDGE captures semantic and structural relationships within codebases, enabling precise documentation that highlights architectural insights and module dependencies. Our methodology employs a parser based approach to construct dependency graphs, stored in MemGraph using Cypher queries, and utilizes vector embeddings for similarity based retrieval. Evaluated on diverse open source repositories, LEDGE demonstrates comparable semantic alignment with existing documentation while providing enhanced structural context, as evidenced by qualitative and quantitative analyses. The framework enhances software maintainability, developer onboarding, and knowledge transfer, offering a scalable solution for modern software development. Our code and data are available at https://github.com/MihirRajeshPanchal/LEDGE.",
issn="1573-7535",
doi="10.1007/s10515-026-00596-y",
url="https://doi.org/10.1007/s10515-026-00596-y"
}


@inproceedings{10.1007/978-3-031-66459-5_9,
 abstract = {ChatGPT can advise developers and provide code on how to fix bugs, add new features, refactor, reuse, and secure their code but currently, there is little knowledge about whether the developers trust ChatGPT's responses and actually use the provided code. In this context, this study aims to identify patterns that describe the interaction of developers with ChatGPT with respect to the characteristics of the prompts and the actual use of the provided code by the developer. We performed a case study on 267,098 lines of code provided by ChatGPT related to commits, pull requests, files of code, and discussions between ChatGPT and developers. Our findings show that developers are more likely to integrate the given code snapshot in their code base when they have provided information to ChatGPT through several rounds of brief prompts that include problem-related specific words instead of using large textual or code prompts. Results also highlight the ability of ChatGPT to handle efficiently different types of problems across different programming languages.},
 address = {Cham},
 author = {Terzi, Anastasia
and Bibi, Stamatia
and Tsitsimiklis, Nikolaos
and Angelidis, Pantelis},
 booktitle = {Reuse and Software Quality},
 editor = {Achilleos, Achilleas
and Fuentes, Lidia
and Papadopoulos, George Angelos},
 isbn = {978-3-031-66459-5},
 pages = {137--152},
 publisher = {Springer Nature Switzerland},
 title = {Using Code from ChatGPT: Finding Patterns in the Developers' Interaction with ChatGPT},
 url = {https://link.springer.com/chapter/10.1007/978-3-031-66459-5_9},
 year = {2024}
}

@inproceedings{10.1007/978-3-031-62495-7_27,
 abstract = {Automated Program Repair (APR) is a domain of research in software engineering that focuses on providing computationally-generated fixes to buggy code. The primary objective is to alleviate the challenges associated with identifying and rectifying errors that exist within large-scale projects. Fault localization is a critical stage of the APR pipeline, dedicated to identifying the locations of bugs within the code. Despite Python now being one of the most popular programming languages, most existing fault localization techniques are limited to real-world Java and C repositories. This paper proposes a graph-based representation of buggy code that utilizes flow of control and data to capture both semantic and syntactic information. We also present an analysis of a novel approach, Class-Imbalanced Learning on Graphs (CILG) for fault localization, as an alternative to conventional methods of calculating program element suspiciousness scores. The proposed approach is trained and tested on a real-world dataset containing buggy Python code snippets extracted from the PyTraceBugs dataset, achieving a notable macro Area Under the Curve-Receiver Operating Characteristic (AUC-ROC) score of 0.85. We have also provided a comparison with Graph Neural Network (GNN) models and gpt-3.5-turbo to demonstrate the effectiveness of our technique.},
 address = {Cham},
 author = {Kulkarni, Apoorva Anand
and Niranjan, Divya G.
and Saju, Noel
and Shenoy, P. Rakshith
and Arya, Arti},
 booktitle = {Engineering Applications of Neural Networks},
 editor = {Iliadis, Lazaros
and Maglogiannis, Ilias
and Papaleonidas, Antonios
and Pimenidis, Elias
and Jayne, Chrisina},
 isbn = {978-3-031-62495-7},
 pages = {354--368},
 publisher = {Springer Nature Switzerland},
 title = {Graph-Based Fault Localization in Python Projects with Class-Imbalanced Learning},
 url = {https://link.springer.com/chapter/10.1007/978-3-031-62495-7_27},
 year = {2024}
}

@inproceedings{10.1007/978-3-032-00630-1_2,
 abstract = {The growing and evolving landscape of cybersecurity threats necessitates the development of supporting tools and platforms that allow for the creation of realistic IT environments operating within virtual, controlled settings as Cyber Ranges (CRs). CRs can be exploited for analyzing vulnerabilities and experimenting with the effectiveness of devised countermeasures, as well as serving as training environments for building cyber security skills and abilities for IT operators. This paper proposes ARCeR as an innovative solution for the automatic definition and deployment of CRs, starting from user-provided descriptions in a natural language. ARCeR relies on the Agentic RAG paradigm, which allows it to fully exploit state-of-art AI technologies. Experimental results show that ARCeR is able to successfully process prompts even in cases that LLMs or basic RAG systems are not able to cope with. Furthermore, ARCeR is able to target any CR framework provided that specific knowledge is made available to it.},
 address = {Cham},
 author = {Lupinacci, Matteo
and Blefari, Francesco
and Romeo, Francesco
and Pironti, Francesco Aurelio
and Furfaro, Angelo},
 booktitle = {Availability, Reliability and Security},
 editor = {Coppens, Bart
and Volckaert, Bruno
and Naessens, Vincent
and De Sutter, Bjorn},
 isbn = {978-3-032-00630-1},
 pages = {23--40},
 publisher = {Springer Nature Switzerland},
 title = {ARCeR: An Agentic RAG for the Automated Definition of Cyber Ranges},
 url = {https://link.springer.com/chapter/10.1007/978-3-032-00630-1_2},
 year = {2025}
}

@Article{Rafi2025,
author="Rafi, Md Nakhla
and Pacheco, Lorena Barreto Simedo
and Chen, An Ran
and Yang, Jinqiu
and Chen, Tse-Hsun Peter",
title="SBEST: Spectrum-based fault localization without fault-triggering tests",
journal="Empirical Software Engineering",
year="2025",
month="Nov",
day="15",
volume="31",
number="1",
pages="16",
abstract="Fault localization is a critical step in software maintenance. Yet, many existing techniques, such as Spectrum-Based Fault Localization (SBFL), rely heavily on the availability of fault-triggering tests to be effective. In practice, especially for crash-related bugs, such tests are frequently unavailable. Meanwhile, bug reports containing stack traces often serve as the only available evidence of runtime failures and provide valuable context for debugging. This study investigates the feasibility of using stack traces from crash reports as proxies for fault-triggering tests in SBFL. Our empirical analysis of 60 crash-report bugs in Defects4J reveals that only 3.33{\%} of these bugs have fault-triggering tests available at the time of the bug report creation. However, 98.3{\%} of bug fixes directly address the exception observed in the stack trace, and 78.3{\%} of buggy methods are reachable within an average of 0.34 method calls from the stack trace. These findings underscore the diagnostic value of stack traces in the absence of failing tests. Motivated by these findings, we propose SBEST, a novel approach that integrates stack trace information with test coverage data to perform fault localization when fault-triggering tests are missing. SBEST shows an improvement, with a 32.22{\%} increase in Mean Average Precision (MAP) and a 17.43{\%} increase in Mean Reciprocal Rank (MRR) compared to baseline approaches under the scenario where fault-triggering tests are absent.",
issn="1573-7616",
doi="10.1007/s10664-025-10761-8",
url="https://doi.org/10.1007/s10664-025-10761-8"
}


@inproceedings{10.1007/978-3-031-48796-5_12,
 abstract = {Large language models (LLMs) have recently been integrated in a variety of applications including software engineering tasks. In this work, we study the use of LLMs to enhance the explainability of software patches. In particular, we evaluate the performance of GPT 3.5 in explaining patches generated by the search-based automated program repair system ARJA-e for 30 bugs from the popular Defects4J benchmark. We also investigate the performance achieved when explaining the corresponding patches written by software developers. We find that on average 84{\%} of the LLM explanations for machine-generated patches were correct and 54{\%} were complete for the studied categories in at least 1 out of 3 runs. Furthermore, we find that the LLM generates more accurate explanations for machine-generated patches than for human-written ones.},
 address = {Cham},
 author = {Sobania, Dominik
and Geiger, Alina
and Callan, James
and Brownlee, Alexander
and Hanna, Carol
and Moussa, Rebecca
and L{\'o}pez, Mar Zamorano
and Petke, Justyna
and Sarro, Federica},
 booktitle = {Search-Based Software Engineering},
 editor = {Arcaini, Paolo
and Yue, Tao
and Fredericks, Erik M.},
 isbn = {978-3-031-48796-5},
 pages = {147--152},
 publisher = {Springer Nature Switzerland},
 title = {Evaluating Explanations for Software Patches Generated by Large Language Models},
 url = {https://link.springer.com/chapter/10.1007/978-3-031-48796-5_12},
 year = {2024}
}

@inproceedings{10.1007/978-3-031-46002-9_24,
 abstract = {This paper presents an approach to no-code development based on the interplay of formally defined (graphical) Domain-Specific Languages and informal, intuitive Natural Language which is enriched with contextual information to enable referencing of formally defined entities. The paper focuses on the use and automated integration of these enriched intuitive languages via ChatGPT-based code generation to exploit the best of both language paradigms for domain-specific application development. To compensate for the lack of control over the intuitive languages we apply automated system-level validation via automata learning and subsequent model checking. All this is illustrated using the development of point-and-click adventures as a minimal viable example.},
 address = {Cham},
 author = {Busch, Daniel
and Nolte, Gerrit
and Bainczyk, Alexander
and Steffen, Bernhard},
 booktitle = {Bridging the Gap Between AI and Reality},
 editor = {Steffen, Bernhard},
 isbn = {978-3-031-46002-9},
 pages = {375--390},
 publisher = {Springer Nature Switzerland},
 title = {ChatGPT in the Loop: A Natural Language Extension for Domain-Specific Modeling Languages},
 url = {https://link.springer.com/chapter/10.1007/978-3-031-46002-9_24},
 year = {2024}
}

@Article{Mao2024,
author="Mao, Yuren
and Ge, Yuhang
and Fan, Yijiang
and Xu, Wenyi
and Mi, Yu
and Hu, Zhonghao
and Gao, Yunjun",
title="A survey on LoRA of large language models",
journal="Frontiers of Computer Science",
year="2024",
month="Dec",
day="14",
volume="19",
number="7",
pages="197605",
abstract="Low-Rank Adaptation (LoRA), which updates the dense neural network layers with pluggable low-rank matrices, is one of the best performed parameter efficient fine-tuning paradigms. Furthermore, it has significant advantages in cross-task generalization and privacy-preserving. Hence, LoRA has gained much attention recently, and the number of related literature demonstrates exponential growth. It is necessary to conduct a comprehensive overview of the current progress on LoRA. This survey categorizes and reviews the progress from the perspectives of (1) downstream adaptation improving variants that improve LoRA's performance on downstream tasks; (2) cross-task generalization methods that mix multiple LoRA plugins to achieve cross-task generalization; (3) efficiency-improving methods that boost the computation-efficiency of LoRA; (4) data privacy-preserving methods that use LoRA in federated learning; (5) application. Besides, this survey also discusses the future directions in this field.",
issn="2095-2236",
doi="10.1007/s11704-024-40663-9",
url="https://doi.org/10.1007/s11704-024-40663-9"
}


@inproceedings{10.1007/978-3-031-93979-2_1,
 abstract = {This paper addresses the increasing demand for accurate, fair, and efficient grading of essay-style assessments in higher education by integrating institutional requirements with recent advances in large language models (LLMs). We propose a pipeline emphasizing privacy, explainability, consistency, and fairness. To ensure privacy, the system operates on local servers and employs rigorous anonymization of student data. Grading events integrate task prompts, instructor guidelines, student submissions, grader commentary, and final scores into structured records, enhancing evaluation accuracy and transparency. We detail the development and validation process, fine-tuning two local LLMs using historical course data. Results demonstrate the models' ability to effectively replicate original grading decisions while safeguarding student data. Additionally, we discuss how this framework aligns with the interests of students, educators, and policymakers. Our approach establishes a foundational methodology for responsibly integrating AI-driven grading into higher education. This contribution fosters trust among stakeholders and sets a clear direction for future implementation and research efforts.},
 address = {Cham},
 author = {Arz von Straussenburg, Arnold F.
and Wolters, Anna
and Aldenhoff, Timon T.
and Riehle, Dennis M.},
 booktitle = {Local Solutions for Global Challenges},
 editor = {Chatterjee, Samir
and vom Brocke, Jan
and Anderson, Ricardo},
 isbn = {978-3-031-93979-2},
 pages = {3--20},
 publisher = {Springer Nature Switzerland},
 title = {Enabling Responsible LLM-Based Grading in Higher Education -- Design Guidelines and a Reproducible Data Preparation Pipeline},
 url = {https://link.springer.com/chapter/10.1007/978-3-031-93979-2_1},
 year = {2025}
}

@Article{Chen2025,
author="Chen, Hong",
title="SynergyBug: A deep learning approach to autonomous debugging and code remediation",
journal="Scientific Reports",
year="2025",
month="Jul",
day="10",
volume="15",
number="1",
pages="24888",
abstract="Bug detection and resolution are pivotal to maintaining the quality, reliability, and performance of software systems. Manual debugging, along with traditional static rule-based methods, proves inefficient when applied to complex software structures in contemporary times. SynergyBug combines BERT and GPT-3 to autonomously detect and repair bugs across multiple sources. It resolves essential requirements by implementing an automated system that diagnoses and resolves software bugs automatically, thus minimising human involvement. The framework unites BERT as a contextual machinery with GPT-3 to produce bug fix generation capabilities. The semantic pattern within bug reports, together with error logs and documentation, feeds into BERT for contextual embedding generation. GPT-3 applies the generated embeddings to produce code fixes, code snippets, as well as detailed explanations that address detected problems. The system achieves continuous automatic debugging by enhancing both detection and resolution steps into one unified process. The experimental outcomes prove that it achieves superior performance than conventional bug detection methods by reaching 98.79{\%} accuracy alongside 97.23{\%} precision and 96.56{\%} recall. The system demonstrated exceptional detection strength for functional and performance, and security bugs, where the detection rates reached 94{\%} and 90{\%} and 92{\%}, respectively. SynergyBug showed its ability to expand as it processed bug reports exceeding 100,000 cases without noticeably impacting system performance. This proposed system provides faster debugging capabilities to improve the quality of the complete software development process. This paper discusses as a tool that can revolutionise bug management through proactive instead of just reactive strategies. The implementation of human monitoring within safety programs and managing training system biases represent essential organisational factors. The study terminates by recognising SynergyBug as a crucial development leading toward automated debugging tools that maintain operational safety within intricate software systems.",
issn="2045-2322",
doi="10.1038/s41598-025-08226-5",
url="https://doi.org/10.1038/s41598-025-08226-5"
}


@inproceedings{10.1007/978-981-95-3058-8_12,
 abstract = {Recently, Large Language Models (LLMs) like OpenAI's ChatGPT have shown potential in various aspects of software engineering. As a result, practitioners are increasingly exploring their use in code refactoring. While prior research has investigated ChatGPT's capabilities in areas such as requirements engineering, code generation, code review, and unit test generation, its strengths and limitations in code refactoring remain insufficiently explored. This paper evaluates ChatGPT's capability to identify extract method opportunities (EMOs). Our study leverages two widely used benchmark datasets containing synthetic and real EMOs, previously employed to assess extract method refactoring techniques. The results indicate that while ChatGPT successfully identifies EMOs, its low overlap with benchmark-defined refactorings raises concerns about the practicality of its recommendations. Additionally, while the model excels in generating descriptive method names for the extracted methods, its overall effectiveness requires further validation against multiple refactoring tools.},
 address = {Singapore},
 author = {Nyamawe, Ally S.
and Edward, Estomii},
 booktitle = {Knowledge Science, Engineering and Management},
 editor = {Zhu, Tianqing
and Zhou, Wanlei
and Zhu, Congcong},
 isbn = {978-981-95-3058-8},
 pages = {138--149},
 publisher = {Springer Nature Singapore},
 title = {Code Refactoring with ChatGPT: Analysis Based on Real and Synthetic Extract Method Opportunities},
 url = {https://link.springer.com/chapter/10.1007/978-981-95-3058-8_12},
 year = {2026}
}

@inproceedings{10.1007/978-981-99-6529-8_26,
 abstract = {This article analyzes the usage in the programming of ChatGPT, a natural language processing tool, along with how it may enhance teamwork, communication, and code quality. ChatGPT is an effective tool for developers since it can produce code snippets, templates, and functions based on natural language input. The team's capacity to grasp natural language input can assist in closing the communication and cooperation gap between technical and non-technical team members. Furthermore, ChatGPT can assist developers in finding and fixing bugs or errors in their code more quickly and efficiently by increasing the accuracy of automated code review and testing. However, there are potential disadvantages to take into account as well, such as the danger of relying too heavily on automated tools, restrictions on ChatGPT's capacity to comprehend intricate technical concepts, and worries regarding bias in the training data used to create the tool. ChatGPT has the ability to change programming as a whole by making it more approachable, effective, and user-friendly. To ensure its usefulness and acceptance in the programming community, it will be necessary to carefully analyze and solve any potential constraints and difficulties.},
 address = {Singapore},
 author = {Pantelimon, Florin Valeriu
and Posedaru, Bogdan Ștefan},
 booktitle = {Proceedings of 22nd International Conference on Informatics in Economy (IE 2023)},
 editor = {Ciurea, Cristian
and Pocatilu, Paul
and Filip, Florin Gheorghe},
 isbn = {978-981-99-6529-8},
 pages = {307--316},
 publisher = {Springer Nature Singapore},
 title = {Improving Programming Activities Using ChatGPT: A Practical Approach},
 url = {https://link.springer.com/chapter/10.1007/978-981-99-6529-8_26},
 year = {2024}
}

@Article{Sun2024,
author="Sun, Dan
and Boudouaia, Azzeddine
and Yang, Junfeng
and Xu, Jie",
title="Investigating students' programming behaviors, interaction qualities and perceptions through prompt-based learning in ChatGPT",
journal="Humanities and Social Sciences Communications",
year="2024",
month="Oct",
day="31",
volume="11",
number="1",
pages="1447",
abstract="ChatGPT has proven to facilitate computer programming tasks through the strategic use of prompts, which effectively steer the interaction with the language model towards eliciting relevant information. However, the impact of specifically designed prompts on programming learning outcomes has not been rigorously examined through empirical research. This study adopted a quasi-experimental framework to investigate the differential effects of prompt-based learning (PbL) versus unprompted learning (UL) conditions on the programming behaviors, interaction qualities, and perceptions of college students. The study sample consisted of 30 college students who were randomly assigned to two groups. A mixed-methods approach was employed to gather multi-faceted data. Results revealed notable distinctions between the two learning conditions. First, the PbL group students frequently engaged in coding with Python and employed debugging strategies to verify their work, whereas their UL counterparts typically transferred Python code from PyCharm into ChatGPT and posed new questions within ChatGPT. Second, PbL participants were inclined to formulate more complex queries independently, prompted by the guiding questions, and consequently received more precise feedback from ChatGPT compared to the UL group. UL students tended to participate in more superficial-level interactions with ChatGPT, yet they also obtained accurate feedback. Third, there were noticeable differences in perception observed before and after the ChatGPT implementation, UL group reported a more favorable perception in the perceived ease of use in the pre-test, while the PbL group experienced an improvement in their mean scores for perceived usefulness, ease of use, behavioral intention to utilize, and a significant difference regarding the attitude towards utilizing ChatGPT. Specifically, the use of structured output and delimiters enhanced learners' understanding of problem-solving steps and made learning more efficient with ChatGPT. Drawing on these outcomes, the study offers recommendations for the incorporation of ChatGPT into future instructional designs, highlighting the structured prompting benefits in enhancing programming learning experience.",
issn="2662-9992",
doi="10.1057/s41599-024-03991-6",
url="https://doi.org/10.1057/s41599-024-03991-6"
}


@Article{He2025,
author="He, Haitao
and Li, Shibo
and Li, Yuxiang
and Li, Yang",
title="GTVD: a multi-level aggregation vulnerability detection method based on full-dependency program graph",
journal="Cluster Computing",
year="2025",
month="Sep",
day="03",
volume="28",
number="10",
pages="656",
abstract="In modern software development life cycles, proactive vulnerability discovery and remediation play crucial roles in ensuring application security. However, current deep learning-based vulnerability detection methods frequently face limitations due to overly simplistic feature extraction procedures and inadequate handling of long-range dependency relationships. In this paper, we present GTVD, a graph-based vulnerability detection framework for C/C++ source code, which addresses these challenges through three key innovations. First, we introduce the Full Dependency Program Graph (FDPG), a novel intermediate representation that comprehensively encodes both syntactic structures and semantic relationships within source code. This advancement overcomes the feature representation constraints inherent in conventional code attribute graphs. Our architecture employs a hierarchical Graph Neural Network to systematically extract structural patterns from the FDPG representation, ensuring a robust analysis of the program's inherent structures. At the core of our feature extraction mechanism lies the Multi-Level Message Aggregation (MLMA) strategy. This innovative approach enables progressive integration of information across multiple neighborhood orders, effectively capturing both local and global program dependencies. To mitigate feature degradation in long-range dependencies, we develop an Adaptive Weighted Aggregation (WAG) mechanism that dynamically adjusts feature contributions during graph-level representation learning. Comprehensive evaluations on three large-scale public datasets demonstrate GTVD's superior performance. Our method achieves an average improvement of 7.76{\%} across four evaluation metrics compared to the baseline, thereby confirming our method's enhanced capability to identify complex vulnerability patterns.",
issn="1573-7543",
doi="10.1007/s10586-025-05506-7",
url="https://doi.org/10.1007/s10586-025-05506-7"
}


@inproceedings{10.1007/978-3-031-79007-2_12,
 abstract = {As generative artificial intelligence evolves, understanding the capabilities in the cybersecurity domain becomes crucial. This paper examines the capability of Large Language Models (LLMs) models in solving cybersecurity certification Multiple Choice Question Answering (MCQA) exams, comparing proprietary and open-weights models. Challenges related to test-set leakage, notably on the widely used MMLU benchmark, emphasize the need for continuous validation of benchmarking results. Open-weights models, namely Mistral Large 2, Qwen 2, and Phi 3, seem to overfit the MMLU Computer Security and indicate less usability for cybersecurity knowledge tasks. The study also introduces the first visual cybersecurity MCQA benchmark, assessing the capability of Large Multimodal Models (LMMs) in interpreting and responding to visual questions. Among the tested models, the proprietary Anthropic Claude 3.5 Sonnet and OpenAI GPT-4o outperformed others in the language and vision-language setting. However, Llama 3.1 model series demonstrated significant advancement in the open-weights domain, signaling potential parity in cybersecurity knowledge with proprietary models in the near future. Code and datasets are available at: https://github.com/GKeppler/GenAICyberSecMCQA.},
 address = {Cham},
 author = {Keppler, Gustav
and Kunz, Jeremy
and Hagenmeyer, Veit
and Elbez, Ghada},
 booktitle = {Secure IT Systems},
 editor = {Horn Iwaya, Leonardo
and Kamm, Liina
and Martucci, Leonardo
and Pulls, Tobias},
 isbn = {978-3-031-79007-2},
 pages = {219--238},
 publisher = {Springer Nature Switzerland},
 title = {Evaluating Large Language Models in Cybersecurity Knowledge with Cisco Certificates},
 url = {https://link.springer.com/chapter/10.1007/978-3-031-79007-2_12},
 year = {2025}
}

@Article{Zhou2025,
author="Zhou, Chunying
and Xie, Xiaoyuan
and Chen, Gong
and He, Peng
and Li, Bing",
title="Multi-view adaptive contrastive learning for information retrieval based fault localization",
journal="Automated Software Engineering",
year="2025",
month="Nov",
day="06",
volume="33",
number="1",
pages="29",
abstract="Most studies focused on information retrieval-based techniques for fault localization, which built representations for bug reports and source code files and matched their semantic vectors through similarity measurement. However, such approaches often ignore some useful information that might help improve localization performance, such as 1) the interaction relationship between bug reports and source code files; 2) the similarity relationship between bug reports; and 3) the co-citation relationship between source code files. In this paper, we propose a novel approach named Multi-View Adaptive Contrastive Learning for Information Retrieval Fault Localization (MACL-IRFL) to learn the above-mentioned relationships for software fault localization. Specifically, we first generate data augmentations from report-code interaction view, report-report similarity view and code-code co-citation view separately, and adopt graph neural network to aggregate the information of bug reports or source code files from the three views in the embedding process. Moreover, we perform contrastive learning across these views. Our design of contrastive learning task will force the bug report representations to encode information shared by report-report and report-code views, and the source code file representations shared by code-code and report-code views, thereby alleviating the noise from auxiliary information. Finally, to evaluate the performance of our approach, we conduct extensive experiments on five open-source Java projects. The results show that our model can improve over the best baseline up to 28.93{\%}, 25.57{\%} and 20.35{\%} on Accuracy@1, MAP and MRR, respectively.",
issn="1573-7535",
doi="10.1007/s10515-025-00573-x",
url="https://doi.org/10.1007/s10515-025-00573-x"
}


@inproceedings{10.1007/978-3-031-76459-2_4,
 abstract = {In recent years, code security has become increasingly important, especially with the rise of interconnected technologies. Detecting vulnerabilities early in the software development process has demonstrated numerous benefits. Consequently, the scientific community started using machine learning for automated detection of source code vulnerabilities. This work explores and refines the CVEFixes dataset, which is commonly used to train models for code-related tasks, specifically the C/C++ subset. To this purpose, the Source Code Processing Engine (SCoPE), a framework composed of strategized techniques that can be used to reduce the size and normalize C/C++ functions is presented. The output generated by SCoPE was used to create a new version of CVEFixes. This refined dataset was then employed in a feature representation analysis to assess the effectiveness of the tool's code processing techniques, consisting of fine-tuning three pre-trained LLMs for software vulnerability detection. The results show that SCoPE successfully helped to identify 905 duplicates within the evaluated subset. The LLM results corroborate with the literature regarding their suitability for software vulnerability detection, with the best model achieving 53{\%} F1-score.},
 address = {Cham},
 author = {Gon{\c{c}}alves, Jos{\'e}
and Dias, Tiago
and Maia, Eva
and Pra{\c{c}}a, Isabel},
 booktitle = {Distributed Computing and Artificial Intelligence, Special Sessions I, 21st International Conference},
 editor = {Mehmood, Rashid
and Hern{\'a}ndez, Guillermo
and Pra{\c{c}}a, Isabel
and Wikarek, Jaroslaw
and Loukanova, Roussanka
and Monteiro dos Reis, Ars{\'e}nio
and Skarmeta, Antonio
and Lombardi, Eleonora},
 isbn = {978-3-031-76459-2},
 pages = {34--43},
 publisher = {Springer Nature Switzerland},
 title = {SCoPE: Evaluating LLMs for Software Vulnerability Detection},
 url = {https://link.springer.com/chapter/10.1007/978-3-031-76459-2_4},
 year = {2025}
}

@inproceedings{10.1007/978-3-031-36272-9_79,
 abstract = {In introductory programming courses, automated repair tools (ARTs) are used to provide feedback to students struggling with debugging. Most successful ARTs take advantage of context-specific educational data to construct repairs to students' buggy codes. Recent work in student program repair using large language models (LLMs) has also started to utilize such data. An underexplored area in this field is the use of ARTs in combination with LLMs. In this paper, we propose to transfer the repairing capabilities of existing ARTs to open large language models by finetuning LLMs on ART corrections to buggy codes. We experiment with this approach using three large datasets of Python programs written by novices. Our results suggest that a finetuned LLM provides more reliable and higher-quality repairs than the repair tool used for finetuning the model. This opens venues for further deploying and using educational LLM-based repair techniques.},
 address = {Cham},
 author = {Koutcheme, Charles},
 booktitle = {Artificial Intelligence in Education},
 editor = {Wang, Ning
and Rebolledo-Mendez, Genaro
and Matsuda, Noboru
and Santos, Olga C.
and Dimitrova, Vania},
 isbn = {978-3-031-36272-9},
 pages = {830--835},
 publisher = {Springer Nature Switzerland},
 title = {Training Language Models for Programming Feedback Using Automated Repair Tools},
 url = {https://link.springer.com/chapter/10.1007/978-3-031-36272-9_79},
 year = {2023}
}

@inproceedings{10.1007/978-3-031-76934-4_7,
 abstract = {The rapid deployment of Large Language Models (LLMs) requires careful consideration of their effect on cybersecurity. Our work aims to improve the selection process of LLMs that are suitable for facilitating secure coding (SC). This raises challenging research questions, such as (RQ1) Which functionality can streamline the LLM evaluation? (RQ2) What should the evaluation measure? (RQ3) How to attest that the evaluation process is impartial? To address these questions, we introduce LLMSecCode, an open-source evaluation framework designed to assess LLM SC capabilities objectively. We validate the LLMSecCode implementation through experiments. We find a 10{\%} and 9{\%} difference in performance when varying parameters and prompts, respectively. We also compare some results to reliable external actors, where our results show a 5{\%} difference. We strive to ensure the ease of use of our open-source framework and encourage further development by external actors. With LLMSecCode, we hope to encourage the standardization and benchmarking of LLMs' capabilities in security-oriented code and tasks.},
 address = {Cham},
 author = {Ryd{\'e}n, Anton
and N{\"a}slund, Erik
and Schiller, Elad Michael
and Almgren, Magnus},
 booktitle = {Cyber Security, Cryptology, and Machine Learning},
 editor = {Dolev, Shlomi
and Elhadad, Michael
and Kuty{\l}owski, Miros{\l}aw
and Persiano, Giuseppe},
 isbn = {978-3-031-76934-4},
 pages = {100--118},
 publisher = {Springer Nature Switzerland},
 title = {LLMSecCode: Evaluating Large Language Models for Secure Coding},
 url = {https://link.springer.com/chapter/10.1007/978-3-031-76934-4_7},
 year = {2025}
}

@Article{Zheng2026,
author="Zheng, Tianming
and Meng, Fanchao
and Yi, Ping
and Wu, Yue",
title="Automating fuzz driver generation for deep learning libraries with large language models",
journal="Cybersecurity",
year="2026",
month="Jan",
day="04",
volume="9",
number="1",
pages="7",
abstract="The widespread adoption of deep learning (DL) libraries has raised concerns about their reliability and security. While prior works leveraged large language models (LLMs) to generate test programs for DL library APIs, the hardcoded program behaviors and low code validity rates render them impractical for real-world testing. To address these challenges, we propose FD-FACTORY, a fully automated framework that leverages LLMs to generate fuzz drivers for DL API testing. The fuzz driver programs accept mutated inputs from fuzzing engines to achieve effective code analysis. Inspired by the modular design of industrial production lines, FD-FACTORY decomposes the generation process into eight distinct stages: Preparation, Initial Fuzz Driver Generation, Early Stop Checks, Verification, Issue Diagnosis, Decision Making, Repair Loop, and Deployment. Each stage is handled by dedicated agents or tools to enhance construction efficiency. Experimental results demonstrate that FD-FACTORY achieves 73.67{\%} and 65.33{\%} success rates in generating fuzz drivers for PyTorch and TensorFlow, producing an improvement of 34.66 to {\$}{\$}-{\$}{\$} 54.66{\%} than existing approaches. In addition, FD-FACTORY provides more comprehensive coverage tracking by supporting both Python and native C/C++ code. It achieves a total coverage of 308,351 lines on PyTorch and 528,427 lines on TensorFlow, substantially surpassing the results reported by previous approaches. Unlike prior approaches relying on repeated interactions with the LLM servers throughout the entire testing process, our framework confines the use of LLMs strictly to the fuzz driver generation stages before deployment. Once generated, the fuzz drivers can be reused without further LLM involvement, thereby enhancing the practicality and sustainability of LLM-assisted fuzzing in real-world scenarios.",
issn="2523-3246",
doi="10.1186/s42400-025-00532-9",
url="https://doi.org/10.1186/s42400-025-00532-9"
}


@Article{Asare2023,
author="Asare, Owura
and Nagappan, Meiyappan
and Asokan, N.",
title="Is GitHub's Copilot as bad as humans at introducing vulnerabilities in code?",
journal="Empirical Software Engineering",
year="2023",
month="Sep",
day="23",
volume="28",
number="6",
pages="129",
abstract="Several advances in deep learning have been successfully applied to the software development process. Of recent interest is the use of neural language models to build tools, such as Copilot, that assist in writing code. In this paper we perform a comparative empirical analysis of Copilot-generated code from a security perspective. The aim of this study is to determine if Copilot is as bad as human developers. We investigate whether Copilot is just as likely to introduce the same software vulnerabilities as human developers. Using a dataset of C/C++ vulnerabilities, we prompt Copilot to generate suggestions in scenarios that led to the introduction of vulnerabilities by human developers. The suggestions are inspected and categorized in a 2-stage process based on whether the original vulnerability or fix is reintroduced. We find that Copilot replicates the original vulnerable code about 33{\%} of the time while replicating the fixed code at a 25{\%} rate. However this behaviour is not consistent: Copilot is more likely to introduce some types of vulnerabilities than others and is also more likely to generate vulnerable code in response to prompts that correspond to older vulnerabilities. Overall, given that in a significant number of cases it did not replicate the vulnerabilities previously introduced by human developers, we conclude that Copilot, despite performing differently across various vulnerability types, is not as bad as human developers at introducing vulnerabilities in code.",
issn="1573-7616",
doi="10.1007/s10664-023-10380-1",
url="https://doi.org/10.1007/s10664-023-10380-1"
}


@inproceedings{10.1007/978-3-031-64315-6_9,
 abstract = {The integration of ChatGPT as a supportive tool in education, notably in programming courses, addresses the unique challenges of programming education by providing assistance with debugging, code generation, and explanations. Despite existing research validating ChatGPT's effectiveness, its application in university-level programming education and a detailed understanding of student interactions and perspectives remain limited. This paper explores ChatGPT's impact on learning in a Python programming course tailored for first-year students over eight weeks. By analyzing responses from surveys, open-ended questions, and student-ChatGPT dialog data, we aim to provide a comprehensive view of ChatGPT's utility and identify both its advantages and limitations as perceived by students. Our study uncovers a generally positive reception toward ChatGPT and offers insights into its role in enhancing the programming education experience. These findings contribute to the broader discourse on AI's potential in education, suggesting paths for future research and application.},
 address = {Cham},
 author = {Ma, Boxuan
and Chen, Li
and Konomi, Shin'ichi},
 booktitle = {Artificial Intelligence in Education. Posters and Late Breaking Results, Workshops and Tutorials, Industry and Innovation Tracks, Practitioners, Doctoral Consortium and Blue Sky},
 editor = {Olney, Andrew M.
and Chounta, Irene-Angelica
and Liu, Zitao
and Santos, Olga C.
and Bittencourt, Ig Ibert},
 isbn = {978-3-031-64315-6},
 pages = {113--126},
 publisher = {Springer Nature Switzerland},
 title = {Enhancing Programming Education with ChatGPT: A Case Study on Student Perceptions and Interactions in a Python Course},
 url = {https://link.springer.com/chapter/10.1007/978-3-031-64315-6_9},
 year = {2024}
}

@inproceedings{10.1007/978-3-031-99474-6_20,
 abstract = {This paper explores the potential of Large Language Models (LLM) to optimize various stages of the software development lifecycle, including requirements elicitation, architecture design, diagram creation, and implementation. The study is grounded in a real-world case, where development time and result quality are compared with and without LLM assistance. This research underscores the possibility of applying prompt patterns in LLM to support and enhance software development activities, focusing on a B2C digital commerce platform centered on fashion retail, designated LUNA. The methodology adopted is Design Science, which follows a practical and iterative approach. Requirements, design suggestions, and code samples are analyzed before and after the application of language models. The results indicate substantial advantages in the development process, such as improved task efficiency, faster identification of requirement gaps, and enhanced code readability. Nevertheless, challenges were observed in interpreting complex business logic. Future work should explore the integration of LLM with domain-specific ontologies and business rule engines to improve contextual accuracy in code and model generation. Additionally, refining prompt engineering strategies and combining LLM with interactive development environments could further enhance code quality, traceability, and explainability.},
 address = {Cham},
 author = {Santos, Gon{\c{c}}alo
and Silveira, Clara
and Santos, Vitor
and Santos, Arnaldo
and Mamede, Henrique},
 booktitle = {New Trends in Disruptive Technologies, Tech Ethics and Artificial Intelligence},
 editor = {de la Iglesia, Daniel H.
and de Paz Santana, Juan F.
and L{\'o}pez Rivero, Alfonso J.},
 isbn = {978-3-031-99474-6},
 pages = {226--239},
 publisher = {Springer Nature Switzerland},
 title = {Applying Large Language Models to Software Development: Enhancing Requirements, Design and Code},
 url = {https://link.springer.com/chapter/10.1007/978-3-031-99474-6_20},
 year = {2025}
}

@Article{Yue2025,
author="Yue, Songhui",
title="A Multimodal conceptual framework to achieve automated software evolution for context-rich intelligent applications",
journal="Innovations in Systems and Software Engineering",
year="2025",
month="Sep",
day="01",
volume="21",
number="3",
pages="1091--1105",
abstract="While AI is extensively transforming Software Engineering (SE) fields, SE is still in need of a framework to consider overall all phases to facilitate Automated Software Evolution (ASEv), particularly for intelligent applications that are context-rich instead of conquering each division independently. Its complexity comes from the intricacy of the intelligent applications, the heterogeneity of the data sources, and the constant changes in the context. This study proposes a conceptual framework for achieving automated software evolution, emphasizing the importance of multimodality learning. A Selective Sequential Scope Model (3 S) model is developed based on the conceptual framework, and it can be used to categorize existing and future research when it covers different SE phases and multimodal learning tasks. This research is a preliminary step toward the blueprint of a higher-level ASEv. The proposed conceptual framework can act as a practical guideline for practitioners to prepare themselves for diving into this area. Although the study is about intelligent applications, the framework and analysis methods may be adapted for other types of software as AI brings more intelligence into their life cycles.",
issn="1614-5054",
doi="10.1007/s11334-024-00591-0",
url="https://doi.org/10.1007/s11334-024-00591-0"
}


@inproceedings{10.1007/978-3-032-07132-3_2,
 abstract = {This is an introduction to the track `AI Assisted Programming' (AIAP), organized at the third instance of the AISoLA conference during the period November 1--5, 2025. AISoLA as a whole aims to study opportunities and risks of late advances of AI. The motivation behind the AIAP track in particular, which also takes place the third time, is the emerging use of large language models for the construction and analysis of software artifacts. An overview of the track presentations is provided.},
 address = {Cham},
 author = {Ahrendt, Wolfgang
and Aichernig, Bernhard K.
and Havelund, Klaus},
 booktitle = {Bridging the Gap Between AI and Reality},
 editor = {Steffen, Bernhard},
 isbn = {978-3-032-07132-3},
 pages = {11--17},
 publisher = {Springer Nature Switzerland},
 title = {AI Assisted Programming (AISoLA 2025 Track Introduction)},
 url = {https://link.springer.com/chapter/10.1007/978-3-032-07132-3_2},
 year = {2026}
}

@Article{Ehsani2025,
author="Ehsani, Ramtin
and Pathak, Sakshi
and Parra, Esteban
and Haiduc, Sonia
and Chatterjee, Preetha",
title="What characteristics make ChatGPT effective for software issue resolution? An empirical study of task, project, and conversational signals in GitHub issues",
journal="Empirical Software Engineering",
year="2025",
month="Nov",
day="18",
volume="31",
number="1",
pages="22",
abstract="Conversational large-language models (LLMs), such as ChatGPT, are extensively used for issue resolution tasks, particularly for generating ideas to implement new features or resolve bugs. However, not all developer-LLM conversations are useful for effective issue resolution and it is still unknown what makes some of these conversations not helpful. In this paper, we analyze 686 developer-ChatGPT conversations shared within GitHub issue threads to identify characteristics that make these conversations effective for issue resolution. First, we empirically analyze the conversations and their corresponding issue threads to distinguish helpful from unhelpful conversations. We begin by categorizing the types of tasks developers seek help with (e.g., code generation, bug identification and fixing, test generation), to better understand the scenarios in which ChatGPT is most effective. Next, we examine a wide range of conversational, project, and issue-related metrics to uncover statistically significant factors associated with helpful conversations. Finally, we identify common deficiencies in unhelpful ChatGPT responses to highlight areas that could inform the design of more effective developer-facing tools. We found that only 62{\%} of the ChatGPT conversations were helpful for successful issue resolution. Among different tasks related to issue resolution, ChatGPT was most helpful in assisting with code generation, and tool/library/API recommendations, but struggled with generating code explanations. Our conversational metrics reveal that helpful conversations are shorter, more readable, and exhibit higher semantic and linguistic alignment. Our project metrics reveal that larger, more popular projects and experienced developers benefit more from ChatGPT's assistance. Our issue metrics indicate that ChatGPT is more effective on simpler issues characterized by limited developer activity and faster resolution times. These typically involve well-scoped technical problems such as compilation errors and tool feature requests. In contrast, it performs less effectively on complex issues that demand deep project-specific understanding, such as system-level code debugging and refactoring. The most common deficiencies in unhelpful ChatGPT responses include incorrect information and lack of comprehensiveness. Our findings have wide implications including guiding developers on effective interaction strategies for issue resolution, informing the development of tools or frameworks to support optimal prompt design, and providing insights on fine-tuning LLMs for issue resolution tasks.",
issn="1573-7616",
doi="10.1007/s10664-025-10745-8",
url="https://doi.org/10.1007/s10664-025-10745-8"
}


@inproceedings{10.1007/978-981-97-1814-6_12,
 abstract = {As software engineering educators, we are always looking for ways to engage our students and help them develop the skills they will require in their future careers. Generative artificial intelligence offers an exciting new tool with the potential to change software engineering education. As a case in point, in this paper we explore how large language models like ChatGPT can be used as interactive teaching assistants in undergraduate software engineering courses. We provide a catalog of prompt examples tailored for 12 key knowledge areas of software engineering. These prompts can be used to generate code, tests, explanations and more, enabling students to experiment and get personalized guidance. Students can brainstorm requirements, simulate APIs, get automated feedback, and much more. Using ChatGPT and similar AI systems prepares students for tools they may encounter in industry and gives them valuable experience in human-AI collaboration. While generative AI offers many benefits, it also brings challenges that must be addressed. We discuss concerns around the correctness, risks and quality of AI-generated answers. Our goal is to inspire and motivate educators to consider using generative AI to create engaging learning experiences for students. We provide resources to help educators get started, encouraging experimentation and sharing of ideas and prompts.},
 address = {Singapore},
 author = {Pereira, Juanan
and L{\'o}pez, Juan Miguel
and Azanza, Maider
and D{\'i}az, {\'O}scar
and Garmendia, Xabier},
 booktitle = {Proceedings of TEEM 2023},
 editor = {Gon{\c{c}}alves, Jos{\'e} Alexandre de Carvalho
and Lima, Jos{\'e} Lu{\'i}s Sousa de Magalh{\~a}es
and Coelho, Jo{\~a}o Paulo
and Garc{\'i}a-Pe{\~{n}}alvo, Francisco Jos{\'e}
and Garc{\'i}a-Holgado, Alicia},
 isbn = {978-981-97-1814-6},
 pages = {125--134},
 publisher = {Springer Nature Singapore},
 title = {Leveraging on Generative AI to Teach Undergraduate Software Engineering: A Collaborative Prompt Catalogue},
 url = {https://link.springer.com/chapter/10.1007/978-981-97-1814-6_12},
 year = {2024}
}

@Article{Sheikhaei2024,
author="Sheikhaei, Mohammad Sadegh
and Tian, Yuan
and Wang, Shaowei
and Xu, Bowen",
title="An empirical study on the effectiveness of large language models for SATD identification and classification",
journal="Empirical Software Engineering",
year="2024",
month="Oct",
day="01",
volume="29",
number="6",
pages="159",
abstract="Self-Admitted Technical Debt (SATD), a concept highlighting sub-optimal choices in software development documented in code comments or other project resources, poses challenges in the maintainability and evolution of software systems. Large language models (LLMs) have demonstrated significant effectiveness across a broad range of software tasks, especially in software text generation tasks. Nonetheless, their effectiveness in tasks related to SATD is still under-researched. In this paper, we investigate the efficacy of LLMs in both identification and classification of SATD. For both tasks, we investigate the performance gain from using more recent LLMs, specifically the Flan-T5 family, across different common usage settings. Our results demonstrate that for SATD identification, all fine-tuned LLMs outperform the best existing non-LLM baseline, i.e., the CNN model, with a 4.4{\%} to 7.2{\%} improvement in F1 score. In the SATD classification task, while our largest fine-tuned model, Flan-T5-XL, still led in performance, the CNN model exhibited competitive results, even surpassing four of six LLMs. We also found that the largest Flan-T5 model, i.e., Flan-T5-XXL, when used with a zero-shot in-context learning (ICL) approach that only provides instructions for SATD identification, yields competitive results with traditional approaches but performs 6.4{\%} to 9.2{\%} worse than fine-tuned LLMs. For SATD classification, few-shot ICL approach, incorporating examples and category descriptions in prompts, outperforms the zero-shot approach and even surpasses the fine-tuned smaller Flan-T5 models. Moreover, our experiments demonstrate that incorporating contextual information, such as surrounding code, into the SATD classification task enables larger fine-tuned LLMs to improve their performance. Our study highlights the capabilities and limitations of LLMs for SATD tasks and the role of contextual information in achieving higher performance with larger LLMs, setting a foundation for future efforts to enhance these models for more effective technical debt management.",
issn="1573-7616",
doi="10.1007/s10664-024-10548-3",
url="https://doi.org/10.1007/s10664-024-10548-3"
}


@inproceedings{10.1007/978-3-032-10444-1_13,
 abstract = {Students in computing education increasingly use large language models (LLMs) such as ChatGPT. Yet, the role of LLMs in supporting cognitively demanding tasks, like deductive program verification, remains poorly understood. This paper investigates how students interact with an LLM when solving formal verification exercises in Dafny, a language that supports functional correctness by allowing programmers to write formal specifications and automatically verifying that the implementation satisfies the specification. We conducted a mixed-methods study with master's students enrolled in a formal methods course. Each participant completed two verification problems, one with access to a custom ChatGPT interface that logged all interactions and the other without. We identified strategies used by successful students and assessed the level of trust students place in LLMs. Our findings show that students perform significantly better when using ChatGPT; however, performance gains are tied to prompt quality. We conclude with practical recommendations for integrating LLMs into formal methods courses more effectively, including designing LLM-aware challenges that promote learning.},
 address = {Cham},
 author = {Carreira, Carolina
and Silva, {\'A}lvaro
and Abreu, Alexandre
and Mendes, Alexandra},
 booktitle = {Software Engineering and Formal Methods},
 editor = {Bianculli, Domenico
and G{\'o}mez-Mart{\'i}nez, Elena},
 isbn = {978-3-032-10444-1},
 pages = {203--220},
 publisher = {Springer Nature Switzerland},
 title = {Can Large Language Models Help Students Prove Software Correctness? An Experimental Study with Dafny},
 url = {https://link.springer.com/chapter/10.1007/978-3-032-10444-1_13},
 year = {2026}
}

@Article{Zhang2024,
author="Zhang, Huangzhao
and Zhang, Kechi
and Li, Zhuo
and Li, Jia
and Li, Jia
and Li, Yongmin
and Zhao, Yunfei
and Zhu, Yuqi
and Liu, Fang
and Li, Ge
and Jin, Zhi",
title="Deep learning for code generation: a survey",
journal="Science China Information Sciences",
year="2024",
month="Aug",
day="20",
volume="67",
number="9",
pages="191101",
abstract="In the past decade, thanks to the powerfulness of deep-learning techniques, we have witnessed a whole new era of automated code generation. To sort out developments, we have conducted a comprehensive review of solutions to deep learning-based code generation. In this survey, we generally formalize the pipeline and procedure of code generation and categorize existing solutions according to taxonomy from perspectives of architecture, model-agnostic enhancing strategy, metrics, and tasks. In addition, we outline the challenges faced by current dominant large models and list several plausible directions for future research. We hope that this survey may provide handy guidance to understanding, utilizing, and developing deep learning-based code-generation techniques for researchers and practitioners.",
issn="1869-1919",
doi="10.1007/s11432-023-3956-3",
url="https://doi.org/10.1007/s11432-023-3956-3"
}


@Article{Song2025,
author="Song, Jiayin
and Li, Yike
and Tian, Yunzhe
and Ma, Haoxuan
and Li, Honglei
and Zuo, Jie
and Liu, Jiqiang
and Niu, Wenjia",
title="Investigating the bugs in reinforcement learning programs: Insights from Stack Overflow and GitHub",
journal="Automated Software Engineering",
year="2025",
month="Sep",
day="23",
volume="33",
number="1",
pages="9",
abstract="Reinforcement learning (RL) is increasingly applied in areas such as gaming, robotic control, and autonomous driving. Like to deep learning, RL systems also encounter failures during operation. However, RL differs from deep learning in terms of its error causes and symptom manifestations. What are the differences in error causes and symptoms between RL and deep learning? How are RL errors and their symptoms related? Understanding the symptoms and causes of RL failures can advance research on RL failure detection and repair. In this paper, we conducted a comprehensive empirical study by collecting 1,155 error reports from the popular Q{\&}A forum Stack Overflow and four GitHub repositories: baselines, stable-baselines3, tianshou and keras-rl. We analyzed the root causes and symptoms of these failures and examined the differences in resolution times across various root causes. Additionally, we analyzed the correlations between causes and symptoms. Our study yielded 14 key findings, and six implications for developing RL detection and failure repair tools. Our work is the first to integrate LLM-based analysis with manual validation for RL bug studies, providing actionable insights for tool development and testing strategies.",
issn="1573-7535",
doi="10.1007/s10515-025-00555-z",
url="https://doi.org/10.1007/s10515-025-00555-z"
}


@inproceedings{10.1007/978-3-032-12457-9_3,
 abstract = {Introductory programming education is constantly challenged with how to provide effective, personalized guidance to struggling novices. AST-based hints generation emerges as a potential solution, marrying abstract syntax tree analysis with generative AI to offer tailored, instructive feedback for Python learners. Existing hint generation systems like ITAP and GPT4Hints-GPT3.5Val have approached hint generation through path construction and generative models, respectively. Both approaches to hint generation have shown promise in generating human-like hints, but each has its own limitations. These approaches either provide highly instructive hints that are often too explicit or more abstract but may lack the specificity necessary for effective guidance. Our study combines the strengths of both approaches to provide students with hints that are both instructive and abstract but do not give away the solution. We provide a detailed overview of the AST-based hints system, including requirements gathering, the system architecture and features. The system is evaluated through path construction testing and A/B testing with speculative analysis. The results from path construction and A/B testing demonstrate that AST-Hints is moderately effective at generating human-like hints, faster than human tutors, with its success strongly related to the quality of the goal solution and hint relevance.},
 address = {Cham},
 author = {Levin, Marc
and Kandjimi, Herman
and Safla, Aslam},
 booktitle = {ICT Education},
 editor = {Nel, Liezel
and Stott, Tanya
and Calitz, Andr{\'e}},
 isbn = {978-3-032-12457-9},
 pages = {33--50},
 publisher = {Springer Nature Switzerland},
 title = {Leveraging Abstract Syntax Trees To Generate Instructive Hints In Programming},
 url = {https://link.springer.com/chapter/10.1007/978-3-032-12457-9_3},
 year = {2026}
}

@inproceedings{10.1007/978-981-97-6352-8_50,
 abstract = {As natural language processing (NLP) models develop, the question of whether they can be used to solve problems across a variety of industries becomes more important. This research paper offers a thorough examination of the possibilities of ChatGPT, a cutting-edge language model, as a flexible NLP problem solver. We research its performance across a range of tasks, evaluate its applicability to various fields, consider its moral ramifications, and look at its function in assisting human problem-solving. We list the benefits and drawbacks of ChatGPT and discuss its potential contributions to AI-powered solutions through empirical studies and practical implementations. Because ChatGPT can provide excellent responses to input from humans and automatically fix previous mistakes based on new talks, it has recently attracted a lot of interest from the natural language processing (NLP) field. We demonstrate the merits and drawbacks of the current ChatGPT version using in-depth empirical studies. We find that while ChatGPT does well on many tasks that favor reasoning skills (like arithmetic reasoning), it still has difficulties with more specialized tasks like sequence labeling.},
 address = {Singapore},
 author = {Nagender, Y.
and Kumar, S. Vijaya
and Chakravarthi, M. Kalyan
and Guha, Shouvik Kumar
and Moharekar, Tejashree Tejpal
and Sajida Bhanu, P.},
 booktitle = {International Conference on Signal, Machines, Automation, and Algorithm},
 editor = {Malik, Hasmat
and Mishra, Sukumar
and Sood, Y. R.
and Garc{\'i}a M{\'a}rquez, Fausto Pedro
and Ustun, Taha Selim},
 isbn = {978-981-97-6352-8},
 pages = {699--710},
 publisher = {Springer Nature Singapore},
 title = {A Study on Potentiality of ChatGPT as Task Solver Based on Natural Language Processing},
 url = {https://link.springer.com/chapter/10.1007/978-981-97-6352-8_50},
 year = {2024}
}

@Article{Wang2025,
author="Wang, Liping
and Lu, Guilong
and Chen, Xiang
and Dai, Xiaofeng
and Qiu, Jianlin",
title="SIFT: enhance the performance of vulnerability detection by incorporating structural knowledge and multi-task learning",
journal="Automated Software Engineering",
year="2025",
month="Apr",
day="11",
volume="32",
number="2",
pages="38",
abstract="Software vulnerabilities pose significant risks to software systems, leading to security breaches, data loss, operational disruptions, and substantial financial damage. Therefore, accurately detecting these vulnerabilities is of paramount importance. In recent years, pre-trained language models (PLMs) have demonstrated powerful capabilities in code representation and understanding, emerging as a promising method for vulnerability detection. However, integrating code structure knowledge while fine-tuning PLMs remains a significant challenge. To alleviate this limitation, we propose a novel vulnerability detection approach called SIFT. SIFT extracts the code property graph (CPG) to serve as the source of graph structural information. It constructs a code structure matrix from this information and measures the difference between the code structure matrix and the attention matrix using Sinkhorn Divergence to obtain the structural knowledge loss. This structural knowledge loss is then used alongside the cross-entropy loss for vulnerability detection in a multi-task learning framework to enhance overall detection performance. To evaluate the effectiveness of SIFT, we conducted experiments on three vulnerability detection datasets: FFmpeg+Qemu, Chrome+Debian, and Big-Vul. The results demonstrate that SIFT outperforms nine state-of-the-art vulnerability detection baselines, achieving performance improvements of 1.74{\%}, 10.19{\%}, and 2.87{\%} in terms of F1 score, respectively. Our study shows the effectiveness of incorporating structural knowledge and multi-task learning in enhancing the performance of PLMs for vulnerability detection.",
issn="1573-7535",
doi="10.1007/s10515-025-00507-7",
url="https://doi.org/10.1007/s10515-025-00507-7"
}


@Article{Quan2025,
author="Quan, Vu Le Anh
and Phat, Chau Thuan
and Van Nguyen, Kiet
and The Duy, Phan
and Pham, Van-Hau",
title="XGV-BERT: Leveraging contextualized language model and graph neural network for efficient software vulnerability detection",
journal="The Journal of Supercomputing",
year="2025",
month="Apr",
day="16",
volume="81",
number="6",
pages="750",
abstract="With the advancement of deep learning in various fields, there are many attempts to reveal software vulnerabilities by data-driven approach. Natural language processing has emerged as a powerful tool for bridging the semantic gap between programming languages and natural language. However, a significant disparity between the two still exists. In this work, we propose XGV-BERT, a framework that combines the pre-trained CodeBERT model and graph neural network to detect software vulnerabilities. By jointly training the CodeBERT and graph neural network modules within XGV-BERT, the proposed model leverages the advantages of large-scale pre-training, harnessing vast raw data, and transfer learning by learning representations for training data through graph convolution. The research results demonstrate that the XGV-BERT method significantly improves vulnerability detection accuracy compared to two existing methods such as VulDeePecker and SySeVR. For the VulDeePecker dataset, XGV-BERT achieves an impressive F1-score of 97.5{\%}, significantly outperforming VulDeePecker, which achieved an F1-score of 78.3{\%}. Again, with the SySeVR dataset, XGV-BERT achieves an F1-score of 95.5{\%}, surpassing the results of SySeVR with an F1-score of 83.5{\%}.",
issn="1573-0484",
doi="10.1007/s11227-025-07198-7",
url="https://doi.org/10.1007/s11227-025-07198-7"
}


@inproceedings{10.1007/978-981-96-9275-0_38,
 abstract = {In today's world, our day-to-day lives depend on many software programs. The outage of the services of some crucial software like banking and airline will affect our lives badly. It is the necessity of this era to locate and repair the bug in software in less time. This paper presents a novel method to locate the bug at statement-level with the help of a code graph. The project code base is converted to a graph and stored in the graph database. The sub-graph related to the bug report is pulled from the graph database by using the bug report as the query. The nodes in the sub-graph are then listed in the decreasing order of their similarity score. Each node in the graph represents the statements in the code. Hence, the proposed method ensures fine-grained bug localization with the help of a code graph. Moreover, it gives the dependencies of the buggy code, which helps in test case generation. The proposed method reduces the effort for software maintenance and increases the productivity of the maintenance team.},
 address = {Singapore},
 author = {Sunitha, E. V.},
 booktitle = {ICT for Intelligent Systems},
 editor = {Choudrie, Jyoti
and Mahalle, Parikshit N.
and Perumal, Thinagaran
and Joshi, Amit},
 isbn = {978-981-96-9275-0},
 pages = {451--460},
 publisher = {Springer Nature Singapore},
 title = {Fine-Grained Bug Localization in Software Projects Using Code Graph},
 url = {https://link.springer.com/chapter/10.1007/978-981-96-9275-0_38},
 year = {2026}
}

@Article{Chen2024,
author="Chen, Xiangping
and Hu, Xing
and Huang, Yuan
and Jiang, He
and Ji, Weixing
and Jiang, Yanjie
and Jiang, Yanyan
and Liu, Bo
and Liu, Hui
and Li, Xiaochen
and Lian, Xiaoli
and Meng, Guozhu
and Peng, Xin
and Sun, Hailong
and Shi, Lin
and Wang, Bo
and Wang, Chong
and Wang, Jiayi
and Wang, Tiantian
and Xuan, Jifeng
and Xia, Xin
and Yang, Yibiao
and Yang, Yixin
and Zhang, Li
and Zhou, Yuming
and Zhang, Lu",
title="Deep learning-based software engineering: progress, challenges, and opportunities",
journal="Science China Information Sciences",
year="2024",
month="Dec",
day="24",
volume="68",
number="1",
pages="111102",
abstract="Researchers have recently achieved significant advances in deep learning techniques, which in turn has substantially advanced other research disciplines, such as natural language processing, image processing, speech recognition, and software engineering. Various deep learning techniques have been successfully employed to facilitate software engineering tasks, including code generation, software refactoring, and fault localization. Many studies have also been presented in top conferences and journals, demonstrating the applications of deep learning techniques in resolving various software engineering tasks. However, although several surveys have provided overall pictures of the application of deep learning techniques in software engineering, they focus more on learning techniques, that is, what kind of deep learning techniques are employed and how deep models are trained or fine-tuned for software engineering tasks. We still lack surveys explaining the advances of subareas in software engineering driven by deep learning techniques, as well as challenges and opportunities in each subarea. To this end, in this study, we present the first task-oriented survey on deep learning-based software engineering. It covers twelve major software engineering subareas significantly impacted by deep learning techniques. Such subareas spread out through the whole lifecycle of software development and maintenance, including requirements engineering, software development, testing, maintenance, and developer collaboration. As we believe that deep learning may provide an opportunity to revolutionize the whole discipline of software engineering, providing one survey covering as many subareas as possible in software engineering can help future research push forward the frontier of deep learning-based software engineering more systematically. For each of the selected subareas, we highlight the major advances achieved by applying deep learning techniques with pointers to the available datasets in such a subarea. We also discuss the challenges and opportunities concerning each of the surveyed software engineering subareas.",
issn="1869-1919",
doi="10.1007/s11432-023-4127-5",
url="https://doi.org/10.1007/s11432-023-4127-5"
}


@inproceedings{10.1007/978-3-031-70879-4_15,
 abstract = {Security-critical software comes with numerous side-channel leakages left unpatched due to a lack of resources or experts. The situation will only worsen as the pace of code development accelerates, with developers relying on Large Language Models (LLMs) to automatically generate code. Compiler-based approaches are limited to only certain types of leakages and languages, and there is no automated method to solve the issue in the source code. In this work, we explore the use of LLMs in generating patches for vulnerable code with microarchitectural side-channel leakages in the source code. Automatic patching with LLMs in the source code provides portability to interpreted languages as well, eases the maintenance burden on the developers, and provides flexibility for different types of leakages.},
 address = {Cham},
 author = {Tol, M. Caner
and Sunar, Berk},
 booktitle = {Computer Security -- ESORICS 2024},
 editor = {Garcia-Alfaro, Joaquin
and Kozik, Rafa{\l}
and Chora{\'{s}}, Micha{\l}
and Katsikas, Sokratis},
 isbn = {978-3-031-70879-4},
 pages = {290--310},
 publisher = {Springer Nature Switzerland},
 title = {ZeroLeak: Automated Side-Channel Patching in Source Code Using LLMs},
 url = {https://link.springer.com/chapter/10.1007/978-3-031-70879-4_15},
 year = {2024}
}

@inproceedings{10.1007/978-981-99-7587-7_13,
 abstract = {Recent development of large language models (LLMs), such as ChatGPT has been widely applied to a wide range of software engineering tasks. Many papers have reported their analysis on the potential advantages and limitations of ChatGPT for writing code, summarization, text generation, etc. However, the analysis of the current state of ChatGPT for log processing has received little attention. Logs generated by large-scale software systems are complex and hard to understand. Despite their complexity, they provide crucial information for subject matter experts to understand the system status and diagnose problems of the systems. In this paper, we investigate the current capabilities of ChatGPT to perform several interesting tasks on log data, while also trying to identify its main shortcomings. Our findings show that the performance of the current version of ChatGPT for log processing is limited, with a lack of consistency in responses and scalability issues. We also outline our views on how we perceive the role of LLMs in the log processing discipline and possible next steps to improve the current capabilities of ChatGPT and the future LLMs in this area. We believe our work can contribute to future academic research to address the identified issues.},
 address = {Singapore},
 author = {Mudgal, Priyanka
and Wouhaybi, Rita},
 booktitle = {AI-generated Content},
 editor = {Zhao, Feng
and Miao, Duoqian},
 isbn = {978-981-99-7587-7},
 pages = {148--169},
 publisher = {Springer Nature Singapore},
 title = {An Assessment of ChatGPT on Log Data},
 url = {https://link.springer.com/chapter/10.1007/978-981-99-7587-7_13},
 year = {2024}
}

@Article{Zuo2024,
author="Zuo, Fei
and Rhee, Junghwan",
title="Vulnerability discovery based on source code patch commit mining: a systematic literature review",
journal="International Journal of Information Security",
year="2024",
month="Apr",
day="01",
volume="23",
number="2",
pages="1513--1526",
abstract="In recent years, there has been a remarkable surge in the adoption of open-source software (OSS). However, with the growing usage of OSS components in both free and proprietary software, vulnerabilities that are present within them can be spread to a vast array of underlying applications. Even worse, a myriad of vulnerabilities are fixed secretly via patch commits, which causes other software re-using the vulnerable code snippets to be left in the dark. Thus, source code patch commit mining toward vulnerability discovery is receiving immense attention, and a variety of approaches are proposed. Despite that, there is no comprehensive survey summarizing and discussing the current progress within this field. To fill this gap, we survey, evaluate, and systematize a list of literature and provide the community with our insights on both successes and remaining issues in this space. Special attention is paid on the work toward vulnerability discovery. In this paper, we also provide an introductory panorama with our replicable hands-on experience, which can help readers quickly understand and step into the pertinent field. Our empirical study reveals noteworthy challenges which need to be highlighted and addressed in this field. We also discuss potential directions for the future work. To the best of knowledge, we provide the first literature review to study source code patch commit mining in the vulnerability discovery context. The systematic framework, hands-on practices, and list of potential challenges provide new knowledge for mining source code patch commit toward a more robust software eco-system. The research gaps found in this literature review show the need for future research, such as the concern on data quality, high false alarms, and the significance of textual information.",
issn="1615-5270",
doi="10.1007/s10207-023-00795-8",
url="https://doi.org/10.1007/s10207-023-00795-8"
}


@inproceedings{10.1007/978-3-031-10542-5_6,
 abstract = {The application of Artificial Intelligence (AI) in the Software Engineering (SE) field is always a bit delayed compared to state-of-the-art research results. While the Generative Pre-trained Transformer (GPT-2) model was published in 2018, only a few recent works used it for SE tasks. One of such tasks is Automated Program Repair (APR), where the applied technique should find a fix to software bugs without human intervention. One problem emerges here: the creation of proper training data is resource-intensive and requires several hours of additional work from researchers. The sole reason for it is that training a model to repair programs automatically requires both the buggy program and the fixed one on large scale and presumably in an already pre-processed form. There are currently few such databases, so teaching and fine-tuning models is not an easy task. In this work, we wanted to investigate how the GPT-2 model performs when it is not fine-tuned for the APR task, compared to when it is fine-tuned. From previous work, we already know that the GPT-2 model can automatically generate patches for buggy programs, although the literature lacks studies where no fine-tuning has taken place. For the sake of the experiment we evaluated the GPT-2 model out-of-the-box and also fine-tuned it before the evaluation on 1559 JavaSript code snippets. Based on our results we can conclude that although the fine-tuned model was able to learn how to write syntactically correct source code almost on every attempt, the non-fine-tuned model lacked some of these positive features.},
 address = {Cham},
 author = {Lajk{\'o}, M{\'a}rk
and Horv{\'a}th, D{\'a}niel
and Csuvik, Viktor
and Vid{\'a}cs, L{\'a}szl{\'o}},
 booktitle = {Computational Science and Its Applications -- ICCSA 2022 Workshops},
 editor = {Gervasi, Osvaldo
and Murgante, Beniamino
and Misra, Sanjay
and Rocha, Ana Maria A. C.
and Garau, Chiara},
 isbn = {978-3-031-10542-5},
 pages = {79--91},
 publisher = {Springer International Publishing},
 title = {Fine-Tuning GPT-2 to Patch Programs, Is It Worth It?},
 url = {https://link.springer.com/chapter/10.1007/978-3-031-10542-5_6},
 year = {2022}
}

@Article{Karmakar2023,
author="Karmakar, Anjan
and Allamanis, Miltiadis
and Robbes, Romain",
title="JEMMA: An extensible Java dataset for ML4Code applications",
journal="Empirical Software Engineering",
year="2023",
month="Mar",
day="10",
volume="28",
number="2",
pages="54",
abstract="Machine Learning for Source Code (ML4Code) is an active research field in which extensive experimentation is needed to discover how to best use source code's richly structured information. With this in mind, we introduce JEMMA: An Extensible Java Dataset for ML4Code Applications, which is a large-scale, diverse, and high-quality dataset targeted at ML4Code. Our goal with JEMMA is to lower the barrier to entry in ML4Code by providing the building blocks to experiment with source code models and tasks. JEMMA comes with a considerable amount of pre-processed information such as metadata, representations (e.g., code tokens, ASTs, graphs), and several properties (e.g., metrics, static analysis results) for 50,000 Java projects from the 50K-C dataset, with over 1.2 million classes and over 8 million methods. JEMMA is also extensible allowing users to add new properties and representations to the dataset, and evaluate tasks on them. Thus, JEMMA becomes a workbench that researchers can use to experiment with novel representations and tasks operating on source code. To demonstrate the utility of the dataset, we also report results from two empirical studies on our data, ultimately showing that significant work lies ahead in the design of context-aware source code models that can reason over a broader network of source code entities in a software project---the very task that JEMMA is designed to help with.",
issn="1573-7616",
doi="10.1007/s10664-022-10275-7",
url="https://doi.org/10.1007/s10664-022-10275-7"
}


@inproceedings{10.1007/978-3-032-04403-7_10,
 abstract = {AI-powered software tools are widely used to assist software engineers. However, there is still a need to understand the productivity benefits of such tools for software engineers. In addition to short-term benefits, there is a question of how adopting AI-generated solutions affects the quality of software over time (e.g., maintainability).},
 address = {Cham},
 author = {Amasanti, Giorgio
and Jahi{\'{c}}, Jasmin},
 booktitle = {Software Architecture. ECSA 2025 Tracks and Workshops},
 editor = {Bianculli, Domenico
and Sartaj, Hassan
and Andrikopoulos, Vasilios
and Pautasso, Cesare
and Mikkonen, Tommi
and Perez, Jennifer
and Bure{\v{s}}, Tom{\'a}{\v{s}}
and De Sanctis, Martina
and Muccini, Henry
and Navarro, Elena
and Soliman, Mohamed
and Zdun, Uwe},
 isbn = {978-3-032-04403-7},
 pages = {89--104},
 publisher = {Springer Nature Switzerland},
 title = {The Impact of AI-Generated Solutions on Software Architecture and Productivity: Results from a Survey Study},
 url = {https://link.springer.com/chapter/10.1007/978-3-032-04403-7_10},
 year = {2026}
}

@inproceedings{10.1007/978-3-032-00630-1_5,
 abstract = {Software vulnerabilities pose significant security threats, requiring effective mitigation. While Automated Program Repair (APR) has advanced in fixing general bugs, vulnerability patching---a security-critical aspect of APR---remains underexplored. This study investigates pre-trained language models, CodeBERT and CodeT5, for automated vulnerability patching across six datasets and four languages. We evaluate their accuracy and generalization to unknown vulnerabilities. Results show that while both models face challenges with fragmented or sparse context, CodeBERT performs comparatively better in such scenarios, whereas CodeT5 excels in capturing complex vulnerability patterns. CodeT5 also demonstrates superior scalability. Furthermore, we test fine-tuned models on both in-distribution (trained) and out-of-distribution (unseen) datasets. While fine-tuning improves in-distribution performance, models struggle to generalize to unseen data, highlighting challenges in robust vulnerability detection. This study benchmarks model performance, identifies limitations in generalization, and provides actionable insights to advance automated vulnerability patching for real-world security applications.},
 address = {Cham},
 author = {Khan, Zanis Ali
and Garg, Aayush
and Tang, Qiang},
 booktitle = {Availability, Reliability and Security},
 editor = {Coppens, Bart
and Volckaert, Bruno
and Naessens, Vincent
and De Sutter, Bjorn},
 isbn = {978-3-032-00630-1},
 pages = {73--87},
 publisher = {Springer Nature Switzerland},
 title = {A Multi-dataset Evaluation of Models for Automated Vulnerability Repair},
 url = {https://link.springer.com/chapter/10.1007/978-3-032-00630-1_5},
 year = {2025}
}

@inproceedings{10.1007/978-3-031-89063-5_56,
 abstract = {Modern software systems undergo frequent updates, continuously evolving with new versions and variants to offer new features, improve functionality, and expand usability. Given the rapid pace of software evolution, organizations require effective tools and methods to mitigate the challenges associated with these changes, also called deltas. To address these challenges, the international SmartDelta Project joined industry and academia to develop and test solutions for incremental development and quality assurance. This paper provides insights into the SmartDelta project achievements and highlights one main contribution: the SmartDelta Methodology, a domain-unspecific concept for delta management in incremental software engineering. This methodology enables companies to identify gaps in their continuous engineering environment across six stages and helps to discover new tools in various technical areas. Additionally, the paper presents seven selected tools at different stages of the methodology.},
 address = {Cham},
 author = {Dornauer, Benedikt
and Felderer, Michael
and Saadatmand, Mehrdad
and Abbas, Muhammad
and Bonnotte, Nicolas
and Dreschinski, Andreas
and Enoiu, Eduard Paul
and T{\"u}z{\"u}n, Eray
and U{\c{c}}ar, Baykal Mehmet
and Devran, {\"O}mercan
and Gr{\"o}pler, Robin},
 booktitle = {The 22nd International Conference on Information Technology-New Generations (ITNG 2025)},
 editor = {Latifi, Shahram},
 isbn = {978-3-031-89063-5},
 pages = {648--659},
 publisher = {Springer Nature Switzerland},
 title = {SmartDelta Methodology: Automated Quality Assurance and Optimization for Incremental System Engineering},
 url = {https://link.springer.com/chapter/10.1007/978-3-031-89063-5_56},
 year = {2025}
}

@Article{Romera-Paredes2024,
author="Romera-Paredes, Bernardino
and Barekatain, Mohammadamin
and Novikov, Alexander
and Balog, Matej
and Kumar, M. Pawan
and Dupont, Emilien
and Ruiz, Francisco J. R.
and Ellenberg, Jordan S.
and Wang, Pengming
and Fawzi, Omar
and Kohli, Pushmeet
and Fawzi, Alhussein",
title="Mathematical discoveries from program search with large language models",
journal="Nature",
year="2024",
month="Jan",
day="01",
volume="625",
number="7995",
pages="468--475",
abstract="Large language models (LLMs) have demonstrated tremendous capabilities in solving complex tasks, from quantitative reasoning to understanding natural language. However, LLMs sometimes suffer from confabulations (or hallucinations), which can result in them making plausible but incorrect statements1,2. This hinders the use of current large models in scientific discovery. Here we introduce FunSearch (short for searching in the function space), an evolutionary procedure based on pairing a pretrained LLM with a systematic evaluator. We demonstrate the effectiveness of this approach to surpass the best-known results in important problems, pushing the boundary of existing LLM-based approaches3. Applying FunSearch to a central problem in extremal combinatorics---the cap set problem---we discover new constructions of large cap sets going beyond the best-known ones, both in finite dimensional and asymptotic cases. This shows that it is possible to make discoveries for established open problems using LLMs. We showcase the generality of FunSearch by applying it to an algorithmic problem, online bin packing, finding new heuristics that improve on widely used baselines. In contrast to most computer search approaches, FunSearch searches for programs that describe how to solve a problem, rather than what the solution is. Beyond being an effective and scalable strategy, discovered programs tend to be more interpretable than raw solutions, enabling feedback loops between domain experts and FunSearch, and the deployment of such programs in real-world applications.",
issn="1476-4687",
doi="10.1038/s41586-023-06924-6",
url="https://doi.org/10.1038/s41586-023-06924-6"
}


@inproceedings{10.1007/978-981-95-5719-6_5,
 abstract = {Programming skills are an essential competency in today's digital age. As online judgement systems have gained popularity, students are contributing vast amounts of behavioral data through programming exercises. Traditional knowledge tracing can be employed to predict students' performance in programming. Nevertheless, these methods frequently fall short by neglecting the role of code faults in modeling programming learning state. Furthermore, they fail to capture the dynamic and semantic characteristics of source code due to reliance on general-purpose code representation techniques. In this study, we propose a novel method called Fault-Aware Code Knowledge Tracing (FACKT), which integrates a dual-LSTM architecture with attention mechanisms and memory decay theory to track the evolution of both fault patterns and knowledge states of students. Moreover, by incorporating test case analysis with Large Language Models (LLMs), FACKT autonomously generates fault types, thereby enriching the code representation. Experimental results indicate that FACKT achieves an 8.3{\%} improvement in AUC and a 3.5{\%} increase in ACC compared to baseline methods. Ablation studies further demonstrate the effectiveness of modeling the fault evolution process. Additionally, we publicly release FACKT{\_}2024, a dataset comprising 74,269 code submissions from university students.},
 address = {Singapore},
 author = {Zhou, Mengshuang
and Zhou, Qing
and Xia, Wenyuan
and Jiang, Ying
and Lin, Cifa
and Zhao, Subo},
 booktitle = {Web and Big Data},
 editor = {Li, Jiajia
and Chbeir, Richard
and Li, Lei
and Zong, Chuanyu
and Zhang, Yanfeng
and Zhang, Mengxuan},
 isbn = {978-981-95-5719-6},
 pages = {67--80},
 publisher = {Springer Nature Singapore},
 title = {FACKT: A Fault-Aware Model for Code Knowledge Tracing},
 url = {https://link.springer.com/chapter/10.1007/978-981-95-5719-6_5},
 year = {2026}
}

@Article{Sharma2025,
author="Sharma, Digvijay Narayan
and Yadav, Dilip Kumar",
title="ROS-XGB: a machine learning model for software defect prediction",
journal="International Journal of System Assurance Engineering and Management",
year="2025",
month="Aug",
day="18",
abstract="The software defect prediction (SDP) research aims to predict defects early in the software lifecycle. It permits stakeholders to improve software quality, functionality, scalability, reliability, and information security aspects for the targeted software. With the digitalisation of enterprises and processes, its extent has grown since enterprises look for reliable, high-quality software applications. Since most of the Software defect identification is done manually during the development and testing phase, SDP has been an area of research in software engineering. Researchers have been trying SDP modelling using Machine Learning (ML) and Deep Learning (DL) techniques from classified feature metrics and semantic information that can be extracted from the Abstract Syntax Tree (AST) and the software's source code. Both ML and DL based models suffer from the class imbalance problem, and predictions are biased towards the majority class. This study conducts an empirical comparative analysis of 7 benchmark machine learning (ML) classification models trained on the PROMISE code metrics repository for both balanced and unbalanced classified code metrics and proposes ROS-XGB (Random Oversampling based XGBoost) as the best model for SDP. The experiment indicates that the ROS-XGB-based SDP model outperformed the recent models on evaluation metrics like accuracy, precision, recall, and F1 score. By using the ROS-XGB framework, organisations can optimise human resource allocation, maintain project schedules, and ensure the successful delivery of quality software.",
issn="0976-4348",
doi="10.1007/s13198-025-02906-1",
url="https://doi.org/10.1007/s13198-025-02906-1"
}


@inproceedings{10.1007/978-981-97-4522-7_30,
 abstract = {As vulnerability detection tools evolve, more and more vulnerabilities are found. Due to the high labor cost required to fix vulnerabilities manually, the AI-based approaches are introduced. Recently, Neural Machine Translation (NMT) has been introduced for application in the field of vulnerability repairing. However, However, most of the existing NMT approaches focus on bug fixing tasks in Automated Programmed Repair (APR) tasks and lack specialization in vulnerability fixing tasks. In this paper, we present PAVR, an approach based on deep learning models combined with vulnerability-related pre-training tasks and attention enhancement. We propose three customized pre-training tasks to enable our deep learning model to acquire code generation capabilities related to vulnerability repair during the pre-training process. At the same time, we impose certain restrictions on the output based on the Decoder part, which makes the model's attention to vulnerability types enhanced. We demonstrate that our pre-training tasks and augmented attention mechanism are effective, and we implement PAVR.},
 address = {Singapore},
 author = {Peng, Kaifeng
and Fu, Yulong
and Yang, Jincheng
and Yi, Wei
and Cao, Jin
and Li, Hui},
 booktitle = {Network Simulation and Evaluation},
 editor = {Gu, Zhaoquan
and Zhou, Wanlei
and Zhang, Jiawei
and Xu, Guandong
and Jia, Yan},
 isbn = {978-981-97-4522-7},
 pages = {427--441},
 publisher = {Springer Nature Singapore},
 title = {PAVR: A Pre-Training Approach with Self-attention for Vulnerability Repair},
 url = {https://link.springer.com/chapter/10.1007/978-981-97-4522-7_30},
 year = {2024}
}

@inproceedings{10.1007/978-981-95-3182-0_3,
 abstract = {Database Management Systems (DBMSs) serve as critical infrastructure software, underpinning data storage, management, and access control across a wide range of applications. In practice, DBMS development often follows a fork-based model, where the vendors build upon native systems to implement customized features. However, native DBMSs tend to contain silent vulnerability fixes - fixes applied to vulnerabilities without public disclosure - which lead to vulnerability information leakage and leave downstream DBMSs exposed to unfixed vulnerabilities. This work presents the first systematic study of silent vulnerability fixes in DBMSs and their security implications under fork-based development. Firstly, we introduce NightHawk, a detection framework that analyzes inheritance relationships and identifies silent fixes. NightHawk demonstrates superior performance in silent vulnerability fix detection, with higher precision and more than a 10{\%} improvement in recall compared to existing approaches. Using NightHawk, we analyze 16 DBMSs, over 60 latest releases, and more than 9,000 commits. As a result, we identify 1,951 silent vulnerability fixes, the inheritance of which by downstream DBMSs poses significant security risks. We believe the findings offer new insights into DBMS security and reveal overlooked threats in software supply chains.},
 address = {Singapore},
 author = {Dong, Jialiang
and Ni, Zihan
and Susilo, Willy
and Ma, Siqi},
 booktitle = {Data Security and Privacy Protection},
 editor = {Chen, Xiaofeng
and Hu, Haibo
and Wang, Ding},
 isbn = {978-981-95-3182-0},
 pages = {38--56},
 publisher = {Springer Nature Singapore},
 title = {Ghosts in DBMS: Revealing the Security Impacts of Silent Fixes},
 url = {https://link.springer.com/chapter/10.1007/978-981-95-3182-0_3},
 year = {2026}
}

@inproceedings{10.1007/978-981-97-1326-4_7,
 abstract = {In an era where there's a lot of information and a big demand for things to be done automatically, combining the power of understanding human language with computer programming has become really important. This research paper introduces an achievement in this domain. The software which we have developed can convert natural language problem statements into their equivalent Python code, hence making it easier to write code for a normal human. The heart of our software lies in the transformer model which is trained on an extensive corpus of diverse Python codes. This corpus encompasses a wide spectrum of programming concepts and syntactic structures, enabling our model to discern intricate patterns and nuances in the language of code. This idea is really important for things like quickly testing ideas, making software, and teaching. It helps connect regular language with computer language, making it easier for people and machines to work together. This could change the way we use computers in a big way. We have also done an analysis regarding the related works in this domain and shared our findings in this paper. Our methodology includes how we processed the data and the steps taken to build a fully functional software prototype. This section also offers the architecture used to build this software. A brief comparison between the existing solutions has also been done. In conclusion, this research represents a milestone in the pursuit of human--computer interaction. Our model has the potential to revolutionize the way programs are written.},
 address = {Singapore},
 author = {Pavitha, N.
and Patrawala, Alimurtuza
and Kulkarni, Tejas
and Talati, Vidit
and Dahiya, Shubham},
 booktitle = {Smart Trends in Computing and Communications},
 editor = {Senjyu, Tomonobu
and So--In, Chakchai
and Joshi, Amit},
 isbn = {978-981-97-1326-4},
 pages = {73--83},
 publisher = {Springer Nature Singapore},
 title = {NL2Code: Harnessing Transformers for Automatic Code Generation from Natural Language Descriptions},
 url = {https://link.springer.com/chapter/10.1007/978-981-97-1326-4_7},
 year = {2024}
}

@Article{Pei2025,
author="Pei, Wenlong
and Huang, Yilin
and Chen, Xiang
and Lu, Guilong
and Liu, Yong
and Ni, Chao",
title="Semi-supervised software vulnerability assessment via code lexical and structural information fusion",
journal="Automated Software Engineering",
year="2025",
month="Jun",
day="03",
volume="32",
number="2",
pages="57",
abstract="In recent years, data-driven approaches have become popular for software vulnerability assessment (SVA). However, these approaches need a large amount of labeled SVA data to construct effective SVA models. This process demands security expertise for accurate labeling, incurring significant costs and introducing potential errors. Therefore, collecting the training datasets for SVA can be a challenging task. To effectively alleviate the SVA data labeling cost, we propose an approach SURF, which makes full use of a limited amount of labeled SVA data combined with a large amount of unlabeled SVA data to train the SVA model via semi-supervised learning. Furthermore, SURF incorporates lexical information (i.e., treat the code as plain text) and structural information (i.e., treat the code as the code property graph) as bimodal inputs for the SVA model training, which can further improve the performance of SURF. Through extensive experiments, we evaluated the effectiveness of SURF on a dataset that contains C/C++ vulnerable functions from real-world software projects. The results show that only by labeling 30% of the SVA data, SURF can reach or even exceed the performance of state-of-the-art SVA baselines (such as DeepCVA and Func), even if these supervised baselines use 100% of the labeled SVA data. Furthermore, SURF can also exceed the performance of the state-of-the-art Positive-unlabeled learning baseline PILOT when both are trained on 30% of the labeled SVA data.",
issn="1573-7535",
doi="10.1007/s10515-025-00526-4",
url="https://doi.org/10.1007/s10515-025-00526-4"
}


@Article{Haque2025,
author="Haque, Radowanul
and Ali, Aftab
and McClean, Sally
and Khan, Naveed",
title="A zero-shot framework for cross-project vulnerability detection in source code",
journal="Empirical Software Engineering",
year="2025",
month="Oct",
day="29",
volume="31",
number="1",
pages="3",
abstract="The growing prevalence of software vulnerabilities has increased the need for effective detection methods, particularly in cross-project settings where domain differences create significant challenges. Existing vulnerability detection models often struggle to generalise across projects due to variations in coding styles, feature distributions, and the absence of labelled target data. This paper presents ZSVulD, a zero-shot, cross-project vulnerability detection framework designed to operate without target-domain labels. ZSVulD uses domain-agnostic CodeBERT embeddings to capture both syntactic and semantic features of source code, enabling knowledge transfer between projects. The framework applies an iterative pseudo-labelling process in which a neural network and XGBoost classifier collaboratively refine predictions for the target domain. Feature alignment is incorporated as a diagnostic technique to assess and visualise distributional differences between source and target datasets. Experiments on the Devign and REVEAL datasets show that ZSVulD achieves higher recall, F1, and F2 scores compared to existing methods, with an emphasis on reducing false negatives. These findings indicate that ZSVulD can support automated vulnerability detection pipelines, contributing to more reliable security assessments across different software projects.",
issn="1573-7616",
doi="10.1007/s10664-025-10749-4",
url="https://doi.org/10.1007/s10664-025-10749-4"
}


@Article{Taromirad2025,
author="Taromirad, Masoumeh
and Runeson, Per",
title="Assertions in software testing: survey, landscape, and trends",
journal="International Journal on Software Tools for Technology Transfer",
year="2025",
month="Feb",
day="01",
volume="27",
number="1",
pages="117--135",
abstract="Assertions are one of the most useful automated techniques for checking program's behaviour and hence have been used for different verification and validation tasks. We provide an overview of the last two decades of research involving ``assertions'' in software testing. Based on a term-based search, we filtered the inclusion of relevant papers and synthesised them with respect to the problem addressed, the solution designed, and the evaluation conducted. The survey rendered 145 papers on assertions in software testing. After test oracle, the dominant problem is test generation, followed by engineering aspects of assertions. Solutions are typically embedded in tool prototypes and evaluated throughout a limited number of cases, whereas using large-scale industrial settings is still a noticeable method. We conclude that assertions would be worth more attention in future research, particularly regarding the new and emerging demands (e.g., verification of programs with uncertainty), for effective, applicable, and domain-specific solutions.",
issn="1433-2787",
doi="10.1007/s10009-025-00794-1",
url="https://doi.org/10.1007/s10009-025-00794-1"
}


@Article{Buchberger2023,
author="Buchberger, Bruno",
title="Automated programming, symbolic computation, machine learning: my personal view",
journal="Annals of Mathematics and Artificial Intelligence",
year="2023",
month="Oct",
day="01",
volume="91",
number="5",
pages="569--589",
abstract="In this note, I present my personal view on the interaction of the three areas Automated Programming, Symbolic Computation, and Machine Learning. Programming is the activity of finding a (hopefully) correct program (algorithm) for a given problem. Programming is central to automation in all areas and is considered one of the most creative human activities. However, already very early in the history of programming, people started to ``jump to the meta-level'' of programming, i.e., started to develop procedures that automate, or semi-automate, (various aspects or parts of) the process of programming. This area has various names like ``Automated Programming'', ``Automated Algorithm Synthesis'', etc. Developing compilers can be considered an early example of a problem in automated programming. Automated reasoners for proving the correctness of programs with respect to a specification is an advanced example of a topic in automated programming. ChatGPT producing (amazingly good) programs from problem specifications in natural language is a recent example of automated programming. Programming tends to become the most important activity as the level of technological sophistication increases. Therefore, automating programming is maybe the most exciting and relevant technological endeavor today. It also will have enormous impact on the global job market in the software industry. Roughly, I see two main approaches to automated programming:symbolic computationand machine learning.In this note, I explain how the two approaches work and that they are fundamentally different because they address two completely different ways of how problems are specified. Together, the two approaches constitute (part of) what some people like to call ``artificial intelligence''. In my analysis, both approaches are just part of (algorithmic) mathematics. The approaches, like all non-trivial mathematical methods, need quite some intelligence on the side of the human inventors of the methods. However, applying the methods is just ``machine execution'' of algorithms. It is misleading to call the application ``machine intelligence'' or ``artificial intelligence''. The analysis of the two approaches to automated programming also suggests that the two approaches, in the future, should be combined to achieve even higher levels of sophistication. At the end of this note, I propose some research questions for this new direction.",
issn="1573-7470",
doi="10.1007/s10472-023-09894-7",
url="https://doi.org/10.1007/s10472-023-09894-7"
}


@Article{Yang2024,
author="Yang, Xueqi
and Jakubowski, Mariusz
and Kang, Li
and Yu, Haojie
and Menzies, Tim",
title="SparseCoder: Advancing source code analysis with sparse attention and learned token pruning",
journal="Empirical Software Engineering",
year="2024",
month="Dec",
day="10",
volume="30",
number="1",
pages="38",
abstract="As software projects rapidly evolve, software artifacts become more complex and defects behind them get harder to identify. The emerging Transformer-based approaches, though achieving remarkable performance, struggle with long code sequences due to their self-attention mechanism, which scales quadratically with the sequence length. This paper introduces SparseCoder, an innovative approach incorporating sparse attention and learned token pruning (LTP) method (adapted from natural language processing) to address this limitation. Compared to previous state-of-the-art models (CodeBERT, RoBERTa and CodeT5), our experiments demonstrate that SparseCoder can handle significantly longer input sequences -- at least twice as long, within the limits of our hardware resources and data statistics. Additionally, SparseCoder is four times faster than other methods measured in runtime, achieving a 50{\%} reduction in floating point operations per second (FLOPs) with a negligible performance drop of less than 1{\%} compared to Transformers using sparse attention (Sparse Atten). Plotting FLOPs of model inference against token lengths reveals that SparseCoder scales linearly, whereas other methods, including the current state-of-the-art model CodeT5, scale quadratically. Moreover, SparseCoder enhances interpretability by visualizing non-trivial tokens layer-wise.",
issn="1573-7616",
doi="10.1007/s10664-024-10558-1",
url="https://doi.org/10.1007/s10664-024-10558-1"
}


@Article{Hao2024,
author="Hao, Huizi
and Hasan, Kazi Amit
and Qin, Hong
and Macedo, Marcos
and Tian, Yuan
and Ding, Steven H. H.
and Hassan, Ahmed E.",
title="An empirical study on developers' shared conversations with ChatGPT in GitHub pull requests and issues",
journal="Empirical Software Engineering",
year="2024",
month="Sep",
day="16",
volume="29",
number="6",
pages="150",
abstract="ChatGPT has significantly impacted software development practices, providing substantial assistance to developers in various tasks, including coding, testing, and debugging. Despite its widespread adoption, the impact of ChatGPT as an assistant in collaborative coding remains largely unexplored. In this paper, we analyze a dataset of 210 and 370 developers' shared conversations with ChatGPT in GitHub pull requests (PRs) and issues. We manually examined the content of the conversations and characterized the dynamics of the sharing behavior, i.e., understanding the rationale behind the sharing, identifying the locations where the conversations were shared, and determining the roles of the developers who shared them. Our main observations are: (1) Developers seek ChatGPT's assistance across 16 types of software engineering inquiries. In both conversations shared in PRs and issues, the most frequently encountered inquiry categories include code generation, conceptual questions, how-to guides, issue resolution, and code review. (2) Developers frequently engage with ChatGPT via multi-turn conversations where each prompt can fulfill various roles, such as unveiling initial or new tasks, iterative follow-up, and prompt refinement. Multi-turn conversations account for 33.2{\%} of the conversations shared in PRs and 36.9{\%} in issues. (3) In collaborative coding, developers leverage shared conversations with ChatGPT to facilitate their role-specific contributions, whether as authors of PRs or issues, code reviewers, or collaborators on issues. Our work serves as the first step towards understanding the dynamics between developers and ChatGPT in collaborative software development and opens up new directions for future research on the topic.",
issn="1573-7616",
doi="10.1007/s10664-024-10540-x",
url="https://doi.org/10.1007/s10664-024-10540-x"
}


@inproceedings{10.1007/978-3-031-50385-6_7,
 abstract = {Edge-cloud system aims to reduce the processing time of Big data by bringing massive infrastructures closer to the source of data. Infrastructure as Code (IaC) supports the automatic deployment and management of these infrastructures through reusable code, and Ansible is the most popular IaC tool. As the quality of Ansible script directly influences the quality of Edge-cloud system, many researchers have studied improving the quality of Ansible scripts. However, there has yet to be an attempt to leverage the power of ChatGPT. Thus, we study to explore the feasibility of ChatGPT to improve the quality of Ansible scripts. Three raters evaluate ChatGPT's code recommendation ability on 48 code revision cases from 25 Ansible project GitHub repositories, and we analyze the rating results. As a result, we can confirm that ChatGPT can recognize and understand Ansible script. However, its ability largely depends on how to user formulates the questions. Thus, we can confirm the need for prompt engineering for ChatGPT to acquire stable code recommendation results.},
 address = {Cham},
 author = {Kwon, Sunjae
and Lee, Sungu
and Kim, Taehyoun
and Ryu, Duksan
and Baik, Jongmoon},
 booktitle = {Current Trends in Web Engineering},
 editor = {Casteleyn, Sven
and Mikkonen, Tommi
and Garc{\'i}a Sim{\'o}n, Alberto
and Ko, In-Young
and Loseto, Giuseppe},
 isbn = {978-3-031-50385-6},
 pages = {75--83},
 publisher = {Springer Nature Switzerland},
 title = {Exploring the Feasibility of ChatGPT for Improving the Quality of Ansible Scripts in Edge-Cloud Infrastructures Through Code Recommendation},
 url = {https://link.springer.com/chapter/10.1007/978-3-031-50385-6_7},
 year = {2024}
}

@Article{Kalyani2025,
author="Kalyani, P.
and Rao, C. Prakasa
and Goparaju, Bhargavi
and Babu, Kommu Kishore
and Kandimalla, Purna Chandra Rao",
title="BugPrioritizeAI for multimodal test case prioritisation using bug reports, code changes, and test metadata",
journal="Scientific Reports",
year="2025",
month="Dec",
day="08",
volume="16",
number="1",
pages="1539",
abstract="Regression testing is necessary in modern software development with continuous integration and delivery, but running it in full after every change is often too expensive. Test case prioritisation (TCP) can aid this process by prioritising test cases that reveal faults earliest. Still, current TCP approaches focus on single information sources (coverage, change history, and/or past faults) and do not model semantic relationships across software artefacts. Meanwhile, current deep learning--based methods still suffer from cross--project generalisation and misinterpretation. These gaps can be alleviated with BugPrioritizeAI, an explainable, multimodal TCP framework we propose that jointly uses bug reports, source code changes, and test metadata to rank test cases. At the core of BugTestRankNet is a component responsible for generating a priority score that allows for quicker fault detection. BugPrioritizeAI is an AI-enhanced approach to bug triage that operates at the bug report level and ranks potentially buggy files in the bug repo using textual features. This framework reduces testing overhead and provides SHAP-based explanations, giving developers insight into the reasons for prioritising individual test cases.",
issn="2045-2322",
doi="10.1038/s41598-025-31851-z",
url="https://doi.org/10.1038/s41598-025-31851-z"
}


@Article{Liu2025,
author="Liu, Lu
and Wei, Lili
and Zhang, Wuqi
and Li, Shuqing
and Zhou, Yifan
and Liu, Yepang
and Cheung, Shing-Chi
and Lyu, Michael R.",
title="On state reverting in solidity smart contracts: Developer practices, fault categorization, and tool evaluation",
journal="Empirical Software Engineering",
year="2025",
month="Jul",
day="17",
volume="30",
number="5",
pages="141",
abstract="Smart contracts are computer programs deployed on blockchains to facilitate transactions. A critical aspect of smart contract security is the use of state-reverting statements (e.g., require, if...revert, if...throw). These statements protect transactions from abnormal behaviors or malicious attacks by reverting a contract to its previous state when certain input constraints or security properties are violated. While essential, the correct use of these state-reverting (SR) statements is nontrivial. Improper use can lead to security vulnerabilities, resulting in substantial financial losses or other severe consequences. It is, therefore, highly important to understand developers' practices of state reverting in smart contracts and the common mistakes they make. To achieve this goal, we conduct the first comprehensive empirical study on the use of SR statements and their related faults in Solidity smart contracts. First, we analyze the prevalence and purposes of SR statements in 21,414 verified contracts from popular decentralized applications (dapps) and manually examine 381 SR statements, leading to a taxonomy of their uses. Second, we collect 320 real-world state-reverting faults (SR faults) from open-source projects on GitHub and audit reports on Code4rena. We categorize the SR faults into 17 types and summarize 12 distinct fixing strategies. This knowledge can help researchers and practitioners to better understand the common usages of SR statements and learn how to prevent or cope with SR faults. Lastly, the variety of SR fault types and the presence of high-risk issues highlight the need for automated tools to identify and mitigate these faults. This further motivates us to assess the SR fault detection performance of state-of-the-art security analyzers, with the aim of understanding their capability and identifying their deficiencies. Via evaluating 12 representative tools on a benchmark comprising 243 contracts with six types of SR faults and the corresponding patched versions, we observe that existing tools exhibit limited capabilities in detecting SR faults (the average detection rate is 14.4{\%}). This result underscores the need for more advanced security analysis tools specifically tailored for SR faults. To facilitate the development of such tools, we further provide a comprehensive analysis of three common limitations of existing tools.",
issn="1573-7616",
doi="10.1007/s10664-025-10685-3",
url="https://doi.org/10.1007/s10664-025-10685-3"
}


@Article{Maile2025,
author="Maile, Thomas
and Masoumi, Salim
and Wang, Carl
and Riddell, Anna
and McClusky, Simon",
title="Analysis of the AUSPOS v3.0 positioning service after alignment to ITRF2020",
journal="GPS Solutions",
year="2025",
month="Dec",
day="29",
volume="30",
number="1",
pages="52",
abstract="AUSPOS is an openly available online Global Positioning System (GPS) data processing service provided by Geoscience Australia and is recommended by the Intergovernmental Committee on Surveying and Mapping (ICSM) for control surveys by Global Navigation Satellite System (GNSS) in the Australian region as a method for providing connection to the Geocentric Datum of Australia 2020 (GDA2020). A new and improved global reference frame, the International Terrestrial Reference Frame 2020 (ITRF2020), was released in April 2022 by the International Earth Rotation and Reference Systems Service (IERS), which was followed by the International GNSS Service (IGS) realisation, using modernised analysis standards and processing strategies, called IGS20. This paper outlines the adoption of ITRF2020/IGS20 for the AUSPOS processing service, which includes the introduction of a two-step transformation strategy to provide access to the national datum, GDA2020. The paper also provides relevant analysis and validation of the new AUSPOS system's (v3.0) ability to produce consistent GDA2020 coordinates when compared to those produced by the previous AUSPOS system (v2.4 - aligned to ITRF2014) with observed differences of {\$}{\$}{\backslash}:-0.2{\backslash}pm{\backslash}:6mm,{\backslash}:-0.2{\backslash}pm{\backslash}:4mm{\backslash}:and-0.3{\backslash}pm{\backslash}:21mm{\$}{\$}in Easting, Northing and Height components, respectively. Geoscience Australia is committed to being Australia's trusted provider of analytic products and services, by enhancing the accuracy and reliability of positioning within Australia.",
issn="1521-1886",
doi="10.1007/s10291-025-02003-7",
url="https://doi.org/10.1007/s10291-025-02003-7"
}


@inproceedings{10.1007/978-3-031-47765-2_22,
 abstract = {Teaching programming is essential for science and technology development in any country. Studies indicate high failure rates in programming subjects, which often lead to frustration or discouragement among students when they encounter coding and compiler error message. This is due to the language and interpretation of such error messages. Several efforts have been made to improve this situation, using neural networks or machine learning techniques to either fix the compiler error or give a more understandable error message. In this work, error-based learning is considered, and instead of fixing errors in the code, compiler error messages are used to provide feedback on syntax errors to programming students. The feedback consists of four components: a translation of the message to Spanish, syntax information about the language item, the error's relation to possible causes and a reference to relevant topics for review. All the given information is intended to help students understand the error, allowing them to rewrite the code and compile it successfully. To achieve that goal, supervised learning was used to build a classifier using the compiler error messages. A set of documents was generated by injecting errors into model programs, thereafter, labelled according to the type of syntax error. The Machine Learning algorithms used were Decision Tree, Support Vector Machine, Random Forest, Multi-layer Perceptron and K-Nearest Neighbors. These classifiers were trained using 80{\%} of the documents and evaluated with the remaining 20{\%}, achieving an accuracy of over 90{\%} for making new predictions and providing feedback.},
 address = {Cham},
 author = {Mart{\'i}nez, V{\'i}ctor Gonzalo Rivero
and L{\'o}pez, Maricela Quintana
and Chau, Asdr{\'u}bal L{\'o}pez
and Moreno, V{\'i}ctor Manuel Landassuri},
 booktitle = {Advances in Computational Intelligence},
 editor = {Calvo, Hiram
and Mart{\'i}nez-Villase{\~{n}}or, Lourdes
and Ponce, Hiram},
 isbn = {978-3-031-47765-2},
 pages = {296--308},
 publisher = {Springer Nature Switzerland},
 title = {Using Compiler Errors Messages to Feedback High School Students Through Machine Learning Methods},
 url = {https://link.springer.com/chapter/10.1007/978-3-031-47765-2_22},
 year = {2024}
}

@Article{Tang2025,
author="Tang, Xunzhu
and Tian, Haoye
and Pian, Weiguo
and Ezzini, Saad
and Kabor{\'e}, Abdoul Kader
and Habib, Andrew
and Kim, Kisub
and Klein, Jacques
and Bissyand{\'e}, Tegawend{\'e} F.",
title="Learning to represent code changes",
journal="Empirical Software Engineering",
year="2025",
month="Dec",
day="17",
volume="31",
number="3",
pages="50",
abstract="Code change representation plays a pivotal role in automating numerous software engineering tasks, such as classifying code change correctness or generating natural language summaries of code changes. Recent studies have leveraged deep learning to derive effective code change representation, primarily focusing on capturing changes in token sequences or Abstract Syntax Trees (ASTs). However, these current state-of-the-art representations do not explicitly calculate the intention semantic induced by the change on the AST, nor do they effectively explore the surrounding contextual information of the modified lines. To address this, we propose a new code change representation methodology, Patcherizer, which we refer to as our tool. This innovative approach explores the intention features of the context and structure, combining the context around the code change along with two novel representations. These new representations capture the sequence intention inside the code changes in the code change and the graph intention inside the structural changes of AST graphs before and after the code change. This comprehensive representation allows us to better capture the intentions underlying a code change. Patcherizer builds on graph convolutional neural networks for the structural input representation of the intention graph and on transformers for the intention sequence representation. We assess the generalizability of Patcherizer 's learned embeddings on three tasks: (1) Generating code change description in NL, (2) Predicting code change correctness in program repair, and (3) Code change intention detection. Experimental results show that the learned code change representation is effective for all three tasks and achieves superior performance to the state-of-the-art (SOTA) approaches. For instance, on the popular task of code change description generation (a.k.a. commit message generation), Patcherizer achieves an average improvement of 19.39{\%}, 8.71{\%}, and 34.03{\%} in terms of BLEU, ROUGE-L, and METEOR metrics, respectively.",
issn="1573-7616",
doi="10.1007/s10664-025-10763-6",
url="https://doi.org/10.1007/s10664-025-10763-6"
}


@Article{Ali2025,
author="Ali, Usama
and Naseer, Mehwish",
title="Artificial intelligence and machine learning in enhancing software project management processes: A systematic literature review",
journal="Automated Software Engineering",
year="2025",
month="Nov",
day="11",
volume="33",
number="1",
pages="31",
abstract="The growing complexity of software systems and the increasing demand for global software development necessitate innovative approaches to enhance project management, quality assurance, and risk mitigation. In this study, Artificial Intelligence (AI) and Machine Learning (ML) techniques are examined for overcoming problems in software project management, notably effort estimation, scheduling, resource allocation, risk management, and defect prediction. By systematically reviewing the literature, we show that AI/ML models like Support Vector Machines, neural networks, and ensemble learning can enhance estimation accuracy, maximize resource utilization, and reduce risks. Furthermore, the practical benefits and challenges of implementing an AI/ML system into a real-world system are discussed using real-world case studies, which include data quality and integration issues, and the interpretability of the model. In addition, advanced models, such as graph convolutional networks and deep neural networks, hold great promise as a defect prediction and bug severity classifier. The focus of this research is to leverage the transformative capabilities of AI/ML toward defect-free, efficient, and customer-centric software development. Finally, it suggests future research interests, including integrating explanation model AI, managing data in a better way, and implementing the scalable hybrid approach to meet the newer needs of the industry.",
issn="1573-7535",
doi="10.1007/s10515-025-00578-6",
url="https://doi.org/10.1007/s10515-025-00578-6"
}


@Article{Zhang2025,
author="Zhang, Yu-Wei
and Jin, Zhi
and Wang, Ze-Jun
and Xing, Ying
and Li, Ge",
title="SAGA: Summarization-Guided Assert Statement Generation",
journal="Journal of Computer Science and Technology",
year="2025",
month="Jan",
day="01",
volume="40",
number="1",
pages="138--157",
abstract="Generating meaningful assert statements is one of the key challenges in automated test case generation, which requires understanding the intended functionality of the tested code. Recently, deep learning based models have shown promise in improving the performance of assert statement generation. However, the existing models only rely on the test prefixes along with their corresponding focal methods, yet ignore the developer-written summarization. Based on our observations, the summarization contents usually express the intended program behavior or contain parameters that will appear directly in the assert statement. Such information will help existing models address their current inability to accurately predict assert statements. This paper presents a summarization-guided approach for automatically generating assert statements. To derive generic representations for natural language (i.e., summarization) and programming language (i.e., test prefixes and focal methods), we leverage a pre-trained language model as the reference architecture and fine-tune it on the task of assert statement generation. To the best of our knowledge, the proposed approach makes the first attempt to leverage the summarization of focal methods as the guidance for making the generated assert statements more accurate. We demonstrate the effectiveness of our approach on two real-world datasets compared with state-of-the-art models.",
issn="1860-4749",
doi="10.1007/s11390-023-2878-6",
url="https://doi.org/10.1007/s11390-023-2878-6"
}


@Article{He2024,
author="He, Junda
and Xu, Bowen
and Yang, Zhou
and Han, DongGyun
and Yang, Chengran
and Liu, Jiakun
and Zhao, Zhipeng
and Lo, David",
title="PTM4Tag+: Tag recommendation of stack overflow posts with pre-trained models",
journal="Empirical Software Engineering",
year="2024",
month="Nov",
day="20",
volume="30",
number="1",
pages="28",
abstract="Stack Overflow is one of the most influential Software Question {\&} Answer (SQA) websites, hosting millions of programming-related questions and answers. Tags play a critical role in efficiently organizing the contents on Stack Overflow and are vital to support various site operations, such as querying relevant content. Poorly chosen tags often lead to issues such as tag ambiguity and tag explosion. Therefore, a precise and accurate automated tag recommendation technique is needed. Inspired by the recent success of pre-trained models (PTMs) in natural language processing (NLP), we present PTM4Tag+, a tag recommendation framework for Stack Overflow posts that utilize PTMs in language modeling. PTM4Tag+ is implemented with a triplet architecture, which considers three key components of a post, i.e., Title, Description, and Code, with independent PTMs. We utilize a number of popular pre-trained models, including BERT-based models (e.g., BERT, RoBERTa, CodeBERT, BERTOverflow, and ALBERT), and encoder-decoder models (e.g., PLBART, CoTexT, and CodeT5). Our results show that leveraging CodeT5 under the PTM4Tag+ framework achieves the best performance among the eight considered PTMs and outperforms the state-of-the-art Convolutional Neural Network-based approach by a substantial margin in terms of average Precision@k, Recall@k, and F1-score@k (k ranges from 1 to 5). Specifically, CodeT5 improves the performance of F1-score@1-5 by 8.8{\%}, 12.4{\%}, 15.3{\%}, 16.4{\%}, and 16.6{\%}, respectively. Moreover, to address the concern with inference latency, we experimented PTM4Tag+ using smaller PTM models (i.e., DistilBERT, DistilRoBERTa, CodeBERT-small, and CodeT5-small). We find that although smaller PTMs cannot outperform larger PTMs, they still maintain over 93.96{\%} of the performance on average while reducing the mean inference time by more than 47.2{\%}.",
issn="1573-7616",
doi="10.1007/s10664-024-10576-z",
url="https://doi.org/10.1007/s10664-024-10576-z"
}


@Article{Onan2025,
author="Onan, Aytug
and Alhumyani, Hesham A.",
title="CodeDiffuSe: A masked diffusion framework for structure-aware code completion and repair",
journal="Journal of King Saud University Computer and Information Sciences",
year="2025",
month="Sep",
day="24",
volume="37",
number="8",
pages="230",
abstract="Code completion and code repair have become fundamental tasks in software engineering and machine learning research. However, existing large language models (LLMs) for code generation, predominantly based on autoregressive modeling (ARM), exhibit limitations when dealing with incomplete or buggy code snippets, especially when the missing or erroneous spans are located arbitrarily within the sequence. In this paper, we propose CodeDiffuSe, a novel masked diffusion framework specifically designed for structure-aware code completion and repair. Unlike traditional ARM approaches, CodeDiffuSe learns to recover missing code spans by leveraging both left and right context, enabling bidirectional reasoning and flexible infilling. We introduce a syntax-aware masking strategy that randomly masks entire Abstract Syntax Tree (AST) subtrees during training, and a semantic consistency regularization that encourages type-correct and syntactically valid predictions. During inference, we propose an error-aware remasking mechanism that dynamically identifies uncertain or invalid tokens and selectively refines them across adaptive reverse diffusion steps. Extensive experiments on standard code completion and bug repair benchmarks, including CodeXGLUE, Defects4J, and QuixBugs, demonstrate that CodeDiffuSe consistently outperforms strong autoregressive baselines such as CodeGen, CodeEditorBench, and InCoder across multiple programming languages. Our work introduces a structure- and semantics-aware diffusion-based alternative for code completion and repair, offering consistent gains in both syntactic validity and functional correctness across diverse benchmarks. Rather than claiming to replace existing paradigms, we demonstrate how diffusion models can complement and improve upon autoregressive and retrieval-based approaches in structure-sensitive settings.",
issn="2213-1248",
doi="10.1007/s44443-025-00237-6",
url="https://doi.org/10.1007/s44443-025-00237-6"
}


@Article{Yang2024,
author="Yang, Shaoyu
and Chen, Xiang
and Liu, Ke
and Yang, Guang
and Yu, Chi",
title="Automatic bi-modal question title generation for Stack Overflow with prompt learning",
journal="Empirical Software Engineering",
year="2024",
month="May",
day="03",
volume="29",
number="3",
pages="63",
abstract="When drafting question posts for Stack Overflow, developers may not accurately summarize the core problems in the question titles, which can cause these questions to not get timely help. Therefore, improving the quality of question titles has attracted the wide attention of researchers. An initial study aimed to automatically generate the titles by only analyzing the code snippets in the question body. However, this study ignored the helpful information in their corresponding problem descriptions. Therefore, we propose an approach SOTitle+ by considering bi-modal information (i.e., the code snippets and the problem descriptions) in the question body. Then we formalize the title generation for different programming languages as separate but related tasks and utilize multi-task learning to solve these tasks. Later we fine-tune the pre-trained language model CodeT5 to automatically generate the titles. Unfortunately, the inconsistent inputs and optimization objectives between the pre-training task and our investigated task may make fine-tuning hard to fully explore the knowledge of the pre-trained model. To solve this issue, SOTitle+ further prompt-tunes CodeT5 with hybrid prompts (i.e., mixture of hard and soft prompts). To verify the effectiveness of SOTitle+, we construct a large-scale high-quality corpus from recent data dumps shared by Stack Overflow. Our corpus includes 179,119 high-quality question posts for six popular programming languages. Experimental results show that SOTitle+ can significantly outperform four state-of-the-art baselines in both automatic evaluation and human evaluation. In addition, our ablation studies also confirm the effectiveness of component settings (such as bi-modal information, prompt learning, hybrid prompts, and multi-task learning) of SOTitle+. Our work indicates that considering bi-modal information and prompt learning in Stack Overflow title generation is a promising exploration direction.",
issn="1573-7616",
doi="10.1007/s10664-024-10466-4",
url="https://doi.org/10.1007/s10664-024-10466-4"
}


@Article{Improta2024,
author="Improta, Cristina
and Liguori, Pietro
and Natella, Roberto
and Cukic, Bojan
and Cotroneo, Domenico",
title="Enhancing robustness of AI offensive code generators via data augmentation",
journal="Empirical Software Engineering",
year="2024",
month="Oct",
day="19",
volume="30",
number="1",
pages="7",
abstract="Since manually writing software exploits for offensive security is time-consuming and requires expert knowledge, AI-base code generators are an attractive solution to enhance security analysts' productivity by automatically crafting exploits for security testing. However, the variability in the natural language and technical skills used to describe offensive code poses unique challenges to their robustness and applicability. In this work, we present a method to add perturbations to the code descriptions to create new inputs in natural language (NL) from well-intentioned developers that diverge from the original ones due to the use of new words or because they miss part of them. The goal is to analyze how and to what extent perturbations affect the performance of AI code generators in the context of offensive code. First, we show that perturbed descriptions preserve the semantics of the original, non-perturbed ones. Then, we use the method to assess the robustness of three state-of-the-art code generators against the newly perturbed inputs, showing that the performance of these AI-based solutions is highly affected by perturbations in the NL descriptions. To enhance their robustness, we use the method to perform data augmentation, i.e., to increase the variability and diversity of the NL descriptions in the training data, proving its effectiveness against both perturbed and non-perturbed code descriptions.",
issn="1573-7616",
doi="10.1007/s10664-024-10569-y",
url="https://doi.org/10.1007/s10664-024-10569-y"
}


@inproceedings{10.1007/978-3-032-07106-4_10,
 abstract = {Adversarial examples undermine the reliability of neural networks. To defend against attacks, multiple approaches have been proposed. However, many of them introduce high training overhead or high inference overhead, some significantly decrease the network's accuracy or insufficiently increase the network's robustness, and others do not scale to deep networks. To mitigate all these shortcomings, we propose a new form of defense: optimal program synthesis of short repair programs, integrated into a trained network. A repair program modifies a few neurons by using a few other neurons. The challenge is to identify the most successful combination of neurons to enhance the network's robustness while maintaining high accuracy. We introduce DefEnSyn, a stochastic synthesizer of repair programs. To cope with the exponential number of neuron combinations, DefEnSyn learns the effective combinations by synthesizing repair programs of increasing length. We evaluate DefEnSyn on classifiers for ImageNet and CIFAR-10 and show it enhances the robustness of networks to {\$}{\$}L{\_}{\backslash}infty {\$}{\$}L∞-, {\$}{\$}L{\_}2{\$}{\$}L2-, and {\$}{\$}L{\_}0{\$}{\$}L0- black-box adversarial example attacks and to backdoor attacks. DefEnSyn 's repair programs enhance the networks' robustness on average by {\$}{\$}+40{\backslash}{\%}{\$}{\$}+40{\%}and up to {\$}{\$}+71{\backslash}{\%}{\$}{\$}+71{\%}. DefEnSyn decreases the network's accuracy by only {\$}{\$}{\backslash}approx -1{\backslash}{\%}{\$}{\$}≈-1{\%}. We demonstrate that DefEnSyn outperforms existing state-of-the-art defenses based on adversarial training, randomization, and repair, in both robustness and accuracy.},
 address = {Cham},
 author = {Yuviler, Tom
and Drachsler-Cohen, Dana},
 booktitle = {Static Analysis},
 editor = {Oh, Hakjoo
and Sui, Yulei},
 isbn = {978-3-032-07106-4},
 pages = {221--248},
 publisher = {Springer Nature Switzerland},
 title = {Enhancing Neural Network Robustness via Synthesis of Repair Programs},
 url = {https://link.springer.com/chapter/10.1007/978-3-032-07106-4_10},
 year = {2026}
}

@Article{Atkinson2023,
author="Atkinson, Cameron F.",
title="ChatGPT and computational-based research: benefits, drawbacks, and machine learning applications",
journal="Discover Artificial Intelligence",
year="2023",
month="Dec",
day="04",
volume="3",
number="1",
pages="42",
abstract="Generative artificial intelligence (GenAI) systems are disrupting how research is conducted across a wide range of disciplines. Many journals have decided not to allow these tools to be co-authors for the purposes of publication, but rather they must be acknowledged by authors as having been utilised in the writing process. Furthermore, due to the hallucinations that these models sometimes produce, authors are to review what is generated and recognise that they hold it to be true and accurate. To date, there has been varying research conducted on the accuracy of GenAI systems and their production of written text. However, new functions that allow GenAI systems to produce coding for constructing tools in computer programming languages highlights a new area that warrants investigation. Therefore, this article puts forth an account of using ChatGPT 3.5 to construct coding to be utilised for a Latent Dirichlet Allocation Topic Model (LDA-TM) for use in a Systematic Literature Review. This is hoped to address three elements of using ChatGPT 3.5 for coding: code review, error resolution, and scripting new code. The code will be aimed at designating an appropriate Hyper-parameter for the Random State for use in the LDA-TM. Within this context, this article will discuss the advantages and drawbacks of utilising this new tool and what it means for researchers who wish to augment their work with computer programming-based applications. To the authors knowledge, this is the first time this has been discussed within the context of the research being conducted.",
issn="2731-0809",
doi="10.1007/s44163-023-00091-3",
url="https://doi.org/10.1007/s44163-023-00091-3"
}


@Article{Salzano2025,
author="Salzano, Francesco
and Marchesi, Lodovica
and Antenucci, Cosmo Kevin
and Scalabrino, Simone
and Tonelli, Roberto
and Oliveto, Rocco
and Pareschi, Remo",
title="Bridging the gap: a comparative study of academic and developer approaches to smart contract vulnerabilities",
journal="Empirical Software Engineering",
year="2025",
month="Dec",
day="05",
volume="31",
number="2",
pages="37",
abstract="In this paper, we investigate the strategies adopted by Solidity developers to fix security vulnerabilities in smart contracts. Vulnerabilities are categorized using the DASP TOP 10 taxonomy, and fixing strategies are extracted from 364 commits collected from open-source Solidity projects on GitHub. Each commit was selected through a two-phase process: an initial filter using natural language processing techniques, followed by manual validation. We assessed whether these fixes adhere to established academic guidelines. Our analysis shows that 60.55{\%} of the commits aligned with at least one literature-based recommendation, particularly for well-documented vulnerability types such as Reentrancy and Arithmetic. However, adherence dropped significantly for categories like Denial of Service, Time Manipulation, and Bad Randomness, highlighting gaps between academic best practices and real-world developer behavior. From the remaining 143 non-aligned commits, we identified 27 novel fixing strategies not previously discussed in the literature. To evaluate their quality, we conducted a structured questionnaire involving 9 experts from both academia and industry. Their feedback indicated high perceived effectiveness of the new fixes, especially for vulnerabilities like Reentrancy and Unchecked Return Values. Generalizability received more varied responses, suggesting context-specific applicability. Finally, we performed a post-fix evolution analysis on over 6700 subsequent commits to assess the long-term stability of the fixes. Most patches remained unchanged, confirming their persistence in production code. Our findings offer practical insights into how vulnerabilities are fixed in smart contracts today, reveal promising emerging patterns, and help bridge the gap between academic guidelines and developer practices.",
issn="1573-7616",
doi="10.1007/s10664-025-10780-5",
url="https://doi.org/10.1007/s10664-025-10780-5"
}


@Article{Côté2024,
author="C{\^o}t{\'e}, Pierre-Olivier
and Nikanjam, Amin
and Ahmed, Nafisa
and Humeniuk, Dmytro
and Khomh, Foutse",
title="Data cleaning and machine learning: a systematic literature review",
journal="Automated Software Engineering",
year="2024",
month="Jun",
day="11",
volume="31",
number="2",
pages="54",
abstract="Machine Learning (ML) is integrated into a growing number of systems for various applications. Because the performance of an ML model is highly dependent on the quality of the data it has been trained on, there is a growing interest in approaches to detect and repair data errors (i.e., data cleaning). Researchers are also exploring how ML can be used for data cleaning; hence creating a dual relationship between ML and data cleaning. To the best of our knowledge, there is no study that comprehensively reviews this relationship. This paper's objectives are twofold. First, it aims to summarize the latest approaches for data cleaning for ML and ML for data cleaning. Second, it provides future work recommendations. We conduct a systematic literature review of the papers published between 2016 and 2022 inclusively. We identify different types of data cleaning activities with and for ML: feature cleaning, label cleaning, entity matching, outlier detection, imputation, and holistic data cleaning. We summarize the content of 101 papers covering various data cleaning activities and provide 24 future work recommendations. Our review highlights many promising data cleaning techniques that can be further extended. We believe that our review of the literature will help the community develop better approaches to clean data.",
issn="1573-7535",
doi="10.1007/s10515-024-00453-w",
url="https://doi.org/10.1007/s10515-024-00453-w"
}


@Article{Yuan2024,
author="Yuan, Dawei
and Peng, Xiao
and Chen, Zijie
and Zhang, Tao
and Lei, Ruijia",
title="Code context-based reviewer recommendation",
journal="Frontiers of Computer Science",
year="2024",
month="Nov",
day="11",
volume="19",
number="1",
pages="191202",
abstract="Code review is a critical process in software development, contributing to the overall quality of the product by identifying errors early. A key aspect of this process is the selection of appropriate reviewers to scrutinize changes made to source code. However, in large-scale open-source projects, selecting the most suitable reviewers for a specific change can be a challenging task. To address this, we introduce the Code Context Based Reviewer Recommendation (CCB-RR), a model that leverages information from changesets to recommend the most suitable reviewers. The model takes into consideration the paths of modified files and the context derived from the changesets, including their titles and descriptions. Additionally, CCB-RR employs KeyBERT to extract the most relevant keywords and compare the semantic similarity across changesets. The model integrates the paths of modified files, keyword information, and the context of code changes to form a comprehensive picture of the changeset. We conducted extensive experiments on four open-source projects, demonstrating the effectiveness of CCB-RR. The model achieved a Top-1 accuracy of 60{\%}, 55{\%}, 51{\%}, and 45{\%} on the Android, OpenStack, QT, and LibreOffice projects respectively. For Mean Reciprocal Rank (MRR), CCB achieved 71{\%}, 62{\%}, 52{\%}, and 68{\%} on the same projects respectively, thereby highlighting its potential for practical application in code reviewer recommendation.",
issn="2095-2236",
doi="10.1007/s11704-023-3256-9",
url="https://doi.org/10.1007/s11704-023-3256-9"
}


@Article{Vasileiou2025,
author="Vasileiou, Zoe
and Kumara, Indika
and Meditskos, Georgios
and Tokmakov, Kamil
and Radolovi{\'{c}}, Dragan
and Cruz, Jes{\'u}s Gorro{\~{n}}ogoitia
and Di Nitto, Elisabetta
and Tamburri, Damian Andrew
and Van Den Heuvel, Willem-Jan
and Vrochidis, Stefanos",
title="A knowledge-based approach for guided development of Infrastructure as Code",
journal="Software and Systems Modeling",
year="2025",
month="Jun",
day="23",
abstract="Infrastructure as Code (IaC) uses versionable software code to define, deploy, and configure physical computational resources, software execution platforms, and applications. As a result, IaC enables the scalable management of complex computing environments while preventing environment drift. IaC frameworks typically offer specific languages such as the industrial Terraform, Ansible, Chef, or TOSCA---standing for Topology and Orchestration Specification for Cloud Applications---the OASIS (Organization for the Advancement of Structured Information Standards) open standard approach to IaC. Developing high-quality IaC for deploying and managing applications demands expertise and knowledge in specific IaC languages, infrastructure resources, resource providers, quality issues in IaC scripts, and so on. While several model-driven engineering (MDE) approaches have been proposed to simplify IaC development, they cannot capture and use expert knowledge to assist with modeling tasks and MDE processes by providing interactive recommendations. This paper presents a knowledge-based framework for guiding the model-driven development of IaC. We use TOSCA as the target IaC language as it is an open standard. We enable IaC and resource experts to share their IaC and resource-related knowledge with application operational experts to help simplify the development of application deployment models. We use an ontology to record the relevant deployment knowledge and ontology reasoning to implement modeling guidance capabilities such as TOSCA model auto-completion, code smell and error detection, and model element matchmaking. We show the flexibility of our methodology by applying it to three industrial applications, covering cloud, edge, and HPC (High-Performance Computing) domains. Moreover, we also assess the use acceptance of our approach and framework by conducting controlled experiments with expert and non-expert IaC users. The results indicate that our method can simplify IaC development by providing appropriate recommendations.",
issn="1619-1374",
doi="10.1007/s10270-025-01294-1",
url="https://doi.org/10.1007/s10270-025-01294-1"
}


@Article{Huang2024,
author="Huang, Zixin
and Dutta, Saikat
and Misailovic, Sasa",
title="Debugging convergence problems in probabilistic programs via program representation learning with SixthSense",
journal="International Journal on Software Tools for Technology Transfer",
year="2024",
month="Jun",
day="01",
volume="26",
number="3",
pages="249--268",
abstract="Probabilistic programming aims to open the power of Bayesian reasoning to software developers and scientists, but identification of problems during inference and debugging are left entirely to the developers and typically require significant statistical expertise. A common class of problems when writing probabilistic programs is the lack of convergence of the probabilistic programs to their posterior distributions.",
issn="1433-2787",
doi="10.1007/s10009-024-00737-2",
url="https://doi.org/10.1007/s10009-024-00737-2"
}


@Article{Nabavi2023,
author="Nabavi, Saeed
and Gholampour, Sirous
and SadeghpourHaji, Maedeh",
title="Damage identification in structure elements by grasshopper optimization algorithm using dynamic structure behaviors",
journal="Evolutionary Intelligence",
year="2023",
month="Aug",
day="01",
volume="16",
number="4",
pages="1369--1382",
abstract="Timely inspection of structures is necessary to prevent damage progression and ensure the safety and efficiency of structures at the lowest cost. The existence of defects and partial failures in structures is the source of general failures and threats to the behavior of structures. In many cases, impending failure may not be apparent from its appearance. Hence, the study of structural failure is one of the essential areas of research that helps increase the structure's life and control the failure of structures. This research aims to investigate an approach for detecting damage in the structure, especially in the plates. An efficient method utilizing the Grasshopper Optimization Algorithm (GOA) as an optimization solver is presented here to detect the multiple-damage of structural systems. Accelerations changes of a structure are considered as a criterion for damage occurrence. The structural damage detection problem is first transmuted into a standard optimization problem dealing with continuous variables. Then the GOA is utilized to solve the optimization problem for finding the site and severity of structural damage. To assess the performance of the proposed method for damage identification, numerical examples with considering measurement noise are considered. All the results demonstrate the effectiveness of the proposed method for accurately determining the site and severity of multiple-damage. Also, the performance of the GOA for damage detection compared to the differential evolution algorithm (DEA) is confirmed by a numerical example.",
issn="1864-5917",
doi="10.1007/s12065-022-00748-5",
url="https://doi.org/10.1007/s12065-022-00748-5"
}


@Article{Lan2023,
author="Lan, Jinpeng
and Gong, Lina
and Zhang, Jingxuan
and Zhang, Haoxiang",
title="BTLink : automatic link recovery between issues and commits based on pre-trained BERT model",
journal="Empirical Software Engineering",
year="2023",
month="Jul",
day="12",
volume="28",
number="4",
pages="103",
abstract="Data traceability in software development can connect different software artifacts to enhance the observability of developer practices. In particular, traceability links between issues and commits (i.e., issue-commit links) play a key role in software maintenance tasks (e.g., bug localization and bug prediction). In practice, developers typically manually make the issue-commit links by adding the issue identifier into the message of the corresponding commits, which results in missing issue commit links being prevalent in software projects. To recover the missing issue commit links, previous studies have proposed some automatic approaches. However, due to the difference between heuristic rules and real-world behavior, as well as insufficient semantic understanding, these approaches cannot achieve the expected performance. Since the text contained in issues and commits contains highly related information, thorough text understanding can improve traceability links. Meanwhile, pre-trained models (i.e., PTMs) have been successfully used to explore the semantic information of text in various software engineering tasks (e.g., software code generation). Therefore, our study proposes a novel BERT -based method (i.e., BTLink) that employs the pre-trained models to automatically recover the issue-commits links. Our proposed BTlink method includes a BERT embedding layer, a fusion layer, and a classifier layer. First, we build two pre-trained BERT encoders to respectively explore the feature representation of the issue text in combination with commit code and commit text. Then we build the fusion layer to examine the joint feature vector. Finally, we build the classifier layer to identify the links between issue and commit. In addition, to further our investigation and verify the effectiveness of BTLink, we conduct an extensive case study on 12 issue-commit links datasets from open source software projects, and observe that: (i) compared to state-of-the-art approaches, our proposed BTLink improves the performance of automatic issue-commit links recovery on all studied measures; (ii) both text and code information in the issues and commits are effective to recover more accurate issue-commit links; (iii) our proposed BTLink is more applicable to the cross-project context compared to state-of-the-art approaches.",
issn="1573-7616",
doi="10.1007/s10664-023-10342-7",
url="https://doi.org/10.1007/s10664-023-10342-7"
}


@inproceedings{10.1007/978-3-031-19849-6_11,
 abstract = {This white paper presents the vision of a novel methodology for developing safety-critical software, which is inspired by late developments in learning based co-piloting of implementations. The methodology, called TriCo, integrates formal methods with learning based approaches to co-pilot the agile, simultaneous development of three artefacts: implementation, specification, and tests. Whenever the user changes any of these, a TriCo empowered IDE would suggest changes to the other two artefacts in such a way that the three are kept consistent. The user has the final word on whether the changes are accepted, rejected, or modified. In the latter case, consistency will be checked again and re-established. We discuss the emerging trends which put the community in a good position to realise this vision, describe the methodology and workflow, as well as challenges and possible solutions for the realisation of TriCo.},
 address = {Cham},
 author = {Ahrendt, Wolfgang
and Gurov, Dilian
and Johansson, Moa
and R{\"u}mmer, Philipp},
 booktitle = {Leveraging Applications of Formal Methods, Verification and Validation. Verification Principles},
 editor = {Margaria, Tiziana
and Steffen, Bernhard},
 isbn = {978-3-031-19849-6},
 pages = {174--187},
 publisher = {Springer International Publishing},
 title = {TriCo---Triple Co-piloting of Implementation, Specification and Tests},
 url = {https://link.springer.com/chapter/10.1007/978-3-031-19849-6_11},
 year = {2022}
}

@Article{Chen2024,
author="Chen, Xiang
and Pei, Wenlong
and Yang, Shaoyu
and Zhou, Yanlin
and Zhang, Zichen
and Pei, Jiahua",
title="Automatic title completion for Stack Overflow posts and GitHub issues",
journal="Empirical Software Engineering",
year="2024",
month="Jul",
day="25",
volume="29",
number="5",
pages="120",
abstract="Title quality is important for different software engineering communities. For example, in Stack Overflow, posts with low-quality question titles often discourage potential answerers. In GitHub, issues with low-quality titles can make it difficult for developers to grasp the core idea of the problem. In previous studies, researchers mainly focused on generating titles from scratch by analyzing the body contents, such as the post body for Stack Overflow question title generation (SOTG) and the issue body for issue title generation (ISTG). However, the quality of the generated titles is still limited by the information available in the body contents. A more effective way is to provide accurate completion suggestions when developers compose titles. Inspired by this idea, we are the first to study the problem of automatic title completion for software engineering title generation tasks and propose the approach TC4SETG. Specifically, we first preprocess the gathered titles to form incomplete titles (i.e., tip information provided by developers) for simulating the title completion scene. Then we construct the input by concatenating the incomplete title with the body's content. Finally, we fine-tune the pre-trained model CodeT5 to learn the title completion patterns effectively. To evaluate the effectiveness of TC4SETG, we selected 189,655 high-quality posts from Stack Overflow by covering eight popular programming languages for the SOTG task and 333,563 issues in the top-200 starred repositories on GitHub for the ISTG task. Our empirical results show that compared with the approaches of generating question titles from scratch, our proposed approach TC4SETG is more practical in automatic and human evaluation. Our experimental results demonstrate that TC4SETG outperforms corresponding state-of-the-art baselines in the SOTG task by a minimum of 25.82{\%} and in the ISTG task by at least 45.48{\%} in terms of ROUGE-L. Therefore, our study provides a new direction for studying automatic software engineering title generation and calls for more researchers to investigate this direction in the future.",
issn="1573-7616",
doi="10.1007/s10664-024-10513-0",
url="https://doi.org/10.1007/s10664-024-10513-0"
}

@Article{Zapata2025,
author="L{\'o}pez Zapata, Erwin Daniel
and Tang, Cheng
and {\v{S}}v{\'a}bensk{\'y}, Valdemar
and Okubo, Fumiya
and Shimada, Atsushi",
title="LECTOR: Summarizing E-book Reading Content for Personalized Student Support",
journal="International Journal of Artificial Intelligence in Education",
year="2025",
month="Dec",
day="01",
volume="35",
number="4",
pages="2495--2533",
abstract="Educational e-book platforms provide valuable information to teachers and researchers through two main sources: reading activity data and reading content data. While reading activity data is commonly used to analyze learning strategies and predict low-performing students, reading content data is often overlooked in these analyses. To address this gap, this study proposes LECTOR (Lecture slides and Topic Relationships), a model that summarizes information from reading content in a format that can be easily integrated with reading activity data. Our first experiment compared LECTOR to representative Natural Language Processing (NLP) models in extracting key information from 2,255 lecture slides, showing an average improvement of 5{\%} in F1-score. These results were further validated through a human evaluation involving 28 students, which showed an average improvement of 21{\%} in F1-score over a model predominantly used in current educational tools. Our second experiment compared reading preferences extracted by LECTOR with traditional reading activity data in predicting low-performing students using 600,712 logs from 218 students. The results showed a tendency to improve the predictive performance by integrating LECTOR. Finally, we proposed examples showing the potential application of the reading preferences extracted by LECTOR in designing personalized interventions for students.",
issn="1560-4306",
doi="10.1007/s40593-025-00478-6",
url="https://doi.org/10.1007/s40593-025-00478-6"
}

@Article{Rocco2025,
author="Di Rocco, Juri
and Di Ruscio, Davide
and Di Sipio, Claudio
and Nguyen, Phuong T.
and Rubei, Riccardo",
title="On the use of large language models in model-driven engineering",
journal="Software and Systems Modeling",
year="2025",
month="Jun",
day="01",
volume="24",
number="3",
pages="923--948",
abstract="Model-driven engineering (MDE) has seen significant advancements with the integration of machine learning (ML) and deep learning techniques. Building upon the groundwork of previous investigations, our study provides a concise overview of current large language models (LLMs) applications in MDE, emphasizing their role in automating tasks like model repository classification and developing advanced recommender systems. The paper also outlines the technical considerations for seamlessly integrating LLMs in MDE, offering a practical guide for researchers and practitioners. Looking forward, the paper proposes a focused research agenda for the future interplay of LLMs and MDE, identifying key challenges and opportunities. This concise roadmap envisions the deployment of LLM techniques to enhance the management, exploration, and evolution of modeling ecosystems. Moreover, we also discuss the adoption of LLMs in various domains by means of model-driven techniques and tools, i.e., MDE for supporting LLMs. By offering a compact exploration of LLMs in MDE, this paper contributes to the ongoing evolution of MDE practices, providing a forward-looking perspective on the transformative role of large language models in software engineering and model-driven practices.",
issn="1619-1374",
doi="10.1007/s10270-025-01263-8",
url="https://doi.org/10.1007/s10270-025-01263-8"
}

@Article{Du2024,
author="Du, Xiaoting
and Liu, Zhihao
and Li, Chenglong
and Ma, Xiangyue
and Li, Yingzhuo
and Wang, Xinyu",
title="LLM-BRC: A large language model-based bug report classification framework",
journal="Software Quality Journal",
year="2024",
month="Sep",
day="01",
volume="32",
number="3",
pages="985--1005",
abstract="Deep learning frameworks serve as the cornerstone for constructing robust deep learning systems. However, bugs within these frameworks can have severe consequences, negatively affecting various applications. Accurately classifying and understanding these bugs is essential to ensure framework reliability. By doing so, developers can proactively take appropriate measures to mitigate potential risks associated with specific bug types in both current and future software releases. Despite the significance of bug report classification, existing methods fall short in terms of performance, rendering them impractical for real-world applications. To address this limitation, we propose a bug report classification framework for deep learning frameworks, called LLM--BRC, leveraging OpenAI's latest embedding model, text-embedding-ada-002. Our LLM--BRC framework achieves an impressive accuracy range of 92{\%} to 98.75{\%} in bug report classification for three deep learning frameworks: TensorFlow, MXNET, and PaddlePaddle. This represents a substantial improvement of 17.21{\%} to 69.15{\%} compared to existing methods. Furthermore, we conduct a comprehensive investigation into the impact of different bug report components and different models.",
issn="1573-1367",
doi="10.1007/s11219-024-09675-3",
url="https://doi.org/10.1007/s11219-024-09675-3"
}


@Article{Li2025,
author="Li, Yingling
and Cai, Muxin
and Chen, Junjie
and Xu, Yang
and Huang, Lei
and Li, Jianping",
title="Context-aware prompting for LLM-based program repair",
journal="Automated Software Engineering",
year="2025",
month="Apr",
day="18",
volume="32",
number="2",
pages="42",
abstract="Automated program repair (APR) plays a crucial role in ensuring the quality of software code, as manual bug-fixing is extremely time-consuming and labor-intensive. Traditional APR tools (e.g., template-based approaches) face the challenge of generalizing to different bug patterns, while deep learning (DL)-based methods heavily rely on training datasets and struggle to fix unseen bugs. Recently, large language models (LLMs) have shown great potential in APR due to their ability to generate patches, having achieved promising results. However, their effectiveness is still constrained by the casually-determined context (e.g., being unable to adaptively select the specific context according to the situation of each defect). Therefore, a more effective APR approach is highly needed, which provides more precise and comprehensive context for the given defect to enhance the robustness of LLM-based APRs. In this paper, we propose a context-aware APR approach named CodeCorrector, which designs a Chain-of-Thought (CoT) approach to follow developers' program repair behaviors. Given a failing test and its buggy file, CodeCorrector first analyzes why the test fails based on the failure message to infer repair direction; then selects the relevant context information to this repair direction; finally builds the context-aware repair prompt to guide LLMs for patch generation. Our motivation is to offer a novel perspective for enhancing LLM-based program repair through context-aware prompting, which adaptively selects specific context for a given defect. The evaluation on the widely-used Defects4J (i.e., v1.2 and v2.0) benchmark shows that overall, by executing a small number of repairs (i.e., as few as ten rounds), CodeCorrector outperforms all the state-of-the-art baselines on the more complex defects in Defects4J v2.0 and the defects without fine-grained defect localization information in Defects4J v1.2. Specifically, a total of 38 defects are fixed by only CodeCorrector. We further analyze the contributions of two core components (i.e., repair directions, global context selection) to the performance of CodeCorrector, especially repair directions, which improve CodeCorrector by 112{\%} in correct patches and 78{\%} in plausible patches on Defects4J v1.2. Moreover, CodeCorrector generates more valid and correct patches, achieving a 377{\%} improvement over the base LLM GPT-3.5 and a 268{\%} improvement over GPT-4.",
issn="1573-7535",
doi="10.1007/s10515-025-00512-w",
url="https://doi.org/10.1007/s10515-025-00512-w"
}


@inproceedings{10.1007/978-981-95-2961-2_22,
 abstract = {The last 18 months have seen an explosion of activity in both industry and research in the Generative AI space, specifically Large Language Models (LLMs). Smart contract vulnerability detection is no exception; as smart contracts exist on public chains and can have billions of dollars transacted daily, continuous improvement in vulnerability detection is crucial. This has led to many researchers investigating the usage of generative large language models (LLMs) to aid in detecting vulnerabilities in smart contracts. This paper presents a systemic review of the current LLM-based smart contract vulnerability detection tools, comparing them against traditional static and dynamic analysis tools like Slither and Mythril. Our analysis highlights key areas where each performs better and shows that while these tools show promise, the LLM-based tools available for testing are not ready to replace more traditional tools. We conclude with recommendations on how LLMs are best used in the vulnerability detection process and offer insights for improving on the state-of-the-art via hybrid approaches and targeted pre-training of much smaller models.},
 address = {Singapore},
 author = {Ince, Peter
and Yu, Jiangshan
and Liu, Joseph K.
and Du, Xiaoning
and Luo, Xiapu},
 booktitle = {Provable and Practical Security},
 editor = {Yang, Guomin
and Liu, Shengli
and Su, Chunhua
and Otsuka, Akira
and Lian, Zhuotao},
 isbn = {978-981-95-2961-2},
 pages = {426--445},
 publisher = {Springer Nature Singapore},
 title = {GenDetect: Generative Large Language Model Usage in Smart Contract Vulnerability Detection},
 url = {https://link.springer.com/chapter/10.1007/978-981-95-2961-2_22},
 year = {2026}
}

@Article{Wang2025,
author="Wang, Shaosheng
and Lu, Lu
and Qiu, Shaojian
and Tian, Qingyan
and Lin, Haishan",
title="DALO-APR: LLM-based automatic program repair with data augmentation and loss function optimization",
journal="The Journal of Supercomputing",
year="2025",
month="Mar",
day="21",
volume="81",
number="5",
pages="640",
abstract="Automatic program repair (APR) has made significant strides with the advent of large language models (LLMs) such as T5 and CodeT5. However, LLM-based APR models may rely on repetitive repair patterns due to limited training data diversity, resulting in suboptimal performance. Additionally, common loss functions, such as cross-entropy, may not fully prioritize repair locations or optimize the model's output probability distribution to favor more accurate repair candidates. To address these challenges, this paper proposes a method for LLM-Based APR with Data Augmentation and Loss Function Optimization (DALO-APR). The data augmentation strategy expands the variety of repair patterns by randomly deleting, inserting, swapping tokens, and injecting errors. The optimized loss function helps the model rank more accurate repair candidates higher. Experimental results on Java, JavaScript, Python, and C datasets demonstrate that DALO-APR improves both error localization and bug fixing. Compared to baseline models, DALO-APR shows improvements across multiple metrics, especially with a 105.65{\%} increase in 100{\%} accuracy.",
issn="1573-0484",
doi="10.1007/s11227-025-07102-3",
url="https://doi.org/10.1007/s11227-025-07102-3"
}


@Article{Kang2024,
author="Kang, Sungmin
and Chen, Bei
and Yoo, Shin
and Lou, Jian-Guang",
title="Explainable automated debugging via large language model-driven scientific debugging",
journal="Empirical Software Engineering",
year="2024",
month="Dec",
day="18",
volume="30",
number="2",
pages="45",
abstract="Automated debugging techniques have the potential to reduce developer effort in debugging. However, while developers want rationales for the provided automatic debugging results, existing techniques are ill-suited to provide them, as their deduction process differs significantly froof human developers. Inspired by the way developers interact with code when debugging, we propose Automated Scientific Debugging (AutoSD), a technique that prompts large language models to automatically generate hypotheses, uses debuggers to interact with buggy code, and thus automatically reach conclusions prior to patch generation. In doing so, we aim to produce explanations of how a specific patch has been generated, with the hope that these explanations will lead to enhanced developer decision-making. Our empirical analysis on three program repair benchmarks shows that AutoSDperforms competitively with other program repair baselines, and that it can indicate when it is confident in its results. Furthermore, we perform a human study with 20 participants to evaluate AutoSD-generated explanations. Participants with access to explanations judged patch correctness more accurately in five out of six real-world bugs studied. Furthermore, 70{\%} of participants answered that they wanted explanations when using repair tools, and 55{\%} answered that they were satisfied with the Scientific Debugging presentation.",
issn="1573-7616",
doi="10.1007/s10664-024-10594-x",
url="https://doi.org/10.1007/s10664-024-10594-x"
}


@inproceedings{10.1007/978-981-97-8540-7_17,
 abstract = {Path Traversal Vulnerability is a significant security flaw that allows attackers to exploit the file system structure of web applications by manipulating user input to access files outside the intended directory structure. This vulnerability can lead to unauthorized access to sensitive files and directories, resulting in severe consequences such as information disclosure, data manipulation, and system compromise. Despite its high likelihood of exploitation, as ranked eighth in the 2023 CWE Top 25 Most Dangerous Software Weaknesses, automated repair methods for this vulnerability, particularly in Java, remain underdeveloped. This paper introduces a methodology, named PTFix, which is a rule-based and LLM technique for repairing Java Path Traversal vulnerability. PTFix includes two stages: 1) analyze and patch Java path traversal vulnerability based on pre-defined rules; 2) integrate with LLMs to patch Java codes that do not match the rule. A comparative study was conducted using four large language models: Meta Llama2 7B, Codellama Instruct 34B, Claude3, and ChatGPT-4. The results revealed that while Meta Llama2 7B and Codellama Instruct 34B failed to correctly fix any examples, Claude3 successfully repaired two instances, and ChatGPT-4 outperformed the other models by correctly repairing four examples. These findings highlight the potential of combining static rule-based methods with LLMs to improve the automated repair of path traversal vulnerabilities in Java applications.},
 address = {Singapore},
 author = {Zhang, Xiaowei
and Liu, Shigang
and Zhang, Jun
and Xiang, Yang},
 booktitle = {Data Security and Privacy Protection},
 editor = {Chen, Xiaofeng
and Huang, Xinyi
and Yung, Moti},
 isbn = {978-981-97-8540-7},
 pages = {276--293},
 publisher = {Springer Nature Singapore},
 title = {PTFix: Rule-Based and LLM Techniques for Java Path Traversal Vulnerability},
 url = {https://link.springer.com/chapter/10.1007/978-981-97-8540-7_17},
 year = {2025}
}

@Article{Zhao2025,
author="Zhao, Qianhui
and Zhang, Li
and Liu, Fang
and Liu, Yang
and Yan, Zhen
and Chen, Zhenghao
and Zhou, Yufei
and Jiang, Jing
and Li, Ge
and Sun, Zian
and Li, Zhongqi
and Ma, Yuchi",
title="Peer-aided repairer: empowering large language models to repair advanced student assignments",
journal="Empirical Software Engineering",
year="2025",
month="Dec",
day="05",
volume="31",
number="2",
pages="33",
abstract="Automated generation of feedback on programming assignments holds significant benefits for programming education, especially when it comes to advanced assignments. Automated Program Repair techniques, especially Large Language Model-based approaches, have gained notable recognition for their potential in fixing introductory assignments. However, the programs used for evaluation are relatively simple. It remains unclear how existing approaches perform in repairing programs from higher-level programming courses. To address these limitations, we curate a new advanced student assignment dataset named Defects4DS from a higher-level programming course. Subsequently, we identify the challenges related to fixing bugs in advanced assignments. Based on the analysis, we develop a framework called PaR that is powered by the Large Language Models. PaR works in three phases: Peer Solution Selection, Multi-Source Prompt Generation, and Program Repair. Peer Solution Selection identifies the closely related peer programs based on lexical, semantic, and syntactic criteria. Then Multi-Source Prompt Generation adeptly combines multiple sources of information to create a comprehensive and informative prompt for the last Program Repair stage. Evaluation reveals that PaR achieves state-of-the-art performance on Defects4DS compared to baseline approaches, with the impressive improvement of 16.13{\%} in repair rate. And experimental results on several introductory programming assignment datasets further demonstrate the effectiveness of PaR, achieving state-of-the-art results on ITSP and IntroClass datasets.",
issn="1573-7616",
doi="10.1007/s10664-025-10716-z",
url="https://doi.org/10.1007/s10664-025-10716-z"
}


@Article{Yang2025,
author="Yang, Ruofan
and Xu, Xianghua
and Wang, Ran",
title="LLM-enhanced evolutionary test generation for untyped languages",
journal="Automated Software Engineering",
year="2025",
month="Feb",
day="17",
volume="32",
number="1",
pages="20",
abstract="Dynamic programming languages, such as Python, are widely used for their flexibility and support for rapid development. However, the absence of explicit parameter type declarations poses significant challenges in generating automated test cases. This often leads to random assignment of parameter types, increasing the search space and reducing testing efficiency. Current evolutionary algorithms, which rely heavily on random mutations, struggle to handle specific data types and frequently fall into local optima, making it difficult to generate high-quality test cases. Moreover, the resulting test suites often contain errors, preventing immediate usage in real-world applications. To address these challenges, this paper proposes the use of large language models to enhance test case generation for dynamic programming languages. Our method involves three key steps: analyzing parameter types to narrow the search space, introducing meaningful data during mutations to increase test case relevance, and using large language models to automatically repair errors in the generated test suites. Experimental results demonstrate a 16{\%} improvement in test coverage, faster evolutionary cycles, and an increase in the number of executable test suites. These findings highlight the potential of large language models in improving both the efficiency and reliability of test case generation for dynamic programming languages.",
issn="1573-7535",
doi="10.1007/s10515-025-00496-7",
url="https://doi.org/10.1007/s10515-025-00496-7"
}


@Article{Zhang2025,
author="Zhang, Yayun
and Li, Yuying
and Fang, Minying
and Yuan, Xing
and Du, Junwei",
title="BRMDS: an LLM-based multi-dimensional summary generation approach for bug reports",
journal="Automated Software Engineering",
year="2025",
month="Sep",
day="23",
volume="33",
number="1",
pages="10",
abstract="Bug report summarization aims to generate concise and accurate descriptions to help developers understand and maintain. The existing methodologies prioritize simplifying reporting content but fail to provide a structured and well-rounded description of bugs, limiting developers' understanding efficiency. In this paper, we leverage large language models (LLMs) to generate detailed, multi-dimensional summaries. Our intuition is based on the following facts: (1) LLMs establish robust semantic connections through extensive pre-training on paired data; (2) Real-world bug reports contain multi-dimensional information. We propose the Bug Report Multi-Dimensional Summary (BRMDS) approach, defining five dimensions: environment, actual behavior, expected behavior, bug category, and solution suggestions, and use specific instructions for each dimension to guide LLM in Parameter Efficient Fine-Tuning (PEFT). We construct a dataset in multi-dimensional information for PEFT and experimental evaluation, thereby addressing the gaps in existing datasets within this domain. The experimental results show that multi-dimensional summaries enhance developers' understanding of bug reports. BRMDS approach outperforms baseline approaches in both automatic and human evaluations. Our datasets are publicly available at https://github.com/yunjua/bug-reports-multi-dimensional.",
issn="1573-7535",
doi="10.1007/s10515-025-00553-1",
url="https://doi.org/10.1007/s10515-025-00553-1"
}


@Article{Wang2025,
author="Wang, Bo
and Deng, Ming
and Chen, Mingda
and Lin, Youfang
and Zhou, Jianyi
and Zhang, Jie M.",
title="Assessing the effectiveness of recent closed-source large language models in fault localization and automated program repair",
journal="Automated Software Engineering",
year="2025",
month="Oct",
day="28",
volume="33",
number="1",
pages="26",
abstract="Large Language Models (LLMs) have made significant advancements in code-related tasks. In the field of automated debugging, fault localization (FL) and automated program repair (APR) are two prevalent topics attracting significant research effort. Recently, in the field of FL and APR, many novel LLM-based approaches have emerged. However, most existing LLM-based studies primarily focus on the GPT models from OpenAI or open-source LLMs. With the rapid development of LLMs, various internet giants have introduced new closed-source models. In addition, due to policy restrictions, some regions can only access the commercial LLMs provided by specified companies. Despite the LLMs of OpenAI, the effectiveness of the other closed-source LLMs in FL and APR remains unknown. To better understand the effectiveness of contemporary closed-source models, we conduct a large-scale empirical study on their performance with respect to FL and APR. Specifically, our study involves 4 recent commercial closed-source LLMs (i.e., GPT-4o-Mini, Ernie-3.5, Qwen-turbo, and Doubao-pro) and 1 open-source LLM (i.e., DeepSeek-V3-chat). Note that only the GPT models have region restrictions among all LLMs we studied. We designed a total of 12 distinct prompt templates, 6 each for FL and APR, incorporating various formats and information sources. We conducted experiments to evaluate the effectiveness of FL and APR on 1036 real Java bugs from two datasets, Defects4J 2.0 and ConDefects. The key findings of the experiments indicate that (1) different LLMs tend to succeed on different sets of bugs in both FL and APR, with relatively little overlap among successful cases, implying the models possess distinct strengths in handling specific kinds of bugs, (2) the effectiveness of prompt templates varies across different models, and (3) the effectiveness of FL and APR capabilities of the studied models is significantly correlated with the bug type. We summarized all 14 findings obtained into 3 implications, which could help researchers further improve the performance of LLMs on FL and APR.",
issn="1573-7535",
doi="10.1007/s10515-025-00549-x",
url="https://doi.org/10.1007/s10515-025-00549-x"
}


@inproceedings{10.1007/978-3-032-10444-1_16,
 abstract = {Debugging and repairing faults when programs fail to formally verify can be complex and time-consuming. Automated Program Repair (APR) can ease this burden by automatically identifying and fixing faults. However, traditional APR techniques often rely on test suites for validation, but these may not capture all possible scenarios. In contrast, formal specifications provide strong correctness criteria, enabling more effective automated repair.},
 address = {Cham},
 author = {Wu, Valentina
and Mendes, Alexandra
and Abreu, Alexandre},
 booktitle = {Software Engineering and Formal Methods},
 editor = {Bianculli, Domenico
and G{\'o}mez-Mart{\'i}nez, Elena},
 isbn = {978-3-032-10444-1},
 pages = {261--278},
 publisher = {Springer Nature Switzerland},
 title = {Specification-Guided Repair of Arithmetic Errors in Dafny Programs Using LLMs},
 url = {https://link.springer.com/chapter/10.1007/978-3-032-10444-1_16},
 year = {2026}
}

@Article{Lin2025,
author="Lin, Renze
and Wang, Ran
and Hu, Guanghuan
and Xu, Xianghua",
title="LMFuzz: Program repair fuzzing based on large language models",
journal="Automated Software Engineering",
year="2025",
month="Oct",
day="28",
volume="33",
number="1",
pages="25",
abstract="Generating programs using large language models (LLMs) for fuzz testing has emerged as a significant testing methodology. While traditional fuzzers can produce correct programs, their effectiveness is limited by excessive constraints and restricted API combinations, resulting in insufficient coverage of the target system's code and impacting testing efficiency. Unlike traditional methods, large language model based fuzzers can generate more diverse code, effectively addressing key issues of conventional fuzzers. However, the lack of constraints on API combinations during the generation process often leads to reduced program validity. Therefore, a crucial challenge is to enhance the validity of generated code while maintaining its diversity. To address this issue, we propose a novel and universal fuzzer, LMFuzz. To ensure the fuzzer's generation capability, we utilize a large language model as the primary generator and model the operator selection problem within the fuzzing loop as a multi-armed bandit problem. We introduce the Thompson Sampling algorithm to enhance both the diversity and validity of program generation. To improve the validity of the generated code, we incorporate a program repair loop that iteratively corrects the generated programs, thereby reducing errors caused by the lack of API combination constraints. Experimental results demonstrate that LMFuzz significantly surpasses existing state-of-the-art large language model based fuzzers in terms of coverage and validity, and also exhibits notable advantages in generating diverse programs. Furthermore, LMFuzz has identified 24 bugs across five popular programming languages and their corresponding systems.",
issn="1573-7535",
doi="10.1007/s10515-025-00568-8",
url="https://doi.org/10.1007/s10515-025-00568-8"
}


@Article{Jie2025,
author="Jie, Wanqing
and Qiu, Wangjie
and Yang, Haofu
and Guo, Muyuan
and Huang, Xinpeng
and Lei, Tianyu
and Zhang, Qinnan
and Zheng, Hongwei
and Zheng, Zhiming",
title="Agent4Vul: multimodal LLM agents for smart contract vulnerability detection",
journal="Science China Information Sciences",
year="2025",
month="May",
day="19",
volume="68",
number="6",
pages="160101",
abstract="Smart contract vulnerabilities have emerged as a significant threat to blockchain system security under the Web 3.0 ecosystem. According to recent research, large language models (LLMs) have demonstrated immense potential in smart contract security audits but still lack the capability for effective vulnerability detection. Consequently, leveraging the capabilities of LLMs to effectively enhance the performance of smart contract vulnerability detection remains a critical challenge. In this paper, we propose Agent4Vul, a novel framework utilizing multimodal LLM agents to enhance smart contract vulnerability detection. Specifically, we design two LLM-based agents: Commentator and Vectorizer. The Commentator agent generates comments for the source code, while the Vectorizer agent converts contents into vector representations. Subsequently, we develop a multimodal learning architecture comprising the semantic branch and the graph branch, which collectively integrate features from the source code, generated comments, and the bytecode control flow graph (CFG). We empirically evaluate a large-scale real dataset of smart contracts and compare 19 state-of-the-art baseline approaches. The results show that Agent4Vul achieves (1) superior performance over all baseline approaches on the four types of common vulnerabilities in real attacks; (2) 3.61{\%}--16.32{\%} higher F1-scores than existing artificial intelligence (AI) approaches, outperforming even advanced LLMs like GPT-4o and o1. This work lays a solid foundation for LLM-driven smart contract security and introduces innovative applications of LLMs in software engineering.",
issn="1869-1919",
doi="10.1007/s11432-024-4402-2",
url="https://doi.org/10.1007/s11432-024-4402-2"
}


@Article{Wang2025,
author="Wang, Ruoke
and Li, Zongjie
and Gao, Cuiyun
and Wang, Chaozheng
and Xiao, Yang
and Wang, Xuan",
title="SPVR: syntax-to-prompt vulnerability repair based on large language models",
journal="Automated Software Engineering",
year="2025",
month="Dec",
day="18",
volume="33",
number="1",
pages="38",
abstract="Purpose: In the field of vulnerability repair, previous research has leveraged pre-trained models and LLM-based prompt engineering, among which LLM-based approaches show better generalizability and achieve the best performance. However, the LLM-based approaches generally regard vulnerability repair as a sequence-to-sequence task, and do not explicitly capture the syntax patterns for different vulnerability types, leading to limited accuracy. We aim to create a method that ensures the specificity of prompts targeting vulnerable code while also leveraging the generative capabilities of Large Language Models. Methods: We propose SPVR (Syntax-to-Prompt Vulnerability Repair), a novel framework that collects information from syntax trees, and generates corresponding prompts. Our method consists of three steps: rule design, prompt generation, and patch generation. In the rule design step, our method parses code patches and designs rules to extract relevant contextual information. These rules aid in identifying vulnerability-related issues. In the prompt generation step, our method extracts information from vulnerable code with pre-defined rules, automatically converting them into prompts. We also incorporate the description of CWE (Common Weakness Enumeration) as known information into the prompts. Finally, in the patch generation step, this prompt will serve as input to any conversational LLM to obtain code patches. Results: Extensive experiments validate that our method achieves excellent results in assisting LLMs to fix vulnerabilities accurately. We utilize multiple Large Language Models to validate the effectiveness of our work, repairing 143 of 547 vulnerable code using ChatGPT-4. We conducted a comparison of our approach against several existing vulnerability repair approaches (including fine-tuning-based and prompt-based), across multiple metrics. Conclusion: Our method is a novel framework that combines the Abstract Syntax Tree structure of code, providing targeted prompts of repair code for vulnerabilities. Our method demonstrates promising potential for real-world code vulnerability repair.",
issn="1573-7535",
doi="10.1007/s10515-025-00579-5",
url="https://doi.org/10.1007/s10515-025-00579-5"
}


@Article{Pan2025,
author="Pan, Xinlong
and Li, Jianhua
and Zhou, Zhihong
and Li, Gaolei
and Chen, Xiuzhen
and Ma, Jin
and Wu, Jun
and Zhang, Quanhai",
title="Large language model-enhanced probabilistic modeling for effective static analysis alarms",
journal="Frontiers of Information Technology {\&} Electronic Engineering",
year="2025",
month="Oct",
day="01",
volume="26",
number="10",
pages="1926--1941",
abstract="Static analysis presents significant challenges in alarm handling, where probabilistic models and alarm prioritization are essential methods for addressing these issues. These models prioritize alarms based on user feedback, thereby alleviating the burden on users to manually inspect alarms. However, they often encounter limitations related to efficiency and issues such as false generalization. While learning-based approaches have demonstrated promise, they typically incur high training costs and are constrained by the predefined structures of existing models. Moreover, the integration of large language models (LLMs) in static analysis has yet to reach its full potential, often resulting in lower accuracy rates in vulnerability identification. To tackle these challenges, we introduce BinLLM, a novel framework that harnesses the generalization capabilities of LLMs to enhance alarm probability models through rule learning. Our approach integrates LLM-derived abstract rules into the probabilistic model, using alarm paths and critical statements from static analysis. This integration enhances the model's reasoning capabilities, improving its effectiveness in prioritizing genuine bugs while mitigating false generalizations. We evaluated BinLLM on a suite of C programs and observed 40.1{\%} and 9.4{\%} reduction in the number of checks required for alarm verification compared to two state-of-the-art baselines, Bingo and BayeSmith, respectively, underscoring the potential of combining LLMs with static analysis to improve alarm management.",
issn="2095-9230",
doi="10.1631/FITEE.2500038",
url="https://doi.org/10.1631/FITEE.2500038"
}


@Article{Bistarelli2025,
author="Bistarelli, Stefano
and Fiore, Marco
and Mercanti, Ivan
and Mongiello, Marina",
title="Usage of Large Language Model for Code Generation Tasks: A Review",
journal="SN Computer Science",
year="2025",
month="Jul",
day="23",
volume="6",
number="6",
pages="673",
abstract="Large Language Models have received a lot of attention in recent years due to their outstanding performance on various Natural Language Processing tasks. They can be used for lots of applications, including assistance in code generation tasks. Actual literature lacks an exhaustive analysis of the benefits and drawbacks of using a Large Language Model for the generation of simple and complex code. This paper aims to overcome the issue: we perform a Literature Review to explore the state-of-the-art of the proposed topic, answering 4 Research Questions. Using the PRISMA methodology, we reviewed 66 papers published between 2021 and 2023. Our analysis reveals Python's dominance as the preferred language and identifies a significant research gap in addressing ethical constraints. Additionally, we provide insights into the performance of models such as GPT-4 and CodeLlama, and their comparative utility in tasks ranging from debugging to multi-turn program synthesis. The findings offer a foundation for future research aimed at optimizing LLMs for code generation.",
issn="2661-8907",
doi="10.1007/s42979-025-04241-5",
url="https://doi.org/10.1007/s42979-025-04241-5"
}


@inproceedings{10.1007/978-3-031-78386-9_28,
 abstract = {Large Language Model-based Automated Program Repair (LLM-APR) has recently received significant attention as a debugging assistance. Our objective is to improve the performance of LLM-APR. In this study, we focus on semantic information contained in the source code. Semantic information refers to elements used by the programmer to understand the source code, which does not contribute to compilation or execution. We picked out specification, method names and variable names as semantic information. In the investigation, we prepared eight prompts, each consisting of all combinations of three types of semantic information. The experimental results showed that all semantic information improves the performance of LLM-APR, and variable names are particularly significant.},
 address = {Cham},
 author = {Hori, Shota
and Matsumoto, Shinsuke
and Higo, Yoshiki
and Kusumoto, Shinji
and Yasuda, Kazuya
and Ito, Shinji
and Huyen, Phan Thi Thanh},
 booktitle = {Product-Focused Software Process Improvement},
 editor = {Pfahl, Dietmar
and Gonzalez Huerta, Javier
and Kl{\"u}nder, Jil
and Anwar, Hina},
 isbn = {978-3-031-78386-9},
 pages = {377--385},
 publisher = {Springer Nature Switzerland},
 title = {The Effects of Semantic Information on LLM-Based Program Repair},
 url = {https://link.springer.com/chapter/10.1007/978-3-031-78386-9_28},
 year = {2025}
}

@Article{Liu2025,
author="Liu, Zixin
and Du, Xiaozhi
and Liu, Hairui",
title="ReAPR: Automatic program repair via retrieval-augmented large language models",
journal="Software Quality Journal",
year="2025",
month="Jul",
day="22",
volume="33",
number="3",
pages="30",
abstract="Automatic Program Repair (APR) aims to automatically fix software defects, significantly reducing the efforts of manual debugging. Recent studies have demonstrated impressive results in utilizing Large Language Models (LLMs) for software bug fixing. Current LLM-based approaches depend solely on the pre-trained knowledge of LLMs, overlooking the prior knowledge contained in historical bug repair records, which increases the likelihood of hallucinations. To address this challenge, this paper proposes ReAPR, a retrieval-augmented framework for APR. We first curate a high-quality retrieval database by carefully compiling and filtering the existing datasets for APR. Subsequently, ReAPR leverages a retriever to fetch bug-fix pairs similar to the target bug from a retrieval database, providing contextual hints to guide the LLMs in the repair process. We then investigate two techniques to retrieve bug-fix pairs associated with the function to be fixed: BM25 and Dense Passage Retrieval (DPR). After retrieving the relevant bug-fix pair, we construct a prompt and integrate the retrieved pair into it. Besides, we also compare the proposed RAG-based approach with the parameter-efficient fine-tuning (PEFT) approaches on repair performance. To validate the effectiveness of ReAPR, we conduct extensive experiments based on the widely-used benchmark dataset Defects4j 2.0 as well as the latest benchmark GitBug-Java. The results show that ReAPR, based on the CodeLlama(7B) backbone, successfully fixes 68 and 59 bugs in the DPR and BM25 settings, respectively, in Defects4j 2.0, outperforming the best baseline approach by 18 and 9 bugs under the same repair settings.",
issn="1573-1367",
doi="10.1007/s11219-025-09728-1",
url="https://doi.org/10.1007/s11219-025-09728-1"
}


@Article{Li2025,
author="Li, Mengliang
and Shen, Qiang
and Ren, Xiaoxue
and Fu, Han
and Li, Zhuo
and Sun, Jianling",
title="HMF: Enhancing reentrancy vulnerability detection and repair with a hybrid model framework",
journal="Automated Software Engineering",
year="2025",
month="Sep",
day="13",
volume="33",
number="1",
pages="5",
abstract="Smart contracts have revolutionized the credit landscape. However, their security remains intensely scrutinized due to numerous hacking incidents and inherent logical challenges. One well-known issue is reentrancy vulnerability, exemplified by DAO attacks that lead to substantial economic losses. Previous approaches have employed rule-based and deep learning-based (DL) algorithms to detect and repair reentrancy vulnerability. Large language models (LLM) have been distinguished in recent years for their excellent understanding of text and code. However, less attention has been paid to LLM-based reentrancy vulnerability detection and repair, and direct prompt-based approaches often suffer from inefficiencies and high false positives. To overcome the above shortcomings, this paper proposes a hybrid model framework combining LLM with DL to enhance the detection and repair of reentrancy vulnerabilities. This unified framework comprises three crucial phases: the data processing phase, the vulnerability detection phase, and the vulnerability repair phase. Extensive experimental results validate the superiority of our approach over state-of-the-art baselines, and ablation studies demonstrate the effectiveness of each component. Our approach demonstrates significant improvements in vulnerability detection, with increases of 3.51{\%} in accuracy, 2.31{\%} in recall, 0.42{\%} in precision, and 0.85{\%} in F1-score. Furthermore, our approach can achieve a notable 9.62{\%} enhancement in the repair rate. Finally, we also conducted a user study to emphasize its potential to fortify the security of smart contracts.",
issn="1573-7535",
doi="10.1007/s10515-025-00546-0",
url="https://doi.org/10.1007/s10515-025-00546-0"
}


@Article{Rong2025,
author="Rong, Yi
and Du, Tianfeng
and Li, Roubing
and Bao, Wenting",
title="Integrating LLM-based code optimization with human-like exclusionary reasoning for computational education",
journal="Journal of King Saud University Computer and Information Sciences ",
year="2025",
month="Jun",
day="23",
volume="37",
number="5",
pages="87",
abstract="Large Language Models (LLMs) are increasingly deployed as intelligent tutors that not only generate but also refine source code for educational purposes. Yet existing end-to-end fine-tuning strategies compel models to transform every input, often introducing superfluous or even detrimental edits that undermine both software quality and pedagogical clarity. We address this limitation by formulating exclusionary reasoning-the human practice of asking ``Should I optimize?'' before acting-as an explicit decision layer in the code-optimization pipeline. Concretely, we devise a two-stage framework in which an LLM first diagnoses whether a code segment merits modification and proceeds with optimization only when necessary, otherwise returning the original snippet verbatim. Implemented on a suite of open-source models and trained with publicly available Python corpora, our method proves model-agnostic and lightweight. Experiments on three standard benchmarks show consistent gains in functional correctness (pass@1/3/5) over conventional fine-tuning, yielding feedback that is both more accurate and easier for students to interpret. By aligning automated optimization with human selective judgment, the proposed framework transforms LLMs from indiscriminate code generators into credible virtual teaching assistants that intervene sparingly, explain clearly, and foster deeper learning of principled programming practices.",
issn="2213-1248",
doi="10.1007/s44443-025-00074-7",
url="https://doi.org/10.1007/s44443-025-00074-7"
}


@Article{TaghaviFar2025,
author="Taghavi Far, Seyed Mohammad
and Feyzi, Farid",
title="Large language models for software vulnerability detection: a guide for researchers on models, methods, techniques, datasets, and metrics",
journal="International Journal of Information Security",
year="2025",
month="Feb",
day="14",
volume="24",
number="2",
pages="78",
abstract="Large language models (LLMs) have emerged as transformative tools in the domain of software vulnerability detection and management, offering sophisticated capabilities in identifying, analyzing, and mitigating security risks. This article delves into the utilization of LLMs, examining their role in revolutionizing traditional approaches to software vulnerability detection. We explore the various categories of LLMs, such as bidirectional encoder representations from transformers (BERT) and generative pre-trained transformer (GPT), and how these models are being leveraged to improve the accuracy and efficiency of vulnerability detection. This article reviews how LLMs are being integrated into existing software security frameworks, synthesizing research findings on their performance in various contexts. It includes insights into how LLM-based methods complement traditional techniques like static analysis and fuzz testing, without engaging in a direct comparative analysis of these approaches. The comparison highlights the strengths of LLMs, such as their ability to generalize across diverse codebases and programming languages, while also addressing their limitations, such as susceptibility to biases from training data and the hallucination. The article synthesizes findings from recent research, showcasing how LLMs have been successfully employed to detect a range of vulnerabilities, from buffer overflows to SQL injections, and outlines how these models enhance productivity by automating the detection and reporting of security flaws. Additionally, we discuss the inherent challenges in applying LLMs to software vulnerability detection, such as the need for high-quality datasets, and the ethical implications related to the deployment of LLM-based systems in security-critical applications. Addressing these challenges is crucial for the future advancement of LLM technologies in the cybersecurity domain. A comprehensive introduction to foundational and specialized datasets is provided, including datasets such as CVEfixes, Big-Vul, and LineVul, which are tailored for software vulnerability detection. These datasets serve as crucial resources for training and benchmarking LLMs. Moreover, we introduce evaluation metrics such as F1-score, precision, recall, and AUC-ROC that are used to assess the performance of models in detecting and mitigating vulnerabilities, offering a structured way to gauge the success and limitations of LLMs. In addition, the article explores fine-tuning techniques such as full fine-tuning, feature extraction, adapter-based fine-tuning, and LoRA (low-rank adaptation), highlighting how each method can enhance LLM performance in vulnerability detection. By focusing on parameter-efficient fine-tuning approaches, such as adapter layers and prefix-tuning, and LoRa, we outline ways to optimize model performance while reducing computational overhead. By providing a comprehensive review of the literature and practical insights into LLM integration, this article aims to fill the gap in existing research and serve as a foundational guide for future investigations. Researchers and practitioners in the field of software security will benefit from the comparative analyses, detailed case studies, and strategic recommendations provided herein, which collectively highlight the potential of LLMs to complement and enhance traditional software vulnerability detection techniques.",
issn="1615-5270",
doi="10.1007/s10207-025-00992-7",
url="https://doi.org/10.1007/s10207-025-00992-7"
}


@Article{Zhang2026,
author="Zhang, Chengming
and Wang, Haoye
and Xu, Chuyang
and Liu, Jiakun
and Liu, Kui
and Liu, Zhongxin",
title="Can test cases generated by large language models facilitate automated program repair?",
journal="Empirical Software Engineering",
year="2026",
month="Feb",
day="07",
volume="31",
number="3",
pages="68",
abstract="Automated program repair (APR) is proposed to reduce manual debugging efforts by automatically fixing buggy programs. Traditional APR techniques rely heavily on test cases, categorized into trigger-based and trigger-free approaches. While trigger-based methods achieve higher accuracy, their dependence on well-established test suites limits real-world applicability. Trigger-free methods, though more flexible, suffer from inferior fault localization performance due to the absence of trigger tests. Recent advances leverage large language models (LLMs) to generate bug-reproducing test cases, yet their systematic integration across the APR pipeline remains unexplored. This paper presents the first comprehensive study on leveraging LLM-generated tests throughout the APR workflow. We conduct experiments on 374 single-function bugs from Defects4J, systematically evaluating the impact of LLM-generated tests on fault localization, patch generation, and patch validation. Key findings reveal that: (1) Even LLM-generated tests with incorrect assertions can enhance fault localization for trigger-free APR by providing supplementary execution traces, improving Top@1 bug detection by 61{\%} ; (2) Incorporating these tests into patch ranking boosts repair effectiveness by 12.5{\%}--24.6{\%} across Top@1--Top@5 metrics. Then we propose a novel APR framework GT-Repair which incorporates LLM-generated tests into different repair stages. Compared to the trigger-free APR pipeline without LLM-generated tests, GT-Repair achieves a 38.8{\%} improvement in Top@1 repair performance. Besides, GT-Repair achieves state-of-the-art performance on Defects4J in trigger-free scenarios, compared to state-of-the-art test-based approaches. This work demonstrates the feasibility of LLM-generated tests in APR, and provides actionable insights for future works.",
issn="1573-7616",
doi="10.1007/s10664-026-10802-w",
url="https://doi.org/10.1007/s10664-026-10802-w"
}


@Article{Omari2024,
author="Omari, Safwan
and Basnet, Kshitiz
and Wardat, Mohammad",
title="Investigating large language models capabilities for automatic code repair in Python",
journal="Cluster Computing",
year="2024",
month="Nov",
day="01",
volume="27",
number="8",
pages="10717--10731",
abstract="Developers often encounter challenges with their introductory programming tasks as part of the development process. Unfortunately, rectifying these mistakes manually can be time-consuming and demanding. Automated program repair (APR) techniques offer a potential solution by synthesizing fixes for such errors. Previous research has investigated the utilization of both symbolic and neural techniques within the APR domain. However, these approaches typically demand significant engineering efforts or extensive datasets and training. In this paper, we explore the potential of using a large language model trained on code, specifically, we assess ChatGPT's capability to detect and repair bugs in simple Python programs. The experimental evaluation encompasses two benchmarks: QuixBugs and Textbook. Each benchmark consists of simple Python functions that implement well-known algorithms and each function contains a single bug. To gauge repair performance in various settings, several benchmark variations were introduced including addition of plain English documentation and code obfuscation. Based on thorough experiments, we found that ChatGPT was able to correctly detect and fix about 50{\%} of the methods, when code is documented. Repair performance drops to 25{\%} when code is obfuscated, and 15{\%} when documentation is removed and code is obfuscated. Furthermore, when compared to existing APR systems, ChatGPT considerably outperformed them.",
issn="1573-7543",
doi="10.1007/s10586-024-04490-8",
url="https://doi.org/10.1007/s10586-024-04490-8"
}


@Article{Zhang2025,
author="Zhang, Shaobo
and Wang, Qianzhi
and Liu, Qin
and Luo, Entao
and Peng, Tao",
title="VulTrLM: LLM-assisted vulnerability detection via AST decomposition and comment enhancement",
journal="Empirical Software Engineering",
year="2025",
month="Nov",
day="05",
volume="31",
number="1",
pages="10",
abstract="Software vulnerability detection is a critical task in software engineering to ensure system security. Recently, pre-trained models have shown great potential in automating vulnerability detection. However, existing pre-trained model-based vulnerability detection approaches still suffer from the following issues: (1) They struggle to accurately parse the complex expressions with multiple operators and pointers. (2) They fail to capture various code execution sequences effectively. To mitigate these issues, we propose VulTrLM, a vulnerability detection framework that decomposes Abstract Syntax Trees (ASTs) with the assistance of Large Language Models (LLMs). To improve code semantics, VulTrLM leverages an improved decomposition algorithm to partition the AST into multiple subtrees, each paired with comments generated by LLMs. To enhance execution sequence sensitivity, we designed a Transformer-based semantic aggregator, which models relationships between subtrees and merges their features into a target vulnerability vector. Finally, VulTrLM incorporates the vulnerability vector to effectively capture vulnerability patterns. Experimental results showed that VulTrLM achieves a higher F1 score than baselines with improvements of 1.87{\%} on FFMPeg+Qemu, 4.82{\%} on Reveal, and 12.46{\%} on SVulD. Moreover, VulTrLM can remain effective across different pre-trained models, and the ablation studies also validate the effectiveness of VulTrLM's core design.",
issn="1573-7616",
doi="10.1007/s10664-025-10738-7",
url="https://doi.org/10.1007/s10664-025-10738-7"
}


@inproceedings{10.1007/978-981-96-0055-7_14,
 abstract = {The extensive utilization of Large Language Models (LLMs) has significantly influenced academic research in mobile device-related fields, encompassing application testing, malware detection, voice control, and software development enhancement. Concurrently, the increasing demand for user access to LLMs on mobile devices for tasks like question answering has introduced new research directions, such as developing native LLMs by reducing parameter sizes. We aim to study the relationship between the evolution of LLMs and mobile device-related research, exploring their integration into traditional tasks and their adaptation to mobile platforms, from early transformer-based models to modern architectures like GPT-4. We have reviewed 55 recent papers, including 50 novel approaches, 1 benchmark, and 4 empirical studies, covering various aspects of LLM applications in mobile devices.},
 address = {Singapore},
 author = {Chen, Chong
and Wang, Bo
and Lin, Youfang},
 booktitle = {Web and Big Data. APWeb-WAIM 2024 International Workshops},
 editor = {Zhang, Wenjie
and Tung, Anthony
and Zheng, Zhonglong
and Yang, Zhengyi
and Wang, Xiaoyang
and Guo, Hongjie},
 isbn = {978-981-96-0055-7},
 pages = {163--174},
 publisher = {Springer Nature Singapore},
 title = {A Systematic Mapping Study of LLM Applications in Mobile Device Research},
 url = {https://link.springer.com/chapter/10.1007/978-981-96-0055-7_14},
 year = {2025}
}

@Article{Hu2025,
author="Hu, Tianmin
and Fan, Zhenye
and Ye, Zhanbo
and Ye, Guixin
and He, Lu",
title="SSFuzz: Synthesizing and scheduling bug-triggering code segments for history-driven compiler testing",
journal="Empirical Software Engineering",
year="2025",
month="Dec",
day="05",
volume="31",
number="2",
pages="27",
abstract="History-driven fuzzing is a viable method to expose compiler bugs. It leverages historical information, such as historical test programs and historical execution information, to locate error-prone modules and conduct continuous testing. However, existing works are inefficient in utilizing historical information, resulting in a limited bug-triggering capability. In this work, we propose SSFuzz, a compiler fuzzer that synthesizes bug-triggering code segments into test cases and schedules both seed programs and code segments. SSFuzz collects bug-triggering code segments from bug-related pull requests and synthesizes them into test cases using different strategies. During the period of testing, SSFuzz uses an entropy-based scheduler to prioritize code segments based on the behavior of target JVMs, and retain test cases as seed programs according to the anomalous behavior feedback and code coverage feedback. During the bug submission phase, SSFuzz successfully uncovered 20 previously undetected bugs across all three target JVMs. Out of these, 14 have been verified, and 3 have already been fixed. Compared with state-of-the-art fuzzers, SSFuzz uncovers {\$}{\$}1.3 {\backslash}sim 3.1{\backslash}times{\$}{\$}more bugs and achieves the highest code coverage.",
issn="1573-7616",
doi="10.1007/s10664-025-10777-0",
url="https://doi.org/10.1007/s10664-025-10777-0"
}


@inproceedings{10.1007/978-981-96-1624-4_12,
 abstract = {Obfuscation is a method that safeguards intellectual property rights against malicious analysts by altering the structure, logic, and other aspects of a program. However, malicious developers utilize obfuscation methods in their malware to avoid detection and analysis. To deobfuscate malware, analysts leverage their analysis skills alongside deobfuscation methodology. Although obfuscation is widely used in malware, heuristic-based deobfuscation methodology has limitations, including reliance on specific obfuscation tools and inefficiency in large-scale processing. In this paper, we propose ChatDEOB, an effective deobfuscation method that utilizes a Large Language Model (LLM). We focus on the LLM's application in various software engineering areas, such as code analysis, generation, and fuzzing, and employ it in our deobfuscation method. To effectively deobfuscate, we fine-tune the LLM model in detail and implement ChatDEOB using well-designed prompt engineering methods. To the best of our knowledge, ChatDEOB is the first method to deobfuscate code using a fine-tuned LLM model. To demonstrate the effectiveness of ChatDEOB, we utilize SacreBLEU, a published obfuscation evaluation method, along with the Obfuscation Quality Quantification Framework. The experiment resulted in the SacreBLEU score increasing from an initial average of 22.71 to 49.12, achieving a 116.27{\%} improvement and demonstrating significant effectiveness. Additionally, when measuring the six evaluation indicators of the Obfuscation Quality Quantification Framework, the deobfuscation effect shows an average improvement of 85{\%} compared to the obfuscated code.},
 address = {Singapore},
 author = {Choi, Byunggeon
and Jin, Hongjoo
and Lee, Dong Hoon
and Choi, Wonsuk},
 booktitle = {Information Security Applications},
 editor = {Lee, Jong-Hyouk
and Emura, Keita
and Lee, Sokjoon},
 isbn = {978-981-96-1624-4},
 pages = {151--163},
 publisher = {Springer Nature Singapore},
 title = {ChatDEOB: An Effective Deobfuscation Method Based on Large Language Model},
 url = {https://link.springer.com/chapter/10.1007/978-981-96-1624-4_12},
 year = {2025}
}

@inproceedings{10.1007/978-3-032-09321-9_9,
 abstract = {This paper explores research on Large Language Models (LLMs) integration into a DevSecOps pipeline for vulnerability detection in C/C++ code. We propose a two-stage model combining LLM-generated embeddings with a binary classifier trained to identify 18 Common Weakness Enumeration (CWE) categories. The model was deployed inside a GitHub Actions workflow, where it ran in parallel with standard SAST tools, analyzing code changes on a per-function basis. We conducted an empirical evaluation using over 500 labeled functions from two vulnerability datasets. Our results show that LLM-enhanced detection improves recall for complex vulnerabilities, outperforming traditional SAST tools in cases requiring higher-level reasoning. However, increased false-positive rates and computational costs introduce practical trade-offs. We discuss the implications of LLM integration in CI/CD environments, including pipeline optimization strategies, developer feedback mechanisms, and potential security risks associated with proprietary LLM models. This study contributes a framework for leveraging AI-driven security checks in modern DevSecOps pipelines, enhancing both security posture and developer efficiency.},
 address = {Cham},
 author = {Kisielewicz, Maciej
and Kotzbach, Pawe{\l}
and K{\k{e}}dziora, Micha{\l}},
 booktitle = {Computational Collective Intelligence},
 editor = {Nguyen, Ngoc Thanh
and Dinh Duc Anh, Vu
and Kozierkiewicz, Adrianna
and Nguyen Van, Sinh
and N{\'u}{\~{n}}ez, Manuel
and Treur, Jan
and Vossen, Gottfried},
 isbn = {978-3-032-09321-9},
 pages = {124--137},
 publisher = {Springer Nature Switzerland},
 title = {Enhancing DevSecOps Through Large Language Model Integration: A Pipeline-Centric Approach},
 url = {https://link.springer.com/chapter/10.1007/978-3-032-09321-9_9},
 year = {2026}
}

@inproceedings{10.1007/978-981-96-4509-1_28,
 abstract = {Automated Program Repair (APR) is currently receiving increasing attention in the field of software engineering because more and more bugs are emerging with the continuous expansion of program size. Recently, Large Language Models (LLMs) have got an outstanding progress in automated debugging field. Some researches utilized Language Models that pre-trained on source code, such as UnixCoder, Incoder, etc., to generate patches for bugs, and achieved impressive performance. Noteworthy, large language models are proved to benefit from in-context learning (ICL). By receiving well designed prompt, models can understand user needs better and provide a more accurate response. However, we find that existing prompted LLM-based APR methods either used too simple prompts or too complex ones. Therefore, this study explores and validates the proper structure of prompt that helps LLM fix more bugs. In this paper, we present a medium length prompt format to enhance CodeLlama's repairing capability. Specifically, we design prompt that include three part: role-profile, bug informations and repair instruction. For each bug, we extract their corresponding informations such as function description and bug location, fill in the prompt frame, and finally send it to CodeLlama to generate a patch. Our experiments show that the proper length of prompt can help CodeLlama model fix more bugs correctly.},
 address = {Singapore},
 author = {Liang, HanZhao
and Gao, Cuiyun
and Jia, Yan},
 booktitle = {Cyberspace Simulation and Evaluation},
 editor = {Xu, Guangxia
and Zhou, Wanlei
and Zhang, Jiawei
and Zhang, Yanchun
and Jia, Yan},
 isbn = {978-981-96-4509-1},
 pages = {413--427},
 publisher = {Springer Nature Singapore},
 title = {Exploration Study About LLM with Proper Prompt in Automated Program Repair},
 url = {https://link.springer.com/chapter/10.1007/978-981-96-4509-1_28},
 year = {2025}
}

@Article{Yousofvand2025,
author="Yousofvand, Leila
and Soleimani, Seyfollah
and Rafe, Vahid
and Nikanjam, Amin",
title="Graph neural networks for precise bug localization through structural program analysis",
journal="Automated Software Engineering",
year="2025",
month="Oct",
day="18",
volume="33",
number="1",
pages="17",
abstract="Bug localization (BL) is known as one of the major steps in the program repair process, which generally seeks to find a set of commands causing a program to crash or fail. At the present time, locating bugs and their sources quickly seems to be impossible as the complexity of modern software development and scaling is soaring. Accordingly, there is a huge demand for BL techniques with minimal human intervention. A graph representing source code typically encodes valuable information about both the syntactic and semantic structures of programs. Many software bugs are associated with these structures, making graphs particularly suitable for bug localization (BL). Therefore, the key contributions of this work involve labeling graph nodes, classifying these nodes, and addressing imbalanced classifications within the graph data structure to effectively locate bugs in code. A graph-based bug classifier is initially introduced in the method proposed in this paper. For this purpose, the program source codes are mapped to a graph representation. Since the graph nodes do not have labels, the Gumtree algorithm is then exploited to label them by comparing the buggy graphs and the corresponding bug-free ones. Afterward, a trained, supervised node classifier, developed based on a graph neural network (GNN), is applied to classify the nodes into buggy or bug-free ones. Given the imbalance in the data, accuracy, precision, recall, and F1-score metrics are used for evaluation. Experimental results on identical datasets show that the proposed method outperforms other related approaches. The proposed approach effectively localizes a broader spectrum of bug types, such as undefined properties, functional bugs, variable naming errors, and variable misuse issues.",
issn="1573-7535",
doi="10.1007/s10515-025-00556-y",
url="https://doi.org/10.1007/s10515-025-00556-y"
}


@inproceedings{10.1007/978-981-97-3442-9_34,
 abstract = {Fixing bugs in the wild is a challenging task. Automating it would help software development immensely. Recently, Deep Learning-based approaches, including Large Language Models, have made great progress in this direction. Unfortunately, not many of these models have been open sourced, and proprietary models often outperform open-sourced ones. In this paper we investigate whether the recently open-sourced model, CodeLlama, can be improved through fine-tuning to be competitive on the bugfixing task. We investigate two kinds of tasks: singular bugfixing and whole program bugfixing. For fine-tuning, we use code from GitHub for the former and Project CodeNet published by IBM Research for the latter. The base and fine-tuned models are evaluated on the QuixBugs dataset for singular bugfixing and a collection of incorrect assignment submissions coded by students for whole program bugfixing. We find that our fine-tuned models outperform ChatGPT (GPT-3.5) on singular bugfixing. On whole program bugixing, they come close in the number of incorrect programs fixed while corrupting only half as many.},
 address = {Singapore},
 author = {Szalontai, Bal{\'a}zs
and Vad{\'a}sz, Andr{\'a}s
and M{\'a}rton, Tam{\'a}s
and Pint{\'e}r, Bal{\'a}zs
and Gregorics, Tibor},
 booktitle = {Proceedings of International Conference on Recent Innovations in Computing},
 editor = {Ill{\'e}s, Zolt{\'a}n
and Verma, Chaman
and Gon{\c{c}}alves, Paulo J. Sequeira
and Singh, Pradeep Kumar},
 isbn = {978-981-97-3442-9},
 pages = {497--509},
 publisher = {Springer Nature Singapore},
 title = {Fine-Tuning CodeLlama to Fix Bugs},
 url = {https://link.springer.com/chapter/10.1007/978-981-97-3442-9_34},
 year = {2024}
}

@Article{Zhang2025,
author="Zhang, Jie
and Bu, Haoyu
and Wen, Hui
and Liu, Yongji
and Fei, Haiqiang
and Xi, Rongrong
and Li, Lun
and Yang, Yun
and Zhu, Hongsong
and Meng, Dan",
title="When LLMs meet cybersecurity: a systematic literature review",
journal="Cybersecurity",
year="2025",
month="Feb",
day="05",
volume="8",
number="1",
pages="55",
abstract="The rapid development of large language models (LLMs) has opened new avenues across various fields, including cybersecurity, which faces an evolving threat landscape and demand for innovative technologies. Despite initial explorations into the application of LLMs in cybersecurity, there is a lack of a comprehensive overview of this research area. This paper addresses this gap by providing a systematic literature review, covering the analysis of over 300 works, encompassing 25 LLMs and more than 10 downstream scenarios. Our comprehensive overview addresses three key research questions: the construction of cybersecurity-oriented LLMs, the application of LLMs to various cybersecurity tasks, the challenges and further research in this area. This study aims to shed light on the extensive potential of LLMs in enhancing cybersecurity practices and serve as a valuable resource for applying LLMs in this field. We also maintain and regularly update a list of practical guides on LLMs for cybersecurity at https://github.com/tmylla/Awesome-LLM4Cybersecurity.",
issn="2523-3246",
doi="10.1186/s42400-025-00361-w",
url="https://doi.org/10.1186/s42400-025-00361-w"
}


@Article{Brownlee2025,
author="Brownlee, Alexander E. I.
and Callan, James
and Even-Mendoza, Karine
and Geiger, Alina
and Hanna, Carol
and Petke, Justyna
and Sarro, Federica
and Sobania, Dominik",
title="Large language model based mutations in genetic improvement",
journal="Automated Software Engineering",
year="2025",
month="Jan",
day="21",
volume="32",
number="1",
pages="15",
abstract="Ever since the first large language models (LLMs) have become available, both academics and practitioners have used them to aid software engineering tasks. However, little research as yet has been done in combining search-based software engineering (SBSE) and LLMs. In this paper, we evaluate the use of LLMs as mutation operators for genetic improvement (GI), an SBSE approach, to improve the GI search process. In a preliminary work, we explored the feasibility of combining the Gin Java GI toolkit with OpenAI LLMs in order to generate an edit for the JCodec tool. Here we extend this investigation involving three LLMs and three types of prompt, and five real-world software projects. We sample the edits at random, as well as using local search. We also conducted a qualitative analysis to understand why LLM-generated code edits break as part of our evaluation. Our results show that, compared with conventional statement GI edits, LLMs produce fewer unique edits, but these compile and pass tests more often, with the OpenAI model finding test-passing edits 77{\%} of the time. The OpenAI and Mistral LLMs are roughly equal in finding the best run-time improvements. Simpler prompts are more successful than those providing more context and examples. The qualitative analysis reveals a wide variety of areas where LLMs typically fail to produce valid edits commonly including inconsistent formatting, generating non-Java syntax, or refusing to provide a solution.",
issn="1573-7535",
doi="10.1007/s10515-024-00473-6",
url="https://doi.org/10.1007/s10515-024-00473-6"
}


@Article{Zheng2024,
author="Zheng, Zibin
and Ning, Kaiwen
and Zhong, Qingyuan
and Chen, Jiachi
and Chen, Wenqing
and Guo, Lianghong
and Wang, Weicheng
and Wang, Yanlin",
title="Towards an understanding of large language models in software engineering tasks",
journal="Empirical Software Engineering",
year="2024",
month="Dec",
day="26",
volume="30",
number="2",
pages="50",
abstract="Large Language Models (LLMs) have drawn widespread attention and research due to their astounding performance in text generation and reasoning tasks. Derivative products, like ChatGPT, have been extensively deployed and highly sought after. Meanwhile, the evaluation and optimization of LLMs in software engineering tasks, such as code generation, have become a research focus. However, there is still a lack of systematic research on applying and evaluating LLMs in software engineering. Therefore, this paper comprehensively investigate and collate the research and products combining LLMs with software engineering, aiming to answer two questions: (1) What are the current integrations of LLMs with software engineering? (2) Can LLMs effectively handle software engineering tasks? To find the answers, we have collected related literature as extensively as possible from seven mainstream databases and selected 123 timely papers published starting from 2022 for analysis. We have categorized these papers in detail and reviewed the current research status of LLMs from the perspective of seven major software engineering tasks, hoping this will help researchers better grasp the research trends and address the issues when applying LLMs. Meanwhile, we have also organized and presented papers with evaluation content to reveal the performance and effectiveness of LLMs in various software engineering tasks, guiding researchers and developers to optimize.",
issn="1573-7616",
doi="10.1007/s10664-024-10602-0",
url="https://doi.org/10.1007/s10664-024-10602-0"
}


@Article{Chen2025,
author="Chen, Li-Guo
and Xiao, Zheng
and Xu, Yi-Jiang
and An, Rui-Chuan
and Wang, Xin
and Li, Yang-Ning
and Li, Ying-Hui
and Wang, Yi-Dong
and Zeng, Zheng-Ran
and Gao, Qing
and Zhang, Shi-Kun",
title="CodeRankEval: Benchmarking and Analyzing LLM Performance for Code Ranking",
journal="Journal of Computer Science and Technology",
year="2025",
month="Sep",
day="01",
volume="40",
number="5",
pages="1220--1233",
abstract="Large language models (LLMs) are increasingly applied across diverse software engineering tasks. Consequently, their ability to effectively rank code quality is crucial for applications like selecting optimal solutions and aiding code review. However, evaluating this essential code ranking capability is hampered by a lack of benchmarks covering diverse paradigms and robustness testing. To address this, we introduce CodeRankEval, a benchmark suite for multi-paradigm evaluation, and CodeRankEval-Perturbed for robustness testing against common code flaws. Our empirical study reveals key insights: pairwise ranking yields the highest accuracy but is costly; listwise is the cheapest and shows comparable performance with pairwise; pointwise generally exhibits lower performance with intermediate cost. Besides, ranking ability correlates positively with generation ability, models show reasonable robustness to perturbations but may exhibit positional bias. Overall, this work provides valuable resources and insights for understanding and improving LLM-based code ranking evaluation.",
issn="1860-4749",
doi="10.1007/s11390-025-5514-9",
url="https://doi.org/10.1007/s11390-025-5514-9"
}


@Article{Dikici2025,
author="Dikici, Sena
and Bilgin, Turgay Tugay",
title="Advancements in automated program repair: a comprehensive review",
journal="Knowledge and Information Systems",
year="2025",
month="Jun",
day="01",
volume="67",
number="6",
pages="4737--4783",
abstract="This review paper presents a comprehensive examination of automated program repair (APR) and its significant contribution to the field of modern software engineering. It elucidates how APR methodologies markedly mitigate manual debugging needs by automating the detection and resolution of software glitches. The study encompasses an in-depth exploration of three primary categories of APR techniques: template-based, machine learning, and deep learning approaches, drawing from an exhaustive evaluation of 41 APR tools. Each category showcases distinct strategies for managing diverse software errors, underscoring the breadth and effectiveness of current APR methodologies. Template-based APR solutions utilize pre-established patterns to efficiently tackle common coding issues, while machine learning-driven approaches dynamically devise repair strategies from historical bug-fix datasets. Deep learning methods extend error rectification boundaries by delving into the semantic context of code, yielding more precise adjustments. The ongoing advancement of APR technologies necessitates researchers to address critical challenges, including the integration of semantic-syntactic analyses, mitigation of data scarcity, optimization of cross-platform tools, development of context-aware approaches, enhancement of fault localization and patch validation processes, and establishment of standardized performance evaluation metrics. This comprehensive analysis underscores the pivotal role of APR in enhancing software efficiency and reliability, representing significant progress in software development and maintenance practices.",
issn="0219-3116",
doi="10.1007/s10115-025-02383-9",
url="https://doi.org/10.1007/s10115-025-02383-9"
}


@Inbook{Tihanyi2026,
author="Tihanyi, Norbert
and Bisztray, Tamas
and Ferrag, Mohamed Amine
and Cherif, Bilel
and Dubniczky, Richard A.
and Jain, Ridhi
and Cordeiro, Lucas C.",
editor="Nowroozi, Ehsan
and Taheri, Rahim
and Cordeiro, Lucas",
title="Vulnerability Detection: From Formal Verification to Large Language Models and Hybrid Approaches: A Comprehensive Overview",
bookTitle="Adversarial Example Detection and Mitigation Using Machine Learning",
year="2026",
publisher="Springer Nature Switzerland",
address="Cham",
pages="33--47",
abstract="Software testing and verification are critical for ensuring the reliability and security of modern software systems. Traditionally, formal verification techniques, such as model checking and theorem proving, have provided rigorous frameworks for detecting bugs and vulnerabilities. However, these methods often face scalability challenges when applied to complex, real-world programs. Recently, the advent of Large Language Models (LLMs) has introduced a new paradigm for software analysis, leveraging their ability to understand insecure coding practices. Although LLMs demonstrate promising capabilities in tasks such as bug prediction and invariant generation, they lack the formal guarantees of classical methods. This paper presents a comprehensive study of state-of-the-art software testing and verification, focusing on three key approaches: classical formal methods, LLM-based analysis, and emerging hybrid techniques, which combine their strengths. We explore each approach's strengths, limitations, and practical applications, highlighting the potential of hybrid systems to address the weaknesses of standalone methods. We analyze whether integrating formal rigor with LLM-driven insights can enhance the effectiveness and scalability of software verification, exploring their viability as a pathway toward more robust and adaptive testing frameworks.",
isbn="978-3-031-99447-0",
doi="10.1007/978-3-031-99447-0_3",
url="https://doi.org/10.1007/978-3-031-99447-0_3"
}


@inproceedings{10.1007/978-981-95-3543-9_28,
 abstract = {In recent years, cyberattacks have been on the rise globally, causing substantial economic losses and severe social risks to governments, enterprises, and other organizations. Against this backdrop, Cyber Threat Intelligence (CTI) has emerged as a crucial resource for combating cyberattacks, with its importance increasingly highlighted. However, the unstructured nature of CTI data poses challenges for manual extraction of valuable information. Therefore, Named Entity Recognition (NER) based on CTI has become a key technology for the prevention and response to cyberattacks. Although deep learning-based NER models have achieved remarkable success in many fields, their application in the cybersecurity domain has been relatively slow due to the high specialization of CTI and the scarcity of labeled data. To address this, this paper proposes a CTI Named Entity Recognition framework for low-resource scenarios---CyberNER-LLM. This framework effectively integrates LLMs with NER through a redesigned output format and achieves efficient and accurate extraction of cybersecurity entities through two stages: initial entity recognition and model self-validation. Experimental results show that, supported by a 3B-scale LLM, CyberNER-LLM significantly outperforms traditional NER models.},
 address = {Singapore},
 author = {Liu, Xinzheng
and Lin, Wangqun
and Ding, Zhaoyun},
 booktitle = {Information and Communications Security },
 editor = {Han, Jinguang
and Xiang, Yang
and Cheng, Guang
and Susilo, Willy
and Chen, Liquan},
 isbn = {978-981-95-3543-9},
 pages = {513--530},
 publisher = {Springer Nature Singapore},
 title = {CyberNER-LLM: Cyber Threat Intelligence Named Entity Recognition With Large Language Model},
 url = {https://link.springer.com/chapter/10.1007/978-981-95-3543-9_28},
 year = {2026}
}

@Article{Hemberg2024,
author="Hemberg, Erik
and Moskal, Stephen
and O'Reilly, Una-May",
title="Evolving code with a large language model",
journal="Genetic Programming and Evolvable Machines",
year="2024",
month="Sep",
day="12",
volume="25",
number="2",
pages="21",
abstract="Algorithms that use Large Language Models (LLMs) to evolve code arrived on the Genetic Programming (GP) scene very recently. We present LLM{\_}GP, a general LLM-based evolutionary algorithm designed to evolve code. Like GP, it uses evolutionary operators, but its designs and implementations of those operators significantly differ from GP's because they enlist an LLM, using prompting and the LLM's pre-trained pattern matching and sequence completion capability. We also present a demonstration-level variant of LLM{\_}GP and share its code. By presentations that range from formal to hands-on, we cover design and LLM-usage considerations as well as the scientific challenges that arise when using an LLM for genetic programming.",
issn="1573-7632",
doi="10.1007/s10710-024-09494-2",
url="https://doi.org/10.1007/s10710-024-09494-2"
}


@Article{Su2026,
author="Su, Huading
and Xu, Zhen
and Zhang, Yan
and Tan, Qian",
title="Source code vulnerability detection based on deep learning: a review",
journal="Cybersecurity",
year="2026",
month="Jan",
day="04",
volume="9",
number="1",
pages="2",
abstract="Source code vulnerability detection is a crucial aspect of software security development, and the current use of Large Language Models (LLMs) accelerates not only software development but also the generation and propagation of code vulnerabilities. Traditional code vulnerability detection techniques have limited detection efficiency and accuracy. Deep learning techniques have recently gained distinct advantages in multidimensional feature extraction and large-scale data processing, and their application in code vulnerability detection is evolving from simple classification to multimodal approaches. This paper primarily systematizes and summarises deep learning-based source code vulnerability detection, as well as analyzes and anticipates current challenges and future research directions in this area. The distinction between this review and the preceding reviews: This study investigates the literature of the last four years; Not only does it contain datasets, but also includes model-related research and an analysis of multiple different application scenarios. It's more current and comprehensive than most previous reviews of this type.",
issn="2523-3246",
doi="10.1186/s42400-025-00518-7",
url="https://doi.org/10.1186/s42400-025-00518-7"
}


@Article{Xu2025,
author="Xu, Weikai
and Huang, Chengrui
and Gao, Shen
and Shang, Shuo",
title="LLM-Based Agents for Tool Learning: A Survey",
journal="Data Science and Engineering",
year="2025",
month="Dec",
day="01",
volume="10",
number="4",
pages="533--563",
abstract="Human beings capable of making and using tools can accomplish tasks far beyond their innate abilities, and this paradigm of integration with tools may not be limited to humans themselves. Recently, the large language model (LLM) has demonstrated immense potential across various fields with its unique planning and reasoning abilities. However, there are still many challenges beyond its capabilities due to deficiencies in its training data and inherent illusions. Thus, integrating LLMs and tools into tool learning agents has become a new emerging research direction. To this end, we present a systematic investigation and comprehensive review of tool-learning agents in this paper. We start by introducing the definition of the tool learning task for Agents and then illustrating the  typical architecture of the tool-learning models. Since these tools are all defined by users, LLM does not know what tools there are and what their functions are. Thus, LLMs should first find appropriate tools and split the tool retrieval methods into two categories: training-based and non-training-based. To accurately complete the user task, it is important to decompose the task into several sub-tasks and execute them in the correct order. Following that, we introduce the tool planning methods and organize these works by whether they rely on the model's inherent reasoning capabilities for planning or utilize external reasoning tools. Due to the rapid development of this field, we also introduce an emerging frontier direction: using multimodal tools for LLM. In addition, we compile current open-source benchmarks and evaluation metrics, focusing on their scale, composition, calculation methods, and assessment dimensions. Next, we introduce several application scenarios for the LLM-based tool learning methods. Finally, we discuss the safety and ethical issues involved in tool learning.",
issn="2364-1541",
doi="10.1007/s41019-025-00296-9",
url="https://doi.org/10.1007/s41019-025-00296-9"
}


@inproceedings{10.1007/978-3-031-97576-9_6,
 abstract = {Bug localization and semantic code search within large software repositories is a significant and time-consuming challenge for developers, particularly when dealing with bug reports from end-users who lack technical expertise. Traditional similarity-based code search methods struggle with the inherent domain and vocabulary mismatch between end-user reports and codebase semantics, while directly applying Large Language Models (LLMs) is hampered by their limited context windows and lack of repository-level understanding. To address these limitations, this paper introduces a novel, structure-aware methodology for creating repository-aware LLMs using hierarchical summarization. Our approach comprises a pre-processing phase that constructs an abstract repository tree, creates a context-aware LLM primed with project knowledge, and generates hierarchical summaries at project, directory, and file levels. The inference phase employs a top-down search strategy, guiding the LLM to progressively narrow down the search space from directory-level to file-level, effectively localizing bug-relevant code. This method mitigates the context window bottleneck and leverages LLMs' semantic understanding to overcome domain gap issues. Evaluated on a real-world dataset of Jira issues from a large-scale industrial project, our approach significantly outperforms both Flat Retrieval baselines and state-of-the-art LLM + Retrieval-Augmented Generation (RAG) systems, achieving a Pass@10 of 0.89 and Recall@10 of 0.33. The results demonstrate the efficacy of hierarchical summarization in enabling scalable, task-agnostic, and structure-aware repository-level code comprehension for improved bug localization and code search, particularly in scenarios involving non-technical end-user bug reports.},
 address = {Cham},
 author = {Oskooei, Amirkia Rafiei
and Yukcu, Selcan
and Bozoglan, Mehmet Cevheri
and Aktas, Mehmet S.},
 booktitle = {Computational Science and Its Applications -- ICCSA 2025 Workshops},
 editor = {Gervasi, Osvaldo
and Murgante, Beniamino
and Garau, Chiara
and Karaca, Yeliz
and Faginas Lago, Maria Noelia
and Scorza, Francesco
and Braga, Ana Cristina},
 isbn = {978-3-031-97576-9},
 pages = {88--105},
 publisher = {Springer Nature Switzerland},
 title = {Repository-Level Code Understanding by LLMs via Hierarchical Summarization: Improving Code Search and Bug Localization},
 url = {https://link.springer.com/chapter/10.1007/978-3-031-97576-9_6},
 year = {2026}
}

@Article{Deng2025,
author="Deng, Liangjun
and Zhong, Qi
and Qiu, Yao
and Chen, Jingxue
and Lei, Hang
and Yang, Shunkun
and Zhou, Liming
and Cheng, Hongyuan",
title="LLM-based program analysis for source codes, abstract syntax trees and webassembly instructions",
journal="Cluster Computing",
year="2025",
month="Sep",
day="29",
volume="28",
number="14",
pages="892",
abstract="The advancement of Web3.0 technology has brought about a urgent need for ensuring the safety and reliability of the software systems. Program analysis, a crucial aspect of software security research need a unified solution for various cross-language program. Moreover, the previous studies regard the necessity of capturing structured features from the ASTs, commonly holding a conception that plain text and compiled binary instructions are challenging to represent and identify. This paper proposes three methods for extracting structural information to demonstrate that the underlying principles of source code and instructions align with those of an abstract syntax tree. These methods include (1) embedding program instruction files directly into natural language, (2) embedding formatted source code into natural language, and (3) embedding misformatted, non-compilable source code into natural language. We train large-scale language models (LLMs) to identify defect and non-defect of WebAssembly. Experiment results demonstrate that program instruction analysis surpasses traditional techniques, achieving state-of-the-art accuracy exceeding 98.1{\%}. Our study also suggests a practical approach of plain text embedding using a 7.65 billion LLM. Interestingly, misformatted source codes are readable to humans but un-compilable, and the accuracy remains above 98.63{\%}. This paper not only introduces novel instruction and plain text embedding approach for future program security analysis, but also provides new insights for subsequent research about the three program analysis forms of plain text, ASTs, and instructions.",
issn="1573-7543",
doi="10.1007/s10586-025-05557-w",
url="https://doi.org/10.1007/s10586-025-05557-w"
}


@Article{Sun2024,
author="Sun, Jiaze
and Yin, Zhiqiang
and Zhang, Hengshan
and Chen, Xiang
and Zheng, Wei",
title="Adversarial generation method for smart contract fuzz testing seeds guided by chain-based LLM",
journal="Automated Software Engineering",
year="2024",
month="Dec",
day="31",
volume="32",
number="1",
pages="12",
abstract="With the rapid development of smart contract technology and the continuous expansion of blockchain application scenarios, the security issues of smart contracts have garnered significant attention. However, traditional fuzz testing typically relies on randomly generated initial seed sets. This random generation method fails to understand the semantics of smart contracts, resulting in insufficient seed coverage. Additionally, traditional fuzz testing often ignores the syntax and semantic constraints within smart contracts, leading to the generation of seeds that may not conform to the syntactic rules of the contracts and may even include logic that violates contract semantics, thereby reducing the efficiency of fuzz testing. To address these challenges, we propose a method for adversarial generation for smart contract fuzz testing seeds guided by Chain-Based LLM, leveraging the deep semantic understanding capabilities of LLM to assist in seed set generation. Firstly, we propose a method that utilizes Chain-Based prompts to request LLM to generate fuzz testing seeds, breaking down the LLM tasks into multiple steps to gradually guide the LLM in generating high-coverage seed sets. Secondly, by establishing adversarial roles for the LLM, we guide the LLM to autonomously generate and optimize seed sets, producing high-coverage initial seed sets for the program under test. To evaluate the effectiveness of the proposed method, 2308 smart contracts were crawled from Etherscan for experimental purposes. Results indicate that using Chain-Based prompts to request LLM to generate fuzz testing seed sets improved instruction coverage by 2.94{\%} compared to single-step requests. The method of generating seed sets by establishing adversarial roles for the LLM reduced the time to reach maximum instruction coverage from 60 s to approximately 30 s compared to single-role methods. Additionally, the seed sets generated by the proposed method can directly trigger simple types of vulnerabilities (e.g., timestamp dependency and block number dependency vulnerabilities), with instruction coverage improvements of 3.8{\%} and 4.1{\%}, respectively.",
issn="1573-7535",
doi="10.1007/s10515-024-00483-4",
url="https://doi.org/10.1007/s10515-024-00483-4"
}


@Article{Alsofyani2025,
author="Alsofyani, May
and Wang, Liqiang",
title="Evaluating ChatGPT's strengths and limitations for data race detection in parallel programming via prompt engineering",
journal="The Journal of Supercomputing",
year="2025",
month="Apr",
day="23",
volume="81",
number="6",
pages="776",
abstract="Large Language Models have significantly advanced software engineering, enabling tasks like code comprehension and fault detection. However, their ability to detect complex bugs, such as data races in parallel programming, remains uncertain. Fault detection in parallel programming (Pthreads) requires a deep understanding of thread-based logic, as data races occur when threads access shared data concurrently without proper synchronization. This paper explores ChatGPT's potential in Pthreads fault detection by addressing three questions: (1) Can ChatGPT effectively debug parallel programming threads? (2) How can dialogue assist with the detection of faults? (3) How can prompt engineering help to improve ChatGPT's fault detection performance?. We examine advanced prompt engineering techniques, such as Zero-Shot, Few-Shot, Chain-of-Thought, and Retrieval-Augmented Generation prompts. Additionally, we introduce three hybrid prompting techniques to enhance performance, including Chain-of-Thought with Few-Shot Prompting, Retrieval-Augmented Generation with Few-Shot Prompting, and Prompt Chaining with Few-Shot Prompting, while evaluating ChatGPT's strengths and limitations for data race detection.",
issn="1573-0484",
doi="10.1007/s11227-025-07237-3",
url="https://doi.org/10.1007/s11227-025-07237-3"
}


@Article{Guo2025,
author="Guo, Wenjie
and Xue, Jingfeng
and Liu, Zeyang
and Han, Weijie
and Hu, Jingjing",
title="Malgta: large language model-based guided malware tactical analysis",
journal="The Journal of Supercomputing",
year="2025",
month="Jun",
day="19",
volume="81",
number="9",
pages="1049",
abstract="In High-Performance Computing (HPC) environments, a comprehensive understanding of cybersecurity threats and their underlying attack strategies is essential. However, current research predominantly focuses on maliciousness determination, typically emphasizing the code's operational behaviors rather than the attack strategies employed. The advancements in multimedia computing, particularly Large Language Models (LLMs), have paved the way for innovative solutions to the aforementioned bottleneck. This work proposes MalGTA (Guided Malware Tactical Analysis), an LLM-based system that automates ATT{\&}CK (Adversarial Tactics, Techniques, and Common Knowledge)-aligned malware tactical analysis through Cuckoo Sandbox-driven dynamic profiling. Specifically, we construct a multi-source knowledge base integrated with Retrieval-Augmented Generation (RAG), which mitigates hallucinations in LLMs through context-sensitive threat intelligence retrieval. In addition, we propose a query optimization strategy to address challenges related to input information overload and attention dispersion in LLMs, enabling context-aware data refinement from Cuckoo reports. Finally, this study conducts dynamic analysis on classical VirusShare and Advanced Persistent Threat (APT) samples and constructs an evaluation dataset based on the authoritative malware analysis platform HybridAnalysis. Experimental results show the effectiveness of the method. ",
issn="1573-0484",
doi="10.1007/s11227-025-07545-8",
url="https://doi.org/10.1007/s11227-025-07545-8"
}


@Article{Yu2026,
author="Yu, Fengrui
and Du, Yanhui",
title="VDM-IOG, a framework of inference on graph in retrieval-augmented generation for vulnerability description mapping",
journal="Cybersecurity",
year="2026",
month="Jan",
day="20",
volume="9",
number="1",
pages="14",
abstract="To address the vulnerability description mapping (VDM) task, current approaches employ deep learning and large language models (LLMs) through prompt engineering, framing VDM as multi-class classification, multi-label classification, and text generation problems. However, existing methods exhibit significant limitations, including suboptimal identification accuracy, limited category coverage, inadequate interpretability, susceptibility to hallucinations, and challenges related to class imbalance. To address these limitations, this study proposes VDM-IOG, an inference on graph framework in retrieval-augmented generation for vulnerability description mapping. By transforming the VDM methodology into an intelligence graph, the proposed approach leverages a large language model to perform reasoning through five steps: identification, querying, scoring, questioning, and reflection. Experimental results demonstrate that the proposed method achieves a Macro-F1 score of 74.88{\%} and a Micro-F1 score of 82.17{\%}. Compared to existing research, the proposed approach expands detection coverage across 42 technical categories, effectively mitigates class imbalance, and enhances process interpretability and controllability through explicit reasoning traces.",
issn="2523-3246",
doi="10.1186/s42400-025-00413-1",
url="https://doi.org/10.1186/s42400-025-00413-1"
}


@inproceedings{10.1007/978-3-032-02406-0_12,
 abstract = {Reliability and efficiency are two of the main concerns for software designs. These quality objectives can be met through efficient design and bug management. Developers and stakeholders report a large number of software bugs periodically. These bug reports often lack proper descriptions of the issues and the correct linked entities. Thus, it requires efficient bug localization and mapping the reported bugs to the exact software features for resolution. Most of the related research works primarily focus on bug classification and categorization to identify high- priority bugs. This work proposes a novel methodology towards analyzing the bugs, and accordingly identifying relevant software features. The proposed methodology uses large language models (LLM) for mapping bug reports to relevant software features. The effectiveness of this approach is evaluated using two state-of-the-art LLM models are employed using naive Retrieval-Augmented Generation (RAG) and advanced RAG. The comparative study identifies fascinating distinctions between advanced RAG and naive RAG, results in identifying software features. These findings highlight the potential of LLM-powered retrieval methods in improving automated bug localization, paving the way for more efficient software maintenance and debugging workflows.},
 address = {Cham},
 author = {Ghosh, Tirthankar
and Roy, Mandira
and Das, Souvick
and Chaki, Nabendu
and Cortesi, Agostino},
 booktitle = {Computer Information Systems and Industrial Management},
 editor = {Saeed, Khalid
and Dvorsk{\'y}, Ji{\v{r}}{\'i}
and Fukumoto, Makoto
and Nishiuchi, Nobuyuki},
 isbn = {978-3-032-02406-0},
 pages = {164--178},
 publisher = {Springer Nature Switzerland},
 title = {A Hybrid RAG Framework for Bug-to-Feature Mapping},
 url = {https://link.springer.com/chapter/10.1007/978-3-032-02406-0_12},
 year = {2026}
}

@inproceedings{10.1007/978-3-032-05188-2_11,
 abstract = {REST APIs (Representational State Transfer Application Programming Interfaces) play a vital role in modern cloud-native applications. As these APIs grow in complexity and scale, ensuring their correctness and robustness becomes increasingly important. Automated testing is essential for identifying hidden bugs, particularly those that appear in edge cases or under unexpected inputs. However, creating comprehensive and effective test suites for REST APIs is challenging and often demands significant effort. In this paper, we investigate the use of large language model (LLM) systems---both single-agent and multi-agent setups---for amplifying existing REST API test suites. These systems generate additional test cases that aim to push the boundaries of the API, uncovering behaviors that might otherwise go untested. We present a comparative evaluation of the two approaches across several dimensions, including test coverage, bug detection effectiveness, and practical considerations such as computational cost and energy usage. Our evaluation demonstrates increased API coverage, identification of numerous bugs in the API under test, and insights into the computational cost and energy consumption of both approaches.},
 address = {Cham},
 author = {Nooyens, Robbe
and Bardakci, Tolgahan
and Beyaz{\i}t, Mutlu
and Demeyer, Serge},
 booktitle = {Testing Software and Systems},
 editor = {Bonfanti, Silvia
and Papadopoulos, George Angelos},
 isbn = {978-3-032-05188-2},
 pages = {161--177},
 publisher = {Springer Nature Switzerland},
 title = {Test Amplification for REST APIs via Single and Multi-agent LLM Systems},
 url = {https://link.springer.com/chapter/10.1007/978-3-032-05188-2_11},
 year = {2026}
}

@Article{Xu2025,
author="Xu, Yi-Jiang
and Jia, Hong-Rui
and Chen, Li-Guo
and Wang, Xin
and Zeng, Zheng-Ran
and Wang, Yi-Dong
and Gao, Qing
and Ye, Wei
and Zhang, Shi-Kun
and Wu, Zhong-Hai",
title="ISC4DGF: Enhancing Directed Grey-Box Fuzzing with Initial Seed Corpus Generation Driven by Large Language Models",
journal="Journal of Computer Science and Technology",
year="2025",
month="Nov",
day="01",
volume="40",
number="6",
pages="1662--1677",
abstract="Fuzz testing is crucial for identifying software vulnerabilities, with coverage-guided grey-box fuzzers like AFL and Angora excelling in broad detection. However, as the need for targeted detection grows, directed grey-box fuzzing (DGF) has become essential, focusing on specific vulnerabilities. The initial seed corpus, which consists of carefully selected input samples that the fuzzer uses as a starting point, is fundamental in determining the paths that the fuzzer explores. A well-designed seed corpus can guide the fuzzer more effectively towards critical areas of the code, improving the efficiency and success of the fuzzing process. Even with its importance, much work concentrates on refining guidance mechanisms while paying less attention to optimizing the initial seed corpus. In this paper, we introduce ISC4DGF, a novel approach to generating optimized initial seed corpus for DGF using large language models (LLMs). By leveraging LLMs' deep understanding of software and refined user inputs, ISC4DGF creates a precise seed corpus that efficiently triggers specific vulnerabilities through a multi-round validation process. Implemented on AFL and tested against state-of-the-art fuzzers such as Titan, BEACON, AFLGo, FairFuzz, and Entropic using the Magma benchmark, ISC4DGF achieves a 25.03x speedup with fewer target reaches. Moreover, ISC4DGF improves target vulnerabilities detection accuracy while narrowing the detection scope and reducing code coverage.",
issn="1860-4749",
doi="10.1007/s11390-025-4745-0",
url="https://doi.org/10.1007/s11390-025-4745-0"
}


@Article{Dou2026,
author="Dou, Shihan
and Jia, Haoxiang
and Wu, Shenxi
and Zheng, Huiyuan
and Wu, Muling
and Tao, Yunbo
and Zhang, Ming
and Chai, Mingxu
and Fan, Jessica
and Xi, Zhiheng
and Zheng, Rui
and Wu, Yueming
and Wen, Ming
and Gui, Tao
and Zhang, Qi
and Qiu, Xipeng
and Huang, Xuanjing",
title="What is wrong with your code generated by large language models? An extensive study",
journal="Science China Information Sciences",
year="2026",
month="Jan",
day="04",
volume="69",
number="1",
pages="112107",
abstract="The increasing development of large language models (LLMs) in code generation has drawn significant attention among researchers. To enhance LLM-based code generation ability, current efforts are predominantly directed towards collecting high-quality datasets and leveraging diverse training technologies. However, there is a notable lack of comprehensive studies examining the limitations and boundaries of existing methods. To bridge this gap, we conducted an extensive empirical study evaluating the performance of three leading closed-source LLMs and six popular open-source LLMs on three commonly used benchmarks. Our investigation, which evaluated the length, cyclomatic complexity and API number of the generated code, revealed that these LLMs face challenges in generating successful code for more complex problems, and tend to produce code that is shorter yet more complicated as compared to canonical solutions. Additionally, we developed a taxonomy of bugs for incorrect codes that includes three categories and ten sub-categories, and analyzed the root cause for common bug types. To better understand the performance of LLMs in real-world projects, we also manually created a real-world benchmark RWPB. We analyzed bugs on RWPB to highlight distinct differences in bug distributions between actual scenarios and existing benchmarks. Finally, we propose a novel training-free iterative method that introduces self-critique, enabling LLMs to critique and correct their generated code based on bug types and compiler feedback. Experimental results demonstrate that our approach can significantly mitigate bugs and achieve a repair success rate of 29.2{\%} after two iterations, indicating substantial potential for LLMs to handle more complex problems. Our comprehensive and extensive study provides insights into the current limitations of LLM-based code generation and opportunities for enhancing the accuracy and quality of the generated code.",
issn="1869-1919",
doi="10.1007/s11432-025-4632-8",
url="https://doi.org/10.1007/s11432-025-4632-8"
}


@Article{Niaz2026,
author="Niaz, Asim
and Umraiz, Muhammad
and Alam Zaidi, Syed Farhan
and Akram, Farhan
and Choi, Kwang Nam",
title="Synthesis-guided unsupervised anomaly detection in industrial images with large language model-driven analysis",
journal="Neural Computing and Applications",
year="2026",
month="Jan",
day="27",
volume="38",
number="2",
pages="21",
abstract="Surface anomaly detection is crucial in industrial imaging, requiring accurate identification of deviations from expected patterns. Traditional reconstruction-based models often fail to detect unseen anomalies due to their reliance on learned normal distributions. To overcome this, we propose Synthesis-Guided Unsupervised Anomaly Detection in Industrial Images with Large Language Model-Driven Analysis (SUADA), which frames anomaly detection as a discriminative task rather than a reconstructive one. Our approach generates artificial anomalies by combining Perlin and random noise for diverse anomaly generation. We further employ a transformer-driven attention mechanism within a hierarchical feature extraction framework to enhance anomaly localization and segmentation. The attention mechanism captures long-range dependencies, particularly across skip connections, enabling selective feature fusion for improved robustness against scale variations and efficient parameterization. Unlike conventional methods, SUADA directly localizes anomalies without complex post-processing. Furthermore, SUADA integrates an anomaly analysis module powered by Large Language Models (LLMs), which provides human-interpretable descriptions of detected anomalies, detailing their shape, location, and severity. By analyzing contours, spatial distributions, and shape characteristics, this module translates anomaly features into actionable insights, making industrial anomaly detection more accessible to non-expert users. SUADA achieves state-of-the-art performance on the MVTec dataset, with AUROC (Image) 96.01{\%}, AUROC (Pixel) 86.87{\%}, AP (Image) 96.86{\%}, and AP (Pixel) 56.92{\%}, while maintaining a high inference speed of 82 FPS, making it suitable for real-world industrial applications. These results demonstrate the effectiveness of our approach in enhancing neural computing for anomaly detection. By combining unsupervised learning, synthetic data generation, and language-based analysis, SUADA offers a scalable framework adaptable to diverse industrial inspection tasks.",
issn="1433-3058",
doi="10.1007/s00521-025-11775-5",
url="https://doi.org/10.1007/s00521-025-11775-5"
}


@inproceedings{10.1007/978-981-96-7005-5_11,
 abstract = {CTI (Cyber Threat Intelligence) entity recognition is the task of accurately extracting threat entities from unstructured CTI. At present, it has the problem of inaccurate and incomplete entity extraction. Most of the current large models for threat entity recognition tasks are optimized based on the BERT model, but due to the mask independence assumption, they all have the problem of difficulty in capturing the dependencies between tokens. Large models try to migrate to specific fields, but there is no threat intelligence work for the time being. Based on the consideration that threat intelligence data needs to consider contextual associations, we propose an entity recognition method using the GLM model as the base. Compared with BERT, the results generated by its autoregressive fill-in-the-blank pattern can take into account the association between tokens, thereby extracting entities more accurately. In addition, directly fine-tuning the large model in the threat intelligence field has a serious hallucination problem. Based on this, this paper designs a large language model combined with a contrastive learning method, generates negative samples by using a preliminary model and adding character-level noise to generate negative samples, and adjusts the parameters of the hybrid loss function to effectively alleviate the hallucination problem. Experiments show that the performance of our model in named entity recognition on public threat intelligence datasets is 5.01{\%} higher than the F1 score of the current best model.},
 address = {Singapore},
 author = {Sun, Yuchen
and Liu, Jianyi
and Zhang, Ru},
 booktitle = {Neural Information Processing},
 editor = {Mahmud, Mufti
and Doborjeh, Maryam
and Wong, Kevin
and Leung, Andrew Chi Sing
and Doborjeh, Zohreh
and Tanveer, M.},
 isbn = {978-981-96-7005-5},
 pages = {153--167},
 publisher = {Springer Nature Singapore},
 title = {Threat Intelligence Entity Recognition Based on Large Language Model with Contrastive Learning},
 url = {https://link.springer.com/chapter/10.1007/978-981-96-7005-5_11},
 year = {2025}
}

@Article{Busch2025,
author="Busch, Daniel
and Bainczyk, Alexander
and Smyth, Steven
and Steffen, Bernhard",
title="LLM-based code generation and system migration in language-driven engineering",
journal="International Journal on Software Tools for Technology Transfer",
year="2025",
month="Feb",
day="01",
volume="27",
number="1",
pages="137--147",
abstract="This paper illustrates the power of extending Language Driven Engineering (LDE) with Domain-Specific Natural Languages (DSNLs) through a case study on two levels. Both cases benefit from the characteristic decomposition feature of LDE, resulting in tasks tailored to the application of domain-specific languages, here with a focus on the application of DSNLs supported by LLM-based code generation. In the first case study, we show how DSNL-supported LDE facilitates the development of point-and-click adventures, whereas the second case study focuses on migration: We demonstrate how the entire LDE scenario for point-and-click adventure games can be migrated to output TypeScript instead of JavaScript using LLM-based code generation exclusively, without manually writing any code. This migration not only infers the required types, but also preserves an important property of the original LDE scenario: generated web applications can be automatically validated by design via automata learning and subsequent model checking. Even better, this property can be exploited to automatically validate the correctness of the migration by learning so-called difference automata that characterize the behavioral differences between the generated JavaScript-based and Type-Script-based applications.",
issn="1433-2787",
doi="10.1007/s10009-025-00798-x",
url="https://doi.org/10.1007/s10009-025-00798-x"
}


@Article{Han2025,
author="Han, Linyi
and Pan, Shidong
and Xing, Zhenchang
and Yitagesu, Sofonias
and Zhang, Xiaowang
and Feng, Zhiyong
and Sun, Jiamou
and Huang, Qing",
title="Domain-constrained synthesis of inconsistent key aspects in textual vulnerability descriptions",
journal="Automated Software Engineering",
year="2025",
month="Nov",
day="27",
volume="33",
number="1",
pages="35",
abstract="Textual Vulnerability Descriptions (TVDs) are crucial for security analysts to understand and address software vulnerabilities. However, the key aspect inconsistencies in TVDs from different repositories pose challenges for achieving a comprehensive understanding of vulnerabilities. Existing approaches aim to mitigate inconsistencies by aligning TVDs with external knowledge bases, but they often discard valuable information and fail to synthesize comprehensive representations. In this paper, we propose a domain-constrained LLM-based synthesis framework for unifying key aspects of TVDs. Our framework consists of three stages: 1) Extraction, guided by rule-based templates to ensure all critical details are captured; 2) Self-evaluation, using domain-specific anchor words to assess semantic variability across sources; and 3) Fusion, leveraging information entropy to reconcile inconsistencies and prioritize relevant details. This framework improves synthesis performance, increasing the F1 score for key aspect augmentation from 0.82 to 0.87, while enhancing comprehension and efficiency by over 30{\%}. We further develop Digest Labels, a practical tool for visualizing TVDs, which human evaluations show significantly boosts usability.",
issn="1573-7535",
doi="10.1007/s10515-025-00582-w",
url="https://doi.org/10.1007/s10515-025-00582-w"
}


@inproceedings{10.1007/978-3-031-76459-2_4,
 abstract = {In recent years, code security has become increasingly important, especially with the rise of interconnected technologies. Detecting vulnerabilities early in the software development process has demonstrated numerous benefits. Consequently, the scientific community started using machine learning for automated detection of source code vulnerabilities. This work explores and refines the CVEFixes dataset, which is commonly used to train models for code-related tasks, specifically the C/C++ subset. To this purpose, the Source Code Processing Engine (SCoPE), a framework composed of strategized techniques that can be used to reduce the size and normalize C/C++ functions is presented. The output generated by SCoPE was used to create a new version of CVEFixes. This refined dataset was then employed in a feature representation analysis to assess the effectiveness of the tool's code processing techniques, consisting of fine-tuning three pre-trained LLMs for software vulnerability detection. The results show that SCoPE successfully helped to identify 905 duplicates within the evaluated subset. The LLM results corroborate with the literature regarding their suitability for software vulnerability detection, with the best model achieving 53{\%} F1-score.},
 address = {Cham},
 author = {Gon{\c{c}}alves, Jos{\'e}
and Dias, Tiago
and Maia, Eva
and Pra{\c{c}}a, Isabel},
 booktitle = {Distributed Computing and Artificial Intelligence, Special Sessions I, 21st International Conference},
 editor = {Mehmood, Rashid
and Hern{\'a}ndez, Guillermo
and Pra{\c{c}}a, Isabel
and Wikarek, Jaroslaw
and Loukanova, Roussanka
and Monteiro dos Reis, Ars{\'e}nio
and Skarmeta, Antonio
and Lombardi, Eleonora},
 isbn = {978-3-031-76459-2},
 pages = {34--43},
 publisher = {Springer Nature Switzerland},
 title = {SCoPE: Evaluating LLMs for Software Vulnerability Detection},
 url = {https://link.springer.com/chapter/10.1007/978-3-031-76459-2_4},
 year = {2025}
}

@Article{Tambon2025,
author="Tambon, Florian
and Moradi-Dakhel, Arghavan
and Nikanjam, Amin
and Khomh, Foutse
and Desmarais, Michel C.
and Antoniol, Giuliano",
title="Bugs in large language models generated code: an empirical study",
journal="Empirical Software Engineering",
year="2025",
month="Feb",
day="13",
volume="30",
number="3",
pages="65",
abstract="Large Language Models (LLMs) for code have gained significant attention recently. They can generate code in different programming languages based on provided prompts, fulfilling a long-lasting dream in Software Engineering (SE), i.e., automatic code generation. Similar to human-written code, LLM-generated code is prone to bugs, and these bugs have not yet been thoroughly examined by the community. Given the increasing adoption of LLM-based code generation tools (e.g., GitHub Copilot) in SE activities, it is critical to understand the characteristics of bugs contained in code generated by LLMs. This paper examines samples of 333 bugs collected from code generated using three leading LLMs (i.e., CodeGen, PanGu-Coder, and Codex) and identifies the following 10 distinctive bug patterns: Misinterpretations, Syntax Error, Silly Mistake, Prompt-biased code, Missing Corner Case, Wrong Input Type, Hallucinated Object, Wrong Attribute, Incomplete Generation, and Non-Prompted Consideration. The bug patterns are presented in the form of a taxonomy. The identified bug patterns are validated using online surveys with over 50 LLM practitioners and researchers. The surveyed participants generally asserted the significance and prevalence of the bug patterns. Researchers and practitioners can leverage these findings to develop effective quality assurance techniques for LLM-generated code. This study sheds light on the distinctive characteristics of LLM-generated code.",
issn="1573-7616",
doi="10.1007/s10664-025-10614-4",
url="https://doi.org/10.1007/s10664-025-10614-4"
}


@Inbook{Paramesha2025,
author="Paramesha, Mallikarjuna
and Rane, Nitin Liladhar
and Rane, Jayesh",
editor="Rane, Nitin Liladhar
and Mallick, Suraj Kumar
and Rane, Jayesh
and Pande, Chaitanya Baliram",
title="Integration of Large Language Model (LLM) and Building Information Modeling (BIM) for Enhanced Construction Project Lifecycle Management: A Review",
bookTitle="Large Language Models for Sustainable Urban Development",
year="2025",
publisher="Springer Nature Switzerland",
address="Cham",
pages="235--269",
abstract="This chapter reviews the feasibility of employing Large Language Models (LLMs) within the BIM framework for enhanced efficiencies during design, construction, and even facilities management with a focus on LLM characteristics of usability, automation, and communication. LLMs integrate into the Building Information Modeling (BIM) processes to enhance the usability and interoperability of BIM information. LLMs can understand different types of information across diverse platforms and reformat it into usable information. This conversion not only minimizes data loss or error but also fosters effective communication and translatability across various modalities. Furthermore, usability is enhanced on multiple levels; many options taken by the LLM from reporting error findings to presenting ranges of construction parameters serve to simplify processes and avoid unnecessary costs so that project managers can funnel their time into strategic tactical efforts rather than time-consuming menial tasks. Team members similarly stand to benefit as LLMs answer inquiry-based contexts as opposed to general inquiry searches, which leads to the opportunity for more in-depth reporting and summary complexities for greater understanding and agreement. Likewise, the ability of LLMs to generate designs is acknowledged; simply putting parameters and limitations into the system can yield varying options for review by the architect/engineer, which minimizes design time but offers creative yet suitable solutions. However, challenges associated with blended LLMs and BIM systems arise from data quality, training, extensive computing power prerequisites, and ethical concerns surrounding data privacy and accountability. Ultimately, however, LLMs possess the ability to increase interaction with and usability of the data within BIM systems, improve automation processes, and enhance intra- and inter-team communication and generative efforts. Further studies must be conducted surrounding trained data quality, computational potentials, and ethical facets of Artificial Intelligence (AI) in BIM to bring this potential to fruition. If these obstacles can be overcome, never before has the Architectural, Engineering, and Construction (AEC) industry had a potential evolution of integration such that LLMs can foster creativity and productivity during the design/construction process for a more technologically advanced and efficient sustainable future.",
isbn="978-3-031-86039-3",
doi="10.1007/978-3-031-86039-3_10",
url="https://doi.org/10.1007/978-3-031-86039-3_10"
}


@inproceedings{10.1007/978-3-032-04288-0_6,
 abstract = {This study conducts a Multivocal Literature Review (MLR) to explore the strengths and weaknesses of Artificial Intelligence (AI) in software engineering, focusing on four key areas: reliability, sustainability, security and convenience. AI tools improve efficiency and automation, but they also have challenges such as bias, non-determinism and model transparency. There are threats to the sustainability of AI in software engineering, such as high computational energy demands and legal constraints on proprietary and copyrighted data. Security risks can also emerge from AI-generated code, as models may produce insecure implementations that require human oversight to ensure robustness. Even with these challenges, AI seems to be gaining in popularity due to its convenience, enabling developers to streamline workflows and enhance their productivity. This study explores these strengths and weaknesses and provides insights into the future of AI in software engineering.},
 address = {Cham},
 author = {Walshe, Jed
and Maloney, Robert
and Grant, Evun
and Conde, Carlos
and Marks, Gerard
and Yilmaz, Murat
and Messnarz, Richard
and Clarke, Paul M.
and McCarren, Andrew},
 booktitle = {Systems, Software and Services Process Improvement},
 editor = {Yilmaz, Murat
and Clarke, Paul
and Riel, Andreas
and Messnarz, Richard
and Zelmenis, Mikus
and Buce, Ivi Anna},
 isbn = {978-3-032-04288-0},
 pages = {91--105},
 publisher = {Springer Nature Switzerland},
 title = {Current AI-Based Software Engineering, Strengths and Weaknesses - Results from a MLR},
 url = {https://link.springer.com/chapter/10.1007/978-3-032-04288-0_6},
 year = {2026}
}

@Article{Shaikh2025,
author="Shaikh, Tawseef Ayoub
and Rasool, Tabasum
and Veningston, K.
and Yaseen, Syed Mufassir",
title="The role of large language models in agriculture: harvesting the future with LLM intelligence",
journal="Progress in Artificial Intelligence",
year="2025",
month="Jun",
day="01",
volume="14",
number="2",
pages="117--164",
abstract="Significant accomplishments in many agricultural applications during the past decade attest to the fast progress and use of deep learning and machine learning methods in agricultural systems. However, these conventional models have a few drawbacks: They are not generalizable since they are trained on large, costly labeled datasets, require expert expertise to create and maintain, and are often built for specific applications. Significant accomplishments in language, vision, and decision-making tasks across several domains have been shown recently by massive pre-trained models, also known as large models (LMs). Recent years have seen large language models (LLMs) demonstrate remarkable competence in a variety of fields, including natural language processing (NLP), by encompassing different advancements in terms of architecture, training methods, context duration, fine-tuning, multi-modality, datasets, efficiency, benchmarking, and many other. The massive amounts of data used to train these models span many domains and modalities. After training, they can handle a wide range of tasks with less tweaking and less task-specific labeled data. Despite its effectiveness and promising future, agricultural artificial intelligence (AAI) has received less attention than other applications of LLMs. To better understand the problem area and open up new research pathways in this sector, this work aims to examine the possibilities of LLMs in smart agriculture by offering conceptual tools and a technical base. Herein, we delve into the potential applications of large models in agriculture, primarily categorizing them into four categories: Agricultural applications of large language models (LLMs), large vision models (LVMs) for precise agricultural applications, multimodal large language models (MLLMs) and model assessment, and intelligent and precise agriculture using reinforcement learning large models (RLLMs). Further, we review some of the most prominent LLMs, including three famous LLM families (GPT, LLaMA, PaLM), and discuss their characteristics, contributions, and limitations. Next, we evaluate famous LLM evaluation metrics and look at datasets for training, fine-tuning, and evaluation. Finally, we focus our discussion on issues and possible future research directions of LLMs in the agricultural sector. This review article aims to provide academics and practitioners with a panoramic perspective of the field and a quick reference to help them draw out relevant ideas from the extensive summaries of prior publications to broaden their LLM research.",
issn="2192-6360",
doi="10.1007/s13748-024-00359-4",
url="https://doi.org/10.1007/s13748-024-00359-4"
}


@inproceedings{10.1007/978-981-96-9958-2_17,
 abstract = {Security vulnerabilities in smart contracts pose significant risks to blockchain ecosystems. Traditional mitigation strategies, including static analysis, formal verification, and pattern-based detection, often suffer from high false positives, limited scalability, and dependency on expert-curated rules. To address these limitations, this paper proposes an innovative approach that integrates Large Language Models (LLMs) with contract analysis results to establish a multistage collaborative framework for smart contract vulnerability repair. The LLMfram first generates Control Flow Graphs (CFGs) and Call Graphs, capturing critical code logic. Then, the contract is segmented based on the function. Subsequently, it leverages multiround LLM interactions for hierarchical tasks, including vulnerability detection, root cause analysis, and patch generation. Finally, the generated repair code undergoes LLM-human collaborative verification to ensure functional correctness. Experimental results demonstrate that the LLMfram improves vulnerability localization and repair accuracy by dynamically coordinating LLMs with static analysis results and employing hierarchical diagnostic mechanisms. This work offers a scalable and explainable solution for smart contract repair.},
 address = {Singapore},
 author = {Miao, Peicheng
and Pan, Ziling},
 booktitle = {Advanced Intelligent Computing Technology and Applications},
 editor = {Huang, De-Shuang
and Chen, Wei
and Pan, Yijie
and Chen, Haiming},
 isbn = {978-981-96-9958-2},
 pages = {208--219},
 publisher = {Springer Nature Singapore},
 title = {A Large Language Models-Powered Framework for Smart Contract Repair},
 url = {https://link.springer.com/chapter/10.1007/978-981-96-9958-2_17},
 year = {2025}
}

@Article{Jiao2025,
author="Jiao, Yutong
and Han, Jiaxuan
and Huang, Cheng",
title="DeepVulHunter: enhancing the code vulnerability detection capability of LLMs through multi-round analysis",
journal="Journal of Intelligent Information Systems",
year="2025",
month="Dec",
day="01",
volume="63",
number="6",
pages="2237--2264",
abstract="As the economic losses caused by software vulnerabilities continue to escalate, automated vulnerability detection has emerged as a crucial demand in software engineering. While current Large Language Model (LLM)-based approaches demonstrate promising capabilities for vulnerability detection, they still face significant challenges including susceptibility to non-vulnerability factors like code length, severe hallucination issues, and unsatisfactory detection accuracy and balance. To overcome these limitations, we propose DeepVulHunter, a novel multi-round detection framework that utilizes Retrieval Augmented Generation (RAG) technique to provide code snippets semantically similar to the target code and their associated vulnerability information. Extensive experiments conducted across five representative models from the Llama and Deepseek series confirm that our method effectively mitigates these challenges while enhancing both accuracy and balance in vulnerability detection tasks for general large models. The best-performing Llama-405B model achieves a detection accuracy of up to 75.3{\%}, surpassing the current state-of-the-art approach that utilizes GPT-4 with Chain-of-Thought (CoT) prompting.",
issn="1573-7675",
doi="10.1007/s10844-025-00982-0",
url="https://doi.org/10.1007/s10844-025-00982-0"
}


@inproceedings{10.1007/978-3-032-00828-2_16,
 abstract = {Migrating existing C programs into Rust is increasingly desired, as Rust offers superior memory safety while maintaining C's high performance. Existing automated translation tools, such as C2Rust, may rely too much on syntactic, template-based translation and generate unsafe Rust code that is hard for human developers to read, maintain, or even compile. More semantic-aware translation that produces safer, idiomatic, and runnable Rust code is much needed. This paper introduces a novel dependency-guided and large language model (LLM)-based C-to-Rust translation approach, RustMap, based on three key ideas: (1) Utilize LLM's capabilities to produce idiomatic Rust code from given small pieces of C code, (2) Mitigate LLM's incapability in handling large codebases by breaking project-scale C programs into smaller units for translation according to their usage dependencies, and (3) Enhance the correctness of the translated Rust program by utilizing available test cases to check input/output equivalence between C and Rust code, and iteratively utilizing the feedback from compilation and testing errors for LLM to refine translated Rust code. We have empirically evaluated RustMap on 126 sample real-world programs, including 125 programs from Rosetta Code and a complex bzip2 program containing more than 7000 lines of code, using GPT-4o as the LLM. RustMap shows promising results in guiding GPT-4o to translate most of the C code into more idiomatic, readable, and functional Rust code with significantly less unsafe code than other translation tools, presenting non-trivial translation patterns that may be reusable for future research.},
 address = {Cham},
 author = {Cai, Xuemeng
and Liu, Jiakun
and Huang, Xiping
and Yu, Yijun
and Wu, Haitao
and Li, Chunmiao
and Wang, Bo
and Yusuf, Imam Nur Bani
and Jiang, Lingxiao},
 booktitle = {Engineering of Complex Computer Systems},
 editor = {Zhou, Yuan
and Teo, Sin G.
and Xie, Xiaofei
and Ding, Zuohua
and Liu, Yang},
 isbn = {978-3-032-00828-2},
 pages = {283--302},
 publisher = {Springer Nature Switzerland},
 title = {RustMap: Towards Project-Scale C-to-Rust Migration via Program Analysis and LLM},
 url = {https://link.springer.com/chapter/10.1007/978-3-032-00828-2_16},
 year = {2026}
}

@inproceedings{10.1007/978-3-031-46002-9_23,
 abstract = {Large language models such as OpenAI's GPT and Google's Bard offer new opportunities for supporting software engineering processes. Large language model assisted software engineering promises to support developers in a conversational way with expert knowledge over the whole software lifecycle. Current applications range from requirements extraction, ambiguity resolution, code and test case generation, code review and translation to verification and repair of software vulnerabilities. In this paper we present our position on the potential benefits and challenges associated with the adoption of language models in software engineering. In particular, we focus on the possible applications of large language models for requirements engineering, system design, code and test generation, code quality reviews, and software process management. We also give a short review of the state-of-the-art of large language model support for software construction and illustrate our position by a case study on the object-oriented development of a simple ``search and rescue'' scenario.},
 address = {Cham},
 author = {Belzner, Lenz
and Gabor, Thomas
and Wirsing, Martin},
 booktitle = {Bridging the Gap Between AI and Reality},
 editor = {Steffen, Bernhard},
 isbn = {978-3-031-46002-9},
 pages = {355--374},
 publisher = {Springer Nature Switzerland},
 title = {Large Language Model Assisted Software Engineering: Prospects, Challenges, and a Case Study},
 url = {https://link.springer.com/chapter/10.1007/978-3-031-46002-9_23},
 year = {2024}
}

@inproceedings{10.1007/978-3-032-13513-1_22,
 abstract = {We represent interdependent infrastructure systems and communities alike with a hetero-functional graph (HFG) that encodes the dependencies between functionalities. This graph naturally imposes a partial order of functionalities that can inform the sequence of repair decisions to be made during a disaster across affected communities. However, using such technical criteria alone provides limited guidance at the point where the functionalities directly impact the communities, since these can be repaired in any order without violating the system constraints. To address this gap and improve resilience, we integrate community preferences to refine this partial order from the HFG into a total order. Our strategy involves getting the communities' opinions on their preferred sequence for repair crews to address infrastructure issues, considering potential constraints on resources. Due to the delay and cost associated with real-world survey data, we utilize a Large Language Model (LLM) as a proxy survey tool. We use the LLM to craft distinct personas representing individuals, each with varied disaster experiences. We construct diverse disaster scenarios, and each simulated persona provides input on prioritizing infrastructure repair needs across various communities. Finally, we apply learning algorithms to generate a global order based on the aggregated responses from these LLM-generated personas.},
 address = {Cham},
 author = {Okeukwu-Ogbonnaya, Adaeze
and Amatapu, Rahul
and Bergtold, Jason
and Amariucai, George},
 booktitle = {Social Networks Analysis and Mining},
 editor = {An, Aijun
and Cuzzocrea, Alfredo
and Hu, Hongxin},
 isbn = {978-3-032-13513-1},
 pages = {264--274},
 publisher = {Springer Nature Switzerland},
 title = {LLM-Based Community Surveys for Operational Decision Making in Interconnected Utility Infrastructures},
 url = {https://link.springer.com/chapter/10.1007/978-3-032-13513-1_22},
 year = {2026}
}

@inproceedings{10.1007/978-3-032-03705-3_11,
 abstract = {We propose an automated vulnerability detection system that synergizes static analysis and fuzzing target identification through LLMs and AI agents. Building on the Dante system, our solution integrates various reasoning models and leverages a dynamic dataset from student code contributions. The system employs reinforcement learning, prompt crafting, and test-time computing techniques to refine the detection of critical vulnerabilities in C codebases. A multi-step automated workflow performs detailed static code analysis, extracts fuzzing targets, and iteratively compiles and tests code snippets. Log outputs are summarized using reasoning models, highlighting only the most relevant errors.},
 address = {Cham},
 author = {Kapusta, Pawe{\l}
and Duch, Piotr
and Majchrowicz, Micha{\l}
and Kr{\'o}lik, Adrian},
 booktitle = {Artificial Intelligence and Soft Computing},
 editor = {Rutkowski, Leszek
and Scherer, Rafa{\l}
and Korytkowski, Marcin
and Pedrycz, Witold
and Tadeusiewicz, Ryszard
and Zurada, Jacek M.},
 isbn = {978-3-032-03705-3},
 pages = {114--125},
 publisher = {Springer Nature Switzerland},
 title = {System for Automatic Bug Detection in Code and Programs Using LLMs and AI Agents},
 url = {https://link.springer.com/chapter/10.1007/978-3-032-03705-3_11},
 year = {2026}
}

@Article{Alhanahnah2025,
author="Alhanahnah, Mohannad
and Rashedul Hasan, Md
and Xu, Lisong
and Bagheri, Hamid",
title="An empirical evaluation of pre-trained large language models for repairing declarative formal specifications",
journal="Empirical Software Engineering",
year="2025",
month="Jul",
day="25",
volume="30",
number="5",
pages="149",
abstract="Automatic Program Repair (APR) has garnered significant attention as a practical research domain focused on automatically fixing bugs in programs. While existing APR techniques primarily target imperative programming languages like C and Java, there is a growing need for effective solutions applicable to declarative software specification languages. This paper systematically investigates the capacity of Large Language Models (LLMs) to repair declarative specifications in Alloy, a declarative formal language used for software specification. We designed six different repair settings, encompassing single-agent and dual-agent paradigms, utilizing various LLMs. These configurations also incorporate different levels of feedback, including an auto-prompting mechanism for generating prompts autonomously using LLMs. Our study reveals that dual-agent with auto-prompting setup outperforms the other settings, albeit with a marginal increase in the number of iterations and token usage. This dual-agent setup demonstrated superior effectiveness compared to state-of-the-art Alloy APR techniques when evaluated on a comprehensive set of benchmarks. This work is the first to empirically evaluate LLM capabilities to repair declarative specifications, while taking into account recent trending LLM concepts such as LLM-based agents, feedback, auto-prompting, and tools, thus paving the way for future agent-based techniques in software engineering.",
issn="1573-7616",
doi="10.1007/s10664-025-10687-1",
url="https://doi.org/10.1007/s10664-025-10687-1"
}


@Article{Dakhama2025,
author="Dakhama, Aidan
and Even-Mendoza, Karine
and Langdon, W. B
and Men{\'e}ndez, H{\'e}ctor D.
and Petke, Justyna",
title="Enhancing search-based testing with LLMs for finding bugs in system simulators",
journal="Automated Software Engineering",
year="2025",
month="Jul",
day="10",
volume="32",
number="2",
pages="63",
abstract="Despite the wide availability of automated testing techniques such as fuzzing, little attention has been devoted to testing computer architecture simulators. We propose a fully automated approach for this task. Our approach uses large language models (LLM) to generate input programs, including information about their parameters and types, as test cases for the simulators. The LLM's output becomes the initial seed for an existing fuzzer, AFL++, which has been enhanced with three mutation operators, targeting both the input binary program and its parameters. We implement our approach in a tool called SearchSYS . We use it to test the gem5 system simulator. SearchSYS discovered 21 new bugs in gem5 , 14 where gem5 's software prediction differs from the real behaviour on actual hardware, and 7 where it crashed. New defects were uncovered with each of the 6 LLMs used.",
issn="1573-7535",
doi="10.1007/s10515-025-00531-7",
url="https://doi.org/10.1007/s10515-025-00531-7"
}


@Article{Song2025,
author="Song, Jiayin
and Li, Yike
and Tian, Yunzhe
and Ma, Haoxuan
and Li, Honglei
and Zuo, Jie
and Liu, Jiqiang
and Niu, Wenjia",
title="Investigating the bugs in reinforcement learning programs: Insights from Stack Overflow and GitHub",
journal="Automated Software Engineering",
year="2025",
month="Sep",
day="23",
volume="33",
number="1",
pages="9",
abstract="Reinforcement learning (RL) is increasingly applied in areas such as gaming, robotic control, and autonomous driving. Like to deep learning, RL systems also encounter failures during operation. However, RL differs from deep learning in terms of its error causes and symptom manifestations. What are the differences in error causes and symptoms between RL and deep learning? How are RL errors and their symptoms related? Understanding the symptoms and causes of RL failures can advance research on RL failure detection and repair. In this paper, we conducted a comprehensive empirical study by collecting 1,155 error reports from the popular Q{\&}A forum Stack Overflow and four GitHub repositories: baselines, stable-baselines3, tianshou and keras-rl. We analyzed the root causes and symptoms of these failures and examined the differences in resolution times across various root causes. Additionally, we analyzed the correlations between causes and symptoms. Our study yielded 14 key findings, and six implications for developing RL detection and failure repair tools. Our work is the first to integrate LLM-based analysis with manual validation for RL bug studies, providing actionable insights for tool development and testing strategies.",
issn="1573-7535",
doi="10.1007/s10515-025-00555-z",
url="https://doi.org/10.1007/s10515-025-00555-z"
}


@inproceedings{10.1007/978-981-96-5693-6_31,
 abstract = {Ensuring timely and accurate security patches is crucial for maintaining software integrity in the face of evolving vulnerabilities. This paper presents a comprehensive study on developing and applying advanced AI and deep learning models for improving security patch management. It examines the challenges in automated program repair for complex security vulnerabilities and explores the use of large language models (LLMs) to focus repair efforts on relevant code sections. Deep learning methods for vulnerability detection are also analyzed, including a new dataset of over 18,000 vulnerable functions from security-related commits. The study also reviews machine learning and deep learning applications for detecting vulnerabilities in Internet of Things (IoT) devices, addressing current limitations such as high false-positive rates and generalization difficulties. Promising research directions, such as source code-specific pre-training models, are identified to enhance the future performance of AI-driven vulnerability detection systems.},
 address = {Singapore},
 author = {Shahbazi, Zeinab
and Mesbah, Meshkat},
 booktitle = {Advances in Computer Science and Ubiquitous Computing},
 editor = {Park, Ji Su
and Camacho, David
and Gritzalis, Stefanos
and Park, James J.},
 isbn = {978-981-96-5693-6},
 pages = {199--205},
 publisher = {Springer Nature Singapore},
 title = {Deep Learning Techniques for Enhancing the Efficiency of Security Patch Development},
 url = {https://link.springer.com/chapter/10.1007/978-981-96-5693-6_31},
 year = {2025}
}

@inproceedings{10.1007/978-3-031-71801-4_5,
 abstract = {This study explores the development of a console application utilizing autonomous open-source Large Language Models (LLMs), specifically LLAMA 2, to automate the generation of paragraph highlights(outlines) for scientific articles. The research highlights a critical need to enhance the efficiency of processing and reviewing extensive literature, especially in fields overwhelmed with publications such as artificial intelligence. By automating the extraction of key highlights from paragraphs, the application aids researchers in swiftly identifying relevant studies without extensive manual review. Utilizing a qualitative research methodology, the project assesses various LLMs and integrates Whiteside's method for optimal outlines generation. The findings suggest that the application effectively streamlines the review process, potentially transforming how academic literature is synthesized.},
 address = {Cham},
 author = {Tsypliak, Oleksandr
and Artemchuk, Volodymyr},
 booktitle = {Information Technology for Education, Science, and Technics},
 editor = {Faure, Emil
and Tryus, Yurii
and Vartiainen, Tero
and Danchenko, Olena
and Bondarenko, Maksym
and Bazilo, Constantine
and Zaspa, Grygoriy},
 isbn = {978-3-031-71801-4},
 pages = {53--64},
 publisher = {Springer Nature Switzerland},
 title = {Console Application Development for Articles` Highlights Generation Based on Artificial Intelligence Designed Using Autonomous Large Language Model},
 url = {https://link.springer.com/chapter/10.1007/978-3-031-71801-4_5},
 year = {2024}
}

@Article{Zheng2026,
author="Zheng, Tianming
and Meng, Fanchao
and Yi, Ping
and Wu, Yue",
title="Automating fuzz driver generation for deep learning libraries with large language models",
journal="Cybersecurity",
year="2026",
month="Jan",
day="04",
volume="9",
number="1",
pages="7",
abstract="The widespread adoption of deep learning (DL) libraries has raised concerns about their reliability and security. While prior works leveraged large language models (LLMs) to generate test programs for DL library APIs, the hardcoded program behaviors and low code validity rates render them impractical for real-world testing. To address these challenges, we propose FD-FACTORY, a fully automated framework that leverages LLMs to generate fuzz drivers for DL API testing. The fuzz driver programs accept mutated inputs from fuzzing engines to achieve effective code analysis. Inspired by the modular design of industrial production lines, FD-FACTORY decomposes the generation process into eight distinct stages: Preparation, Initial Fuzz Driver Generation, Early Stop Checks, Verification, Issue Diagnosis, Decision Making, Repair Loop, and Deployment. Each stage is handled by dedicated agents or tools to enhance construction efficiency. Experimental results demonstrate that FD-FACTORY achieves 73.67{\%} and 65.33{\%} success rates in generating fuzz drivers for PyTorch and TensorFlow, producing an improvement of 34.66 to {\$}{\$}-{\$}{\$} 54.66{\%} than existing approaches. In addition, FD-FACTORY provides more comprehensive coverage tracking by supporting both Python and native C/C++ code. It achieves a total coverage of 308,351 lines on PyTorch and 528,427 lines on TensorFlow, substantially surpassing the results reported by previous approaches. Unlike prior approaches relying on repeated interactions with the LLM servers throughout the entire testing process, our framework confines the use of LLMs strictly to the fuzz driver generation stages before deployment. Once generated, the fuzz drivers can be reused without further LLM involvement, thereby enhancing the practicality and sustainability of LLM-assisted fuzzing in real-world scenarios.",
issn="2523-3246",
doi="10.1186/s42400-025-00532-9",
url="https://doi.org/10.1186/s42400-025-00532-9"
}


@inproceedings{10.1007/978-981-95-3495-1_21,
 abstract = {The rapid growth in software development has led to difficulty maintaining high-quality and secure code. The reason for this is the increasing complexity of modern software systems. Traditional methods like static analysis and manual code review and refinement are time-consuming and prone to human errors which states the necessity of finding advanced automated solutions. Our study uses buggy and fixed Java code snippets to fine-tune various pre-trained Large Language Models (LLMs) and compare them based on their performance on code refinement tasks. Models like CodeT5, CodeGen2, and PolyCoder were compared using BLEU, ROUGE, and Exact Match metrics. CodeGen2 and PolyCoder achieved the highest BLEU and ROUGE scores. Hence, this paper highlights the potential of LLMs in automating code refinement tasks and provides insights for future improvements in this field.},
 address = {Singapore},
 author = {Magar, Abhay
and Joshi, Krisha
and Shah, Pooja
and Mishra, Shakti},
 booktitle = {Proceedings of International Conference on Communication and Computational Technologies},
 editor = {Kumar, Sandeep
and Hiranwal, Saroj
and Chouhan, Lokesh
and Chaudhary, Naveen Kumar},
 isbn = {978-981-95-3495-1},
 pages = {291--302},
 publisher = {Springer Nature Singapore},
 title = {Enhancing Code Quality Using Pre-trained Language Models: A Fine-Tuning Approach for Code Refinement},
 url = {https://link.springer.com/chapter/10.1007/978-981-95-3495-1_21},
 year = {2026}
}

@inproceedings{10.1007/978-981-96-9958-2_36,
 abstract = {Fuzzing is an effective method for detecting bugs in deep learning (DL) libraries, which is critical for downstream applications. Large Language Models (LLMs) offer new opportunities to improve fuzzing. However, current LLM-based fuzzing methods face challenges in generating complex and error-prone API sequences for DL libraries. We introduce KL-RAG, a lightweight and versatile fuzzing framework for DL libraries, designed to overcome these challenges. Our key insight is that various DL libraries share commonalities, and numerous GitHub repositories contain extensive bug reports. These reports can guide the generation of API sequences for other libraries, improving their effectiveness. KL-RAG integrates knowledge-based generation and retrieval, collecting bug information from existing libraries to build a vulnerability knowledge base. When generating test code for other APIs, KL-RAG retrieves relevant vulnerability knowledge from the knowledge base and incorporates it into the prompt as context, helping LLMs generate high-quality, error-prone API sequences. We evaluate KL-RAG's effectiveness through experiments conducted on three DL libraries. We have detected 23 bugs in the PaddlePaddle library, including 10 previously unknown bugs. The evaluation results show that KL-RAG significantly outperforms existing fuzzing methods in bug detection, code generation effectiveness, and API coverage.},
 address = {Singapore},
 author = {Liao, RongTao
and Yan, XueHu
and Zhu, KaiLong},
 booktitle = {Advanced Intelligent Computing Technology and Applications},
 editor = {Huang, De-Shuang
and Chen, Wei
and Pan, Yijie
and Chen, Haiming},
 isbn = {978-981-96-9958-2},
 pages = {443--456},
 publisher = {Springer Nature Singapore},
 title = {KLRAG: Deep Learning Library Vulnerability Detection via Knowledge-Level RAG},
 url = {https://link.springer.com/chapter/10.1007/978-981-96-9958-2_36},
 year = {2025}
}

@inproceedings{10.1007/978-3-031-72344-5_27,
 abstract = {Large Language Models trained on Code (LLMCs) have been shown to be effective in Automated Program Repair (APR) tasks, introducing new innovations to the field. Typically, LLMCs do not engage in error localization for APR tasks, instead treating APR more as a code refinement task. This approach often results in larger edit distances, altering the program's original structure. The principle of making minimal edits is crucial in certain scenarios, such as when correcting student programming assignments or software group development, where it's better to preserve the original intent of the code with as few changes as possible. To address these challenges, we introduce a hybrid framework for automated program repair that combines graph neural networks and large language models, which we refer to as HFRepair. HFRepair leverages the precise error localization capability of DrRepair for C programs, combining it with LLMCs to perform the line-level APR task based on the code context, aiming for minimal edits. Our experimental results demonstrate that HFRepair significantly outperforms previous state-of-the-art methods in benchmark tests. For instance, on the DeepFix dataset, HFRepair improves the full repair rate from 67.9{\%} and 71.4{\%} (achieved by DrRepair and BIFI, respectively) to 82.2{\%}, while reducing average edit distance from 33.4 and 27.7 to 11.6.},
 address = {Cham},
 author = {Xu, Zhenyu
and Sheng, Victor S.},
 booktitle = {Artificial Neural Networks and Machine Learning -- ICANN 2024},
 editor = {Wand, Michael
and Malinovsk{\'a}, Krist{\'i}na
and Schmidhuber, J{\"u}rgen
and Tetko, Igor V.},
 isbn = {978-3-031-72344-5},
 pages = {402--416},
 publisher = {Springer Nature Switzerland},
 title = {Towards Minimal Edits in Automated Program Repair: A Hybrid Framework Integrating Graph Neural Networks and Large Language Models},
 url = {https://link.springer.com/chapter/10.1007/978-3-031-72344-5_27},
 year = {2024}
}

@inproceedings{10.1007/978-3-031-97141-9_28,
 abstract = {Smart contracts, predominantly authored in the Solidity programming language, are fundamental components of the blockchain ecosystem; however, they are frequently susceptible to security vulnerabilities. While existing detection tools have made significant advances in identifying such vulnerabilities, a critical gap remains in providing comprehensive explanations and reasoning about the identified issues. This is further complicated by the deficiency of well-annotated, high-quality training datasets specifically tailored for Solidity, which limits the development of advanced machine learning models capable of producing coherent and insightful explanations. In light of these challenges, this paper introduces a novel system designed to furnish detailed, context-aware explanations for vulnerabilities in Solidity-based smart contracts. A cornerstone of our methodology is the construction of a synthetic dataset, which encompasses a wide spectrum of code vulnerabilities matched with high-quality synthetic explanations. This dataset addresses the shortage of existing training data and enhances the capability of our system to offer meaningful insights into the vulnerabilities detected, thereby empowering developers and security auditors to understand and remediate potential security threats more effectively. Our contributions aim to enhance the interpretability of smart contract vulnerabilities and fortify the security of blockchain technologies.},
 address = {Cham},
 author = {Nguyen, Ngoc Minh
and Inoue, Naoya
and Nguyen, Le Minh},
 booktitle = {Natural Language Processing and Information Systems},
 editor = {Ichise, Ryutaro},
 isbn = {978-3-031-97141-9},
 pages = {411--425},
 publisher = {Springer Nature Switzerland},
 title = {Improve Smart Contract Vulnerability Explanation with Synthetic Data and Chain-of-Thought Prompting},
 url = {https://link.springer.com/chapter/10.1007/978-3-031-97141-9_28},
 year = {2026}
}

@Article{Zhang2025,
author="Zhang, Ruxin
and Zhang, Shanxin
and Xie, Linbo",
title="A systematic exploration of C-to-rust code translation based on large language models: prompt strategies and automated repair",
journal="Automated Software Engineering",
year="2025",
month="Oct",
day="18",
volume="33",
number="1",
pages="21",
abstract="C is widely used in system programming due to its low-level flexibility. However, as demands for memory safety and code reliability grow, Rust has become a more favorable alternative owing to its modern design principles. Migrating existing C code to Rust has therefore emerged as a key approach for enhancing the security and maintainability of software systems. Nevertheless, automating such migrations remains challenging due to fundamental differences between the two languages in terms of language design philosophy, type systems, and levels of abstraction. Most current code transformation tools focus on mappings of basic data types and syntactic replacements, such as handling pointers or conversion of lock mechanisms. These approaches often fail to deeply model the semantic features and programming paradigms of the target language. To address this limitation, this paper proposes RustFlow, a C-to-Rust code translation framework based on large language models (LLMs), designed to generate idiomatic and semantically accurate Rust code. This framework employs a multi-stage progressive architecture, which decomposes the overall translation task into several sequential stages, namely translation, validation, and repair. During the translation phase, a collaborative prompting strategy is employed to guide the LLM in achieving cross-language semantic alignment, thereby improving the accuracy of the generated code. Subsequently, a validation mechanism is introduced to perform syntactic and semantic checks on the generated output, and a conversational iterative repair strategy is employed to further enhance the quality of the final result. Experimental results show that RustFlow outperforms most of the latest baseline approaches, achieving an average improvement of 50.67{\%} in translation performance compared to the base LLM. This work offers a novel technical approach and practical support for efficient and reliable cross-language code migration.",
issn="1573-7535",
doi="10.1007/s10515-025-00570-0",
url="https://doi.org/10.1007/s10515-025-00570-0"
}


@inproceedings{10.1007/978-981-97-4522-7_30,
 abstract = {As vulnerability detection tools evolve, more and more vulnerabilities are found. Due to the high labor cost required to fix vulnerabilities manually, the AI-based approaches are introduced. Recently, Neural Machine Translation (NMT) has been introduced for application in the field of vulnerability repairing. However, However, most of the existing NMT approaches focus on bug fixing tasks in Automated Programmed Repair (APR) tasks and lack specialization in vulnerability fixing tasks. In this paper, we present PAVR, an approach based on deep learning models combined with vulnerability-related pre-training tasks and attention enhancement. We propose three customized pre-training tasks to enable our deep learning model to acquire code generation capabilities related to vulnerability repair during the pre-training process. At the same time, we impose certain restrictions on the output based on the Decoder part, which makes the model's attention to vulnerability types enhanced. We demonstrate that our pre-training tasks and augmented attention mechanism are effective, and we implement PAVR.},
 address = {Singapore},
 author = {Peng, Kaifeng
and Fu, Yulong
and Yang, Jincheng
and Yi, Wei
and Cao, Jin
and Li, Hui},
 booktitle = {Network Simulation and Evaluation},
 editor = {Gu, Zhaoquan
and Zhou, Wanlei
and Zhang, Jiawei
and Xu, Guandong
and Jia, Yan},
 isbn = {978-981-97-4522-7},
 pages = {427--441},
 publisher = {Springer Nature Singapore},
 title = {PAVR: A Pre-Training Approach with Self-attention for Vulnerability Repair},
 url = {https://link.springer.com/chapter/10.1007/978-981-97-4522-7_30},
 year = {2024}
}

@inproceedings{10.1007/978-3-031-97992-7_65,
 abstract = {The advancement of Large Language Model (LLMs) has opened new possibilities for automating software development, particularly in generating test codes. This study examines LLM capabilities for autonomously generating Robot Framework test codes using a Retrieval-Augmented Generation (RAG) model. By leveraging a repository with stored contextual information, the model enhances the accuracy and relevance of generated test scripts while reducing human intervention. To assess the generated tests, we employ CodeBLEU to evaluate syntactic correctness, Jaccard Similarity to measure structural alignment, and pass/fail statistics from CI/CD pipelines to determine real-world applicability. Our experimental results demonstrate that integrating RAG with LLMs significantly improves both test quality and reliability. Contextual information retrieval enhances semantic cohesion and functional robustness, while a multi-metric evaluation framework ensures a comprehensive assessment. This study highlights the transformational role of LLMs in software testing automation, emphasizing their potential to streamline continuous integration (CI) practices. By enhancing test code generation accuracy and minimizing manual effort, the proposed approach marks a significant step toward fully automated and contextually aware testing within the Robot Framework.},
 address = {Cham},
 author = {Sezgin, An{\i}l
and {\"O}zkan, G{\"u}rkan
and Co{\c{s}}gun, Esra},
 booktitle = {Intelligent and Fuzzy Systems},
 editor = {Kahraman, Cengiz
and Cebi, Selcuk
and Oztaysi, Basar
and Cevik Onar, Sezi
and Tolga, Cagr{\i}
and Ucal Sari, Irem
and Otay, Irem},
 isbn = {978-3-031-97992-7},
 pages = {585--592},
 publisher = {Springer Nature Switzerland},
 title = {Generating Robot Framework Code with LLM Models: A RAG-Based Approach},
 url = {https://link.springer.com/chapter/10.1007/978-3-031-97992-7_65},
 year = {2025}
}

@inproceedings{10.1007/978-3-031-75434-0_10,
 abstract = {Large Language Model (LLM) Artificial Intelligence (AI) systems have generated significant enthusiasm in the computer science research community for their potential in various computer language processing tasks, such as source code generation and source-to-source translation. We are particularly interested in using LLMs for automated theorem proving, specifically for proof repair. To this end, we introduce CoqDog Copilot, which leverages the neuro-symbolic interplay between generative AI and the Coq theorem prover to form a productive ``generate-and-test'' loop, incrementally improving proofs based on failure information and human hints until valid proofs are achieved. Our research introduces innovative solutions to critical challenges in developing CoqDog Copilot, including addressing context limitations, enhancing the soundness of recommendation systems, defining effective metrics for measuring proof repair progress, and designing a statistically robust evaluation system for conversational quality assessment. We present a comprehensive evaluation of CoqDog Copilot's performance in proof repair across multiple samples from the Copland Coq proofbase, which consists of a total of 21,000 lines of Coq code. We have attained in excess of 60{\%} accuracy for proof generation using GPT-4 in one `shot', with approximately 30{\%} more lemmas proved given one additional user prompt (yielding 90{\%} correctness overall). With three `shots', the overall proof correctness rate increases to 97{\%}. We can generate Coq proofs with up to 50 proof steps using this technique. Our LLM-generated proofbase currently consists of over 1,400 lines of Copland Coq source.},
 address = {Cham},
 author = {Tahat, Amer
and Hardin, David
and Petz, Adam
and Alexander, Perry},
 booktitle = {Bridging the Gap Between AI and Reality},
 editor = {Steffen, Bernhard},
 isbn = {978-3-031-75434-0},
 pages = {145--166},
 publisher = {Springer Nature Switzerland},
 title = {Proof Repair Utilizing Large Language Models: A Case Study on the Copland Remote Attestation Proofbase},
 url = {https://link.springer.com/chapter/10.1007/978-3-031-75434-0_10},
 year = {2025}
}

@Article{Meng2024,
author="Meng, Qianshuang
and Zou, Weiqin
and Cai, Biyu
and Zhang, Jingxuan",
title="KeyTitle: towards better bug report title generation by keywords planning",
journal="Software Quality Journal",
year="2024",
month="Dec",
day="01",
volume="32",
number="4",
pages="1655--1682",
abstract="Bug reports play an important role in the software development and maintenance process. As the eye of a bug report, a concise and fluent title is always preferred and expected by developers as it could help them quickly seize the problem point and make better decisions in handling the bugs. However, in practice, not all titles filled by bug reporters are found to be of high quality; some may not carry essential bug-related information, and some may be hard to understand or contain extra noise. With the aim to reduce the burden of bug reporters and ease developers' life in handling bugs, we propose a deep learning-based technique named KeyTitle, to automatically generate a title for a given bug report. KeyTitle formulates the title generation problem as a one-sentence summarization task. It could be viewed as a Seq2Seq generation model (which generally directly generates target text based on source text) that incorporates keywords planning. Specifically, within KeyTitle, a transformer-based encoder-decoder model is enforced to generate a chain of keywords first from the detailed textual problem description, and then generate the target title by considering both these keywords and description content. Experiments over three large bug datasets collected from GitHub, Eclipse, and Apache shows that KeyTitle could outperform state-of-art title generation models relatively by up to 8.9-18.2{\$}{\$}{\backslash}{\%}{\$}{\$}, 11.4-30.4{\$}{\$}{\backslash}{\%}{\$}{\$}, and 13.0-18.0{\$}{\$}{\backslash}{\%}{\$}{\$}in terms of ROUGE-1, ROUGE-2, and ROUGE-L F1-scores; the titles generated by KeyTitle are also found to be better in terms of Relevance, Accuracy, Conciseness, Fluency in human evaluation. Besides generating titles from textual descriptions, KeyTitle is also found to have great potential in generating titles based on just a few keywords, a task that also has much value in bug reporting/handling practice.",
issn="1573-1367",
doi="10.1007/s11219-024-09695-z",
url="https://doi.org/10.1007/s11219-024-09695-z"
}


@inproceedings{10.1007/978-3-032-07884-1_9,
 abstract = {Software supply chain vulnerabilities arise when attackers exploit weaknesses by injecting vulnerable code into widely used packages or libraries within software repositories. While most existing approaches focus on identifying vulnerable packages or libraries, they often overlook the specific functions responsible for these vulnerabilities. Pinpointing vulnerable functions within packages or libraries is critical, as it can significantly reduce the risks associated with using open-source software. Identifying vulnerable patches is challenging because developers often submit code changes that are unrelated to vulnerability fixes. To address this issue, this paper introduces FuncVul, an innovative code chunk-based model for function-level vulnerability detection in C/C++ and Python, designed to identify multiple vulnerabilities within a function by focusing on smaller, critical code segments. To assess the model's effectiveness, we construct six code and generic code chunk based datasets using two approaches: (1) integrating patch information with large language models to label vulnerable samples and (2) leveraging large language models alone to detect vulnerabilities in function-level code. To design FuncVul vulnerability model, we utilise GraphCodeBERT fine tune model that captures both the syntactic and semantic aspects of code. Experimental results show that FuncVul outperforms existing state-of-the-art models, achieving an average accuracy of 87--92{\%} and an F1 score of 86--92{\%} across all datasets. Furthermore, we have demonstrated that our code-chunk-based FuncVul model improves 53.9{\%} accuracy and 42.0{\%} F1-score than the full function-based vulnerability prediction. The model code and datasets are publicly available on GitHub (https://github.com/sajalhalder/FuncVul).},
 address = {Cham},
 author = {Halder, Sajal
and Ahmed, Muhammad Ejaz
and Camtepe, Seyit},
 booktitle = {Computer Security -- ESORICS 2025},
 editor = {Nicomette, Vincent
and Benzekri, Abdelmalek
and Boulahia-Cuppens, Nora
and Vaidya, Jaideep},
 isbn = {978-3-032-07884-1},
 pages = {166--185},
 publisher = {Springer Nature Switzerland},
 title = {FuncVul: An Effective Function Level Vulnerability Detection Model Using LLM and Code Chunk},
 url = {https://link.springer.com/chapter/10.1007/978-3-032-07884-1_9},
 year = {2026}
}

@inproceedings{10.1007/978-3-032-00630-1_5,
 abstract = {Software vulnerabilities pose significant security threats, requiring effective mitigation. While Automated Program Repair (APR) has advanced in fixing general bugs, vulnerability patching---a security-critical aspect of APR---remains underexplored. This study investigates pre-trained language models, CodeBERT and CodeT5, for automated vulnerability patching across six datasets and four languages. We evaluate their accuracy and generalization to unknown vulnerabilities. Results show that while both models face challenges with fragmented or sparse context, CodeBERT performs comparatively better in such scenarios, whereas CodeT5 excels in capturing complex vulnerability patterns. CodeT5 also demonstrates superior scalability. Furthermore, we test fine-tuned models on both in-distribution (trained) and out-of-distribution (unseen) datasets. While fine-tuning improves in-distribution performance, models struggle to generalize to unseen data, highlighting challenges in robust vulnerability detection. This study benchmarks model performance, identifies limitations in generalization, and provides actionable insights to advance automated vulnerability patching for real-world security applications.},
 address = {Cham},
 author = {Khan, Zanis Ali
and Garg, Aayush
and Tang, Qiang},
 booktitle = {Availability, Reliability and Security},
 editor = {Coppens, Bart
and Volckaert, Bruno
and Naessens, Vincent
and De Sutter, Bjorn},
 isbn = {978-3-032-00630-1},
 pages = {73--87},
 publisher = {Springer Nature Switzerland},
 title = {A Multi-dataset Evaluation of Models for Automated Vulnerability Repair},
 url = {https://link.springer.com/chapter/10.1007/978-3-032-00630-1_5},
 year = {2025}
}

@inproceedings{10.1007/978-981-96-9275-0_38,
 abstract = {In today's world, our day-to-day lives depend on many software programs. The outage of the services of some crucial software like banking and airline will affect our lives badly. It is the necessity of this era to locate and repair the bug in software in less time. This paper presents a novel method to locate the bug at statement-level with the help of a code graph. The project code base is converted to a graph and stored in the graph database. The sub-graph related to the bug report is pulled from the graph database by using the bug report as the query. The nodes in the sub-graph are then listed in the decreasing order of their similarity score. Each node in the graph represents the statements in the code. Hence, the proposed method ensures fine-grained bug localization with the help of a code graph. Moreover, it gives the dependencies of the buggy code, which helps in test case generation. The proposed method reduces the effort for software maintenance and increases the productivity of the maintenance team.},
 address = {Singapore},
 author = {Sunitha, E. V.},
 booktitle = {ICT for Intelligent Systems},
 editor = {Choudrie, Jyoti
and Mahalle, Parikshit N.
and Perumal, Thinagaran
and Joshi, Amit},
 isbn = {978-981-96-9275-0},
 pages = {451--460},
 publisher = {Springer Nature Singapore},
 title = {Fine-Grained Bug Localization in Software Projects Using Code Graph},
 url = {https://link.springer.com/chapter/10.1007/978-981-96-9275-0_38},
 year = {2026}
}

@Article{Sindhwad2025,
author="Sindhwad, Parul V.
and Ranka, Prateek
and Muni, Siddhi
and Kazi, Faruk",
title="VulnArmor: mitigating software vulnerabilities with code resolution and detection techniques",
journal="International Journal of Information Technology",
year="2025",
month="Dec",
day="01",
volume="17",
number="9",
pages="5393--5408",
abstract="In today's swiftly evolving digital environment, the security and dependability of software applications are crucial. In light of industries' increasing reliance on software, identifying and mitigating vulnerabilities is essential for protecting data, systems, and user trust. With data-driven methodologies, there is increased interest in using Artificial Intelligence (AI) and Machine Learning (ML) for software assurance to construct trustworthy software systems. This research addresses the urgent need for an automated and comprehensive approach to code resolution and vulnerability detection, providing a robust solution to improve software security and reduce potential risks. Code resolution is implemented by fine-tuning Large Language Models (LLM) like Generative Pre-Trained Transformers (GPT)-2, Text-to-Text Transfer Transformers (T5), Bidirectional Encoder Representations from Transformers (BERT), and Large Language Model Meta AI (LLaMA). Secondly, vulnerable code detection plays a crucial role in evaluating the correctness of resolved code and identifying any remaining vulnerabilities. This essential step not only validates the efficacy of code resolution but also identifies areas where additional mitigation efforts are required. Utilizing Deep Learning (DL) models, the top performer of the study, Convolutional Neural Network (CNN), achieved a remarkable 93{\%} accuracy rate, demonstrating its prowess in protecting software applications against potential attacks.",
issn="2511-2112",
doi="10.1007/s41870-024-01775-4",
url="https://doi.org/10.1007/s41870-024-01775-4"
}


@Article{Luo2025,
author="Luo, Mingwan
and Liang, Yiqiong",
title="Network security vulnerability detection and repair model based on deep neural networks",
journal="Discover Artificial Intelligence",
year="2025",
month="Dec",
day="07",
volume="6",
number="1",
pages="18",
abstract="Due to the growing complexity of network infrastructures, this investigation intends to create a Deep Learning (DL)-based model for identifying and fixing network security issues in digital infrastructures. Using the CIC-IDS-2017 network security dataset obtained and preprocessed through Forward Fill (FFill) to remove noise and determine the missing values, along with Z-score normalization for data normalization, provides the compatibility network layer. Local Binary Patterns (LBP) is used for the feature extraction stage that transforms structured data into concise and distinctive patterns to detect security threats effectively. Artificial Gorilla Troops Optimizer-driven Malleable Deep Neural Network (AGTO-MDNN) architecture is proposed to accurately determine the patterns associated with various security flaws. The model categorizes the vulnerabilities according to categories and severity levels, which allows for more focused responses. An integrated repair mechanism automatically suggests mitigations like software patches, configuration changes, or policy enforcements. Experimental evaluations establish that the proposed AGTO-MDNN model achieves superior detection performance by providing an accuracy of 97.32{\%}, a precision of 95.27{\%}, a recall of 96.58{\%}, and an F1-score of 95.92{\%} in comparison with the traditional DL-based models. An integrated repair mechanism improves response efficiency and reduces human intervention in the vulnerability management lifecycle. This research proposes a scalable and intelligent solution for proactive cybersecurity management and sets the foundation for integrating Machine Learning (ML) models in real-time vulnerability defense systems.",
issn="2731-0809",
doi="10.1007/s44163-025-00666-2",
url="https://doi.org/10.1007/s44163-025-00666-2"
}


@inproceedings{10.1007/978-3-032-02049-9_9,
 abstract = {The significance of early vulnerability identification in ensuring security during software development cannot be denied. In this research, we introduce CWEpredBELL, a unique automated vulnerability prediction method that makes use of a modified pre-trained language model derived from CodeBERT. With a binary classification layer, an improved optimizer, and a fine-tuned loss function to boost model performance, our method is especially tailored for identifying vulnerabilities in source code. We used cross-validation techniques and the Local Interpretable Model-Agnostic Explanations (LIME) approach to identify particular lines of error in the source code. The experimental comparison demonstrates that CWEpredBELL is an effective method of automatically identifying vulnerabilities.},
 address = {Cham},
 author = {Alam, Syeda Sadia
and Akter, Mst Shapna
and Cuzzocrea, Alfredo},
 booktitle = {Database and Expert Systems Applications},
 editor = {Wrembel, Robert
and Kotsis, Gabriele
and Tjoa, A. Min
and Khalil, Ismail},
 isbn = {978-3-032-02049-9},
 pages = {122--129},
 publisher = {Springer Nature Switzerland},
 title = {Improving Software Security Through a LLM-Based Vulnerability Detection Model},
 url = {https://link.springer.com/chapter/10.1007/978-3-032-02049-9_9},
 year = {2026}
}

@Article{Tihanyi2024,
author="Tihanyi, Norbert
and Bisztray, Tamas
and Ferrag, Mohamed Amine
and Jain, Ridhi
and Cordeiro, Lucas C.",
title="How secure is AI-generated code: a large-scale comparison of large language models",
journal="Empirical Software Engineering",
year="2024",
month="Dec",
day="21",
volume="30",
number="2",
pages="47",
abstract="This study compares state-of-the-art Large Language Models (LLMs) on their tendency to generate vulnerabilities when writing C programs using a neutral zero-shot prompt. Tihanyi et al. introduced the FormAI dataset at PROMISE '23, featuring 112,000 C programs generated by GPT-3.5-turbo, with over 51.24{\%} identified as vulnerable. We extended that research with a large-scale study involving 9 state-of-the-art models such as OpenAI's GPT-4o-mini, Google's Gemini Pro 1.0, TII's 180 billion-parameter Falcon, Meta's 13 billion-parameter Code Llama, and several other compact models. Additionally, we introduce the FormAI-v2 dataset, which comprises 331 000 compilable C programs generated by these LLMs. Each program in the dataset is labeled based on the vulnerabilities detected in its source code through formal verification, using the Efficient SMT-based Context-Bounded Model Checker (ESBMC). This technique minimizes false positives by providing a counterexample for the specific vulnerability and reduces false negatives by thoroughly completing the verification process. Our study reveals that at least 62.07{\%} of the generated programs are vulnerable. The differences between the models are minor, as they all show similar coding errors with slight variations. Our research highlights that while LLMs offer promising capabilities for code generation, deploying their output in a production environment requires proper risk assessment and validation.",
issn="1573-7616",
doi="10.1007/s10664-024-10590-1",
url="https://doi.org/10.1007/s10664-024-10590-1"
}


@inproceedings{10.1007/978-3-032-08124-7_32,
 abstract = {This paper presents BlockLens, a supervised, trace-level framework for detecting malicious Ethereum transactions using large language models (LLMs). Unlike prior approaches limited to static features or storage-level abstractions, BlockLens processes complete execution traces, capturing opcode sequences, memory information, gas usage, and call structures to accurately represent the runtime behavior of each transaction. This framework harnesses the exceptional reasoning capabilities of LLMs for long input sequences and is fine-tuned on transaction data. We design a tokenization strategy aligned with Ethereum Virtual Machine (EVM) semantics, mapping execution traces into interpretable tokens. Each transaction captures its complete execution trace through simulated execution and is then sliced into overlapping chunks using a sliding window, allowing for long-range context modeling within memory constraints. During inference, the model outputs both a binary decision and a probability score indicating the likelihood of malicious behavior. We implement the framework based on LLaMA 3.2-1B backbone and fine-tune the model using Low-Rank Adaptation (LoRA). We evaluate it on a curated dataset containing both real-world attacks and normal DeFi transactions. BlockLens outperforms representative baselines, achieving higher F1 scores and recall at top-k thresholds than representative baselines. Additionally, BlockLens offers interpretable chunk-level outputs by localizing suspicious trace segments that enhance explainability, facilitating rapid forensic analysis and actionable decision-making in security-critical environments.},
 address = {Cham},
 author = {Feng, Chi
and Fan, Lei},
 booktitle = {Information Security},
 editor = {Cha, Sang Kil
and Park, Jeongeun},
 isbn = {978-3-032-08124-7},
 pages = {581--601},
 publisher = {Springer Nature Switzerland},
 title = {BlockLens: Detecting Malicious Transactions in Ethereum Using LLM Techniques},
 url = {https://link.springer.com/chapter/10.1007/978-3-032-08124-7_32},
 year = {2026}
}

@inproceedings{10.1007/978-3-031-97620-9_7,
 abstract = {The Linux kernel, a cornerstone of modern computing across servers, mobile devices, and embedded systems, is increasingly vulnerable due to its vast complexity and continuous evolution. Fuzz testing has emerged as a critical technique for identifying kernel vulnerabilities, with tools like Syzkaller uncovering thousands of bugs through system call (syscall) fuzzing. However, the effectiveness of such tools relies heavily on manually crafted syscall specifications, a process that struggles to keep pace with the kernel{\^a}{\texteuro}{\texttrademark}s dynamic nature and intricate semantics. This paper presents SyzForge, a novel automated framework to generate precise syscall specifications for Linux kernel drivers. SyzForge integrates four stages: static analysis to distill kernel semantics from source code, symbolic execution for dynamic constraint-based parameter solving, fuzzing with Syzkaller to assess coverage, and large language model (LLM)-driven refinement to correct specification errors. Evaluated on Linux kernel version 6.12, SyzForge achieves a 13.3{\%} increase in code coverage compared to default Syzkaller specifications, outperforming KernelGPT by 4.3{\%}, SyzDescribe by 5.5{\%}, and DIFUZE by 24.3{\%}. Furthermore, it identifies 19 previously unreported vulnerabilities, demonstrating its practical impact. By automating a traditionally manual process, SyzForge enhances fuzzing efficiency, improves vulnerability detection, and strengthens kernel security. This work addresses key limitations in existing specification generation methods, offering a scalable and adaptable solution to safeguard the Linux ecosystem amid its ongoing development.},
 address = {Cham},
 author = {Tang, ZhiZhuo
and Lin, Jian
and Dong, Weiyu
and Ma, Hang
and Liu, Tieming},
 booktitle = {Detection of Intrusions and Malware, and Vulnerability Assessment},
 editor = {Egele, Manuel
and Moonsamy, Veelasha
and Gruss, Daniel
and Carminati, Michele},
 isbn = {978-3-031-97620-9},
 pages = {118--139},
 publisher = {Springer Nature Switzerland},
 title = {SyzForge: An Automated System Call Specification Generation Process for Efficient Kernel Fuzzing},
 url = {https://link.springer.com/chapter/10.1007/978-3-031-97620-9_7},
 year = {2025}
}

@inproceedings{10.1007/978-3-031-70879-4_14,
 abstract = {The significant increase in software production driven by automation and faster development lifecycles has resulted in a corresponding surge in software vulnerabilities. In parallel, the evolving landscape of software vulnerability detection, highlighting the shift from traditional methods to machine learning and large language models (LLMs), provides massive opportunities at the cost of resource-demanding computations. This paper thoroughly analyses LLMs' capabilities in detecting vulnerabilities within source code by testing models beyond their usual applications to study their potential in cybersecurity tasks. We evaluate the performance of six open-source models that are specifically trained for vulnerability detection against six general-purpose LLMs, three of which were further fine-tuned on a dataset that we compiled. Our dataset, alongside five state-of-the-art benchmark datasets, were used to create a pipeline to leverage a binary classification task, namely classifying code into vulnerable and non-vulnerable. The findings highlight significant variations in classification accuracy across benchmarks, revealing the critical influence of fine-tuning in enhancing the detection capabilities of small LLMs over their larger counterparts, yet only in the specific scenarios in which they were trained. Further experiments and analysis also underscore the issues with current benchmark datasets, particularly around mislabeling and their impact on model training and performance, which raises concerns about the current state of practice. We also discuss the road ahead in the field suggesting strategies for improved model training and dataset curation.},
 address = {Cham},
 author = {Guo, Yuejun
and Patsakis, Constantinos
and Hu, Qiang
and Tang, Qiang
and Casino, Fran},
 booktitle = {Computer Security -- ESORICS 2024},
 editor = {Garcia-Alfaro, Joaquin
and Kozik, Rafa{\l}
and Chora{\'{s}}, Micha{\l}
and Katsikas, Sokratis},
 isbn = {978-3-031-70879-4},
 pages = {271--289},
 publisher = {Springer Nature Switzerland},
 title = {Outside the Comfort Zone: Analysing LLM Capabilities in Software Vulnerability Detection},
 url = {https://link.springer.com/chapter/10.1007/978-3-031-70879-4_14},
 year = {2024}
}

@Article{He2025,
author="He, Haitao
and Li, Shibo
and Li, Yuxiang
and Li, Yang",
title="GTVD: a multi-level aggregation vulnerability detection method based on full-dependency program graph",
journal="Cluster Computing",
year="2025",
month="Sep",
day="03",
volume="28",
number="10",
pages="656",
abstract="In modern software development life cycles, proactive vulnerability discovery and remediation play crucial roles in ensuring application security. However, current deep learning-based vulnerability detection methods frequently face limitations due to overly simplistic feature extraction procedures and inadequate handling of long-range dependency relationships. In this paper, we present GTVD, a graph-based vulnerability detection framework for C/C++ source code, which addresses these challenges through three key innovations. First, we introduce the Full Dependency Program Graph (FDPG), a novel intermediate representation that comprehensively encodes both syntactic structures and semantic relationships within source code. This advancement overcomes the feature representation constraints inherent in conventional code attribute graphs. Our architecture employs a hierarchical Graph Neural Network to systematically extract structural patterns from the FDPG representation, ensuring a robust analysis of the program's inherent structures. At the core of our feature extraction mechanism lies the Multi-Level Message Aggregation (MLMA) strategy. This innovative approach enables progressive integration of information across multiple neighborhood orders, effectively capturing both local and global program dependencies. To mitigate feature degradation in long-range dependencies, we develop an Adaptive Weighted Aggregation (WAG) mechanism that dynamically adjusts feature contributions during graph-level representation learning. Comprehensive evaluations on three large-scale public datasets demonstrate GTVD's superior performance. Our method achieves an average improvement of 7.76{\%} across four evaluation metrics compared to the baseline, thereby confirming our method's enhanced capability to identify complex vulnerability patterns.",
issn="1573-7543",
doi="10.1007/s10586-025-05506-7",
url="https://doi.org/10.1007/s10586-025-05506-7"
}


@inproceedings{10.1007/978-3-031-95127-5_16,
 abstract = {This study explores the potential of Large Language Models (LLMs) in generating automated test cases to enhance software testing and bug detection. By evaluating 100 GitHub issues, the research investigates whether LLM-generated tests could identify bugs prior to their resolution. Findings reveal that LLMs can be effective in certain cases, particularly with simpler code that has fewer dependencies and lower complexity metrics. However, the success of these tests diminishes as code complexity, imports, and function calls increase. The study highlights the potential of LLMs in software testing, while also noting the challenges associated with testing complex code structures.},
 address = {Cham},
 author = {Pehlke, Marcel
and Eudenbach, Cindy
and Graw, Maximilian
and Jansen, Marc},
 booktitle = {Computational Science and Computational Intelligence},
 editor = {Arabnia, Hamid R.
and Deligiannidis, Leonidas
and Shenavarmasouleh, Farzan
and Amirian, Soheyla
and Ghareh Mohammadi, Farid},
 isbn = {978-3-031-95127-5},
 pages = {205--217},
 publisher = {Springer Nature Switzerland},
 title = {Towards Automated Software Testing: Evaluating LLMs in Generating Effective Test Cases},
 url = {https://link.springer.com/chapter/10.1007/978-3-031-95127-5_16},
 year = {2025}
}

@inproceedings{10.1007/978-981-95-0011-6_10,
 abstract = {Unit tests are critical for software reliability, yet manually writing them is laborious and time-consuming. While traditional tools like EvoSuite achieve high coverage, their low readability limits adoption. Recent LLM-based methods (e.g., AthenaTest, ChatTester) improve test readability but suffer from suboptimal prompt design and rigid template-based instructions that lack adaptability to different focal methods. To address these limitations, we propose AgentTester---an LLM-based method with three core components: 1) AutoPrompting that extracts the essential information of focal method and infers method intent, 2) Prompt Distillation to refine tailored test instructions via multi-temperature sampling before generating the unit tests, and 3) Validation-Repair for iterative error correction of buggy test cases. Experimental evaluations show AgentTester surpasses EvoSuite in line coverage and outperforms both AthenaTest and ChatTester in compilation rate, test correctness, and overall coverage. These results demonstrate AgentTester's effectiveness in generating reliable, adaptable and universality unit tests.},
 address = {Singapore},
 author = {Chen, Honghui
and Chen, Kaiqing
and Zhang, Fanlong
and Wang, Tao
and Cheng, Lianglun},
 booktitle = {Advanced Intelligent Computing Technology and Applications},
 editor = {Huang, De-Shuang
and Zhang, Chuanlei
and Zhang, Qinhu
and Pan, Yijie},
 isbn = {978-981-95-0011-6},
 pages = {114--126},
 publisher = {Springer Nature Singapore},
 title = {AgentTester: An LLM-Based Tool for Unit Test Generation with Automatically Generated Prompts},
 url = {https://link.springer.com/chapter/10.1007/978-981-95-0011-6_10},
 year = {2025}
}

@Article{Jiao2025,
author="Jiao, Junfeng
and Afroogh, Saleh
and Xu, Yiming
and Phillips, Connor",
title="Navigating LLM ethics: advancements, challenges, and future directions",
journal="AI and Ethics",
year="2025",
month="Dec",
day="01",
volume="5",
number="6",
pages="5795--5819",
abstract="This study addresses ethical issues surrounding Large Language Models (LLMs) within the field of artificial intelligence. It explores the common ethical challenges posed by both LLMs and other AI systems, such as privacy and fairness, as well as ethical challenges uniquely arising from LLMs. It highlights challenges such as hallucination, verifiable accountability, and decoding censorship complexity, which are unique to LLMs and distinct from those encountered in traditional AI systems. The study underscores the need to tackle these complexities to ensure accountability, reduce biases, and enhance transparency in the influential role that LLMs play in shaping information dissemination. It proposes mitigation strategies and future directions for LLM ethics, advocating for interdisciplinary collaboration. It recommends ethical frameworks tailored to specific domains and dynamic auditing systems adapted to diverse contexts. This roadmap aims to guide responsible development and integration of LLMs, envisioning a future where ethical considerations govern AI advancements in society.",
issn="2730-5961",
doi="10.1007/s43681-025-00814-5",
url="https://doi.org/10.1007/s43681-025-00814-5"
}


@inproceedings{10.1007/978-3-032-03215-7_18,
 abstract = {With the continuous progress of mobile communication technology and the continuous growth of network demand, the network structure is becoming increasingly complex. However, traditional network management is difficult to meet the needs of future development. In the future, intelligent autonomous networks could perform flexibly and efficiently with the help of AI-driven automated analysis and multidimensional data perception. Still, at the same time, this also requires a more intelligent approach to network management. The large language models (LLMs) represented by generative pre-trained transformer (GPT) will play an important role in promoting intelligent autonomy of communication networks. Therefore, this paper studies the specific methods of GPT promoting intelligent autonomy of communication networks, and analyzes how GPT enables intelligent autonomy in communication networks from different perspectives. Specifically, it includes GPT-assisted base station site selection, antenna design optimization and virtualized intelligent slicing, as well as network operations and maintenance from anomaly detection to automatic recovery, and network traffic optimization, coverage optimization and signaling tracing. Finally, we also propose some challenges, such as the inconsistent quality of training data sets, insufficient computing resources, and high risks to network privacy and security. We also propose some corresponding solutions and predict future development trends.},
 address = {Cham},
 author = {Yang, Yifan
and Yang, Zheng
and Zeng, Jie
and Dan, Yuran
and Bai, Zhenming
and Xu, Chen},
 booktitle = {Communications and Networking},
 editor = {Ning, Zhaolong
and Guo, Song
and Wang, Xiaojie},
 isbn = {978-3-032-03215-7},
 pages = {262--275},
 publisher = {Springer Nature Switzerland},
 title = {GPT Promotes Intelligent Autonomy in Communication Networks},
 url = {https://link.springer.com/chapter/10.1007/978-3-032-03215-7_18},
 year = {2026}
}

@inproceedings{10.1007/978-3-032-01486-3_20,
 abstract = {Fuzz testing is a widely adopted testing methodology in software engineering that offers efficient means of testing software and identifying vulnerabilities. This paper presents a universal framework aimed at improving the efficiency of fuzz testing for Autonomous Systems (AS), particularly Unmanned Aerial Vehicle (UAV) autonomous systems. At its core is SaFliTe (Safe Flight Testing), a predictive component that evaluates whether a test case meets predefined safety criteria. By leveraging the large language model (LLM) with information about the test objective and the AS state, SaFliTe assesses the relevance of each test case. We evaluated SaFliTe by instantiating it with various LLMs, including GPT-3.5, Mistral-7B, and Llama2-7B, and integrating it into four fuzz testing tools: PGFuzz, DeepHyperion-UAV, CAMBA, and TUMB. These tools are designed specifically for testing autonomous drone control systems. The experimental results demonstrate that, compared to PGFuzz, SaFliTe increased the likelihood of selecting operations that triggered bug occurrences in each fuzzing iteration by an average of 93.1{\%}. Additionally, after integrating SaFliTe, the ability of DeepHyperion-UAV, CAMBA, and TUMB to generate test cases that caused system safety violations increased by 234.5{\%}, 33.3{\%}, and 17.8{\%}, respectively. The benchmark used in evaluation was from CPS-UAV Tool Competition 2024.},
 address = {Cham},
 author = {Zhu, Taohong
and Skapars, Adrians
and Mackenzie, Fardeen
and Kehoe, Declan
and Newton, William
and Embury, Suzanne
and Sun, Youcheng},
 booktitle = {Towards Autonomous Robotic Systems},
 editor = {Cavalcanti, Ana
and Foster, Simon
and Richardson, Robert},
 isbn = {978-3-032-01486-3},
 pages = {245--258},
 publisher = {Springer Nature Switzerland},
 title = {SaFliTe: Fuzzing Autonomous Systems via Large Language Models},
 url = {https://link.springer.com/chapter/10.1007/978-3-032-01486-3_20},
 year = {2026}
}

@inproceedings{10.1007/978-981-95-3537-8_10,
 abstract = {In the software supply chain, vulnerability impacts extend beyond the initially reported package, affecting dependent packages within the ecosystem. This presents a challenge for maintainers to determine if their systems are vulnerable when invoking the affected packages through indirect or nested dependencies. To address this challenge, we propose PackShield to efficiently assess the impact of vulnerabilities within package management systems. PackShield integrates code-based detection and testcase-based verification to accurately confirm if a package is affected by the vulnerability. Specifically, it extracts vulnerability and patch code from vulnerability reports and locates this code within target packages. If the vulnerability exists without an applied patch, it further generates a Proof-of-Concept via directed fuzzing to verify its presence. The directed fuzzing is performed on a sliced harness containing the critical path to trigger the vulnerability rather than the whole application. We evaluate PackShield on two popular package management systems (i.e., Ubuntu's APT system and Fedora's DNF system) with a dataset of 3,321 vulnerability reports. We identified 345 security issues in 6 Ubuntu versions and 40 security issues in 4 Fedora versions, especially pointing out the security issues in old OS versions. We also show that PackShield outperforms other tools (i.e., Dependency-Check and V1SCAN).},
 address = {Singapore},
 author = {Wang, Zibo
and Jia, Xiangkun
and Yan, Jia
and Yang, Yi
and Huang, Huafeng
and Su, Purui},
 booktitle = {Information and Communications Security },
 editor = {Han, Jinguang
and Xiang, Yang
and Cheng, Guang
and Susilo, Willy
and Chen, Liquan},
 isbn = {978-981-95-3537-8},
 pages = {175--194},
 publisher = {Springer Nature Singapore},
 title = {Towards Efficient C/C++ Vulnerability Impact Assessment in Package Management Systems},
 url = {https://link.springer.com/chapter/10.1007/978-981-95-3537-8_10},
 year = {2026}
}

@inproceedings{10.1007/978-3-032-11358-0_21,
 abstract = {Social security programs aim to protect vulnerable populations; however, accurately identifying individuals with significantly lower incomes than their peers (accounting for age, occupation, and education level) remains an operational challenge. This article proposes an innovative method for detecting economic vulnerability by combining income data enrichment with large language models in a multi-agent architecture, unsupervised clustering techniques, and statistical heuristics. The developed algorithm analyzes demographic and labor-related variables to estimate expected annual income by profile, thereby identifying atypical discrepancies that suggest vulnerability. This approach not only optimizes the prioritization of beneficiaries for targeted assistance but also serves as a preventive mechanism against the inadvertent exclusion of eligible groups. Preliminary results demonstrate the method's effectiveness in detecting hidden vulnerability particularly among young adults aged 17--23, whose high underemployment rates ({\$}{\$}{\backslash}approx 40{\backslash}{\%}{\$}{\$}≈40{\%}) in recent national statistics closely align with the concentration of vulnerability detected. These findings underscore its potential as a complementary tool to enhance equity and efficiency in social policy implementation.},
 address = {Cham},
 author = {Herrera-Semenets, Vitali
and Bustio-Mart{\'i}nez, L{\'a}zaro
and Berg, Jan van den
and {\'A}lvarez-Carmona, Miguel {\'A}ngel},
 booktitle = {Progress in Artificial Intelligence and Pattern Recognition},
 editor = {Hern{\'a}dez Heredia, Yanio
and Mili{\'a}n N{\'u}{\~{n}}ez, Vladimir
and Ruiz-Shulcloper, Jos{\'e}},
 isbn = {978-3-032-11358-0},
 pages = {255--267},
 publisher = {Springer Nature Switzerland},
 title = {Detecting Economic Vulnerability via Multi-Agent LLM Architecture and Context-Aware Cluster Analysis},
 url = {https://link.springer.com/chapter/10.1007/978-3-032-11358-0_21},
 year = {2026}
}

@Inbook{Dakhel2024,
author="Dakhel, Arghavan Moradi
and Nikanjam, Amin
and Khomh, Foutse
and Desmarais, Michel C.
and Washizaki, Hironori",
editor="Nguyen-Duc, Anh
and Abrahamsson, Pekka
and Khomh, Foutse",
title="Generative AI for Software Development: A Family of Studies on Code Generation",
bookTitle="Generative AI for Effective Software Development",
year="2024",
publisher="Springer Nature Switzerland",
address="Cham",
pages="151--172",
abstract="The rapid advancements in generative artificial intelligence (AI) offer multiple opportunities for its application in various domains, including software engineering (SE). This chapter explores the benefits and challenges of utilizing generative AI for different activities in the software development cycle that involve code generation. We review different approaches leveraging generative AI, either independently or in combination with traditional SE techniques, to complete a diverse set of tasks including feature implementation, generating test cases, and repairing programs. Additionally, we discuss the potential pitfalls of using generative AI to perform such SE tasks, as well as the quality of the code generated by these models. Finally, we explore research opportunities in harnessing generative AI, with a particular emphasis on tasks that require code generation.",
isbn="978-3-031-55642-5",
doi="10.1007/978-3-031-55642-5_7",
url="https://doi.org/10.1007/978-3-031-55642-5_7"
}


@Inbook{Bianchini2025,
author="Bianchini, Filippo
and Marinacci, Matteo",
editor="De Luzi, Francesca
and Monti, Flavia
and Mecella, Massimo",
title="Exploring Large Language Models in Information Systems: A Survey",
bookTitle="Engineering Information Systems with Large Language Models",
year="2025",
publisher="Springer Nature Switzerland",
address="Cham",
pages="77--109",
abstract="The integration of large language models (LLMs) into the life cycle of information systems (ISs) is transforming traditional methodologies across multiple phases, from planning and design to monitoring and maintenance. This chapter provides a comprehensive survey of the most impactful research on LLMs in key IS life cycle stages, analyzing their applications, benefits, and limitations. Through a literature review, we examine how LLMs are utilized and can be leveraged across different phases of the IS life cycle: planning, design, development, testing and validation, and monitoring and maintenance. Additionally, we analyze their role in tasks that, while not strictly part of the IS life cycle, are increasingly relevant, such as information extraction and data analytics. This chapter provides an in-depth analysis of recent LLMs applications in each phase, evaluating their impact on traditional methodologies and assessing their effectiveness in automating processes, improving decision-making, and enhancing overall system efficiency.",
isbn="978-3-031-92285-5",
doi="10.1007/978-3-031-92285-5_5",
url="https://doi.org/10.1007/978-3-031-92285-5_5"
}


@Article{Yousuf2025,
author="Yousuf, Mir Mohammad
and Sofi, Shabir Ahmad",
title="Bug Classification in quantum software: a rule-based framework and its evaluation",
journal="Automated Software Engineering",
year="2025",
month="Dec",
day="18",
volume="33",
number="1",
pages="39",
abstract="Accurate bug classification is essential for improving software quality, particularly in the emerging and complex domain of quantum computing. This paper introduces a rule-based framework for automated classification of quantum software issues across five dimensions: bug type, bug category, severity, quality attribute, and quantum-specific subtype. The framework integrates weighted keyword heuristics, TF--IDF similarity, and domain-specific rules to capture both general software defects and quantum-domain failure modes. The proposed approach was applied to 12,910 issues from 36 Qiskit repositories and validated on a stratified subset of 4,984 manually annotated issues. On this manually labeled subset, the framework achieved accuracies between 0.82 and 0.85 and macro-F1 scores ranging from 0.68 to 0.77, demonstrating strong agreement with human annotations without requiring supervised training. When compared with standard machine-learning baselines (Logistic Regression, Decision Tree, Random Forest, Gradient Boosting), the rule-based approach consistently outperformed all models across tasks, showing particularly large improvements in fine-grained and low-frequency categories such as Category (macro-F1: 0.26 vs. 0.69) and Quantum-Specific Subtype (0.15 vs. 0.77). Beyond predictive accuracy, the framework was applied to real-world 12,910 issues from 36 Qiskit repositories for large-scale distributional analysis. The results revealed that approximately 67{\%} of issues were classical and 27{\%} quantum-specific, with interoperability, usability, and maintainability identified as the most frequently affected quality attributes. Low-severity issues dominated (68.8{\%}), while critical bugs accounted for around 21{\%}. Quantum-specific defects were most prevalent at the circuit and gate abstraction levels, reflecting the hybrid and hardware-constrained nature of current quantum software development. Overall, the proposed rule-based framework offers a transparent, interpretable, and empirically validated approach for automated bug triaging in quantum software. Beyond its immediate practical utility, it provides a reproducible methodological framework that can support future hybrid and learning-based advances in quantum software engineering.",
issn="1573-7535",
doi="10.1007/s10515-025-00585-7",
url="https://doi.org/10.1007/s10515-025-00585-7"
}


@inproceedings{10.1007/978-981-96-6008-7_22,
 abstract = {The advancement of software vulnerability detection tools has accelerated in recent years. However, the prevalence and severity of vulnerabilities continue to escalate, posing significant threats to computer security and information safety. Numerous detection methodologies have been proposed to address this, with machine learning-based approaches demonstrating notable promise. In this paper, we present a comprehensive review of state-of-the-art (SOTA) architectures that leverage Deep Learning (DL) and Natural Language Processing (NLP) or Large Language Models (LLMs) for identifying vulnerabilities. We systematically examine the efficiency of these cutting-edge architectures and performance analysis. We aim to uncover novel approaches for maximizing the potential of existing architectures to enhance vulnerability detection. During our research, we identified three key research questions: effective integration of NLP and DL technologies, strengths and limitations of LLMs in this domain, and comparative analysis of LLMs versus integrated NLP-DL approaches. In addition, we discuss the challenges and experimental constraints encountered in this domain, offering insights into future research directions. This study aims to inspire further exploration of innovative methodologies and contribute to the development of more robust cybersecurity solutions.},
 address = {Singapore},
 author = {Tran Dinh, Khoa
and Bui Vuong Tam, Anh
and Nguyen Vo Tien, Loc
and Nguyen Phan Quoc, Dat
and To, Trong-Nghia
and The Duy, Phan
and Pham, Van-Hau},
 booktitle = {Intelligent Information and Database Systems},
 editor = {Nguyen, Ngoc Thanh
and Matsuo, Tokuro
and Gaol, Ford Lumban
and Manolopoulos, Yannis
and Fujita, Hamido
and Hong, Tzung-Pei
and Wojtkiewicz, Krystian},
 isbn = {978-981-96-6008-7},
 pages = {301--313},
 publisher = {Springer Nature Singapore},
 title = {An Empirical Review of the Effectiveness of Different Language Processing Approaches in Software Code Vulnerability Detection},
 url = {https://link.springer.com/chapter/10.1007/978-981-96-6008-7_22},
 year = {2025}
}

@Article{Rafi2025,
author="Rafi, Md Nakhla
and Pacheco, Lorena Barreto Simedo
and Chen, An Ran
and Yang, Jinqiu
and Chen, Tse-Hsun Peter",
title="SBEST: Spectrum-based fault localization without fault-triggering tests",
journal="Empirical Software Engineering",
year="2025",
month="Nov",
day="15",
volume="31",
number="1",
pages="16",
abstract="Fault localization is a critical step in software maintenance. Yet, many existing techniques, such as Spectrum-Based Fault Localization (SBFL), rely heavily on the availability of fault-triggering tests to be effective. In practice, especially for crash-related bugs, such tests are frequently unavailable. Meanwhile, bug reports containing stack traces often serve as the only available evidence of runtime failures and provide valuable context for debugging. This study investigates the feasibility of using stack traces from crash reports as proxies for fault-triggering tests in SBFL. Our empirical analysis of 60 crash-report bugs in Defects4J reveals that only 3.33{\%} of these bugs have fault-triggering tests available at the time of the bug report creation. However, 98.3{\%} of bug fixes directly address the exception observed in the stack trace, and 78.3{\%} of buggy methods are reachable within an average of 0.34 method calls from the stack trace. These findings underscore the diagnostic value of stack traces in the absence of failing tests. Motivated by these findings, we propose SBEST, a novel approach that integrates stack trace information with test coverage data to perform fault localization when fault-triggering tests are missing. SBEST shows an improvement, with a 32.22{\%} increase in Mean Average Precision (MAP) and a 17.43{\%} increase in Mean Reciprocal Rank (MRR) compared to baseline approaches under the scenario where fault-triggering tests are absent.",
issn="1573-7616",
doi="10.1007/s10664-025-10761-8",
url="https://doi.org/10.1007/s10664-025-10761-8"
}


@Article{Xu2025,
author="Xu, Caixu
and Guo, Hui
and Cen, Caicun
and Chen, Minglang
and Tao, Xiongjie
and He, Jie",
title="Efficient program optimization through knowledge-enhanced LoRA fine-tuning of large language models",
journal="The Journal of Supercomputing",
year="2025",
month="Jun",
day="10",
volume="81",
number="8",
pages="1006",
abstract="Source code optimization enables developers to enhance programs at the human--computer interaction level, thereby improving development efficiency and product quality. With the rise of large language models (LLMs), fine-tuning and prompting have become mainstream solutions for this task. However, both approaches present challenges: fine-tuning is resource-intensive due to the exponential growth in the scale of LLMs, whereas prompting, although resource-efficient, struggles to generate high-quality optimized programs. In this paper, we present CodeOPT, a LoRA-driven approach for fine-tuning LLMs to optimize C/C++ code. Instead of fine-tuning all LLM parameters, CodeOPT leverages LoRA to fine-tune only an optimization adapter, significantly reducing the number of trainable parameters. Additionally, we incorporate prior optimization knowledge during fine-tuning and introduce optimization-based instruction fine-tuning, enabling LLMs to effectively learn from external knowledge sources to improve program optimization. To evaluate the effectiveness of CodeOPT, we benchmarked it against several baselines on challenging programming tasks from different code completion platforms. Experimental results demonstrate that CodeOPT outperforms all baselines, including the state of the art, while keeping modifications to the original program minimal.",
issn="1573-0484",
doi="10.1007/s11227-025-07378-5",
url="https://doi.org/10.1007/s11227-025-07378-5"
}


@Article{Dathathri2024,
author="Dathathri, Sumanth
and See, Abigail
and Ghaisas, Sumedh
and Huang, Po-Sen
and McAdam, Rob
and Welbl, Johannes
and Bachani, Vandana
and Kaskasoli, Alex
and Stanforth, Robert
and Matejovicova, Tatiana
and Hayes, Jamie
and Vyas, Nidhi
and Merey, Majd Al
and Brown-Cohen, Jonah
and Bunel, Rudy
and Balle, Borja
and Cemgil, Taylan
and Ahmed, Zahra
and Stacpoole, Kitty
and Shumailov, Ilia
and Baetu, Ciprian
and Gowal, Sven
and Hassabis, Demis
and Kohli, Pushmeet",
title="Scalable watermarking for identifying large language model outputs",
journal="Nature",
year="2024",
month="Oct",
day="01",
volume="634",
number="8035",
pages="818--823",
abstract="Large language models (LLMs) have enabled the generation of high-quality synthetic text, often indistinguishable from human-written content, at a scale that can markedly affect the nature of the information ecosystem1--3. Watermarking can help identify synthetic text and limit accidental or deliberate misuse4, but has not been adopted in production systems owing to stringent quality, detectability and computational efficiency requirements. Here we describe SynthID-Text, a production-ready text watermarking scheme that preserves text quality and enables high detection accuracy, with minimal latency overhead. SynthID-Text does not affect LLM training and modifies only the sampling procedure; watermark detection is computationally efficient, without using the underlying LLM. To enable watermarking at scale, we develop an algorithm integrating watermarking with speculative sampling, an efficiency technique frequently used in production systems5. Evaluations across multiple LLMs empirically show that SynthID-Text provides improved detectability over comparable methods, and standard benchmarks and human side-by-side ratings indicate no change in LLM capabilities. To demonstrate the feasibility of watermarking in large-scale-production systems, we conducted a live experiment that assessed feedback from nearly 20{\thinspace}million Gemini6 responses, again confirming the preservation of text quality. We hope that the availability of SynthID-Text7 will facilitate further development of watermarking and responsible use of LLM systems.",
issn="1476-4687",
doi="10.1038/s41586-024-08025-4",
url="https://doi.org/10.1038/s41586-024-08025-4"
}


@inproceedings{10.1007/978-3-032-09694-4_31,
 abstract = {Proving the security and robustness of software systems is a longstanding challenge in computer science and cybersecurity, particularly when programs are exposed to unexpected or invalid inputs that could trigger vulnerabilities. Fuzz testing has become an effective method to evaluate software resilience by generating arbitrary inputs. However, traditional fuzzing approaches require substantial manual effort, including selecting a fuzzing framework, crafting suitable targets, and optimizing input generation to achieve desired coverage levels. This research investigates the potential of large language models (LLMs) as a generative tool to automate key fuzz testing processes, reducing manual overhead while enhancing coverage and effectiveness. By analyzing the applicability of LLMs to fuzz testing, this study highlights emerging trends, presents successful methodologies, and discusses the opportunities for improving software reliability and security through a more adaptive, intelligent approach to input generation. Additionally, this work explores the integration of multiple LLMs, the use of prompt engineering, and the potential for LLM-driven bug detection and remediation, offering new insights into advancing fuzz testing practices.},
 address = {Cham},
 author = {Hardgrove, Ian
and Xu, Shengjie},
 booktitle = {Proceedings of the International Symposium on Intelligent Computing and Networking 2025},
 editor = {Rodriguez Martinez, Manuel
and Lu, Kejie
and Ye, Feng
and Qian, Yi},
 isbn = {978-3-032-09694-4},
 pages = {404--419},
 publisher = {Springer Nature Switzerland},
 title = {Towards Intelligent Fuzzing: Leveraging Large Language Models for Improved Software Security},
 url = {https://link.springer.com/chapter/10.1007/978-3-032-09694-4_31},
 year = {2026}
}

@inproceedings{10.1007/978-981-95-0172-4_5,
 abstract = {Formal verification is a method used to verify the security of cryptographic protocols. In formal verification, the specifications of a protocol and its security properties are described using the input language of a software tool called a verification tool, enabling automated verification. However, protocol specifications and security properties are generally described and explained in natural language, and rewriting them in the tool's language requires specialized knowledge and time. This study attempts to efficiently make formal descriptions of cryptographic protocols by utilizing an LLM chatbot. Specifically, it explains the process by which the LLM chatbot understands protocol specifications described in natural language and converts them into formal descriptions. By using this approach, we aim to support the initial step of creating inputs for formal verification tools and reduce the effort required when starting to use such tools.},
 address = {Singapore},
 author = {Sakurada, Hideki
and Sakurai, Kouichi},
 booktitle = {Mobile Internet Security},
 editor = {Jo, Hyungrok
and Shin, Seonghan
and Merlo, Alessio
and You, Ilsun},
 isbn = {978-981-95-0172-4},
 pages = {69--83},
 publisher = {Springer Nature Singapore},
 title = {Utilizing LLM Chatbots for Formal Descriptions of Cryptographic Protocols},
 url = {https://link.springer.com/chapter/10.1007/978-981-95-0172-4_5},
 year = {2026}
}

@Article{Chen2025,
author="Chen, Kezhou
and Wang, Tao
and Ni, Mingzhe
and Cheng, Lianglun
and Wang, Zhuowei
and Chen, Chong",
title="Collaborative LLM agents for flexible software development of intelligent industrial robot control systems",
journal="Complex {\&} Intelligent Systems",
year="2025",
month="Oct",
day="24",
volume="11",
number="12",
pages="476",
abstract="Software plays a crucial role in robot control systems, and its efficient, flexible development is essential for production. Such software must be customized to specific production processes, requiring developers with specialized expertise in these areas---the high development threshold results in reduced efficiency and increased costs. Recently, significant progress has been made in automated problem-solving through societies of agents based on large language models (LLMs). To automate software development for industrial robot control systems, this paper introduces an Industrial Robot Control Software Auto-Development (IRCSAD) framework with multi-agent collaboration, and a Low-code Industrial Software Platform (LISP) for validating IRCSAD-developed software. IRCSAD automates software development and iterative optimization of prompts through the collaboration of multiple LLM-based agents. This work also proposes a software testing methodology for robot control systems based on reliability assessment. An experimental study that develops software for the robot control system for assembly, sorting, and inspection tasks is implemented. The results show that collaboration with LISP enhances IRCSAD's ability to solve complex problems in the development process, saves development costs, and improves development efficiency.",
issn="2198-6053",
doi="10.1007/s40747-025-02051-z",
url="https://doi.org/10.1007/s40747-025-02051-z"
}


@Article{Liu2025,
author="Liu, Jingqiang
and Liang, Ruigang
and Zhu, Xiaoxi
and Zhang, Yue
and Liu, Yuling
and Liu, Qixu",
title="LLM4TDG: test-driven generation of large language models based on enhanced constraint reasoning",
journal="Cybersecurity",
year="2025",
month="May",
day="15",
volume="8",
number="1",
pages="32",
abstract="With the evolution of modern software development paradigms, component reuse, and low-code approaches have emerged as mainstream in software development. However, developers often lack an in-depth understanding of reused code. The inability of components to operate autonomously leads to insufficient testing of software functionalities and security, further exacerbating the contradiction between the increasing complexity of software architectures and the demand for accurate and efficient software automation testing. This, in turn, increases the frequency of software supply chain security incidents. This paper proposes a test-driven generation framework, LLM4TDG, based on large language models (LLMs). By formally defining the constraint dependency graph and converting it into context constraints, LLMs' ability to understand natural language descriptions such as test requirements and documents is enhanced. Constraint reasoning and backtracking mechanisms are then used to generate test drivers that satisfy the defined constraints automatically. Using the EvalPlus dataset, we evaluate the comprehensive capabilities of LLM4TDG in test case generation using four general-domain LLMs and five code-generation-domain LLMs. The experimental results indicate that our approach significantly enhances LLMs' ability to comprehend constraints in testing objectives, achieving a 47.62{\%} increase in constraint understanding across 147 testing tasks. Employing LLM4TDG significantly improves the average pass@k metric of all LLMs by 10.41{\%}. The pass@k metric for CodeQwen-chat has improved by up to 18.66{\%}. The metric surpasses the state-of-the-art GPT-4, with a performance of 92.16{\%} on HUMANEVAL and 87.14{\%} on HUMANEVAL+, which enhances the error correction and functional correctness in test-driven code generation. Meanwhile, Our experiments were conducted on a dataset of Python third-party libraries containing malicious behavior in the context of security testing tasks, validating the effectiveness of our method in real-world applications and its generalization capabilities.",
issn="2523-3246",
doi="10.1186/s42400-024-00335-4",
url="https://doi.org/10.1186/s42400-024-00335-4"
}


@inproceedings{10.1007/978-3-032-03751-0_41,
 abstract = {A significant advancement in Software Defect Prediction can be achieved through an innovative methodology utilizing Large Language Models (LLMs) with fine-tuning and few-shot prompting techniques. This approach differs from conventional machine learning and deep learning methodologies by utilizing the inherent knowledge embedded within LLMs, which have undergone pretraining on comprehensive codebases to improve contextual comprehension. The methodology leverages concealed patterns in extensive code repositories, where specific prompting and fine-tuning customize LLMs to the subtleties of software defect prediction, thereby improving defect identification accuracy. Our empirical analysis reveals the superior efficacy of this methodology compared to traditional approaches, yielding accuracy rates exceeding 90{\%}, approximately 20{\%} higher than elementary prompting strategies. This investigation makes a valuable contribution to the evolving domain of software defect prediction. The potential benefits of this approach include diminished reliance on domain-specific characteristics and enhanced detection of nuanced code defects, while significant challenges encompassed hyperparameter optimization and management of computational demands for model training.},
 address = {Cham},
 author = {Khokhar, Dhruv
and Nashier, Harshil
and Malhotra, Ruchika},
 booktitle = {Proceedings of Data Analytics and Management},
 editor = {Swaroop, Abhishek
and Virdee, Bal
and Correia, S{\'e}rgio Duarte
and Polkowski, Zdzislaw},
 isbn = {978-3-032-03751-0},
 pages = {493--503},
 publisher = {Springer Nature Switzerland},
 title = {Fine-Tuned Large Language Models in Software Defect Prediction},
 url = {https://link.springer.com/chapter/10.1007/978-3-032-03751-0_41},
 year = {2026}
}

@Article{Zhang2026,
author="Zhang, Shuoming
and Zhao, Jiacheng
and Yu, Qiuchu
and Xia, Chunwei
and Wang, Zheng
and Feng, Xiaobing
and Cui, Huimin",
title="The new compiler stack: a survey on the synergy of LLMs and compilers",
journal="CCF Transactions on High Performance Computing",
year="2026",
month="Jan",
day="09",
abstract="This survey has provided a systematic overview of the emerging field of LLM-enabled compilation by addressing several key research questions. We first answered how LLMs are being integrated by proposing a comprehensive, multi-dimensional taxonomy that categorizes works based on their Design Philosophy (Selector, Translator, Generator), LLM Methodology, their operational Level of Code Abstraction, and the specific Task Type they address. In answering what advancements these approaches offer, we identified three primary benefits: the democratization of compiler development, the discovery of novel optimization strategies, and the broadening of the compiler's traditional scope. Finally, in addressing the field's challenges and opportunities, we highlighted the critical hurdles of ensuring correctness and achieving scalability, while identifying the development of hybrid systems as the most promising path forward. By providing these answers, this survey serves as a foundational roadmap for researchers and practitioners, charting the course for a new generation of LLM-powered, intelligent, adaptive and synergistic compilation tools.",
issn="2524-4930",
doi="10.1007/s42514-025-00270-x",
url="https://doi.org/10.1007/s42514-025-00270-x"
}


@inproceedings{10.1007/978-3-031-81573-7_6,
 abstract = {This work addresses a research challenge in automating the translation of natural language inputs into programming language specifications. We consider the case of bug reports, which are informally written by users, and that must be specifying into executable test cases for reproducing the bug on the target software. Software bugs are indeed largely reported in natural language by users. Yet, we lack reliable tools to automatically address reported bugs (i.e., enabling their analysis, reproduction, and bug fixing). We therefore build on the recent promises brought by ChatGPT for various tasks, including in software engineering, and establish the following research question: What if Conversational Artificial Intelligence (AI) models could be used to explore the semantics of bug reports as well as to automate their reproduction? We evaluate the capabilities of ChatGPT, a state-of-the-art conversational AI, i.e., chatbot, using the popular Defects4J benchmark with its associated bug reports. The results reveal that ChatGPT can generate executable test cases that could trigger 50{\%} of the bugs reported in natural language. These results are promising not only for the research community, but also for practitioners.},
 address = {Cham},
 author = {Saban{\'e}, Aminata
and Plein, Laura
and Bissyand{\'e}, Tegawend{\'e} F.},
 booktitle = {Towards new e-Infrastructure and e-Services for Developing Countries},
 editor = {Sere, Abdoulaye
and Sie, Oumarou
and Saeed, Rashid A.},
 isbn = {978-3-031-81573-7},
 pages = {81--88},
 publisher = {Springer Nature Switzerland},
 title = {Leveraging Conversational AI for Accelerating User-Driven Software Testing},
 url = {https://link.springer.com/chapter/10.1007/978-3-031-81573-7_6},
 year = {2025}
}

@inproceedings{10.1007/978-3-031-94855-8_3,
 abstract = {Deep Learning (DL) has emerged as a powerful tool for vulnerability detection, often outperforming traditional solutions. However, developing effective DL models requires large amounts of real-world data, which can be difficult to obtain in sufficient quantities. To address this challenge, DiverseVul dataset has been curated as one of the largest datasets of vulnerable and non-vulnerable C/C++ functions extracted exclusively from real-world projects. Its goal is to provide high-quality, large-scale samples for training DL models. Nevertheless, during our study several inconsistencies were identified in the raw dataset while applying pre-processing techniques, highlighting the need for a refined version. In this work, we present a refined version of DiverseVul dataset, which is used to fine-tune a large language model, LLaMA 3.2, for vulnerability detection. Experimental results show that the use of pre-processing techniques led to an improvement in performance, with the model achieving an F1-Score of 66{\%}, a competitive result when compared to our baseline, which achieved a 47{\%} F1-Score in software vulnerability detection.},
 address = {Cham},
 author = {Gon{\c{c}}alves, Jos{\'e}
and Silva, Miguel
and Cabral, Bernardo
and Dias, Tiago
and Maia, Eva
and Pra{\c{c}}a, Isabel
and Severino, Ricardo
and Ferreira, Lu{\'i}s Lino},
 booktitle = {Cybersecurity},
 editor = {Pra{\c{c}}a, Isabel
and Bernardi, Simona
and In{\'a}cio, Pedro R.M.},
 isbn = {978-3-031-94855-8},
 pages = {38--51},
 publisher = {Springer Nature Switzerland},
 title = {Evaluating LLaMA 3.2 for Software Vulnerability Detection},
 url = {https://link.springer.com/chapter/10.1007/978-3-031-94855-8_3},
 year = {2025}
}

@inproceedings{10.1007/978-3-031-95127-5_24,
 abstract = {Artificial intelligence has significantly affected various domains recently, notably code-level application security. Vulnerable code snippets can easily propagate across different software platforms, making early detection and correction crucial. Existing machine learning applications often fail to provide comprehensive and accurate results, necessitating time-consuming manual inspections by developers.},
 address = {Cham},
 author = {Zia, Ilmaan
and Basit Ur Rahim, Muhammad Abdul
and Raj, Ankit
and Hussain, Shahid},
 booktitle = {Computational Science and Computational Intelligence},
 editor = {Arabnia, Hamid R.
and Deligiannidis, Leonidas
and Shenavarmasouleh, Farzan
and Amirian, Soheyla
and Ghareh Mohammadi, Farid},
 isbn = {978-3-031-95127-5},
 pages = {324--339},
 publisher = {Springer Nature Switzerland},
 title = {Leveraging LLM to Detect and Correct Vulnerabilities in Code},
 url = {https://link.springer.com/chapter/10.1007/978-3-031-95127-5_24},
 year = {2025}
}

@inproceedings{10.1007/978-3-032-04403-7_14,
 abstract = {As software architecture grows in complexity, understanding the implications of design decisions becomes increasingly challenging. Large Language Models (LLMs) offer new opportunities for enhancing explainability during architecture modeling and evaluation by generating natural language explanations that support comprehension, learning, and decision-making. This potential is particularly valuable in domains with increased technical complexity---such as elasticity in cloud-based systems. In this work, we integrate and evaluate LLM-based explanations in supporting design-time evaluation of software architectures, focusing on the detection of elasticity antipatterns. Elasticity antipatterns are flawed autoscaling policy configurations that potentially lead to inefficient or unreliable system behavior. We extend an existing modeling and simulation approach with a novel feature that generates contextualized, textual explanations derived from simulation data. These explanations aim to guide architects in understanding scaling behaviors, identifying design issues, and refining their models. Our contribution includes the conceptualization of explanation types relevant to elasticity modeling, the design of prompt templates to elicit effective responses from LLMs, and an evaluation of the generated explanations' usefulness and quality. Results indicate that LLM-assisted feedback enhances the interpretability of elasticity models and supports the early identification of antipatterns, albeit with some limitations in precision and conciseness with only a slight agreement between expert evaluations ({\$}{\$}{\backslash}kappa {\$}{\$}$\kappa$ = 0.202). The explanation quality across types of explanations differs. Even though most explanations contain factual information, a large portion was deemed as imprecise especially in explaining problem and solution, the policy and target and service level objectives.},
 address = {Cham},
 author = {Klinaku, Floriment
and Lammert, Jonas
and Becker, Steffen},
 booktitle = {Software Architecture. ECSA 2025 Tracks and Workshops},
 editor = {Bianculli, Domenico
and Sartaj, Hassan
and Andrikopoulos, Vasilios
and Pautasso, Cesare
and Mikkonen, Tommi
and Perez, Jennifer
and Bure{\v{s}}, Tom{\'a}{\v{s}}
and De Sanctis, Martina
and Muccini, Henry
and Navarro, Elena
and Soliman, Mohamed
and Zdun, Uwe},
 isbn = {978-3-032-04403-7},
 pages = {141--154},
 publisher = {Springer Nature Switzerland},
 title = {LLM-Based Explainability at Design Time: Detecting Elasticity Antipatterns in Software Architectures},
 url = {https://link.springer.com/chapter/10.1007/978-3-032-04403-7_14},
 year = {2026}
}

@Article{Massenon2026,
author="Massenon, Rhodes
and Gambo, Ishaya
and Khan, Javed Ali",
title="Unveiling hidden permissions: an LLM framework for detecting privacy and security concerns in AI mobile apps reviews",
journal="Automated Software Engineering",
year="2026",
month="Jan",
day="09",
volume="33",
number="2",
pages="42",
abstract="Mobile AI applications enhance functionality but introduce complex privacy and security challenges. This research develops and evaluates an automated framework that leverages Large Language Models (LLMs) to analyze user reviews and unveil ``hidden permissions'' defined not as technically undeclared functionalities, but as declared permissions whose purpose or necessity is opaque to users, leading to perceived privacy risks. The framework integrates static analysis of permission manifests with a hybrid Natural Language Processing (NLP) pipeline that combines Term Frequency-Inverse Document Frequency (TF-IDF) with BERT embeddings. A fine-tuned RoBERTa model then classifies user-reported concerns into predefined risk categories. We correlate these user-reported behaviors with declared permissions to identify potential mismatches and prioritize them using a risk-scoring methodology validated against the MITRE Common Weakness Enumeration (CWE) database. In an evaluation against other LLM architectures (GPT-3.5, DistilBERT, XLNet, and LLaMA-2), our fine-tuned RoBERTa model demonstrates superior performance, achieving an F1-score of 0.90 in classifying reviews related to unauthorized tracking. The framework effectively surfaces and prioritizes user-perceived privacy risks, offering actionable insights for developers to address mismatches between an app's declared permissions and its user-experienced behavior, thereby fostering a more secure and trustworthy AI mobile ecosystem.",
issn="1573-7535",
doi="10.1007/s10515-025-00567-9",
url="https://doi.org/10.1007/s10515-025-00567-9"
}


@Inbook{Rezaei2025,
author="Rezaei, Hirad
and Beheshti, Amin
and Rabhi, Fethi",
editor="Xu, Wei",
title="Training LLM with Human Feedback",
bookTitle="Handbook of Human-Centered Artificial Intelligence",
year="2025",
publisher="Springer Nature Singapore",
address="Singapore",
pages="1--62",
abstract="Large language models (LLMs) have significantly advanced the field of natural language processing by generating human-like text and performing complex language tasks. However, traditional training methods often lack the nuanced understanding of human preferences and ethical considerations. This chapter examines the integration of human feedback into the fine-tuning of LLMs to enhance their accuracy, reliability, and alignment with human values. Foundational concepts, techniques such as reinforcement learning from human feedback (RLHF) and active learning, and the design of effective feedback mechanisms are discussed. Additionally, case studies are presented, challenges like scalability and bias are analyzed, and future directions in human--AI collaboration are explored. Integrating human insights enables LLMs to achieve improved performance and contributes to the development of ethical and human-centered AI systems. Throughout, we adopt a human-centered AI (HCAI) lens to make explicit where humans participate in data creation, preference modeling, evaluation, and governance.",
isbn="978-981-97-8440-0",
doi="10.1007/978-981-97-8440-0_53-1",
url="https://doi.org/10.1007/978-981-97-8440-0_53-1"
}


@inproceedings{10.1007/978-3-031-96590-6_5,
 abstract = {Reusing open-source software (OSS) code has become standard in software development. When vulnerabilities are discovered in reused code, maintainers typically apply security patches. However, these patches often include non-vulnerability-related changes, such as code refactoring or updating a setting file. Applying a patch without distinguishing these changes can lead to unintended software malfunctions. Existing techniques do not account for non-remediation code lines in security patches. This study aims to mitigate unexpected failures caused by indiscriminate patch application.},
 address = {Cham},
 author = {Arakawa, Reika Nishimura
and Kanemoto, Yo
and Akiyama, Mitsuaki},
 booktitle = {Data and Applications Security and Privacy XXXIX},
 editor = {Katsikas, Sokratis
and Shafiq, Basit},
 isbn = {978-3-031-96590-6},
 pages = {73--95},
 publisher = {Springer Nature Switzerland},
 title = {Towards the Identification of Vulnerability-Fixing Code Lines in OSS Security Patches Using Lexical Code Segmentation and LLMs},
 url = {https://link.springer.com/chapter/10.1007/978-3-031-96590-6_5},
 year = {2025}
}

@inproceedings{10.1007/978-3-032-07992-3_19,
 abstract = {Large Language Models (LLMs) have demonstrated strong capabilities in code generation, yet studies reveal that their outputs may contain critical security vulnerabilities. This study investigates the effectiveness of Parameter-Efficient Fine-Tuning (PEFT) techniques in enhancing the secure code generation capabilities of LLMs. Using the CodeGen-Multi 2B model as the base, applied two state-of-the-art PEFT methods--LoRA and IA3--to fine-tune the model on a curated dataset of C and C++ code. The objective is to evaluate whether fine-tuned models can reduce vulnerability incidence and to compare the relative performance of LoRA and IA3. Security evaluations are conducted using prompts designed to elicit eight common vulnerabilities from the MITRE CWE Top 25 list. Findings show that PEFT fine-tuning significantly improves the model's ability to generate secure code, with both LoRA and IA3 outperforming the base model. Among the two, LoRA exhibits a slight advantage in reducing vulnerabilities. This research highlights the potential of lightweight fine-tuning methods to mitigate security risks in LLM-generated code and offers practical insights for future advancements in secure AI-assisted software development.},
 address = {Cham},
 author = {Hasan, Md Anwarul},
 booktitle = {Proceedings of the Future Technologies Conference (FTC) 2025, Volume 4},
 editor = {Arai, Kohei},
 isbn = {978-3-032-07992-3},
 pages = {290--309},
 publisher = {Springer Nature Switzerland},
 title = {Secure Code Generation with Parameter-Efficient Fine-Tuning of LLMs},
 url = {https://link.springer.com/chapter/10.1007/978-3-032-07992-3_19},
 year = {2026}
}

@inproceedings{10.1007/978-981-95-4155-3_33,
 abstract = {Understanding cyber threats is crucial for effective defense in the field of cybersecurity. If we can automatically map Common Vulnerabilities and Exposures (CVEs) to attack tactics and techniques, it will help practitioners quickly analyze reports and take responsive actions. In this paper, we introduce CyberLLM, leveraging the tailor-made large language model for mapping CVEs to cyber threat tactics and techniques. Specifically, we model the mapping of CVE to tactics and techniques as a multi-label classification problem, given that many CVEs correspond to multiple techniques of ATT{\&}CK. Then, the text description is vectorized through the tokenization process, and we deploy a series of data augmentation techniques to enrich the semantic information. Considering that external knowledge bases are helpful to enhance the contextual information of the queried CVE, CyberLLM designs a retrieval strategy based on the Jaccard distance calculation. Finally, we support flexible model fine-tuning to adapt to the needs. Through extensive experiments, we demonstrate the superiority of CyberLLM compared with 7 representative state-of-the-art methods. We also perform ablation experiments on data augmentation and evaluate the effectiveness of using retrieval information. Furthermore, we provide a series of deep insights in terms of feature attribution and attention weight visualization.},
 address = {Singapore},
 author = {Zhao, Ziming
and Li, Zhaoxuan
and Li, Tingting
and Zhang, Fan},
 booktitle = {Database Systems for Advanced Applications},
 editor = {Zhu, Feida
and Yu, Philip S.
and Nadamoto, Akiyo
and Lim, Ee-peng
and Shim, Kyuseok
and Ding, Wei
and Zhang, Bingxue},
 isbn = {978-981-95-4155-3},
 pages = {473--488},
 publisher = {Springer Nature Singapore},
 title = {CyberLLM: Enable Mapping CVE to Tactics and Techniques of Cyber Threats via LLM},
 url = {https://link.springer.com/chapter/10.1007/978-981-95-4155-3_33},
 year = {2026}
}

@Article{Pereira2025,
author="Pereira, Lucas de Castro Rodrigues
and Marcos Junior, Maykon
and Santos, Guilherme de Brito
and Sabo, Isabela Cristina
and Dal Pont, Thiago Raulino
and Maurmann, Andressa Silveira Viana
and Bollmann, Lu{\'i}sa
and Vieira, Maite Fortes
and Mohr, Jo{\~a}o Gabriel
and Alchini, Cristian Alexandre
and Silva, Bruno Cassol da
and Rover, Aires Jos{\'e}",
title="Using GPT-4o as a factor extractor for Brazilian consumer law judgments*",
journal="Artificial Intelligence and Law",
year="2025",
month="Aug",
day="12",
abstract="This research paper explores the effectiveness of OpenAI's GPT-4o model in extracting factors and structuring judgments (text data) regarding Brazilian Consumer Law. We constructed two datasets: an unstructured one, comprising judgments on air transport service failures (e.g., flight delays, cancellations, and baggage loss), and a structured dataset created by legal experts manually extracting relevant factors. Two prompts-a raw and a refined version-were tested using two experimental setups. The first setup, Singular, involved 900 judgments with three requests per document. The second, Factor-based sets, also used 900 judgments but partitioned the prompts into three segments: pro-factors, con-factors, and dimensions. Metrics such as accuracy, F1-score, precision, recall, and RMSE were used based on the value type (numerical or categorical). The Singular setup presents the best results, with the refined prompt achieving approximately 90{\%} accuracy and 60{\%} F1-score. In this experiment, individual factor analysis showed moderate accuracy for ``Airline assistance'' factor, likely due to the lack of clear parameters defining adequate assistance in cases like flight delays or cancellations. However, individual analysis of the F1-score, precision and recall revealed very low values for factors such as ``Right to regret and repayment claim'' and ``Downgrade'', highlighting the model's difficulty with some unbalanced class distributions. This study demonstrates the potential of LLMs in structuring legal datasets and aiding professionals in extracting factors from legal texts without fine-tuning. Limited model access restricted further experimentation.",
issn="1572-8382",
doi="10.1007/s10506-025-09466-6",
url="https://doi.org/10.1007/s10506-025-09466-6"
}


@inproceedings{10.1007/978-981-96-0808-9_25,
 abstract = {While ensuring the safety of smart contracts during the development stage is crucial, formalizing natural language requirements for their verification can be challenging and time-consuming. When it comes to standard-based contracts, Ethereum provides natural language guidelines to ensure compliant and safe implementations. Deviating from the intended standard behavior can lead to serious security pitfalls. To address this issue, we propose a verification approach for ensuring the safety of ERC-based contracts using model checking. Our approach employs Mistral as a Large Language Model (LLM) to generate Computational Tree Logic (CTL) specifications from natural language requirements. We demonstrate the applicability of our approach through an ERC-20 smart contract.},
 address = {Singapore},
 author = {Fekih, Rim Ben
and Lahami, Mariam
and El Eze, Mohamed Salem
and Bradai, Salma
and Jmaiel, Mohamed},
 booktitle = {Service-Oriented Computing},
 editor = {Gaaloul, Walid
and Sheng, Michael
and Yu, Qi
and Yangui, Sami},
 isbn = {978-981-96-0808-9},
 pages = {331--338},
 publisher = {Springer Nature Singapore},
 title = {Towards an Automated Verification Approach for ERC-Based Smart Contracts},
 url = {https://link.springer.com/chapter/10.1007/978-981-96-0808-9_25},
 year = {2025}
}

@Article{Minna2025,
author="Minna, Francesco
and Massacci, Fabio
and Tuma, Katja",
title="Analyzing and mitigating (with LLMs) the security misconfigurations of Helm charts from Artifact Hub",
journal="Empirical Software Engineering",
year="2025",
month="Jul",
day="04",
volume="30",
number="5",
pages="132",
abstract="Helm is a package manager that allows defining, installing, and upgrading applications with Kubernetes (K8s), a popular container orchestration platform. A Helm chart is a collection of files describing all dependencies, resources, and parameters required for deploying an application within a K8s cluster. This study aimed to mine and empirically evaluate the security of Helm charts, comparing the performance of existing tools in terms of misconfigurations reported by policies available by default, and measuring to what extent LLMs could be used for removing misconfigurations. For these reasons, we proposed a pipeline to mine Helm charts from Artifact Hub, a popular centralized repository, and analyze them using state-of-the-art open-source tools like Checkov and KICS. First, the pipeline runs several chart analyzers and identifies the common and unique misconfigurations reported by each tool. Secondly, it uses LLMs to suggest a mitigation for each misconfiguration. Finally, the LLM refactored chart previously generated is analyzed again by the same tools to see whether it satisfies the tool's policies. We also performed a manual analysis on a subset of charts to evaluate whether there are false positive misconfigurations from the tool's reporting and in the LLM refactoring. We found that (i) there is a significant difference between LLMs, (ii) providing a snippet of the YAML template as input might be insufficient compared to all resources, and (iii) even though LLMs can generate correct fixes, they may also delete other irrelevant configurations that break the application.",
issn="1573-7616",
doi="10.1007/s10664-025-10688-0",
url="https://doi.org/10.1007/s10664-025-10688-0"
}


@inproceedings{10.1007/978-981-96-9921-6_22,
 abstract = {With the rapid development of kernel auto-fuzzing, the volume of Linux kernel crash reports has surged, many of which are duplicates triggered by the same underlying bug. However, existing classification techniques (e.g., heuristic methods based on crash function classification, bug rule matching and filtering, and kernel object similarity computation) often misidentify such duplicate reports as distinct issues, leading to increased manual effort, incomplete patches, and reduced efficiency in bug remediation. Accurately identifying duplicate crash reports thus remains a critical challenge in kernel error management. Different from application-level crashes, kernel crashes stem directly from low-level mechanisms, e.g., hardware exceptions and resource contention, bypassing intermediate abstraction layers. Therefore, traditional natural language processing techniques, originally designed for user-space crashes, struggle to capture the unique semantics of kernel-specific system call paths. To address this gap, we propose KERMIT, a methodology that combines kernel-specific feature extraction with domain-adaptive finetuning to bridge semantic disparities in crash report analysis. Specifically, KERMIT employs tailored filtering techniques to isolate complete call trace data while removing irrelevant noise, and it utilizes full-parameter fine-tuning of a BERT-based model to adapt its semantic embeddings to the unique characteristics of kernel crash reports. Experimental results demonstrate that KERMIT achieves a recall rate of 92.33{\%}, representing a 7.73{\%} improvement over state-of-the-art methods. Notably, KERMIT, built on a fine-tuned BERT model with only 110 million parameters, outperforms large-scale models like GPT-4 by over 30{\%} in recall, offering a more efficient and resource-effective solution for kernel crash de-duplication.},
 address = {Singapore},
 author = {Yang, Yunshan
and Dong, Pan
and Jiang, Renshuang
and Fang, Xiaoxiang
and Yu, Qirui
and Li, Bao
and Zhang, Jianfeng
and Ding, Yan},
 booktitle = {Advanced Intelligent Computing Technology and Applications},
 editor = {Huang, De-Shuang
and Chen, Wei
and Pan, Yijie
and Chen, Haiming},
 isbn = {978-981-96-9921-6},
 pages = {263--274},
 publisher = {Springer Nature Singapore},
 title = {KERMIT: A BERT-Based Classification Method for Linux Kernel Crashes Through Stack Trace},
 url = {https://link.springer.com/chapter/10.1007/978-981-96-9921-6_22},
 year = {2025}
}

@Article{Reddig2025,
author="Reddig, Jennifer M.
and Arora, Arav
and MacLellan, Christopher J.",
title="Generating In-Context, Personalized Feedback for Intelligent Tutors with Large Language Models",
journal="International Journal of Artificial Intelligence in Education",
year="2025",
month="Dec",
day="01",
volume="35",
number="6",
pages="3459--3500",
abstract="This study explores how large language models (LLMs), specifically GPT-4, could be used to generate personalized feedback within an Intelligent Tutoring System (ITS). The research focuses on evaluating the model's ability to (1) diagnose student errors, (2) generate personalized corrective feedback, and (3) assess the accuracy of diagnoses and helpfulness of the feedback. We analyze student errors from the Apprentice Tutor College Algebra ITS and prompt GPT-4 to give targeted feedback on those errors. The findings suggest that while this model can effectively diagnose a range of student errors, its feedback varies in effectiveness based on the complexity of the problem and the type of error. While GPT-4 generates relevant, specific feedback a majority of the time, 35{\%} of the hints were too general, incorrect, or give away the correct answer. The study also explores methods for using an LLM to automatically evaluate the validity of generated feedback, and finds that only 35{\%} of feedback passes automated helpfulness evaluations.",
issn="1560-4306",
doi="10.1007/s40593-025-00505-6",
url="https://doi.org/10.1007/s40593-025-00505-6"
}


@Inbook{Sekar2026,
author="Sekar, Srinivasan",
title="LLM-Centric Threats: Injection and Poisoning",
bookTitle="The MCP Standard: A Developer's Guide to Building Universal AI Tools with the Model Context Protocol ",
year="2026",
publisher="Apress",
address="Berkeley, CA",
pages="195--205",
abstract="In the last chapter, we made our server less vulnerable to common software problems. We built a strong fence around our playground to keep our tools from running malicious codes or getting files they shouldn't. Our code is now a lot safer. But the most intriguing and difficult part of keeping agentic systems safe isn't keeping the code safe; it's keeping the ``mind'' of the agent itself safe.",
isbn="979-8-8688-2364-0",
doi="10.1007/979-8-8688-2364-0_12",
url="https://doi.org/10.1007/979-8-8688-2364-0_12"
}


@Article{Xu2025,
author="Xu, Jinwei
and Zhang, He
and Zhou, Xin
and Yang, Yanjing
and Mao, Runfeng
and Li, Xiaokang
and Yang, Lanxin
and Shen, Haifeng",
title="Automated detection of affected libraries from vulnerability reports",
journal="Automated Software Engineering",
year="2025",
month="Aug",
day="11",
volume="32",
number="2",
pages="73",
abstract="The growing reuse of third-party libraries in software supply chains increases the risk of being affected by the involved vulnerabilities. To strengthen software security, security vendors such as Snyk manage up-to-date vulnerability databases by associating reported vulnerabilities with their affected libraries, and contemporary digital organizations such as banking and software enterprises detect the third-party libraries they use if affected by these reported vulnerabilities. Existing studies focus on automating the detection process but make few efforts on detecting newly affected libraries, although new libraries (previously healthy) are constantly disclosed to be affected by new vulnerabilities. Moreover, existing studies do not seriously consider digital organizations' concerns only about the libraries they use. In this paper, we propose an approach LibAlarm to address these challenges. We implement LibAlarm as a large language model-powered approach and compare it with the baseline approaches from multiple perspectives. Our experimental evaluation using 16,238 NVD reports indicates that LibAlarm improves the F1 by over 14{\%} compared with baselines and detects over 40{\%} newly affected libraries. For contemporary digital organizations, LibAlarm performs better than the baseline approaches with the F1 above 70{\%} and the reduced false alarm ratio to 20{\%}. Our case analysis using 540 NVD reports and 20 projects from Microsoft and Google demonstrates the effectiveness of LibAlarm. These results indicate that LibAlarm can help security vendors and digital organizations detect affected libraries from vulnerability reports.",
issn="1573-7535",
doi="10.1007/s10515-025-00540-6",
url="https://doi.org/10.1007/s10515-025-00540-6"
}


@inproceedings{10.1007/978-3-032-03997-2_11,
 abstract = {Modern empirical research in machine learning largely relies on developing custom software. Often such software is written by researchers and not professional software engineering. As a result, source code issues and the associated technical debt may accumulate and lead to higher programming effort, obstacles to code reuse, hidden software defects affecting the quality of the research itself. In this paper, we investigate if it is possible to apply automatic tools to prevent or remove these source code issues thus alleviating the need for software engineers in research projects. We analyze the source code of 24 open source research projects in machine learning, identify common issues and propose practical techniques to prevent these issues during coding. We also investigate if an application of an LLM coding assistant can fix common code issues automatically. We found out that 1) frequent source code issues largely the same for different machine learning frameworks 2) most of the issues could be eliminated by following simple coding practices 3) most of the issues could be removed by applying an LLM coding assistant.},
 address = {Cham},
 author = {Xie, Pujun
and Khritankov, Anton S.},
 booktitle = {Data Analytics and Management in Data Intensive Domains},
 editor = {Pardalos, Panos
and Babkin, Eduard
and Zolotykh, Nikolay
and Stupnikov, Sergey},
 isbn = {978-3-032-03997-2},
 pages = {149--175},
 publisher = {Springer Nature Switzerland},
 title = {An LLM Approach to Fixing Common Code Issues in Machine Learning Projects},
 url = {https://link.springer.com/chapter/10.1007/978-3-032-03997-2_11},
 year = {2026}
}

@Article{Chen2025,
author="Chen, Hong",
title="SynergyBug: A deep learning approach to autonomous debugging and code remediation",
journal="Scientific Reports",
year="2025",
month="Jul",
day="10",
volume="15",
number="1",
pages="24888",
abstract="Bug detection and resolution are pivotal to maintaining the quality, reliability, and performance of software systems. Manual debugging, along with traditional static rule-based methods, proves inefficient when applied to complex software structures in contemporary times. SynergyBug combines BERT and GPT-3 to autonomously detect and repair bugs across multiple sources. It resolves essential requirements by implementing an automated system that diagnoses and resolves software bugs automatically, thus minimising human involvement. The framework unites BERT as a contextual machinery with GPT-3 to produce bug fix generation capabilities. The semantic pattern within bug reports, together with error logs and documentation, feeds into BERT for contextual embedding generation. GPT-3 applies the generated embeddings to produce code fixes, code snippets, as well as detailed explanations that address detected problems. The system achieves continuous automatic debugging by enhancing both detection and resolution steps into one unified process. The experimental outcomes prove that it achieves superior performance than conventional bug detection methods by reaching 98.79{\%} accuracy alongside 97.23{\%} precision and 96.56{\%} recall. The system demonstrated exceptional detection strength for functional and performance, and security bugs, where the detection rates reached 94{\%} and 90{\%} and 92{\%}, respectively. SynergyBug showed its ability to expand as it processed bug reports exceeding 100,000 cases without noticeably impacting system performance. This proposed system provides faster debugging capabilities to improve the quality of the complete software development process. This paper discusses as a tool that can revolutionise bug management through proactive instead of just reactive strategies. The implementation of human monitoring within safety programs and managing training system biases represent essential organisational factors. The study terminates by recognising SynergyBug as a crucial development leading toward automated debugging tools that maintain operational safety within intricate software systems.",
issn="2045-2322",
doi="10.1038/s41598-025-08226-5",
url="https://doi.org/10.1038/s41598-025-08226-5"
}


@Article{Modi2025,
author="Modi, Rohan
and Reddy, Navtej
and Kodur, Sai Sreenivas",
title="Debugmate: an AI agent for efficient on-call debugging in complex production systems",
journal="Discover Data",
year="2025",
month="Aug",
day="15",
volume="3",
number="1",
pages="33",
abstract="Production systems are often complex and distributed in nature. Debugging such complex systems is often time-consuming and nuanced, requiring software engineers (SEs) to understand vast codebases and the context of their organization's engineering setup. This leads to a huge cognitive load on SEs, particularly while they are on-call where they are expected to fix critical time-sensitive problems. This often causes sub-optimal human performance, which in turn decreases productivity and potentially impacts system reliability due to prolonged downtimes. In this paper, we introduce DebugMate (Observe.AI. Debugmate. 2024. https://github.com/Observeai-Research/DebugMate), an AI Agent that uniquely integrates an organization's internal context with external knowledge sources. DebugMate connects to the organization's key system resources like documentation, codebase, and knowledge base of historical incidents, along with online developer platforms (eg. StackOverflow, GitHub) and helps SEs respond faster by generating multiple hypotheses for identifying the root cause of a production issue. DebugMate employs Retrieval-Augmented Generation (RAG), ReAct, Tree-of-Thought, and long-term memory to provide grounded hypotheses for debugging via structured and systematic self-planning. In addition, it utilizes graphical representations to build context of not only the organization's code but also of imported code modules, to specifically identify and resolve complex issues that may be caused by unfamiliar external frameworks (eg, SpringBoot). Our proposed approach increases accuracy in identifying an issue by 20{\%} compared to the baseline. On our historical reliability incidents, DebugMate achieves a 77{\%} success rate in identifying root causes and suggesting fixes.",
issn="2731-6955",
doi="10.1007/s44248-025-00074-y",
url="https://doi.org/10.1007/s44248-025-00074-y"
}


@inproceedings{10.1007/978-3-031-85593-1_9,
 abstract = {The increasing frequency of attacks on Android applications coupled with the recent popularity of large language models (LLMs) necessitates a comprehensive understanding of the capabilities of the latter in identifying potential vulnerabilities, which is key to mitigate the overall risk. To this end, the work at hand compares the ability of nine state-of-the-art LLMs to detect Android code vulnerabilities listed in the latest Open Worldwide Application Security Project (OWASP) Mobile Top 10. Each LLM was evaluated against an open dataset of over 100 vulnerable code samples, assessing each model's ability to identify key vulnerabilities. Our analysis reveals the strengths and weaknesses of each LLM, identifying important factors that contribute to their performance. Additionally, we offer insights into context augmentation with retrieval-augmented generation (RAG) for detecting Android code vulnerabilities, which in turn may propel secure application development. Finally, while the reported findings regarding code vulnerability analysis show promise, they also reveal significant discrepancies among the different LLMs.},
 address = {Cham},
 author = {Kouliaridis, Vasileios
and Karopoulos, Georgios
and Kambourakis, Georgios},
 booktitle = {Attacks and Defenses for the Internet-of-Things},
 editor = {Meng, Weizhi
and Yung, Moti
and Shao, Jun},
 isbn = {978-3-031-85593-1},
 pages = {139--154},
 publisher = {Springer Nature Switzerland},
 title = {Assessing the Effectiveness of LLMs in Android Application Vulnerability Analysis},
 url = {https://link.springer.com/chapter/10.1007/978-3-031-85593-1_9},
 year = {2025}
}

@Inbook{Dang2024,
author="Dang, Nguyen Ngoc Hai
and Thanh, Tho Quan
and Nguyen-Duc, Anh",
editor="Nguyen-Duc, Anh
and Abrahamsson, Pekka
and Khomh, Foutse",
title="BERTVRepair: On the Adoption of CodeBERT for Automated Vulnerability Code Repair",
bookTitle="Generative AI for Effective Software Development",
year="2024",
publisher="Springer Nature Switzerland",
address="Cham",
pages="173--196",
abstract="Vulnerable code continues to have a significant impact to software quality, leading to serious consequences such as economic loss, privacy breaches, and threats to national security. Traditional methods of detecting and addressing software security issues are often time-consuming and resource-intensive. This research aims to examine the effectiveness of generative-based methods, particularly those leveraging generative (DL) models like CodeBERT, in repairing code and addressing software vulnerabilities. Our research question (RQ) is: Can the adoption of CodeBERT extend the capabilities of vulnerability code repair, and, if so, to what extent? We proposed a new approach called BERTVRepair that adopts CodeBERT and state-of-the-art transfer learning and tokenization methods to generate vulnerable code patches. We performed an experiment to compare the performance of BERTVRepair with existing models. We showed a marginal improvement in accuracy and perplexity. We conclude that using generative-based methods like CodeBERT, with its code embedding extraction and transfer learning approaches, can potentially enhance the process of software vulnerability repair. This research contributes to adopting large programming language models into software engineering tasks, such as automated code repair.",
isbn="978-3-031-55642-5",
doi="10.1007/978-3-031-55642-5_8",
url="https://doi.org/10.1007/978-3-031-55642-5_8"
}


@inproceedings{10.1007/978-3-031-75110-3_3,
 abstract = {In edge-cloud systems, the quality of infrastructure deployment is crucial for delivering high-quality services, especially when using popular Infrastructure as Code (IaC) tools like Ansible. Ensuring the reliability of such large-scale code systems poses a significant challenge due to the limited testing resources. Software defect prediction (SDP) addresses this limitation by identifying defect-prone software modules, allowing developers to prioritize testing resources effectively. This paper introduces a Large Language Model (LLM)-based approach for SDP in Ansible scripts with Code-Smell-guided Prompting (CSP). CSP leverages code smell indicators extracted from Ansible scripts to refine prompts given to LLMs, enhancing their understanding of code structure concerning defects. Our experimental results demonstrate that CSP variants, particularly the Chain of Thought CSP (CoT-CSP), outperform traditional prompting strategies, as evidenced by improved F1-scores and Recall. To the best of our knowledge, this is the first attempt to employ LLMs for SDP in Ansible scripts. By employing a code smell-guided prompting strategy tailored for Ansible, we anticipate that the proposed method will enhance software quality assurance and reliability, thereby increasing the overall reliability of edge-cloud systems.},
 address = {Cham},
 author = {Hong, Hyunsun
and Lee, Sungu
and Ryu, Duksan
and Baik, Jongmoon},
 booktitle = {Current Trends in Web Engineering},
 editor = {Pautasso, Cesare
and Marcel, Patrick},
 isbn = {978-3-031-75110-3},
 pages = {30--42},
 publisher = {Springer Nature Switzerland},
 title = {Enhancing Software Defect Prediction in Ansible Scripts Using Code-Smell-Guided Prompting with Large Language Models in Edge-Cloud Infrastructures},
 url = {https://link.springer.com/chapter/10.1007/978-3-031-75110-3_3},
 year = {2025}
}

@inproceedings{10.1007/978-3-031-75010-6_23,
 abstract = {This study explores the enhancement of API security testing against Broken Object Level Authorization (BOLA) and Authentication vulnerabilities through the integration of an LLM (Large Language Model)-enhanced framework. By incorporating the OpenHermes 2.5 Mistral 7B model, fine-tuned for domain-specific vulnerabilities, into the Karate testing framework, we demonstrate a novel approach to generating comprehensive and accurate test scenarios. Our methodology emphasizes prompt refinement, dynamic data handling, and endpoint context recognition to address the complexities of API security testing. A case study utilizing the VAmPI API Specification reveals significant improvements in test case generation and complexity, highlighting the potential of fine-tuned LLMs in identifying and mitigating API vulnerabilities. While promising, our exploration uncovers challenges in dataset development, token allocation, and mock data reliance, suggesting areas for future research, including the adoption of Retrieval Augmented Generation (RAG) and Retrieval Aware Fine-Tuning (RAFT) methods. Our findings underscore the transformative impact of LLMs on API security testing, paving the way for more robust and efficient testing frameworks.},
 address = {Cham},
 author = {Pasca, Emil Marian
and Erdei, Rudolf
and Delinschi, Daniela
and Matei, Oliviu},
 booktitle = {The 19th International Conference on Soft Computing Models in Industrial and Environmental Applications SOCO 2024},
 editor = {Quinti{\'a}n, H{\'e}ctor
and Corchado, Emilio
and Troncoso Lora, Alicia
and P{\'e}rez Garc{\'i}a, Hilde
and Jove, Esteban
and Calvo Rolle, Jos{\'e} Luis
and Mart{\'i}nez de Pis{\'o}n, Francisco Javier
and Garc{\'i}a Bringas, Pablo
and Mart{\'i}nez {\'A}lvarez, Francisco
and Herrero Cos{\'i}o, {\'A}lvaro
and Fosci, Paolo},
 isbn = {978-3-031-75010-6},
 pages = {231--240},
 publisher = {Springer Nature Switzerland},
 title = {Enhancing API Security Testing Against BOLA and Authentication Vulnerabilities Through an LLM-Enhanced Framework},
 url = {https://link.springer.com/chapter/10.1007/978-3-031-75010-6_23},
 year = {2025}
}

@inproceedings{10.1007/978-3-032-05120-2_25,
 abstract = {Significant applications in healthcare, education, and finance have been made possible by the rapid development of artificial intelligence (AI), especially large language models (LLMs) like GPT-4 and Vicuna-13B. However, dual-use dangers, such as disinformation, cybercrime, and content moderation evasion, are a concern, as these technologies are becoming more susceptible to adversarial attacks. This study offers a comprehensive governance architecture incorporating hybrid monitoring systems, adaptive governance mechanisms, and region-specific tactics to guarantee responsible development and implementation to address these issues. The usefulness and efficacy of the framework are illustrated through case studies and real-world applications, focusing on doable ways to handle the moral and practical problems raised by adversarial AI tools. To promote confidence, safety, and responsibility in AI systems, this work offers important insights and suggestions for practitioners, researchers, and policymakers.},
 address = {Cham},
 author = {Niloy, Md. Robiul Islam
and Akimoto, Youhei
and Islam, Md. Jahirul},
 booktitle = {Intelligent Systems},
 editor = {Udgata, Siba K.
and Mohapatra, Debasis
and Sethi, Srinivas
and Rana, Muhammad Ehsan},
 isbn = {978-3-032-05120-2},
 pages = {283--295},
 publisher = {Springer Nature Switzerland},
 title = {Governance and Ethical Challenges in AI Vulnerability Research: A Case Study of PAIR},
 url = {https://link.springer.com/chapter/10.1007/978-3-032-05120-2_25},
 year = {2026}
}

@inproceedings{10.1007/978-981-95-4434-9_20,
 abstract = {Vulnerabilities in source code can lead to a wide range of issues, such as privacy and security breaches, system crashes, data leaks, and unpredictable application behavior. These problems typically arise from improper handling of inputs, memory, or access control mechanisms. To address these challenges, we propose a novel Attention-based Code Summarization (ACS) approach for multi-label vulnerability detection in source code. Unlike traditional methods that are limited to binary or multi-class classification, our approach is designed to identify multiple types of vulnerabilities within a single code snippet---an essential capability, as real-world software often contains more than one vulnerability. Our method introduces a specialized, security-focused attention mechanism that utilizes vulnerability-specific queries to generate targeted representations for each vulnerability type. By extending pre-trained code models with a hierarchical summarization framework, we enable the creation of specialized representations tailored to different vulnerability categories. This design ensures a balance between general code understanding and precise vulnerability detection. Extensive evaluations on a multi-label vulnerability dataset show that our model outperforms existing baselines, especially for buffer-related vulnerabilities (e.g., CWE-119, CWE-120) and other complex security issues. Through comprehensive ablation studies, we validate the individual contributions of each architectural component, demonstrating the synergistic effects of our vulnerability-specific attention, security-focused summarization, and adaptive gating mechanisms. Furthermore, the attention visualization component of our model provides explainable outputs, helping security teams identify and understand the root causes of detected vulnerabilities. By enabling efficient and simultaneous detection of multiple vulnerability types, our approach significantly reduces the effort and time required to analyze and secure software systems.},
 address = {Singapore},
 author = {Baby, Maksuda Bilkis
and Atabuzzaman, Md.
and Shajalal, Md.
and Stevens, Gunnar},
 booktitle = {Cryptology and Network Security},
 editor = {Kim, Yongdae
and Miyaji, Atsuko
and Tibouchi, Mehdi},
 isbn = {978-981-95-4434-9},
 pages = {425--447},
 publisher = {Springer Nature Singapore},
 title = {Attention-Based Code Summarization for Multi-label Vulnerability Detection},
 url = {https://link.springer.com/chapter/10.1007/978-981-95-4434-9_20},
 year = {2026}
}

@Article{Mirzaei2024,
author="Mirzaei, Tala
and Amini, Leila
and Esmaeilzadeh, Pouyan",
title="Clinician voices on ethics of LLM integration in healthcare: a thematic analysis of ethical concerns and implications",
journal="BMC Medical Informatics and Decision Making",
year="2024",
month="Sep",
day="09",
volume="24",
number="1",
pages="250",
abstract="This study aimed to explain and categorize key ethical concerns about integrating large language models (LLMs) in healthcare, drawing particularly from the perspectives of clinicians in online discussions.",
issn="1472-6947",
doi="10.1186/s12911-024-02656-3",
url="https://doi.org/10.1186/s12911-024-02656-3"
}


@inproceedings{10.1007/978-3-032-07884-1_15,
 abstract = {The significant increase in software production, driven by the acceleration of development cycles over the past two decades, has led to a steady rise in software vulnerabilities, as shown by statistics published yearly by the CVE program. The automation of the source code vulnerability detection (CVD) process has thus become essential, and several methods have been proposed ranging from the well established program analysis techniques to the more recent AI-based methods. Our research investigates Large Language Models (LLMs), which are considered among the most performant AI models to date, for the CVD task. The objective is to study their performance and apply different state-of-the-art techniques to enhance their effectiveness for this task. We explore various fine-tuning and prompt engineering settings. We particularly suggest one novel approach for fine-tuning LLMs which we call Double Fine-tuning, and also test the understudied Test-Time fine-tuning approach. We leverage the recent open-source Llama-3.1 8B, with source code samples extracted from BigVul and PrimeVul datasets. Our conclusions highlight the importance of fine-tuning to resolve the task, the performance of Double tuning, as well as the potential of Llama models for CVD. Though prompting proved ineffective, Retrieval augmented generation (RAG) performed relatively well as an example selection technique. Overall, some of our research questions have been answered, and many are still on hold, which leaves us many future work perspectives. Code repository is available here: https://github.com/DynaSoumhaneOuchebara/Llama-based-vulnerability-detection.},
 address = {Cham},
 author = {Ouchebara, Dyna Soumhane
and Dupont, St{\'e}phane},
 booktitle = {Computer Security -- ESORICS 2025},
 editor = {Nicomette, Vincent
and Benzekri, Abdelmalek
and Boulahia-Cuppens, Nora
and Vaidya, Jaideep},
 isbn = {978-3-032-07884-1},
 pages = {289--308},
 publisher = {Springer Nature Switzerland},
 title = {Llama-Based Source Code Vulnerability Detection: Prompt Engineering vs Fine Tuning},
 url = {https://link.springer.com/chapter/10.1007/978-3-032-07884-1_15},
 year = {2026}
}

@Article{Mou2025,
author="Mou, Daen
and Wei, Zhihua
and Ni, Lin
and Song, Na
and Sun, Yiwei
and Chu, Weizhong
and Jin, Benkai",
title="LLM-enhanced representation learning for graph collaborative filtering recommendation models",
journal="Journal of Intelligent Information Systems",
year="2025",
month="Aug",
day="01",
volume="63",
number="4",
pages="1179--1202",
abstract="Graph collaborative filtering recommendation models have gained significant attention in recommender systems due to their ability to capture complex user-item relationships through interaction graphs. However, these models often overlook the rich information contained in textual and tabular data, which can provide valuable insights into user preferences and item characteristics. To address this limitation, we propose a model-agnostic multi-task representation learning framework, LLMGCF, which aims to enhance the performance of graph-based recommendation models by integrating textual data enriched by large language models (LLMs) and feature-engineered tabular data. Specifically, our framework leverage contrastive learning to align semantic signals derived from LLM-enhanced textual data, attribute-based tabular signals, and collaborative graph signals, enabling effective cross-modal knowledge fusion. Additionally, LLMGCF utilizes multi-task learning to jointly optimize the supervised recommendation retrieval task and the cross-modal knowledge alignment task. Experimental results on two public datasets demonstrate that LLMGCF outperforms state-of-the-art (SOTA) graph collaborative filtering models, achieving average improvements of approximately 4.17{\%} in Recall and 3.68{\%} in NDCG. Furthermore, our framework exhibits strong robustness against random noise, highlighting its practical applicability in real-world scenarios.",
issn="1573-7675",
doi="10.1007/s10844-025-00933-9",
url="https://doi.org/10.1007/s10844-025-00933-9"
}


@inproceedings{10.1007/978-3-032-07132-3_3,
 abstract = {Cyber-physical systems (CPSs) are complex systems that integrate physical, computational, and communication subsystems. The heterogeneous nature of these systems makes their safety assurance challenging. In this paper, we propose a novel automated approach for guardrailing cyber-physical systems using property-based tests (PBTs) generated by Large Language Models (LLMs). Our approach employs an LLM to extract properties from the code and documentation of CPSs. Next, we use the LLM to generate PBTs that verify the extracted properties on the CPS. The generated PBTs have two uses. First, they are used to test the CPS before it is deployed, i.e., at design time. Secondly, these PBTs can be used after deployment, i.e., at run time, to monitor the behavior of the system and guardrail it against unsafe states. We implement our approach in ChekProp and conduct preliminary experiments to evaluate the generated PBTs in terms of their relevance (how well they match manually crafted properties), executability (how many run with minimal manual modification), and effectiveness (coverage of the input space partitions). The results of our experiments and evaluation demonstrate a promising path forward for creating guardrails for CPSs using LLM-generated property-based tests.},
 address = {Cham},
 author = {Etemadi, Khashayar
and Sirjani, Marjan
and Helali Moghadam, Mahshid
and Strandberg, Per
and Pettersson, Paul},
 booktitle = {Bridging the Gap Between AI and Reality},
 editor = {Steffen, Bernhard},
 isbn = {978-3-032-07132-3},
 pages = {18--46},
 publisher = {Springer Nature Switzerland},
 title = {LLM-Based Property-Based Test Generation for Guardrailing Cyber-Physical Systems},
 url = {https://link.springer.com/chapter/10.1007/978-3-032-07132-3_3},
 year = {2026}
}

@Article{Arikan2025,
author="Arikan, Kaya Emre
and Do{\u{g}}an, Sait Melih
and Yilmaz, Ercan Nurcan
and G{\"o}nen, Serkan",
title="From code to security: machine learning approaches in android vulnerability detection",
journal="International Journal of Information Security",
year="2025",
month="Dec",
day="27",
volume="25",
number="1",
pages="17",
abstract="In today's technology-driven society, an increasing number of individuals rely on mobile devices, leading to a surge in the availability of applications. Smartphone users constantly search for apps that meet their needs, resulting in a flood of options in the marketplace. However, there is a growing concern regarding the security of Android applications, as many have shortcomings in addressing critical security aspects. One reason behind this issue often lies in the lack of automated mechanisms during the design and development stages to identify, test, and rectify vulnerabilities in the source code. It is crucial to address these issues proactively rather than relying solely on updates and patches for already published apps. In response to this challenge, researchers have proposed machine learning techniques to enhance application security by detecting vulnerabilities and malicious code within source code. This systematic literature review delves into this domain by examining 85 carefully selected technical studies published between 2017 and 2024. It aims to shed light on the strengths, weaknesses, and practical applicability of these techniques, while also identifying areas for further improvement. Moreover, the growing focus on advanced approaches---such as Large Language Models (LLMs) and Explainable AI (XAI)---indicates a trend toward more transparent and context-aware vulnerability detection. By synthesizing key insights from the current literature, this review enhances our understanding of Android security approaches, identifies promising directions for future research, and ultimately contributes to the advancement of more secure mobile applications through machine learning-based vulnerability detection.",
issn="1615-5270",
doi="10.1007/s10207-025-01190-1",
url="https://doi.org/10.1007/s10207-025-01190-1"
}


@inproceedings{10.1007/978-3-032-02853-2_9,
 abstract = {Cloud-native architectures demand rapid, continuous delivery, stretching traditional DevOps workflows. Artificial intelligence (AI) and machine learning (ML) now supply the predictive insight and automation required to meet this pace. This paper introduces a unified AI-driven DevOps framework that optimizes the software-development lifecycle (SDLC) for cloud-native applications. We summarise recent advances in AI-enhanced CI/CD, predictive observability, proactive DevSecOps security, and self-healing infrastructure, and examine practical deployments across Amazon Web Services (AWS), Microsoft Azure, and Google Cloud Platform (GCP). Finally, we outline future research directions---explainable and generative AI, federated learning, and responsible AI governance---charting a path toward sustainable, resilient, and secure cloud-native modernization.},
 address = {Cham},
 author = {Mittal, Akshay},
 booktitle = {ICT for Global Innovations and Solutions},
 editor = {Bhattacharya, Saurav},
 isbn = {978-3-032-02853-2},
 pages = {130--147},
 publisher = {Springer Nature Switzerland},
 title = {AI-Driven DevOps Automation for Cloud-Native Application Modernization},
 url = {https://link.springer.com/chapter/10.1007/978-3-032-02853-2_9},
 year = {2026}
}

@Article{Zuo2024,
author="Zuo, Fei
and Rhee, Junghwan",
title="Vulnerability discovery based on source code patch commit mining: a systematic literature review",
journal="International Journal of Information Security",
year="2024",
month="Apr",
day="01",
volume="23",
number="2",
pages="1513--1526",
abstract="In recent years, there has been a remarkable surge in the adoption of open-source software (OSS). However, with the growing usage of OSS components in both free and proprietary software, vulnerabilities that are present within them can be spread to a vast array of underlying applications. Even worse, a myriad of vulnerabilities are fixed secretly via patch commits, which causes other software re-using the vulnerable code snippets to be left in the dark. Thus, source code patch commit mining toward vulnerability discovery is receiving immense attention, and a variety of approaches are proposed. Despite that, there is no comprehensive survey summarizing and discussing the current progress within this field. To fill this gap, we survey, evaluate, and systematize a list of literature and provide the community with our insights on both successes and remaining issues in this space. Special attention is paid on the work toward vulnerability discovery. In this paper, we also provide an introductory panorama with our replicable hands-on experience, which can help readers quickly understand and step into the pertinent field. Our empirical study reveals noteworthy challenges which need to be highlighted and addressed in this field. We also discuss potential directions for the future work. To the best of knowledge, we provide the first literature review to study source code patch commit mining in the vulnerability discovery context. The systematic framework, hands-on practices, and list of potential challenges provide new knowledge for mining source code patch commit toward a more robust software eco-system. The research gaps found in this literature review show the need for future research, such as the concern on data quality, high false alarms, and the significance of textual information.",
issn="1615-5270",
doi="10.1007/s10207-023-00795-8",
url="https://doi.org/10.1007/s10207-023-00795-8"
}


@inproceedings{10.1007/978-3-031-72658-3_24,
 abstract = {We introduce Groma, a Multimodal Large Language Model (MLLM) with grounded and fine-grained visual perception ability. Beyond holistic image understanding, Groma is adept at region-level tasks such as region captioning and visual grounding. Such capabilities are built upon a localized visual tokenization mechanism, where an image input is decomposed into regions of interest and subsequently encoded into region tokens. By integrating region tokens into user instructions and model responses, we seamlessly enable Groma to understand user-specified region inputs and ground its textual output to images. Besides, to enhance the grounded chat ability of Groma, we curate a visually grounded instruction dataset by leveraging the powerful GPT-4V and visual prompting techniques. Compared with MLLMs that rely on the language model or external module for localization, Groma consistently demonstrates superior performances in standard referring and grounding benchmarks, highlighting the advantages of embedding localization into image tokenization. Project page: https://groma-mllm.github.io/.},
 address = {Cham},
 author = {Ma, Chuofan
and Jiang, Yi
and Wu, Jiannan
and Yuan, Zehuan
and Qi, Xiaojuan},
 booktitle = {Computer Vision -- ECCV 2024},
 editor = {Leonardis, Ale{\v{s}}
and Ricci, Elisa
and Roth, Stefan
and Russakovsky, Olga
and Sattler, Torsten
and Varol, G{\"u}l},
 isbn = {978-3-031-72658-3},
 pages = {417--435},
 publisher = {Springer Nature Switzerland},
 title = {Groma: Localized Visual Tokenization for Grounding Multimodal Large Language Models},
 url = {https://link.springer.com/chapter/10.1007/978-3-031-72658-3_24},
 year = {2025}
}

@Article{Qu2025,
author="Qu, Yubin
and Huang, Song
and Li, Yanzhou
and Bai, Tongtong
and Chen, Xiang
and Wang, Xingya
and Li, Long
and Yao, Yongming",
title="BadCodePrompt: backdoor attacks against prompt engineering of large language models for code generation",
journal="Automated Software Engineering",
year="2025",
month="Jan",
day="28",
volume="32",
number="1",
pages="17",
abstract="Using few-shot demonstrations in prompts significantly enhances the generation quality of large language models (LLMs), including code generation. However, adversarial examples injected by malicious service providers via few-shot prompting pose a risk of backdoor attacks in large language models. There is no research on backdoor attacks on large language models in the few-shot prompting setting for code generation tasks. In this paper, we propose BadCodePrompt, the first backdoor attack for code generation tasks targeting LLMS in the few-shot prompting scenario, without requiring access to training data or model parameters and with lower computational overhead. BadCodePrompt exploits the insertion of triggers and poisonous code patterns into examples, causing the output of poisonous source code when there is a backdoor trigger in the end user's query prompt. We demonstrate the effectiveness of BadCodePrompt in conducting backdoor attacks on three LLMS (GPT-4, Claude-3.5-Sonnet, and Gemini Pro-1.5) in code generation tasks without affecting the functionality of the generated code. LLMs with stronger reasoning capabilities are also more vulnerable to BadCodePrompt, with an average attack success rate of up to 98.53{\%} for GPT-4 in two benchmark tasks. Finally, we employ state-of-the-art defenses against backdoor attacks in Prompt Engineering and show their overall ineffectiveness against BadCodePrompt. Therefore, BadCodePrompt remains a serious threat to LLMS, underscoring the urgency of developing effective defense mechanisms.",
issn="1573-7535",
doi="10.1007/s10515-024-00485-2",
url="https://doi.org/10.1007/s10515-024-00485-2"
}


@inproceedings{10.1007/978-3-031-61486-6_11,
 abstract = {Large language models (LLMs) are becoming a powerful transformative force of automation in the areas of software engineering and cybersecurity. In software-centric security research, the LLMs have undertaken a prime role in the identification and repair of security vulnerabilities and bugs. However, in hardware related fields such as logic design and hardware security, use of LLMs has only recently started to get traction. In this work we aim to explore the potential of LLMs in the offensive hardware security domain. More specifically, we explore the level of assistance that LLMs can provide to attackers for the insertion of vulnerabilities, known as hardware trojans (HTs), in complex hardware designs (e.g., CPUs). Having in mind high-level attack outlines, we test the ability of a general-purpose LLM to act as a ``filter'' that correlates system level concepts of security interest with specific module abstractions of hardware designs. By doing so, we tackle the challenges posed by the context length limit of LLMs, that become prevalent during LLM-based analyses of large code bases. Next, we initiate an LLM analysis of the reduced code base, that includes only the register transfer level code of the identified modules and test the LLM's ability to locate the parts that implement the queried security related features. In this way, we reduce the complexity of the overall analysis performed by the LLM. Lastly, we instruct the LLM to insert suitable trojan functionalities by modifying the identified code parts accordingly. To showcase the potential of our automated LLM-based hardware trojan insertion flow, we craft a realistic HT for a modern RISC-V micro-architecture. We test the functionality of the LLM-generated HT on an FPGA board, by attacking the integrity and the availability of the RISC-V CPU. Hence, we demonstrate how general-purpose LLMs can navigate attackers through complex hardware designs and assist them in the implementation of realistic HT attacks.},
 address = {Cham},
 author = {Kokolakis, Georgios
and Moschos, Athanasios
and Keromytis, Angelos D.},
 booktitle = {Applied Cryptography and Network Security Workshops},
 editor = {Andreoni, Martin},
 isbn = {978-3-031-61486-6},
 pages = {176--194},
 publisher = {Springer Nature Switzerland},
 title = {Harnessing the Power of General-Purpose LLMs in Hardware Trojan Design},
 url = {https://link.springer.com/chapter/10.1007/978-3-031-61486-6_11},
 year = {2024}
}

@Article{Asare2023,
author="Asare, Owura
and Nagappan, Meiyappan
and Asokan, N.",
title="Is GitHub's Copilot as bad as humans at introducing vulnerabilities in code?",
journal="Empirical Software Engineering",
year="2023",
month="Sep",
day="23",
volume="28",
number="6",
pages="129",
abstract="Several advances in deep learning have been successfully applied to the software development process. Of recent interest is the use of neural language models to build tools, such as Copilot, that assist in writing code. In this paper we perform a comparative empirical analysis of Copilot-generated code from a security perspective. The aim of this study is to determine if Copilot is as bad as human developers. We investigate whether Copilot is just as likely to introduce the same software vulnerabilities as human developers. Using a dataset of C/C++ vulnerabilities, we prompt Copilot to generate suggestions in scenarios that led to the introduction of vulnerabilities by human developers. The suggestions are inspected and categorized in a 2-stage process based on whether the original vulnerability or fix is reintroduced. We find that Copilot replicates the original vulnerable code about 33{\%} of the time while replicating the fixed code at a 25{\%} rate. However this behaviour is not consistent: Copilot is more likely to introduce some types of vulnerabilities than others and is also more likely to generate vulnerable code in response to prompts that correspond to older vulnerabilities. Overall, given that in a significant number of cases it did not replicate the vulnerabilities previously introduced by human developers, we conclude that Copilot, despite performing differently across various vulnerability types, is not as bad as human developers at introducing vulnerabilities in code.",
issn="1573-7616",
doi="10.1007/s10664-023-10380-1",
url="https://doi.org/10.1007/s10664-023-10380-1"
}


@inproceedings{10.1007/978-3-031-71139-8_5,
 abstract = {In this research, we aim to conduct a systematic mapping study on Large Language Models (LLMs) for Software Engineering (SE). The significantly enhanced capabilities of LLMs have led to their use in many fields, including the important domain of SE. SE processes involve numerous artifacts, such as code, requirements, and documentation, which can serve as input to LLMs. To determine the potential applications of LLMs in SE, it's crucial to understand their capabilities. Therefore, this systematic mapping study will explore the capabilities and potential of LLMs in SE tasks. Additionally, this research will address issues associated with LLMs, such as their non-deterministic nature and the problem of hallucinations. It will serve as a valuable resource for software developers, researchers, and practitioners interested in the intersection of artificial intelligence and SE, guiding their decisions on integrating these technologies.},
 address = {Cham},
 author = {G{\"o}rmez, Muhammet K{\"u}r{\c{s}}at
and Y{\i}lmaz, Murat
and Clarke, Paul M.},
 booktitle = {Systems, Software and Services Process Improvement},
 editor = {Yilmaz, Murat
and Clarke, Paul
and Riel, Andreas
and Messnarz, Richard
and Greiner, Christian
and Peisl, Thomas},
 isbn = {978-3-031-71139-8},
 pages = {64--79},
 publisher = {Springer Nature Switzerland},
 title = {Large Language Models for Software Engineering: A Systematic Mapping Study},
 url = {https://link.springer.com/chapter/10.1007/978-3-031-71139-8_5},
 year = {2024}
}

@Article{Chen2024,
author="Chen, Xiangping
and Hu, Xing
and Huang, Yuan
and Jiang, He
and Ji, Weixing
and Jiang, Yanjie
and Jiang, Yanyan
and Liu, Bo
and Liu, Hui
and Li, Xiaochen
and Lian, Xiaoli
and Meng, Guozhu
and Peng, Xin
and Sun, Hailong
and Shi, Lin
and Wang, Bo
and Wang, Chong
and Wang, Jiayi
and Wang, Tiantian
and Xuan, Jifeng
and Xia, Xin
and Yang, Yibiao
and Yang, Yixin
and Zhang, Li
and Zhou, Yuming
and Zhang, Lu",
title="Deep learning-based software engineering: progress, challenges, and opportunities",
journal="Science China Information Sciences",
year="2024",
month="Dec",
day="24",
volume="68",
number="1",
pages="111102",
abstract="Researchers have recently achieved significant advances in deep learning techniques, which in turn has substantially advanced other research disciplines, such as natural language processing, image processing, speech recognition, and software engineering. Various deep learning techniques have been successfully employed to facilitate software engineering tasks, including code generation, software refactoring, and fault localization. Many studies have also been presented in top conferences and journals, demonstrating the applications of deep learning techniques in resolving various software engineering tasks. However, although several surveys have provided overall pictures of the application of deep learning techniques in software engineering, they focus more on learning techniques, that is, what kind of deep learning techniques are employed and how deep models are trained or fine-tuned for software engineering tasks. We still lack surveys explaining the advances of subareas in software engineering driven by deep learning techniques, as well as challenges and opportunities in each subarea. To this end, in this study, we present the first task-oriented survey on deep learning-based software engineering. It covers twelve major software engineering subareas significantly impacted by deep learning techniques. Such subareas spread out through the whole lifecycle of software development and maintenance, including requirements engineering, software development, testing, maintenance, and developer collaboration. As we believe that deep learning may provide an opportunity to revolutionize the whole discipline of software engineering, providing one survey covering as many subareas as possible in software engineering can help future research push forward the frontier of deep learning-based software engineering more systematically. For each of the selected subareas, we highlight the major advances achieved by applying deep learning techniques with pointers to the available datasets in such a subarea. We also discuss the challenges and opportunities concerning each of the surveyed software engineering subareas.",
issn="1869-1919",
doi="10.1007/s11432-023-4127-5",
url="https://doi.org/10.1007/s11432-023-4127-5"
}


@Inbook{Sarschar2025,
author="Sarschar, Mahja",
title="Theoretical Background",
bookTitle="Pipeline for Automated Code Generation from Backlog Items (PACGBI): Analysis of Potentials and Limitations of Generative AI for Web Development",
year="2025",
publisher="Springer Fachmedien Wiesbaden",
address="Wiesbaden",
pages="3--23",
abstract="In this section, the background of the research topic is explained. In the first part, an introduction to the GenAI is given, with a focus on code generation. This involves large language models, prompting strategies and current GenAI tools. The second part of this section deals with relevant aspects of software development, consisting of the software development process, agile methods, software quality and project hosting platforms.",
isbn="978-3-658-47208-5",
doi="10.1007/978-3-658-47208-5_2",
url="https://doi.org/10.1007/978-3-658-47208-5_2"
}


@Article{Vidaković2025,
author="Vidakovi{\'{c}}, Dragan
and Luburi{\'{c}}, Nikola
and Kova{\v{c}}evi{\'{c}}, Aleksandar
and Slivka, Jelena",
title="Enhancing software and learning with Serbian student feedback corpora",
journal="Language Resources and Evaluation",
year="2025",
month="Dec",
day="01",
volume="59",
number="4",
pages="4193--4221",
abstract="Automated collection and analysis of student feedback within Intelligent Tutoring Systems are vital for the continuous refinement of both educational content and software performance, ensuring that learning environments remain responsive to student needs. This study presents the creation and annotation of Serbian student feedback corpora within an Intelligent Tutoring System, intending to enhance both software functionality and educational experiences. The research addresses gaps in existing studies by implementing a transparent and standardized data annotation process, with Inter-Annotator Agreement scores confirming the reliability of the annotation process. The resulting datasets were then processed using fine-tuned multilingual transformer models, with data augmentation techniques enhancing the analysis. Additionally, a few-shot prompting of a large language model was explored to further improve classification accuracy. The experimental results show that fine-tuned transformer models, combined with data augmentation, significantly enhance the accuracy of feedback analysis, achieving performance levels comparable to human annotators and surpassing baseline models. This automated approach to analyzing student feedback provides substantial time and resource savings for educators and software developers, enabling more efficient and timely improvements to both the software and the educational strategies. This work not only contributes to the development of Serbian language resources but also establishes a foundation for future research in Crowd-based Requirements Engineering and Text-based Emotion Detection within educational contexts.",
issn="1574-0218",
doi="10.1007/s10579-025-09855-y",
url="https://doi.org/10.1007/s10579-025-09855-y"
}


@inproceedings{10.1007/978-981-96-9849-3_17,
 abstract = {Smart contract security serves as a critical safeguard mechanism for ensuring trusted transactional interactions within blockchain ecosystems, emphasizing the imperative for systematic vulnerability detection processes. Deep learning-based techniques have emerged as a promising solution in blockchain security, through their capacity to automate the extraction of high-level semantic features. However, two challenges persist in current methodologies for smart contract vulnerability detection: (1) Feature representation mechanisms exhibit inherent limitations in comprehensively and precisely capturing the intrinsic characteristics of vulnerabilities; (2) Existing frameworks have not explored methods for feature enhancement during training. To overcome the aforementioned challenges, we propose a novel framework integrating feature enhancement techniques with self-supervised training for robust smart contract vulnerability detection. (1) Contract feature vectors are extracted using pre-trained models and convolutional neural networks, followed by retrieval of semantically similar contracts from existing repositories and generation of chain-of-thought vectors through large language models; (2) Contractual features and reasoning patterns are fused to create enhanced representations; (3) Knowledge distillation is employed to optimize feature learning during model training. Empirical findings from three standardized data corroborate that the developed method exhibits enhanced predictive accuracy relative to conventional benchmarks in the field.},
 address = {Singapore},
 author = {Su, Peng
and Hu, Jingyuan},
 booktitle = {Advanced Intelligent Computing Technology and Applications},
 editor = {Huang, De-Shuang
and Chen, Wei
and Pan, Yijie
and Chen, Haiming},
 isbn = {978-981-96-9849-3},
 pages = {199--209},
 publisher = {Springer Nature Singapore},
 title = {Smart Contract Vulnerability Detection with Feature-Enhancement and Self-supervised Training},
 url = {https://link.springer.com/chapter/10.1007/978-981-96-9849-3_17},
 year = {2025}
}

@inproceedings{10.1007/978-3-032-08649-5_7,
 abstract = {This study presents a comparative evaluation of three large language models---Gemini 2.0 Flash, ChatGPT 4.o, and Copilot Enterprise---in the context of automated code generation. The models were assessed across multiple dimensions covered in CS I including decision structures, repetition structures, functions, class {\&} OOP, code quality, documentation, robustness, algorithmic logic, and input validation. The results indicate that all three models achieved A(Excellent) performance in repetition structure and Class {\&} OOP. All three models achieved B (Good) performance in function category. However, the study reveals a significant gap in comments and documentation, with all models scoring at or below the minimum passing grade. The gap found indicated the improvement in AI-generated code to support maintainable and collaborative software development.},
 address = {Cham},
 author = {Qu, Junfeng
and Bai, Shuju
and Jeff, Byron
and Khosravi, Ebrahim},
 booktitle = {Software and Data Engineering},
 editor = {Rahimi, Nick
and Margapuri, Venkat
and Golilarz, Noor Amiri},
 isbn = {978-3-032-08649-5},
 pages = {98--110},
 publisher = {Springer Nature Switzerland},
 title = {Analysis of Programming Capability of LLMs in the Context of Computer Science I},
 url = {https://link.springer.com/chapter/10.1007/978-3-032-08649-5_7},
 year = {2026}
}

@inproceedings{10.1007/978-981-96-6537-2_2,
 abstract = {Subjective manual evaluation of hallucinations in large language models (LLMs) remains a challenging task due to its time-consuming and labored character. It expects human expertise to analyze each LLL response critically to assess cases where the model produces wrong or misleading data. This manual approach is usually small-scale and logical in extent, bringing elements of judgment into the assessment equation. To solve these problems, we have proposed a novel hybrid test automation framework that integrates rules-based and model-based grading methods. The rule-based evaluation, done at the per-commit granularity during the continuous integration (CI) cycle, offers quick feedback on the exact hallucination patterns, while the model-based grading method is carried out during a increment release and is evaluated using a critique LLM. Since there is already known behaviors concerning hallucinations, the framework can readily encode these behaviors in a set of rules and thus be able to alert on potential issues. A more in-depth evaluation is carried out on the LLM by using model-graded evaluation, which is done after the release stage. The first approach focuses on training machine learning models to estimate the probability of hallucination from different characteristics of the LLM's responses. Thus, using both of these methods in the context of our framework allows for the constant review of the potential hallucination risks during each stage of the LLM expansion. This makes it possible to identify problems that might cause problems in the use of LLM-based applications early enough and take necessary measures to address them before their manifestations, thus enabling LLM-based applications to be deployed with higher chances of working as expected.},
 address = {Singapore},
 author = {Chakraborty, Amit
and Mallick, Chirantana
and Chakraborty, Rajdeep
and Das, Saptarshi},
 booktitle = {Data Management, Analytics and Innovation},
 editor = {Goswami, Saptarsi
and Saha, Sajal
and Beed, Romit S.
and Basu, Kanadpriya},
 isbn = {978-981-96-6537-2},
 pages = {13--31},
 publisher = {Springer Nature Singapore},
 title = {HyGen---A Hybrid Automation Testing Approach for Reducing Hallucination in LLM-Based Applications},
 url = {https://link.springer.com/chapter/10.1007/978-981-96-6537-2_2},
 year = {2026}
}

@Article{Shahin2024,
author="Shahin, Mohammad
and Chen, F. Frank
and Hosseinzadeh, Ali
and Maghanaki, Mazdak
and Eghbalian, Ayda",
title="A novel approach to voice of customer extraction using GPT-3.5 Turbo: linking advanced NLP and Lean Six Sigma 4.0",
journal="The International Journal of Advanced Manufacturing Technology",
year="2024",
month="Apr",
day="01",
volume="131",
number="7",
pages="3615--3630",
abstract="This research breaks new ground by utilizing the advanced natural language processing (NLP) capabilities of OpenAI's GPT-3.5 Turbo model for the extraction of voice of customer (VoC) data from online customer support interactions on Twitter. Traditional methods of VoC extraction have typically fallen short in capturing the richness and nuance of customer sentiment. Contemporary machine learning (ML) approaches, while improved, still struggle to interpret the contextual subtleties of digital customer communications effectively. This study showcases the innovative deployment of GPT-3.5 Turbo, demonstrating its superior performance in extracting VoC through a deeper understanding of conversational context and a more intuitive, chat-based data processing. Furthermore, the large-scale, multilingual processing capabilities of this model offer a more comprehensive and inclusive analysis of VoC. The study ties these advancements to Lean Six Sigma 4.0, illustrating how the integration of GPT-3.5 Turbo's transformative capabilities can elevate the customer-centric approach of Lean Six Sigma in the era of Industry 4.0. This innovative exploration points to a significant evolution in VoC analysis, offering potential for more insightful, real-time data--driven customer service strategies and a more substantial foundation for decision-making in product development and process improvement. Future research is encouraged to validate these preliminary findings and to investigate ethical considerations associated with the use of such advanced NLP models.",
issn="1433-3015",
doi="10.1007/s00170-024-13167-w",
url="https://doi.org/10.1007/s00170-024-13167-w"
}


@inproceedings{10.1007/978-3-031-53656-4_8,
 abstract = {We explore the evolving efficacy of three generative pre-trained transformer (GPT) models in generating answers for multiple-choice questions (MCQ) from introductory and intermediate Python programming courses in higher education. We focus on the differences in capabilities of the models prior to the release of ChatGPT (Nov '22), at the time of the release, and today (i.e., Aug '23). Recent studies have established that the abilities of the OpenAI's GPT models to handle assessments originally designed for humans keep increasing as the newer more capable models are released. However, the qualitative differences in the capabilities and limitations of these models to reason about and/or analyze programming MCQs have been under-explored. We evaluated three OpenAI's GPT models on formative and summative MCQ assessments from three Python courses (530 questions) focusing on the qualitative differences in the evolving efficacy of the subsequent models. This study provides further evidence and insight into the trajectory of the current developments where there already exists a technology that can be utilized by students to collect passing scores, with no effort whatsoever, on what today counts as viable programming knowledge and skills assessments. This study could be leveraged by educators and institutions to better understand the recent technological developments in order to adapt the design of programming assessments as well as to fuel the necessary discussions into how assessments in future programming classes should be updated.},
 address = {Cham},
 author = {Savelka, Jaromir
and Agarwal, Arav
and Bogart, Christopher
and Sakr, Majd},
 booktitle = {Computer Supported Education},
 editor = {McLaren, Bruce M.
and Uhomoibhi, James
and Jovanovic, Jelena
and Chounta, Irene-Angelica},
 isbn = {978-3-031-53656-4},
 pages = {160--182},
 publisher = {Springer Nature Switzerland},
 title = {From GPT-3 to GPT-4: On the Evolving Efficacy of LLMs to Answer Multiple-Choice Questions for Programming Classes in Higher Education},
 url = {https://link.springer.com/chapter/10.1007/978-3-031-53656-4_8},
 year = {2024}
}

@Article{Yue2025,
author="Yue, Songhui",
title="A Multimodal conceptual framework to achieve automated software evolution for context-rich intelligent applications",
journal="Innovations in Systems and Software Engineering",
year="2025",
month="Sep",
day="01",
volume="21",
number="3",
pages="1091--1105",
abstract="While AI is extensively transforming Software Engineering (SE) fields, SE is still in need of a framework to consider overall all phases to facilitate Automated Software Evolution (ASEv), particularly for intelligent applications that are context-rich instead of conquering each division independently. Its complexity comes from the intricacy of the intelligent applications, the heterogeneity of the data sources, and the constant changes in the context. This study proposes a conceptual framework for achieving automated software evolution, emphasizing the importance of multimodality learning. A Selective Sequential Scope Model (3 S) model is developed based on the conceptual framework, and it can be used to categorize existing and future research when it covers different SE phases and multimodal learning tasks. This research is a preliminary step toward the blueprint of a higher-level ASEv. The proposed conceptual framework can act as a practical guideline for practitioners to prepare themselves for diving into this area. Although the study is about intelligent applications, the framework and analysis methods may be adapted for other types of software as AI brings more intelligence into their life cycles.",
issn="1614-5054",
doi="10.1007/s11334-024-00591-0",
url="https://doi.org/10.1007/s11334-024-00591-0"
}


@Article{Sajadi2025,
author="Sajadi, Amirali
and Le, Binh
and Nguyen, Anh
and Damevski, Kostadin
and Chatterjee, Preetha",
title="Do LLMs consider security? an empirical study on responses to programming questions",
journal="Empirical Software Engineering",
year="2025",
month="Apr",
day="16",
volume="30",
number="4",
pages="101",
abstract="The widespread adoption of conversational LLMs for software development has raised new security concerns regarding the safety of LLM-generated content. Our motivational study outlines ChatGPT's potential in volunteering context-specific information to the developers, promoting safe coding practices. Motivated by this finding, we conduct a study to evaluate the degree of security awareness exhibited by three prominent LLMs: Claude 3, GPT-4, and Llama 3. We prompt these LLMs with Stack Overflow questions that contain vulnerable code to evaluate whether they merely provide answers to the questions or if they also warn users about the insecure code, thereby demonstrating a degree of security awareness. Further, we assess whether LLM responses provide information about the causes, exploits, and the potential fixes of the vulnerability, to help raise users' awareness. Our findings show that all three models struggle to accurately detect and warn users about vulnerabilities, achieving a detection rate of only 12.6{\%} to 40{\%} across our datasets. We also observe that the LLMs tend to identify certain types of vulnerabilities related to sensitive information exposure and improper input neutralization much more frequently than other types, such as those involving external control of file names or paths. Furthermore, when LLMs do issue security warnings, they often provide more information on the causes, exploits, and fixes of vulnerabilities compared to Stack Overflow responses. Finally, we provide an in-depth discussion on the implications of our findings, and demonstrated a CLI-based prompting tool that can be used to produce more secure LLM responses.",
issn="1573-7616",
doi="10.1007/s10664-025-10658-6",
url="https://doi.org/10.1007/s10664-025-10658-6"
}


@inproceedings{10.1007/978-3-032-01799-4_1,
 abstract = {Securing externally sourced hardware designs is essential to prevent adversaries from embedding hardware Trojans. Trojans are stealthy modifications that leak data or create backdoors. Existing benchmarks like Trust-Hub provide only a limited set of Trojans (106), while the possibilities are virtually infinite. To address this, we propose NETLAM, a comprehensive framework utilizing multiple LLM-based tools to generate previously undiscovered Trojans not included in Trust-Hub. The first tool converts hardware netlists into Directed Acyclic Graphs (DAGs) to identify vulnerable nets and components in digital designs. Using these insights, the second tool generates stealthy Trojan-infected versions of the original design. To evaluate the stealthiness of these Trojans, we use an LLM-based equivalence checker, where stealthier Trojans pass equivalence checks while others are detected. We evaluate NETLAM using the AES dataset from Trust-Hub consisting of 28 Trojans. We identified 5 new Trojans, with high Common Vulnerability Scoring System (CVSS) scores, demonstrating their stealthiness. To prove the efficacy of the NETLAM generated Trojans, we further utilize an open-source formal equivalence checker to perform a functional equivalence check between the golden and the NETLAM generated Trojan-infected circuits. All of the suggested Trojans pass the formal equivalence check. However, the same Trojan-infested circuits fail in the NETLAM equivalence test, thus validating the effectiveness of our proposed framework. We show that LLMs and Generative AI models, such as GPT-4o and Gemini, can enhance Trojan detection by using semantic and probabilistic analysis rather than strict logical equivalence (GitHub Repository: https://github.com/shubhishukla10/NETLAM).},
 address = {Cham},
 author = {Sarkar, Tishya Sarma
and Arya, Kislay
and Chowdhury, Siddhartha
and Mandal, Upasana
and Shukla, Shubhi
and Bhattacharya, Sarani
and Mukhopadhyay, Debdeep},
 booktitle = {Applied Cryptography and Network Security Workshops},
 editor = {Manulis, Mark},
 isbn = {978-3-032-01799-4},
 pages = {3--21},
 publisher = {Springer Nature Switzerland},
 title = {NETLAM: An Automated LLM Framework to Generate and Evaluate Stealthy Hardware Trojans},
 url = {https://link.springer.com/chapter/10.1007/978-3-032-01799-4_1},
 year = {2026}
}

@inproceedings{10.1007/978-3-032-15632-7_15,
 abstract = {As Artificial Intelligence (AI) systems become increasingly integrated into critical domains, conventional risk management methodologies often prove inadequate for addressing their unique and complex challenges, particularly the emergence of novel, unforeseen risks. To address this gap, this paper introduces the LLM-Based AI Risk Management Framework, a structured four-step process that systematically leverages Large Language Models (LLMs) to enhance risk identification and analysis. The framework's efficacy is demonstrated through a detailed case study of an AI-powered matching system and an empirical validation of its core prompt engineering techniques. The results show that this approach enables the generation of comprehensive risk scenarios, including critical compliance and ethical issues initially overlooked by human experts, thereby serving as an objective counter-perspective to organizational biases. The study reveals that the framework's success hinges on a sophisticated human-in-the-loop model where human experts provide strategic direction, not just passive validation. A key finding is that the quality of LLM outputs is dramatically improved by framing requests as concrete `incident scenarios' instead of abstract `risks'. This research contributes an empirically-grounded methodology for integrating LLMs into AI governance, demonstrating that the strategic partnership between human expertise and LLM capabilities can foster a more robust, responsible, and safe approach to managing AI systems.},
 address = {Cham},
 author = {Morozumi, Arisa
and Hayashi, Hisashi},
 booktitle = {Computational Intelligence},
 editor = {Marcelloni, Francesco
and Madani, Kurosh
and van Stein, Niki
and Filipe, Joaquim},
 isbn = {978-3-032-15632-7},
 pages = {269--293},
 publisher = {Springer Nature Switzerland},
 title = {LLM-Based Risk Scenario Generation and Mitigation for AI Systems: A Case Study Approach},
 url = {https://link.springer.com/chapter/10.1007/978-3-032-15632-7_15},
 year = {2026}
}

@inproceedings{10.1007/978-3-031-48796-5_14,
 abstract = {We introduce a novel automated testing technique that combines LLM and search-based fuzzing. We use ChatGPT to parameterise C programs. We compile the resultant code snippets, and feed compilable ones to SearchGEM5, our extension to AFL++ fuzzer with customised new mutation operators. We run thus created 4005 binaries through our system under test, gem5, increasing its existing test coverage by more than 1000 lines. We discover 244 instances where gem5 simulation of the binary differs from the binary's expected behaviour.},
 address = {Cham},
 author = {Dakhama, Aidan
and Even-Mendoza, Karine
and Langdon, W.B.
and Menendez, Hector
and Petke, Justyna},
 booktitle = {Search-Based Software Engineering},
 editor = {Arcaini, Paolo
and Yue, Tao
and Fredericks, Erik M.},
 isbn = {978-3-031-48796-5},
 pages = {160--166},
 publisher = {Springer Nature Switzerland},
 title = {SearchGEM5: Towards Reliable Gem5 with Search Based Software Testing and Large Language Models},
 url = {https://link.springer.com/chapter/10.1007/978-3-031-48796-5_14},
 year = {2024}
}

@Article{Kalyani2025,
author="Kalyani, P.
and Rao, C. Prakasa
and Goparaju, Bhargavi
and Babu, Kommu Kishore
and Kandimalla, Purna Chandra Rao",
title="BugPrioritizeAI for multimodal test case prioritisation using bug reports, code changes, and test metadata",
journal="Scientific Reports",
year="2025",
month="Dec",
day="08",
volume="16",
number="1",
pages="1539",
abstract="Regression testing is necessary in modern software development with continuous integration and delivery, but running it in full after every change is often too expensive. Test case prioritisation (TCP) can aid this process by prioritising test cases that reveal faults earliest. Still, current TCP approaches focus on single information sources (coverage, change history, and/or past faults) and do not model semantic relationships across software artefacts. Meanwhile, current deep learning--based methods still suffer from cross--project generalisation and misinterpretation. These gaps can be alleviated with BugPrioritizeAI, an explainable, multimodal TCP framework we propose that jointly uses bug reports, source code changes, and test metadata to rank test cases. At the core of BugTestRankNet is a component responsible for generating a priority score that allows for quicker fault detection. BugPrioritizeAI is an AI-enhanced approach to bug triage that operates at the bug report level and ranks potentially buggy files in the bug repo using textual features. This framework reduces testing overhead and provides SHAP-based explanations, giving developers insight into the reasons for prioritising individual test cases.",
issn="2045-2322",
doi="10.1038/s41598-025-31851-z",
url="https://doi.org/10.1038/s41598-025-31851-z"
}


@Article{Venkatesh2025,
author="Venkatesh, Ashwin Prasad Shivarpatna
and Sunil, Rose
and Sabu, Samkutty
and Mir, Amir M.
and Reis, Sofia
and Bodden, Eric",
title="An empirical study of large language models for type and call graph analysis in Python and JavaScript",
journal="Empirical Software Engineering",
year="2025",
month="Sep",
day="27",
volume="30",
number="6",
pages="167",
abstract="Large Language Models (LLMs) are increasingly being explored for their potential in software engineering, particularly in static analysis tasks. In this study, we investigate the potential of current LLMs to enhance call-graph analysis and type inference for Python and JavaScript programs. We empirically evaluated 24 LLMs, including OpenAI's GPT series and open-source models like LLaMA and Mistral, using existing and newly developed benchmarks. Specifically, we enhanced TypeEvalPy, a micro-benchmarking framework for type inference in Python, with auto-generation capabilities, expanding its scope from 860 to 77,268 type annotations for Python. Additionally, we introduce SWARM-CG and SWARM-JS, comprehensive benchmarking suites for evaluating call-graph construction tools across multiple programming languages. Our findings reveal a contrasting performance of LLMs in static analysis tasks. For call-graph generation, traditional static analysis tools such as PyCG for Python and Jelly for JavaScript consistently outperform LLMs. While advanced models like mistral-large-it-2407-123b and gpt-4o show promise, they still struggle with completeness and soundness in call-graph analysis across both languages. In contrast, LLMs demonstrate a clear advantage in type inference for Python, surpassing traditional tools like HeaderGen and hybrid approaches such as HiTyper. These results suggest that, while LLMs hold promise in type inference, their limitations in call-graph analysis highlight the need for further research. Our study provides a foundation for integrating LLMs into static analysis workflows, offering insights into their strengths and current limitations.",
issn="1573-7616",
doi="10.1007/s10664-025-10704-3",
url="https://doi.org/10.1007/s10664-025-10704-3"
}


@Article{Zong2025,
author="Zong, Mingyu
and Hekmati, Arvin
and Guastalla, Michael
and Li, Yiyi
and Krishnamachari, Bhaskar",
title="Integrating large language models with internet of things: applications",
journal="Discover Internet of Things",
year="2025",
month="Jan",
day="09",
volume="5",
number="1",
pages="2",
abstract="This paper identifies and analyzes applications in which Large Language Models (LLMs) can make Internet of Things (IoT) networks more intelligent and responsive through three case studies from critical topics: DDoS attack detection, macroprogramming over IoT systems, and sensor data processing. Our results reveal that the GPT model under few-shot learning achieves 87.6{\%} detection accuracy, whereas the fine-tuned GPT increases the value to 94.9{\%}. Given a macroprogramming framework, the GPT model is capable of writing scripts using high-level functions from the framework to handle possible incidents. Moreover, the GPT model shows efficacy in processing a vast amount of sensor data by offering fast and high-quality responses, which comprise expected results and summarized insights. Overall, the model demonstrates its potential to power a natural language interface. We hope that researchers will find these case studies inspiring to develop further.",
issn="2730-7239",
doi="10.1007/s43926-024-00083-4",
url="https://doi.org/10.1007/s43926-024-00083-4"
}


@inproceedings{10.1007/978-3-031-36272-9_74,
 abstract = {In educational settings, automated program repair techniques serve as a feedback mechanism to guide students working on their programming assignments. Recent work has investigated using large language models (LLMs) for program repair. In this area, most of the attention has been focused on using proprietary systems accessible through APIs. However, the limited access and control over these systems remain a block to their adoption and usage in education. The present work studies the repairing capabilities of open large language models. In particular, we focus on a recent family of generative models, which, on top of standard left-to-right program synthesis, can also predict missing spans of code at any position in a program. We experiment with one of these models on four programming datasets and show that we can obtain good repair performance even without additional training.},
 address = {Cham},
 author = {Koutcheme, Charles
and Sarsa, Sami
and Leinonen, Juho
and Hellas, Arto
and Denny, Paul},
 booktitle = {Artificial Intelligence in Education},
 editor = {Wang, Ning
and Rebolledo-Mendez, Genaro
and Matsuda, Noboru
and Santos, Olga C.
and Dimitrova, Vania},
 isbn = {978-3-031-36272-9},
 pages = {798--803},
 publisher = {Springer Nature Switzerland},
 title = {Automated Program Repair Using Generative Models for Code Infilling},
 url = {https://link.springer.com/chapter/10.1007/978-3-031-36272-9_74},
 year = {2023}
}

@Article{Mansouri2025,
author="Mansouri, Nesrine
and Soui, Makram
and Kessentini, Marouane",
title="Mining user reviews for method-level bug localization using transformers in java-based applications",
journal="Neural Computing and Applications",
year="2025",
month="Nov",
day="01",
volume="37",
number="32",
pages="26303--26320",
abstract="Users frequently post hundreds of reviews on mobile applications, often expressing dissatisfaction, reporting bugs, or suggesting new features. These reviews represent a valuable feedback channel that can be leveraged to improve software quality and user satisfaction. In this study, we propose an enhanced transformer-driven approach to automatically link user reviews to relevant method-level code elements in mobile apps. Rather than stopping at the file or class level, our method identifies specific Java methods that are most likely responsible for the issues described in user feedback. Our pipeline begins by filtering reviews through a combined sentiment and intent-aware layer, where reviews are retained if they either exhibit strong negative sentiment or are classified as complaints or feature requests using a zero-shot intent classification model. To group related reviews, we employ BERTopic. This transformer-based topic modeling technique uses Sentence-BERT embeddings and HDBSCAN clustering to form coherent semantic clusters without predefining the number of topics. Each topic is then represented as a dense vector by averaging the embeddings of the clustered reviews. On the code side, we extract and preprocess Java methods using JavaParser and generate contextual embeddings using Sentence-BERT. Cosine similarity is computed between topic vectors and code vectors to identify the Java methods most closely aligned with the concerns in each cluster. We validate our approach on a dataset of 44,683 user reviews spanning 10 open-source Android applications. The results demonstrate the effectiveness of the method in accurately identifying method-level code elements related to user-reported issues, making it a valuable tool for software maintenance and evolution.",
issn="1433-3058",
doi="10.1007/s00521-025-11611-w",
url="https://doi.org/10.1007/s00521-025-11611-w"
}


@Article{Pei2025,
author="Pei, Wenlong
and Huang, Yilin
and Chen, Xiang
and Lu, Guilong
and Liu, Yong
and Ni, Chao",
title="Semi-supervised software vulnerability assessment via code lexical and structural information fusion",
journal="Automated Software Engineering",
year="2025",
month="Jun",
day="03",
volume="32",
number="2",
pages="57",
abstract="In ",
issn="1573-7535",
doi="10.1007/s10515-025-00526-4",
url="https://doi.org/10.1007/s10515-025-00526-4"
}


@Article{Wang2025,
author="Wang, Meng
and Shen, Yangyang
and Zhao, Bingcheng
and Zhou, Xuefeng
and Sun, Lan
and Liu, Xing",
title="Enhancing LLM-based clinical reasoning in anesthesiology via graph-augmented retrieval and explainable generation",
journal="Health Information Science and Systems",
year="2025",
month="Sep",
day="30",
volume="13",
number="1",
pages="62",
abstract="This study aims to enhance the capabilities of large language models (LLMs) in anesthesiology decision support, leveraging a graph-based Retrieval-Augmented Generation (RAG) framework to improve analytical reasoning and deliver evidence-driven results.",
issn="2047-2501",
doi="10.1007/s13755-025-00379-x",
url="https://doi.org/10.1007/s13755-025-00379-x"
}


@Article{Cao2025,
author="Cao, Jialun
and Li, Meiziniu
and Wen, Ming
and Cheung, Shing-Chi",
title="A study on prompt design, advantages and limitations of ChatGPT for deep learning program repair",
journal="Automated Software Engineering",
year="2025",
month="Mar",
day="07",
volume="32",
number="1",
pages="30",
abstract="The emergence of large language models (LLMs) such as ChatGPT has revolutionized many fields. In particular, recent advances in LLMs have triggered various studies examining the use of these models for software development tasks, such as program repair, code understanding, and code generation. Prior studies have shown the capability of ChatGPT in repairing conventional programs. However, debugging deep learning (DL) programs poses unique challenges since the decision logic is not directly encoded in the source code. This requires LLMs to not only parse the source code syntactically but also understand the intention of DL programs. Therefore, ChatGPT's capability in repairing DL programs remains unknown. To fill this gap, our study aims to answer three research questions: (1) Can ChatGPT debug DL programs effectively? (2) How can ChatGPT's repair performance be improved by prompting? (3) In which way can dialogue help facilitate the repair? Our study analyzes the typical information that is useful for prompt design and suggests enhanced prompt templates that are more efficient for repairing DL programs. On top of them, we summarize the dual perspectives (i.e., advantages and disadvantages) of ChatGPT's ability, such as its handling of API misuse and recommendation, and its shortcomings in identifying default parameters. Our findings indicate that ChatGPT has the potential to repair DL programs effectively and that prompt engineering and dialogue can further improve its performance by providing more code intention. We also identified the key intentions that can enhance ChatGPT's program repairing capability.",
issn="1573-7535",
doi="10.1007/s10515-025-00492-x",
url="https://doi.org/10.1007/s10515-025-00492-x"
}


@Article{Alraddadi2025,
author="Alraddadi, Rawan
and Alshayeb, Mohammad",
title="An empirical evaluation of stacked generalization models for binary bug report classification",
journal="Innovations in Systems and Software Engineering",
year="2025",
month="Dec",
day="01",
volume="21",
number="4",
pages="1213--1228",
abstract="Categorizing the reported software bugs into their types is a vital aspect of software development and maintenance. This procedure is initially handled manually by a bug triage. However, the classification approach should be automated to facilitate and improve the process. This research aims to enhance the predictive performance of machine learning models in classifying bug reports. The study proposes a novel framework for integrating chi-square for feature selection with stacked generalization ensemble-based models into the bug report classification process. The study involves an empirical investigation utilizing a set of seven base classifiers and three meta-classifiers (Logistic Regression (LoR), Naive Bayes (NB), and Multilayer Perceptron (MLP)) to construct the stacking ensemble. The models were trained on two open-source Java datasets using the textual data fields for the reported bug. Features were extracted using different variants of N-grams, including uni-grams, bi-grams, and tri-grams. The chi-square feature selection technique was applied to reduce the high dimensionality and select only the informative features. The experimental results were evaluated using the Matthews correlation coefficient and F1 metric and compared with state-of-the-art bug classification methods. The results show that the stacking models' performance is comparatively higher than the standalone classifiers in almost all cases and for both datasets. Increasing the dataset size for all three stacked models improves the chances of achieving higher performance. The analytical comparison among the three stacking models and the statistical results using the Wilcoxon signed-rank test showed that MLP-Stacked and LoR-Stacked ensemble models were the best-performing classifiers among the other models.",
issn="1614-5054",
doi="10.1007/s11334-024-00584-z",
url="https://doi.org/10.1007/s11334-024-00584-z"
}


@Article{Wang2025,
author="Wang, Liping
and Lu, Guilong
and Chen, Xiang
and Dai, Xiaofeng
and Qiu, Jianlin",
title="SIFT: enhance the performance of vulnerability detection by incorporating structural knowledge and multi-task learning",
journal="Automated Software Engineering",
year="2025",
month="Apr",
day="11",
volume="32",
number="2",
pages="38",
abstract="Software vulnerabilities pose significant risks to software systems, leading to security breaches, data loss, operational disruptions, and substantial financial damage. Therefore, accurately detecting these vulnerabilities is of paramount importance. In recent years, pre-trained language models (PLMs) have demonstrated powerful capabilities in code representation and understanding, emerging as a promising method for vulnerability detection. However, integrating code structure knowledge while fine-tuning PLMs remains a significant challenge. To alleviate this limitation, we propose a novel vulnerability detection approach called SIFT. SIFT extracts the code property graph (CPG) to serve as the source of graph structural information. It constructs a code structure matrix from this information and measures the difference between the code structure matrix and the attention matrix using Sinkhorn Divergence to obtain the structural knowledge loss. This structural knowledge loss is then used alongside the cross-entropy loss for vulnerability detection in a multi-task learning framework to enhance overall detection performance. To evaluate the effectiveness of SIFT, we conducted experiments on three vulnerability detection datasets: FFmpeg+Qemu, Chrome+Debian, and Big-Vul. The results demonstrate that SIFT outperforms nine state-of-the-art vulnerability detection baselines, achieving performance improvements of 1.74{\%}, 10.19{\%}, and 2.87{\%} in terms of F1 score, respectively. Our study shows the effectiveness of incorporating structural knowledge and multi-task learning in enhancing the performance of PLMs for vulnerability detection.",
issn="1573-7535",
doi="10.1007/s10515-025-00507-7",
url="https://doi.org/10.1007/s10515-025-00507-7"
}


@inproceedings{10.1007/978-981-96-2376-1_33,
 abstract = {GitHub is one of the most popular collaborative development tools used by many institutions and enterprises. The GitHub issue community serves as the primary tool for project developers to gather program bug information, with GitHub users posting issues based on encountered program bugs. However, a considerable portion of these issues ultimately remain unfixed by developers. We term the issues fixed by developers as `dev-fixed' issues, and conversely, we call them `wontfix' issues. Through manual analysis of 2500 bug-related issues collected from five GitHub projects, we summarize the unresolved issues from four aspects and propose a multi-agent approach based on large language model (LLM) to identify dev-fixed issues. We design the previously summarized four aspects into different agents, independently analyzed by the LLM, and ultimately parallelly provide information to a summary agent for the final determination. Compared to the baseline, our approach shows the best performance in the metric of F1 score, recall rate and precision rate, with the value of 0.852, 0.862 and 0.857 respectively. Ablation experiments demonstrate the critical importance of each agent to our approach.},
 address = {Singapore},
 author = {Han, Zhengru
and Jiang, Bo
and Xue, Weihao
and Dai, Chaoqun
and Huang, Qiao
and Wang, Ye},
 booktitle = {Computer Supported Cooperative Work and Social Computing},
 editor = {Sun, Hailong
and Fan, Hongfei
and Gao, Yongqiang
and Wang, Xiaokang
and Liu, Dongning
and Du, Bowen
and Lu, Tun},
 isbn = {978-981-96-2376-1},
 pages = {459--470},
 publisher = {Springer Nature Singapore},
 title = {A Multi-agent Collaboration Approach for Identifying Developer-Fixed Issues in GitHub Projects},
 url = {https://link.springer.com/chapter/10.1007/978-981-96-2376-1_33},
 year = {2025}
}

@inproceedings{10.1007/978-3-032-07612-0_25,
 abstract = {Current compiler optimization reports often present complex, technical information that is difficult for programmers to interpret and act upon effectively. This paper assesses the capability of large language models (LLM) to understand compiler optimization reports and automatically rewrite the code accordingly.},
 address = {Cham},
 author = {Pirkelbauer, Peter
and Liao, Chunhua},
 booktitle = {High Performance Computing},
 editor = {Neuwirth, Sarah
and Paul, Arnab Kumar
and Weinzierl, Tobias
and Carson, Erin Claire},
 isbn = {978-3-032-07612-0},
 pages = {325--338},
 publisher = {Springer Nature Switzerland},
 title = {CompilerGPT: Leveraging Large Language Models for Analyzing and Acting on Compiler Optimization Reports},
 url = {https://link.springer.com/chapter/10.1007/978-3-032-07612-0_25},
 year = {2026}
}

@inproceedings{10.1007/978-3-032-02782-5_24,
 abstract = {This paper introduces PromptMorph, a design workflow that automates the generation of 3D parametric models using Large Language Models (LLMs) like ChatGPT, Python, and Grasshopper. It bridges natural language and image prompts with computational modeling, enabling architects to create editable parametric models without extensive coding. Central to the method is a novel image-based prompting technique tailored to design disciplines, translating conceptual and visual inputs into parametric geometries.},
 address = {Cham},
 author = {Salta, Styliani Stella
and Katsaros, Miltiadis},
 booktitle = {Formal Methods in Architecture},
 editor = {Marques, Bruno
and Ruivo, Catarina
and Leite Viana, David
and Vieira Vaz, Jorge},
 isbn = {978-3-032-02782-5},
 pages = {430--455},
 publisher = {Springer Nature Switzerland},
 title = {PromptMorph: LLM-Driven Workflow for Text-to-3D Parametric Modeling in Architecture},
 url = {https://link.springer.com/chapter/10.1007/978-3-032-02782-5_24},
 year = {2025}
}

@Article{Ehsani2025,
author="Ehsani, Ramtin
and Pathak, Sakshi
and Parra, Esteban
and Haiduc, Sonia
and Chatterjee, Preetha",
title="What characteristics make ChatGPT effective for software issue resolution? An empirical study of task, project, and conversational signals in GitHub issues",
journal="Empirical Software Engineering",
year="2025",
month="Nov",
day="18",
volume="31",
number="1",
pages="22",
abstract="Conversational large-language models (LLMs), such as ChatGPT, are extensively used for issue resolution tasks, particularly for generating ideas to implement new features or resolve bugs. However, not all developer-LLM conversations are useful for effective issue resolution and it is still unknown what makes some of these conversations not helpful. In this paper, we analyze 686 developer-ChatGPT conversations shared within GitHub issue threads to identify characteristics that make these conversations effective for issue resolution. First, we empirically analyze the conversations and their corresponding issue threads to distinguish helpful from unhelpful conversations. We begin by categorizing the types of tasks developers seek help with (e.g., code generation, bug identification and fixing, test generation), to better understand the scenarios in which ChatGPT is most effective. Next, we examine a wide range of conversational, project, and issue-related metrics to uncover statistically significant factors associated with helpful conversations. Finally, we identify common deficiencies in unhelpful ChatGPT responses to highlight areas that could inform the design of more effective developer-facing tools. We found that only 62{\%} of the ChatGPT conversations were helpful for successful issue resolution. Among different tasks related to issue resolution, ChatGPT was most helpful in assisting with code generation, and tool/library/API recommendations, but struggled with generating code explanations. Our conversational metrics reveal that helpful conversations are shorter, more readable, and exhibit higher semantic and linguistic alignment. Our project metrics reveal that larger, more popular projects and experienced developers benefit more from ChatGPT's assistance. Our issue metrics indicate that ChatGPT is more effective on simpler issues characterized by limited developer activity and faster resolution times. These typically involve well-scoped technical problems such as compilation errors and tool feature requests. In contrast, it performs less effectively on complex issues that demand deep project-specific understanding, such as system-level code debugging and refactoring. The most common deficiencies in unhelpful ChatGPT responses include incorrect information and lack of comprehensiveness. Our findings have wide implications including guiding developers on effective interaction strategies for issue resolution, informing the development of tools or frameworks to support optimal prompt design, and providing insights on fine-tuning LLMs for issue resolution tasks.",
issn="1573-7616",
doi="10.1007/s10664-025-10745-8",
url="https://doi.org/10.1007/s10664-025-10745-8"
}


@Article{Pinna2025,
author="Pinna, Giovanni
and Ravalico, Damiano
and Rovito, Luigi
and Manzoni, Luca
and De Lorenzo, Andrea",
title="Exploring the Effect of Genetic Improvement for Large Language Models-Generated Code",
journal="SN Computer Science",
year="2025",
month="Aug",
day="22",
volume="6",
number="7",
pages="760",
abstract="In recent years, the field of Natural Language Processing (NLP) has made considerable progress with the development of neural network-based models, leading to the creation of various Large Language Models (LLMs). These models have demonstrated strong performance in various NLP tasks, such as language translation, sentiment analysis, and named entity recognition. One notable application of LLMs is their ability to generate code automatically from simple problem descriptions. However, even advanced LLMs frequently generate incorrect code. To address this issue, we extend a recently proposed method that aims to improve the correctness of code generated by LLMs using an evolutionary approach known as Genetic Improvement (GI). Our method involves constructing a dynamic grammar based on the LLM-generated code and using a problem-agnostic fitness function. In our experiments, we evaluated the proposed method on 25 well-known and widely-used problems across four different LLMs, both open-source and proprietary models. We demonstrate that our approach significantly improves the accuracy of code generated by LLMs. Specifically, for problems that the LLM alone does not fully solve, we show that GI significantly improves the initial LLM-generated solution in 50{\%} to 75{\%} of cases across the tested models. Our proposed GI approach remains effective as long as the initial LLM-generated code, despite some errors, provides a solid foundation for constructing a correct program.",
issn="2661-8907",
doi="10.1007/s42979-025-04281-x",
url="https://doi.org/10.1007/s42979-025-04281-x"
}


@Inbook{Tehranipoor2024,
author="Tehranipoor, Mark
and Zamiri Azar, Kimia
and Asadizanjani, Navid
and Rahman, Fahim
and Mardani Kamali, Hadi
and Farahmandi, Farimah",
title="Large Language Models for SoC Security",
bookTitle="Hardware Security: A Look into the Future",
year="2024",
publisher="Springer Nature Switzerland",
address="Cham",
pages="255--299",
abstract="With the increasing prevalence and sophistication of systems-on-chip (SoCs) architectures in electronics, ensuring the security of SoCs has become a complex challenge. Traditional security mechanisms fail to effectively verify contemporary SoC designs, often due to issues of scale, thoroughness, and flexibility. In contrast, Large Language Models (LLMs) have gained acclaim for their exceptional abilities in language comprehension, sophisticated problem-solving, and code generation. This chapter investigates the viability of employing LLMs to bridge the existing deficiencies in SoC security, aiming to develop a verification strategy that is fortified, more scalable, and adaptable. This integration of LLMs into the SoC security verification process marks the dawn of an innovative era, fraught with both opportunities and challenges, in safeguarding complex SoC systems. The chapter provides a detailed examination of the current literature and practical use cases. We discuss the future prospects and hurdles associated with the adoption of LLMs for various SoC security verification processes.",
isbn="978-3-031-58687-3",
doi="10.1007/978-3-031-58687-3_6",
url="https://doi.org/10.1007/978-3-031-58687-3_6"
}


@Inbook{Sekar2026,
author="Sekar, Srinivasan",
title="Server-Side Hardening: Mitigating Common Vulnerabilities",
bookTitle="The MCP Standard: A Developer's Guide to Building Universal AI Tools with the Model Context Protocol ",
year="2026",
publisher="Apress",
address="Berkeley, CA",
pages="175--194",
abstract="It's time to learn to build servers that can withstand these attacks by focusing on the security of our code.",
isbn="979-8-8688-2364-0",
doi="10.1007/979-8-8688-2364-0_11",
url="https://doi.org/10.1007/979-8-8688-2364-0_11"
}


@inproceedings{10.1007/978-3-031-96235-6_31,
 abstract = {The regular patch update of the security vulnerabilities is crucial for an organization to mitigate the possibilities of their potential exploitations for cyber-attacks. Despite their importance, timely updates are not always guaranteed, and many vulnerabilities remain unpatched for extended period my increase the security risks to the organizations. Organizations generally update patches manually, which introduces delays towards mitigation of potential exploitation and requires huge effort and resources. In this context, we propose a novel approach that uses Large Language Model (LLM)-based CodeBERT model to predict the availability of an update or a patch relevant for the vulnerabilities. The approach adopts key trustworthy AI characteristics, including biasness and explainability, to operationalize trustworthy AI practice for the LLM-based CodeBERT model. The work has been evaluated on a real-world use case scenario from Athens International Airport to demonstrate the applicability of the approach through a test environment that emulates the airport{\textasciiacutex}s critical operating systems. Assets from key systems such as flight information display and access control have been considered and linked with vulnerabilities. The results from the study show that the update is predicated for the key vulnerabilities such as CVE-2017--8464 and CVE-2020--1472 which link with Windows 7-based access control system and Oracle-based AODB database server of the use case scenario, respectively. Also, model explainability is improved by the feature importance using SHAP and correlation using Heatmap technique. The key features for the model decision making are exploitability{\_}score, epss, and attack{\_}complexity. Trustworthy AI practice is also operationalized through bias mitigating techniques such as class balancing and equalized odds to ensure fair and balanced training of the model.},
 address = {Cham},
 author = {Basheer, Nihala
and Islam, Shareeful
and Papastergiou, Spyridon
and Mouratidis, Haralambos
and Papagiannopoulos, Nikolaos},
 booktitle = {Artificial Intelligence Applications and Innovations},
 editor = {Maglogiannis, Ilias
and Iliadis, Lazaros
and Andreou, Andreas
and Papaleonidas, Antonios},
 isbn = {978-3-031-96235-6},
 pages = {431--445},
 publisher = {Springer Nature Switzerland},
 title = {Vulnerability Patch Prediction Using LLM Based Bert Model with Trustworthy AI Practice for Cyber Security Enhancement},
 url = {https://link.springer.com/chapter/10.1007/978-3-031-96235-6_31},
 year = {2025}
}

@Article{Tissaoui2026,
author="Tissaoui, Anis",
title="From prompt to persona: a literature review on LLMs as single cognitive agents",
journal="Journal of Ambient Intelligence and Humanized Computing",
year="2026",
month="Jan",
day="01",
volume="17",
number="1",
pages="205--221",
abstract="Recent advancements in large language models (LLMs) have sparked growing interest in their ability to simulate not just linguistic competence, but agent like behavior. Moving beyond chatbots and task based assistants, researchers are now exploring whether LLMs can act as ``single cognitive agents'' entities endowed with memory, persona, planning capabilities, and situatedness in dynamic environments. This article presents the first structured literature review focused specifically on LLMs as single agent simulators. We examine over 70 peer reviewed publications and preprints (2018-2025), organizing them across five core dimensions: memory mechanisms, identity and persona modeling, planning and autonomy, interaction environments, and evaluation protocols. For each dimension, we analyze existing architectures, synthesize taxonomies, and identify recurring limitations. Our findings reveal a fragmented research landscape, where cognitive coherence, temporal memory, and personality persistence remain open challenges. We conclude by proposing a set of key research questions to guide future efforts, including the simulation of theory of mind, self consistent goal formation, and integrated memory persona frameworks. This review lays the groundwork for a unified research agenda aimed at developing cognitively coherent, memory augmented LLM based agents.",
issn="1868-5145",
doi="10.1007/s12652-025-05029-4",
url="https://doi.org/10.1007/s12652-025-05029-4"
}


@inproceedings{10.1007/978-981-95-1581-3_32,
 abstract = {With the increasing complexity of software systems, code vulnerability detection has become a core challenge for ensuring software security. Traditional static analysis, dynamic analysis and hybrid analysis face limitations such as high false alarm rate and insufficient path coverage in detection efficiency and accuracy, making it difficult to cope with the vulnerability detection needs of large-scale complex codes. In recent years, vulnerability detection techniques based on graph convolutional networks show potential, but still have problems such as gradient vanishing, feature over-smoothing, and dependence on high-quality features. Therefore, a code vulnerability detection method is proposed in this paper, which integrates graph convolutional networks, residual connections, and contrastive learning, aiming to improve the detection performance of complex code vulnerabilities. Specifically, the text sequence of the code is first constructed as a graph structure. Then, a large-scale language model is employed to extract both structural and semantic features. Finally, the method is trained on the graph topology to learn node embeddings by leveraging neighborhood and node features, ultimately generating prediction results. Extensive experiments on the ReVeal dataset are conducted to evaluate the effectiveness of the proposed method, and the results indicate that it outperforms other baseline methods in code vulnerability detection.},
 address = {Singapore},
 author = {Tao, Zhi
and Cao, Buqing
and Ye, Hongfan
and Peng, Qian
and Liu, Shanpeng},
 booktitle = {Service Science},
 editor = {Liu, Xuanzhe
and Zhang, Pengcheng
and Ma, Yutao},
 isbn = {978-981-95-1581-3},
 pages = {493--510},
 publisher = {Springer Nature Singapore},
 title = {GRACE: Graph Convolutional Networks with Residual Connections and Contrastive Embedding Learning for Code Vulnerability Detection},
 url = {https://link.springer.com/chapter/10.1007/978-981-95-1581-3_32},
 year = {2026}
}

@inproceedings{10.1007/978-3-032-00972-2_27,
 abstract = {Fault localization aims to automatically localize buggy files, a key step in debugging tasks. Traditional Information-Retrieval-based fault localization (IRFL) methods often struggle due to the lexical gap between bug reports and source code. Inspired by the ability of Large Language Models (LLMs) to process both natural language and programming language, we propose a hybrid fault localization approach that integrates LLM-driven information extraction, semantic search, and relevance-matching techniques to improve fault localization accuracy. Our method utilizes LLMs to extract key information from bug reports, including keywords (variable names, function names, class names), error message verbatims, and technical descriptions. With the extracted keywords, we compute lexical similarity scores between bug reports and source code files and then rank the source code files accordingly. We also utilize text embedding models to encode the extracted bug reports and source code files and compute their semantic similarity to construct a ranked list of suspected buggy files. This semantic ranking is combined with lexical ranking based on the best-rank selection strategy to achieve the final list. Our approach is evaluated on six real-world Java projects from the Bench4BL dataset. Experimental results demonstrate that our approach outperforms the baseline methods by a substantial margin in terms of Top-K, Mean Reciprocal Rank (MRR), and Mean Average Precision (MAP) metrics.},
 address = {Cham},
 author = {Nguyen, Thanh Binh
and Cao, Thi Nham
and Nguyen, Van Tien
and Nguyen, Nhut Tien},
 booktitle = {The 14th Conference on Information Technology and its Applications},
 editor = {Nguyen, Ngoc Thanh
and Huynh, Cong-Phap
and Nguyen, Thanh Thuy
and Le-Khac, Nhien-An
and Seng, Sopheap
and Nguyen, Quang-Vu},
 isbn = {978-3-032-00972-2},
 pages = {359--371},
 publisher = {Springer Nature Switzerland},
 title = {A Hybrid Approach to Fault Localization: Integrating LLMs with IR-Based Methods},
 url = {https://link.springer.com/chapter/10.1007/978-3-032-00972-2_27},
 year = {2026}
}

@Inbook{Dakhel2024,
author="Dakhel, Arghavan Moradi
and Nikanjam, Amin
and Khomh, Foutse
and Desmarais, Michel C.
and Washizaki, Hironori",
editor="Nguyen-Duc, Anh
and Abrahamsson, Pekka
and Khomh, Foutse",
title="An Overview on Large Language Models",
bookTitle="Generative AI for Effective Software Development",
year="2024",
publisher="Springer Nature Switzerland",
address="Cham",
pages="3--21",
abstract="Generative artificial intelligence (AI), propelled by the advancements in large language models (LLMs), has exhibited remarkable capabilities in various software engineering (SE) tasks and beyond. This development has influenced the research studies in this domain. This chapter offers an overview of LLMs, delving into relevant background concepts while exploring advanced techniques at the forefront of LLM research. We review various LLM architectures, in addition to discussing the concepts of training, fine-tuning, and in-context learning. We also discussed different adaptation approaches to LLMs and augmented LLMs. Furthermore, we delve into the evaluation of LLM research, introducing benchmark datasets and relevant tools in this context. The chapter concludes by exploring limitations in leveraging LLMs for SE tasks.",
isbn="978-3-031-55642-5",
doi="10.1007/978-3-031-55642-5_1",
url="https://doi.org/10.1007/978-3-031-55642-5_1"
}


@Article{Quan2025,
author="Quan, Vu Le Anh
and Phat, Chau Thuan
and Van Nguyen, Kiet
and The Duy, Phan
and Pham, Van-Hau",
title="XGV-BERT: Leveraging contextualized language model and graph neural network for efficient software vulnerability detection",
journal="The Journal of Supercomputing",
year="2025",
month="Apr",
day="16",
volume="81",
number="6",
pages="750",
abstract="With the advancement of deep learning in various fields, there are many attempts to reveal software vulnerabilities by data-driven approach. Natural language processing has emerged as a powerful tool for bridging the semantic gap between programming languages and natural language. However, a significant disparity between the two still exists. In this work, we propose XGV-BERT, a framework that combines the pre-trained CodeBERT model and graph neural network to detect software vulnerabilities. By jointly training the CodeBERT and graph neural network modules within XGV-BERT, the proposed model leverages the advantages of large-scale pre-training, harnessing vast raw data, and transfer learning by learning representations for training data through graph convolution. The research results demonstrate that the XGV-BERT method significantly improves vulnerability detection accuracy compared to two existing methods such as VulDeePecker and SySeVR. For the VulDeePecker dataset, XGV-BERT achieves an impressive F1-score of 97.5{\%}, significantly outperforming VulDeePecker, which achieved an F1-score of 78.3{\%}. Again, with the SySeVR dataset, XGV-BERT achieves an F1-score of 95.5{\%}, surpassing the results of SySeVR with an F1-score of 83.5{\%}.",
issn="1573-0484",
doi="10.1007/s11227-025-07198-7",
url="https://doi.org/10.1007/s11227-025-07198-7"
}


@inproceedings{10.1007/978-3-031-82362-6_11,
 abstract = {The increasing volume of publicly disclosed vulnerabilities presents a significant challenge for organizations striving to secure their information systems and data. Traditional vulnerability scanners, reliant on manually coded vulnerability tests, struggle to keep pace with the growing number of vulnerabilities, resulting in delays and inefficiencies. In this work, we propose a novel architecture that leverages Artificial Intelligence (AI) to create modular and scalable vulnerability scanners. Our architecture decouples vulnerability tests from the vulnerability database (VDB), enabling the use of well-known fingerprinting tools and an AI-driven VDB that is regularly updated from Common Vulnerabilities and Exposures records. We evaluate the feasibility and effectiveness of our approach through a series of experiments. Using both heuristic and GPT-based methods, we assess the performance of our approach to automatically create the VDB and to identify known vulnerabilities in arbitrary software using it. The GPT-based methods demonstrate superior accuracy, achieving a perfect precision, recall, and F1 score creating the VDB, albeit with increased execution time compared to heuristic methods. On the vulnerability identification task, the GPT-based approach also shows significant improvement in accuracy over heuristic methods. Our findings indicate that AI models, particularly large language models, can significantly enhance vulnerability scanners to keep up with the latest vulnerabilities. Despite the higher computational costs, the improved accuracy and reduced false positives and false negatives make AI-driven approaches a promising direction for future research and development in cybersecurity.},
 address = {Cham},
 author = {Vargas-Rivera, Andr{\'e}s
and Esquivel-Vargas, Herson},
 booktitle = {Computer Security. ESORICS 2024 International Workshops},
 editor = {Garcia-Alfaro, Joaquin
and Kalutarage, Harsha
and Yanai, Naoto
and Kozik, Rafa{\l}
and Ksieniewicz, Pawe{\l}
and Wo{\'{z}}niak, Micha{\l}
and Abie, Habtamu
and Ranise, Silvio
and Verderame, Luca
and Cambiaso, Enrico
and Ugarelli, Rita
and Pra{\c{c}}a, Isabel
and Katt, Basel
and Pirbhulal, Sandeep
and Shukla, Ankur
and Pawlicki, Marek
and Chora{\'{s}}, Micha{\l}},
 isbn = {978-3-031-82362-6},
 pages = {171--192},
 publisher = {Springer Nature Switzerland},
 title = {Towards AI-Based Identification of Publicly Known Vulnerabilities},
 url = {https://link.springer.com/chapter/10.1007/978-3-031-82362-6_11},
 year = {2025}
}

@Article{Yaacoub2025,
author="Yaacoub, Jean Paul A.
and Noura, Hassan N.
and Salman, Ola
and Pujolle, Guy",
title="Large language models: applications, limitations, challenges, and recommendations in cybersecurity, digital forensics, and ethical hacking",
journal="Annals of Telecommunications",
year="2025",
month="Nov",
day="01",
volume="80",
number="11",
pages="933--973",
abstract="Large Language Models (LLMs) are a significant leap in Artificial Intelligence (AI), providing tremendous capabilities for understanding and creating human-like language. LLMs offer significant advantages in automating and enhancing cybersecurity practices, but their deployment in sensitive domains like cybersecurity, ethical hacking, and digital forensics is a challenge. LLMs improve cybersecurity, digital forensics, and ethical hacking by automating processes such as reconnaissance and advanced attack simulation to confront new digital threats. This study examines LLMs' strengths and weaknesses and proposes mitigation strategies, such as encryption, adversarial training, and strict data governance, to ensure their safe and effective integration in high-stakes situations. The results highlight the necessity of ongoing improvement, supervision, and ethical concerns to fully utilise LLM while reducing dangers and guaranteeing its appropriate deployment.",
issn="1958-9395",
doi="10.1007/s12243-025-01134-9",
url="https://doi.org/10.1007/s12243-025-01134-9"
}


@Article{Wang2025,
author="Wang, Jiyu
and Chen, Xiang
and Pei, Wenlong
and Yang, Shaoyu",
title="Improving prompt tuning-based software vulnerability assessment by fusing source code and vulnerability description",
journal="Automated Software Engineering",
year="2025",
month="May",
day="03",
volume="32",
number="2",
pages="45",
abstract="To effectively allocate resources for vulnerability remediation, it is crucial to prioritize vulnerability fixes based on vulnerability severity. With the increasingnumber of vulnerabilities in recent years, there is an urgent need for automated methods for software vulnerability assessment (SVA). Most of the previous SVA studies mainly rely on traditional machine learning methods. Recently, fine-tuning pre-trained language models has emerged as an intuitive method for improving performance. However, there is a gap between pre-training and fine-tuning, and their performance heavily depends on the dataset's quality of the downstream task. Therefore, we propose a prompt tuning-based method PT-SVA. Different from the fine-tuning paradigm, the prompt-tuning paradigm involves adding prompts to make the training process similar to pre-training, thereby better adapting to downstream tasks. Moreover, previous research aimed to automatically predict severity by only analyzing either the vulnerability descriptions or the source code of the vulnerability. Therefore, we further consider both types of vulnerability information for designing hybrid prompts (i.e., a combination of hard and soft prompts). To evaluate PT-SVA, we construct the SVA dataset based on the CVSS V3 standard, while previous SVA studies only consider the CVSS V2 standard. Experimental results show that PT-SVA outperforms ten state-of-the-art SVA baselines, such as by 13.7{\%} to 42.1{\%} in terms of MCC. Finally, our ablation experiments confirm the effectiveness of PT-SVA's design, specifically in replacing fine-tuning with prompt tuning, incorporating both types of vulnerability information, and adopting hybrid prompts. Our promising results indicate that prompt tuning-based SVA is a promising direction and needs more follow-up studies.",
issn="1573-7535",
doi="10.1007/s10515-025-00525-5",
url="https://doi.org/10.1007/s10515-025-00525-5"
}


@inproceedings{10.1007/978-981-99-6529-8_26,
 abstract = {This article analyzes the usage in the programming of ChatGPT, a natural language processing tool, along with how it may enhance teamwork, communication, and code quality. ChatGPT is an effective tool for developers since it can produce code snippets, templates, and functions based on natural language input. The team's capacity to grasp natural language input can assist in closing the communication and cooperation gap between technical and non-technical team members. Furthermore, ChatGPT can assist developers in finding and fixing bugs or errors in their code more quickly and efficiently by increasing the accuracy of automated code review and testing. However, there are potential disadvantages to take into account as well, such as the danger of relying too heavily on automated tools, restrictions on ChatGPT's capacity to comprehend intricate technical concepts, and worries regarding bias in the training data used to create the tool. ChatGPT has the ability to change programming as a whole by making it more approachable, effective, and user-friendly. To ensure its usefulness and acceptance in the programming community, it will be necessary to carefully analyze and solve any potential constraints and difficulties.},
 address = {Singapore},
 author = {Pantelimon, Florin Valeriu
and Posedaru, Bogdan Ștefan},
 booktitle = {Proceedings of 22nd International Conference on Informatics in Economy (IE 2023)},
 editor = {Ciurea, Cristian
and Pocatilu, Paul
and Filip, Florin Gheorghe},
 isbn = {978-981-99-6529-8},
 pages = {307--316},
 publisher = {Springer Nature Singapore},
 title = {Improving Programming Activities Using ChatGPT: A Practical Approach},
 url = {https://link.springer.com/chapter/10.1007/978-981-99-6529-8_26},
 year = {2024}
}

@inproceedings{10.1007/978-3-031-96093-2_12,
 abstract = {Fuzz test is known as a dynamic testing method, which is reasonably effective at detecting security vulnerabilities and abnormal conditions by providing irregular inputs to the program and observing its response. The efficiency and application scope of fuzz test heavily depends on the quality of the fuzz driver that constructs appropriate inputs for the target function. However, it requires an enormous amount of time and professional knowledge to generate a high-quality fuzz driver. For that reason, numerous studies about automation of fuzz driver generation using LLM are being conducted recently. In this study, the performance difference between prompt engineering and fine-tuning is evaluated by the fuzz driver creation method using LLM. To do so, two types of data set were built based on prompt data used in OSS-Fuzz-gen and a fuzz driver source code collected from the project Introspector, and fine-tuning on GPT-3.5 Turbo Model was conducted. Performance evaluation was carried out based on line, function, and region coverage, as well as the success of target function invocations. As a result of this evaluation, the fine-tuning based model demonstrated overall superior performance compared to prompt-based model.},
 address = {Cham},
 author = {Kim, Sanggu
and Lee, Sun-young},
 booktitle = {Innovative Mobile and Internet Services in Ubiquitous Computing},
 editor = {Barolli, Leonard
and Chen, Hsing-Chung
and Yim, Kangbin},
 isbn = {978-3-031-96093-2},
 pages = {111--120},
 publisher = {Springer Nature Switzerland},
 title = {Performance Comparison of Prompt Engineering and Fine-Tuning Approaches for Fuzz Driver Generation Using Large Language Models},
 url = {https://link.springer.com/chapter/10.1007/978-3-031-96093-2_12},
 year = {2025}
}

@inproceedings{10.1007/978-3-032-08649-5_13,
 abstract = {Achieving high-quality software is a primary goal in software engineering, which requires rigorous validation and verification processes throughout testing activities. Although manual testing can be effective, it is often time-consuming and resource-intensive, leading to an increased demand for automated solutions. Recent advances in large-language models (LLMs) have significantly influenced various domains within software engineering, including requirements analysis, test automation, and debugging. This article explores an agent-oriented frame-work for automated software testing that leverages the capabilities of LLMs to minimize human intervention and improve testing efficiency. The proposed approach integrates LLMs to automate the generation of unit tests, visualization of call graphs, and execution and reporting of tests. The framework is evaluated using multiple applications developed in Python and Java, demonstrating high test coverage and operational efficiency. The findings of the article emphasize the potential of LLM-powered agents to streamline software testing workflows and address critical challenges related to scalability and accuracy.},
 address = {Cham},
 author = {Sherifi, Betim
and Slhoub, Khaled
and Nembhard, Fitzroy},
 booktitle = {Software and Data Engineering},
 editor = {Rahimi, Nick
and Margapuri, Venkat
and Golilarz, Noor Amiri},
 isbn = {978-3-032-08649-5},
 pages = {199--211},
 publisher = {Springer Nature Switzerland},
 title = {The Potential of Large Language Models in Automating Software Testing: From Generation to Reporting},
 url = {https://link.springer.com/chapter/10.1007/978-3-032-08649-5_13},
 year = {2026}
}

@Article{Wang2025,
author="Wang, Jiayi
and Yu, Ping
and Qin, Yi
and Jiang, Yanyan
and Yao, Yuan
and Ma, Xiaoxing",
title="NexuSym: Marrying symbolic path finders with large language models",
journal="Automated Software Engineering",
year="2025",
month="Jun",
day="07",
volume="32",
number="2",
pages="59",
abstract="Symbolic execution is a powerful technique for automated test case generation, ensuring comprehensive coverage of potential scenarios. However, it often struggles with complex, deep paths due to path explosion. Conversely, large language models (LLMs) utilize vast training data to generate test cases that can uncover intricate program behaviors that symbolic execution might miss. Despite their complementary strengths, integrating the systematic nature of symbolic execution with the creative capabilities of LLMs presents a significant challenge. We introduce NexuSym, an innovative tool that integrates symbolic execution with LLMs to facilitate the automatic generation of test cases. To effectively bridge the gap between these two approaches, we have developed a test case reducer, which normalizes the LLM-generated test cases to make them compatible with symbolic execution. Additionally, we propose a search space summarizer, which abstracts and condenses the search space explored by symbolic execution, enabling the LLM to focus on the most promising areas for further exploration. We instantiated NexuSym on KLEE and ChatGPT. Our evaluation of NexuSym involved 99 coreutils programs and 9 large GNU programs. The experimental results demonstrate that NexuSym significantly enhances program test coverage, with improvements of up to 20{\%} in certain cases. Furthermore, we conducted an analysis of the monetary costs associated with using the LLM API, revealing that NexuSym is a highly cost-effective solution.",
issn="1573-7535",
doi="10.1007/s10515-025-00529-1",
url="https://doi.org/10.1007/s10515-025-00529-1"
}


@Article{Salzano2025,
author="Salzano, Francesco
and Marchesi, Lodovica
and Antenucci, Cosmo Kevin
and Scalabrino, Simone
and Tonelli, Roberto
and Oliveto, Rocco
and Pareschi, Remo",
title="Bridging the gap: a comparative study of academic and developer approaches to smart contract vulnerabilities",
journal="Empirical Software Engineering",
year="2025",
month="Dec",
day="05",
volume="31",
number="2",
pages="37",
abstract="In this paper, we investigate the strategies adopted by Solidity developers to fix security vulnerabilities in smart contracts. Vulnerabilities are categorized using the DASP TOP 10 taxonomy, and fixing strategies are extracted from 364 commits collected from open-source Solidity projects on GitHub. Each commit was selected through a two-phase process: an initial filter using natural language processing techniques, followed by manual validation. We assessed whether these fixes adhere to established academic guidelines. Our analysis shows that 60.55{\%} of the commits aligned with at least one literature-based recommendation, particularly for well-documented vulnerability types such as Reentrancy and Arithmetic. However, adherence dropped significantly for categories like Denial of Service, Time Manipulation, and Bad Randomness, highlighting gaps between academic best practices and real-world developer behavior. From the remaining 143 non-aligned commits, we identified 27 novel fixing strategies not previously discussed in the literature. To evaluate their quality, we conducted a structured questionnaire involving 9 experts from both academia and industry. Their feedback indicated high perceived effectiveness of the new fixes, especially for vulnerabilities like Reentrancy and Unchecked Return Values. Generalizability received more varied responses, suggesting context-specific applicability. Finally, we performed a post-fix evolution analysis on over 6700 subsequent commits to assess the long-term stability of the fixes. Most patches remained unchanged, confirming their persistence in production code. Our findings offer practical insights into how vulnerabilities are fixed in smart contracts today, reveal promising emerging patterns, and help bridge the gap between academic guidelines and developer practices.",
issn="1573-7616",
doi="10.1007/s10664-025-10780-5",
url="https://doi.org/10.1007/s10664-025-10780-5"
}


@Inbook{Kaiser2024,
author="Kaiser, Abhinav Krishna
and Meda, Vamshi",
title="AI Infusion in Software Build and Development",
bookTitle="AI Integration in Software Development and Operations: Transformation Through AI Infusion in DevOps, Testing, and SRE",
year="2024",
publisher="Apress",
address="Berkeley, CA",
pages="137--179",
abstract="The advent of AI has heralded a new era in application development, fundamentally changing how software is conceived, built, tested, and maintained. Today, AI is not merely an auxiliary tool; it is a cornerstone of modern software engineering practices, enhancing every phase of the development lifecycle. As organizations strive for agility, efficiency, and innovation, AI emerges as a powerful ally, enabling developers to build robust, scalable, and intelligent applications at an unprecedented pace.",
isbn="979-8-8688-1044-2",
doi="10.1007/979-8-8688-1044-2_6",
url="https://doi.org/10.1007/979-8-8688-1044-2_6"
}


@Article{Tao2025,
author="Tao, Yi
and Dai, Jie
and Ma, Lingna
and Ren, Zhenhui
and Wang, Fei",
title="Structural contrastive learning based automatic bug triaging",
journal="Automated Software Engineering",
year="2025",
month="May",
day="16",
volume="32",
number="2",
pages="51",
abstract="Bug triaging is crucial for software maintenance, as it matches developers with bug reports they are most qualified to handle. This task has gained importance with the growth of the open-source community. Traditionally, methods have emphasized semantic classification of bug reports, but recent approaches focus on the associations between bugs and developers. Leveraging latent patterns from bug-fixing records can enhance triaging predictions; however, the limited availability of these records presents a significant challenge. This scarcity highlights a broader issue in supervised learning: the inadequacy of labeled data and the underutilization of unlabeled data. To address these limitations, we propose a novel framework named SCL-BT (Structural Contrastive Learning-based Bug Triaging). This framework improves the utilization of labeled heterogeneous associations through edge perturbation and leverages unlabeled homogeneous associations via hypergraph sampling. These processes are integrated with a graph convolutional network backbone to enhance the prediction of associations and, consequently, bug triaging accuracy. Experimental results demonstrate that SCL-BT significantly outperforms existing models on public datasets. Specifically, on the Google Chromium dataset, SCL-BT surpasses the GRCNN method by 18.64{\$}{\$}{\backslash}{\%}{\$}{\$}in terms of the Top-9 Hit Ratio metric. The innovative approach of SCL-BT offers valuable insights for the research of automatic bug-triaging.",
issn="1573-7535",
doi="10.1007/s10515-025-00517-5",
url="https://doi.org/10.1007/s10515-025-00517-5"
}


@Article{Cai2025,
author="Cai, Biyu
and Zou, Weiqin
and Meng, Qianshuang
and Xu, Hui
and Zhang, Jingxuan",
title="KBL: a golden keywords-based query reformulation approach for bug localization",
journal="Empirical Software Engineering",
year="2025",
month="Jul",
day="05",
volume="30",
number="5",
pages="135",
abstract="Reformulating initial bug reports to obtain better queries for buggy code retrieval is an important research direction in the bug localization area. Existing query reformulation strategies of bug reports are generally unsupervised and may lack localization guidance, which prevents the generation of better queries for bug localization. Towards this, we propose to develop KBL, a golden keywords-based query reformulation approach for bug localization. Specifically, we first leverage the genetic algorithm and keywords refinement heuristic rules to build a golden keywords benchmark targeted at bug localization. Taking this benchmark as bug localization guidance, we create a keywords classifier for bug reports based on three categories of semantic features. The extracted keywords by the classifier for a bug report are taken as the reformulated start point upon which noise removal and shared keyword expansion with historical bug reports are further performed. The final achieved query, as a replacement for the original bug report, is expected to enhance buggy code retrieval performance. Our experiments show that the contributed keywords benchmark is of high quality in locating bugs, establishing a good basis for further query reformulation to improve localization techniques. Through an analysis of different classifier choices, data balancing strategies, and feature importance, we validate the suitability of the configuration settings for our keyword classifier. A testing dataset of 4,484 bug reports from six projects is used to evaluate our KBL. The results show that KBL is found to substantially outperform both the typical (with a relatively 8{\%}-85{\%} higher Acc@10, 9{\%}-93{\%} higher MAP, and 10{\%}-94{\%} higher MRR), and state-of-the-art (with a relatively 21{\%}-45{\%} higher Acc@10, 31{\%}-47{\%} higher MAP and 32{\%}-50{\%} higher MRR) reformulation strategies. Moreover, based on the reformulated queries of our KBL, the performance of seven representative information retrieval-based bug localization techniques also showed recognizable improvements, including relative increases of 8{\%}-36{\%} in Acc@1, 6{\%}-32{\%} in Acc@5, 4{\%}-24{\%} in Acc@10, 4{\%}-21{\%} in Acc@20, 10{\%}-33{\%} in MAP, and 8{\%}-25{\%} in MRR.",
issn="1573-7616",
doi="10.1007/s10664-025-10694-2",
url="https://doi.org/10.1007/s10664-025-10694-2"
}


@inproceedings{10.1007/978-3-032-00644-8_8,
 abstract = {Vulnerability Detection (VD) in source code is a critical task for ensuring the security of software systems, particularly in C/C++ languages, which are extensively adopted in safety-critical applications. The recent widespread adoption of Large Language Models (LLMs) for software engineering tasks has led to specialized open-source Code-LLMs, tailored to handle programming languages and code-specific challenges. Although these models have achieved promising results for VD through prompt engineering and fine-tuning strategies, existing studies often evaluate them in unrealistic settings, where test data comes from a similar distribution of training data. In this work, we present a comprehensive evaluation of open-source Code-LLMs for VD in C/C++ code, employing both prompt engineering and fine-tuning approaches. We introduce a novel benchmark dataset composed exclusively of functions extracted from real-world, production-level open-source projects, with the aim to conduct a more realistic analysis. Our results highlight the limitations of current Code-LLMs for VD when evaluated under a realistic setup, emphasizing the need for more robust and generalizable solutions for secure software development.},
 address = {Cham},
 author = {Carletti, Vincenzo
and Foggia, Pasquale
and Mazzocca, Carlo
and Parrella, Giuseppe
and Vento, Mario},
 booktitle = {Availability, Reliability and Security},
 editor = {Skopik, Florian
and Naessens, Vincent
and De Sutter, Bjorn},
 isbn = {978-3-032-00644-8},
 pages = {135--152},
 publisher = {Springer Nature Switzerland},
 title = {Evaluating Large Language Models for Vulnerability Detection Under Realistic Conditions},
 url = {https://link.springer.com/chapter/10.1007/978-3-032-00644-8_8},
 year = {2025}
}

@inproceedings{10.1007/978-981-96-5881-7_14,
 abstract = {Large Language Models (LLMs) have significantly advanced in replicating human-like conversational abilities. However, their potential to emulate intricate interpersonal dynamics, particularly the psychologically nuanced `games' described in Eric Berne's Transactional Analysis, remains underexplored. This study investigates the capacity of LLM-based agents to simulate such interactions, focusing on the dialogue-driven game ``Why Don't You - Yes But'', characterized by specific transactional structures and predictable communication patterns. By designing a scenario to evoke this interactions and fine-tuning agent behaviors based on personality traits, we allow the agents to produce dialogue largely independently, without explicit instructions on how to react. The analysis examines how closely the simulated dialogues align with Berne's game structures. It highlights both successes and limitations, including challenges in maintaining consistent emotional depth and personality traits. These findings provide valuable insights into the development of advanced LLM-based agents capable of engaging in complex, psychologically-informed interactions. They also offer a foundation for future research to improve behavioral fidelity and realism.},
 address = {Singapore},
 author = {Zamojska, Monika
and Chudziak, Jaros{\l}aw A.},
 booktitle = {Recent Challenges in Intelligent Information and Database Systems},
 editor = {Nguyen, Ngoc Thanh
and Matsuo, Tokuro
and Gaol, Ford Lumban
and Manolopoulos, Yannis
and Fujita, Hamido
and Hong, Tzung-Pei
and Wojtkiewicz, Krystian},
 isbn = {978-981-96-5881-7},
 pages = {173--187},
 publisher = {Springer Nature Singapore},
 title = {Simulating Human Communication Games: Transactional Analysis in LLM Agent Interactions},
 url = {https://link.springer.com/chapter/10.1007/978-981-96-5881-7_14},
 year = {2025}
}

@Article{Wang2026,
author="Wang, Yifan
and Hou, Yanzhi
and Wu, Bin",
title="Program-plate: a method for identifying the ability to extract vulnerability features",
journal="Cybersecurity",
year="2026",
month="Jan",
day="14",
volume="9",
number="1",
pages="13",
abstract="Although there are numerous advanced and well-established models and methods available for code auditing tasks, their interpretability remains a significant challenge. For machine learning models designed to address code auditing problems, we often know that they can identify vulnerable code but lack insight into their decision-making criteria or whether they have effectively captured the characteristics of vulnerable code. To evaluate the capability of such models in extracting vulnerability-related features, this paper proposes a method called Program-PLATE. By extending a single vulnerable file into a PLATE-dataset, this method enables a more objective assessment of the model's performance on the PLATE-dataset. We applied this method to evaluate multiple models, conducted an in-depth analysis based on the results, and provided suggestions and expectations for future research directions.",
issn="2523-3246",
doi="10.1186/s42400-025-00404-2",
url="https://doi.org/10.1186/s42400-025-00404-2"
}


@inproceedings{10.1007/978-3-032-12827-0_30,
 abstract = {This paper analyzes the efficacy of fine-tuning pre-trained code models for predicting software bug severity, using only the bug report title and description, which is readily available to the bug reporter at submission. We explore the use of various text and code generation language models for this classification task, utilizing the Apache{\'{s}} Impala project bug dataset, focusing on resolved bug reports for accurate severity classifications. GraphCodeBERT demonstrates superior performance (F1: 49.9{\%}) compared to other models studied, such as CodeBERT, CodeT5, and CodeGPT2 (F1 scores ranging from 36.4{\%} to 46.5{\%}), and traditional machine learning methods like Random Forest and XGBoost. Additionally, the Parameter-Efficient Fine-Tuning (PEFT) technique, Low-Rank Adaptation (LoRA), is applied which optimizes computational efficiency. Experiments reveal that GraphCodeBERT, even with LoRA (F1: 45.5{\%}), maintains robust accuracy comparable to its non-LoRA performance.},
 address = {Cham},
 author = {Agarwal, Anurag
and Madhavan, Manu},
 booktitle = {Data Science and Applications},
 editor = {Nanda, Satyasai Jagannath
and Yadav, Rajendra Prasad
and Prasad, Mukesh
and Saraswat, Mukesh},
 isbn = {978-3-032-12827-0},
 pages = {387--398},
 publisher = {Springer Nature Switzerland},
 title = {Fine-Tuning Code Models for Bug Severity Prediction: A Comparative Study with GraphCodeBERT and LoRA},
 url = {https://link.springer.com/chapter/10.1007/978-3-032-12827-0_30},
 year = {2026}
}

@inproceedings{10.1007/978-981-95-6419-4_11,
 abstract = {Multimodal learning provides an effective approach for Android malware detection by integrating heterogeneous data sources such as permissions, intents, and binary content. However, existing fusion algorithms remain simplistic, lacking the capacity to model complex cross-modal relationships. This paper proposes a Large Language Model (LLM)-based feature fusion framework for Android malware classification. The framework combines tabular features (permissions and intents) and visual representations derived from DEX bytecode using a dual-stream architecture with Deep Neural Network (DNN) and Convolutional Neural Network (CNN) backbones. A fusion head inspired by LLMs is employed to capture rich interactions across modalities. Experimental results on the CICMalDroid 2020 dataset show that LLM-based fusion using TinyLLaMA achieves 96.81{\%} accuracy in binary classification and 92.01{\%} in multi-class classification. Under adversarial conditions, the same model reaches 98.24{\%} and 55.74{\%} respectively, outperforming conventional strategies such as self-attention (38.29{\%}, 24.86{\%}) and concatenation (45.10{\%}, 25.87{\%}). Similarly, in obfuscation scenarios, LLM-based fusion (TinyLLaMA and CodeBERT) maintains robust performance, achieving up to 96.66{\%} binary accuracy and 92.83{\%} multi-class accuracy. These results demonstrate improved resilience of the proposed approach against obfuscation and adversarial manipulation.},
 address = {Singapore},
 author = {Minh, Luong Hoang
and Dat, Duong The
and Dang, Bui Tan Hai
and Thinh, Le Duc
and Trung, Doan Minh
and Duy, Phan The},
 booktitle = {Network and System Security},
 editor = {Chen, Jiageng
and He, Debiao
and Xie, Wei},
 isbn = {978-981-95-6419-4},
 pages = {176--195},
 publisher = {Springer Nature Singapore},
 title = {Multimodal Learning with LLM-Fusion Head for Android Malware Detection: Enhancing Cross-Modality Robustness Against Obfuscation and Adversarial Samples},
 url = {https://link.springer.com/chapter/10.1007/978-981-95-6419-4_11},
 year = {2026}
}

@inproceedings{10.1007/978-981-95-5012-8_4,
 abstract = {Large Language Models (LLMs) increasingly depend on external tools and services for complex task execution, creating unprecedented demand for high-quality, reliable tool ecosystems. The Model Context Protocol (MCP) provides a standardized framework for LLM-tool integration, but its growth is severely constrained by prohibitive development costs and pervasive quality deficits in available services. We introduce MCPybarra, a novel multi-agent framework that automates the generation of high-quality MCP services from natural language requirements. Our approach employs three specialized agents---Code Generator, Quality Assurance Inspector, and Code Refiner---operating within a stateful workflow that implements continuous quality-driven iterative refinement. A comprehensive five-dimensional evaluation system provides objective quality assessment and guides automated improvement. Evaluation across 25 diverse MCP services demonstrates that MCPybarra-generated services surpass human-written counterparts in 72{\%} of tasks, with particularly strong performance in non-functional dimensions such as Security and Robustness. The framework achieves this quality improvement at costs of {\$}0.018-0.14 per service. Our work contributes both a novel quality-gated multi-agent paradigm for automated software engineering and a practical solution for scaling high-quality tool ecosystems essential for advanced LLM applications. The code, demo and technical supplementary materials are available at https://github.com/poutonwu/MCPybarra},
 address = {Singapore},
 author = {Peng, Bocheng
and Liu, Mingyi
and Liu, Yanguang
and Tian, Congcong
and Yu, Shuang
and Wang, Zhongjie},
 booktitle = {Service-Oriented Computing},
 editor = {Aiello, Marco
and Deng, Shuiguang
and Murillo, Juan-Manuel
and Georgievski, Ilche
and Benatallah, Boualem
and Wang, Zhongjie},
 isbn = {978-981-95-5012-8},
 pages = {51--65},
 publisher = {Springer Nature Singapore},
 title = {MCPybarra: A Multi-agent Framework for Low-Cost, High-Quality MCP Service Generation},
 url = {https://link.springer.com/chapter/10.1007/978-981-95-5012-8_4},
 year = {2026}
}

@inproceedings{10.1007/978-3-031-91340-2_12,
 abstract = {This paper examines the application of generative AI in automating pull request (PR) reviews, focusing on large-scale pre-trained models . The study highlights the potential of AI-driven tools to generate comprehensive PR descriptions, identify code issues, and enhance developer productivity . Early results indicate that these tools, such as CodeEvaluator, can streamline the review process, reducing the time spent on routine tasks and improving the consistency of code quality checks . Despite the benefits, challenges such as the accuracy of AI-generated content and the necessity for human oversight are discussed, emphasizing the need for balanced integration of AI in software development workflows.},
 address = {Cham},
 author = {Dutta, Sourav
and Sharma, Anjali
and Rajgor, Jaymin},
 booktitle = {Artificial Intelligence and Speech Technology},
 editor = {Sharma, Arun
and Rani, Ritu},
 isbn = {978-3-031-91340-2},
 pages = {147--156},
 publisher = {Springer Nature Switzerland},
 title = {Turbocharging Pull Request Reviews: Exploring Generative AI for Code Review},
 url = {https://link.springer.com/chapter/10.1007/978-3-031-91340-2_12},
 year = {2025}
}

@Article{Vavekanand2026,
author="Vavekanand, Raja
and Ali Laghari, Asif
and Kumar, Teerath",
title="Applications and limitations of large language models to integrate medical context: a comprehensive review",
journal="Iran Journal of Computer Science",
year="2026",
month="Jan",
day="03",
volume="9",
number="1",
pages="12",
abstract="Large Language Models (LLMs) hold transformative potential for healthcare, offering capabilities, such as automated diagnosis, clinical documentation, and patient education. However, their integration into medical practice remains challenging due to critical limitations, including error propensity, bias, and privacy risks. This review provides the first systematic analysis of LLMs across diverse medical applications from electronic health record management to drug discovery while highlighting unresolved barriers to real-world adoption. We evaluate the performance of state-of-the-art models such as GPT-4, Med-PaLM2, and Bio BERT in clinical tasks, revealing disparities in accuracy, 22{\%} hallucination rates in GPT-4 vs. 8{\%} in Med-PaLM2 for radiology. Key challenges include biased outputs, data security vulnerabilities, and inadequate reasoning for complex medical tasks. We propose actionable strategies to mitigate these risks, such as fine-tuning with domain-specific data sets and hybrid human--AI workflows. This review bridges gaps between AI research and clinical practice, offering a roadmap for the safe, equitable, and effective deployment of LLMs in healthcare.",
issn="2520-8446",
doi="10.1007/s42044-025-00360-7",
url="https://doi.org/10.1007/s42044-025-00360-7"
}


@inproceedings{10.1007/978-3-031-80419-9_9,
 abstract = {Capturing the logical structure of programming languages poses a significant challenge for program analysis. Given the complex syntax rules, subjective code vulnerabilities, irrelevant statements, code annotations, and intricate structural information, related studies have explored various semantic comprehension and intermediate representation approaches to extract precise information for program analysis. However, most research in the generic domain ignores defective and non-defective program code, putting them in the same category. In this paper, we introduce a new program analysis method that combines WebAssembly (Wasm) instructions with a 20-billion-parameter transformer model and natural language processing. This approach aims to advance the capabilities of program analysis tools in computer science by jointly embedding Wasm instructions and natural language for more effective program analysis. Our experiments demonstrate that this fused embedding approach achieves state-of-the-art performance, and the accuracy reaches approximately 98 percent, better than traditional small-scale weight models based on intricate conversion tasks such as abstract syntax trees(AST). Moreover, it is more valuable to classify potential vulnerable and non-vulnerable programs in the formal verification special field. Our exploration enhances traditional program classification methods in software security and introduces the application of GPT to offer a more straightforward, convenient, and high-performance approach.},
 address = {Cham},
 author = {Deng, Liangjun
and Zhong, Qi
and Lei, Hang
and Qiu, Yao
and Chen, Jingxue},
 booktitle = {Emerging Information Security and Applications},
 editor = {Li, Wenjuan
and Chen, Liqun
and Lopez, Javier},
 isbn = {978-3-031-80419-9},
 pages = {118--136},
 publisher = {Springer Nature Switzerland},
 title = {GPT-Based Wasm Instruction Analysis for Program Language Processing},
 url = {https://link.springer.com/chapter/10.1007/978-3-031-80419-9_9},
 year = {2025}
}

@inproceedings{10.1007/978-3-031-70879-4_15,
 abstract = {Security-critical software comes with numerous side-channel leakages left unpatched due to a lack of resources or experts. The situation will only worsen as the pace of code development accelerates, with developers relying on Large Language Models (LLMs) to automatically generate code. Compiler-based approaches are limited to only certain types of leakages and languages, and there is no automated method to solve the issue in the source code. In this work, we explore the use of LLMs in generating patches for vulnerable code with microarchitectural side-channel leakages in the source code. Automatic patching with LLMs in the source code provides portability to interpreted languages as well, eases the maintenance burden on the developers, and provides flexibility for different types of leakages.},
 address = {Cham},
 author = {Tol, M. Caner
and Sunar, Berk},
 booktitle = {Computer Security -- ESORICS 2024},
 editor = {Garcia-Alfaro, Joaquin
and Kozik, Rafa{\l}
and Chora{\'{s}}, Micha{\l}
and Katsikas, Sokratis},
 isbn = {978-3-031-70879-4},
 pages = {290--310},
 publisher = {Springer Nature Switzerland},
 title = {ZeroLeak: Automated Side-Channel Patching in Source Code Using LLMs},
 url = {https://link.springer.com/chapter/10.1007/978-3-031-70879-4_15},
 year = {2024}
}

@inproceedings{10.1007/978-3-032-04190-6_2,
 abstract = {Refactoring is a crucial software engineering practice aimed at improving code quality. However, detecting and predicting refactoring activities automatically remains a challenging task due to the limited availability of labeled datasets. This study investigates the role of data augmentation techniques in enhancing refactoring detection models. We apply various augmentation strategies to expand training data and assess their impact on model performance. We also discuss validation and balancing of the resulting dataset in order to provide meaningful data for further applied ML techniques. Our findings highlight the importance of data diversity in automated refactoring detection and provide insights into optimizing augmentation strategies for software engineering applications. Experimental results demonstrate that data augmentation improves the robustness and accuracy of refactoring detection models by mitigating overfitting and enhancing generalization.},
 address = {Cham},
 author = {Moldovan, Vasilica
and Patcas, Rares
and Motogna, Simona},
 booktitle = {Software Engineering and Advanced Applications},
 editor = {Taibi, Davide
and Smite, Darja},
 isbn = {978-3-032-04190-6},
 pages = {20--36},
 publisher = {Springer Nature Switzerland},
 title = {LLMs Based Data Augmentation Techniques for Python Code Refactoring},
 url = {https://link.springer.com/chapter/10.1007/978-3-032-04190-6_2},
 year = {2026}
}

@Inbook{Huang2025,
author="Huang, Jerry
and Huang, Ken
and Hughes, Chris",
editor="Huang, Ken",
title="AI Agents in Offensive Security",
bookTitle="Agentic AI: Theories and Practices",
year="2025",
publisher="Springer Nature Switzerland",
address="Cham",
pages="167--205",
abstract="Chapter 6 explores the use of AI agents in offensive security, emphasizing their growing role in addressing the increasing complexity of cyber threats. Offensive security, traditionally centered on manual penetration testing and adversarial assessments, now benefits significantly from AI Agent--based approaches. These AI agents can autonomously identify vulnerabilities, simulate attack scenarios, and assist in social engineering, software supply chain attacks, and vulnerability discovery. The chapter includes in-depth examples of AI integration in red teaming, social engineering, and software supply chain manipulation. Notably, it provides code examples demonstrating how AI agents like ``BountyAgent'' and ``DeepFuzz'' can be implemented for vulnerability discovery and zero-day identification. It also discusses the architectural evolution of offensive platforms.",
isbn="978-3-031-90026-6",
doi="10.1007/978-3-031-90026-6_6",
url="https://doi.org/10.1007/978-3-031-90026-6_6"
}


@Article{Gerstmayr2024,
author="Gerstmayr, Johannes
and Manzl, Peter
and Pieber, Michael",
title="Multibody Models Generated from Natural Language",
journal="Multibody System Dynamics",
year="2024",
month="Oct",
day="01",
volume="62",
number="2",
pages="249--271",
abstract="Computational models are conventionally created with input data, script files, programming interfaces, or graphical user interfaces. This paper explores the potential of expanding model generation, with a focus on multibody system dynamics. In particular, we investigate the ability of Large Language Model (LLM), to generate models from natural language. Our experimental findings indicate that LLM, some of them having been trained on our multibody code Exudyn, surpass the mere replication of existing code examples. The results demonstrate that LLM have a basic understanding of kinematics and dynamics, and that they can transfer this knowledge into a programming interface. Although our tests reveal that complex cases regularly result in programming or modeling errors, we found that LLM can successfully generate correct multibody simulation models from natural-language descriptions for simpler cases, often on the first attempt (zero-shot).",
issn="1573-272X",
doi="10.1007/s11044-023-09962-0",
url="https://doi.org/10.1007/s11044-023-09962-0"
}


@inproceedings{10.1007/978-981-96-5887-9_5,
 abstract = {This paper generates causal networks from raw text using Large Language Models (LLMs) and compares the accuracy and quality of generated networks with the ones produced by a recently suggested rule-based framework. The LLMs examined in this paper include ChatGPT, Mistral Chat, and Gemini. The evaluation was conducted on raw text drawn from three diverse domains: political, food insecurity, and medical. During the experiments, each LLM was given raw text belonging to these domains, and the generated causal networks were produced in the form of causal Subject-Verb-Object (SVO) triples. The quality of the produced network was assessed by a group of three human annotators. Comparative analysis reveals that, although LLMs show promise in generating causal networks, the rule-based system consistently demonstrates higher reliability in extracting precise causal relationships from raw text. The study highlights the importance of integrating traditional rule-based approaches with modern LLMs to enhance the correctness and comprehensiveness of causal network extraction, particularly in complex domains.},
 address = {Singapore},
 author = {Sheikh, Solat J.
and Haider, Sajjad},
 booktitle = {Recent Challenges in Intelligent Information and Database Systems},
 editor = {Nguyen, Ngoc Thanh
and Matsuo, Tokuro
and Gaol, Ford Lumban
and Manolopoulos, Yannis
and Fujita, Hamido
and Hong, Tzung-Pei
and Wojtkiewicz, Krystian},
 isbn = {978-981-96-5887-9},
 pages = {60--71},
 publisher = {Springer Nature Singapore},
 title = {On Comparing LLM-Generated Causal Networks with a Rule-Based Approach},
 url = {https://link.springer.com/chapter/10.1007/978-981-96-5887-9_5},
 year = {2025}
}

@Article{Elhambakhsh2025,
author="Elhambakhsh, Seyedeh Elham",
title="Evaluating ChatGPT-3's efficacy in solving coding tasks: implications for academic integrity in English language assessments",
journal="Language Testing in Asia",
year="2025",
month="Jul",
day="02",
volume="15",
number="1",
pages="37",
abstract="The purpose of this study was to examine ChatGPT-3's capabilities to generate code solutions for assessment problems commonly assessed by automatic correction tools in the TEFL academic setting, focusing on the Kattis platform. The researcher explored potential implications for academic integrity and the challenges associated with AI-generated solutions. The investigation involved testing ChatGPT on a subset of 124 English language assessment tasks from Kattis, a widely used automatic software grading tool. The results revealed that ChatGPT independently solved 16 tasks successfully. Data analysis demonstrated that while ChatGPT performed well on simpler problems, it faced challenges with more complex assessment tasks. To supplement quantitative findings, a qualitative follow-up investigation was conducted, including interviews with two EFL assessment instructors. The discussion encompassed methodological considerations, the effectiveness of Kattis in preventing cheating, and the limitations in detecting AI-generated code. ChatGPT independently solved 16 out of 124 assessment tasks assessed by Kattis. Performance varied based on task complexity, with better accuracy on simpler problems. Qualitative insights revealed both the strengths and limitations of Kattis in preventing cheating. While ChatGPT demonstrates competence in solving certain assessment problems, challenges persist with more complex tasks. The study emphasizes the need for continuous adaptation in EFL assessment methodologies to maintain academic integrity in the face of evolving AI capabilities. As students gain access to sophisticated AI-generated solutions, the need for vigilant strategies to uphold originality and critical thinking in academic work becomes increasingly crucial. The study's findings have implications for multiple stakeholders, including (1) awareness of AI capabilities in generating code solutions, necessitating vigilant assessment strategies. (2) Understanding the importance of academic integrity and the limitations of AI in mastering complex assessment tasks. (3) Insights into the interplay between AI, automated assessment systems, and academic integrity, guiding future investigations. This performance illustrates the need for careful assessment design to mitigate the risk of AI-assisted academic dishonesty while maintaining rigorous academic standards.",
issn="2229-0443",
doi="10.1186/s40468-024-00333-w",
url="https://doi.org/10.1186/s40468-024-00333-w"
}


@Inbook{Hemberg2025,
author="Hemberg, Erik
and Jorgensen, Steven
and O'Reilly, Una-May",
editor="Winkler, Stephan M.
and Banzhaf, Wolfgang
and Hu, Ting
and Lalejini, Alexander",
title="Survey of Genetic Programming and Large Language Models",
bookTitle="Genetic Programming Theory and Practice XXI",
year="2025",
publisher="Springer Nature Singapore",
address="Singapore",
pages="67--86",
abstract="There now exist algorithms that use both Genetic Programming (GP) and Large Language ModelsLarge Language Model (LLMs) to evolve solutions. This is a composition of two different methods that show some potential. We survey GP and LLM algorithms and their applications from a GP centric perspective. We provide a taxonomy, a breakdown of application domains, and a categorization of the implementation details. Finally, we discuss existing research gaps and possible directions for future workPrompt.",
isbn="978-981-96-0077-9",
doi="10.1007/978-981-96-0077-9_4",
url="https://doi.org/10.1007/978-981-96-0077-9_4"
}


@Article{Çobanlı2025,
author="{\c{C}}obanl{\i}, Furkan Taha
and Kahraman, Mustafa",
title="Leveraging artificial intelligence for computational urban analysis: building StreetRose with ChatGPT",
journal="Spatial Information Research",
year="2025",
month="Mar",
day="29",
volume="33",
number="2",
pages="16",
abstract="The emergence and rapid development of large language models (LLMs) has changed the way we interact with information and solve complex problems. The aim of this study is to explore how an urban analysis tool can be created and used effectively with the aid of a large language model. By focusing on this objective, we highlight both the potential and the limitations of ChatGPT. Using GPT-3.5 Turbo, one of the popular AI-powered Large Language Models, we built ``StreetRose 0.0.1'', a Python-based application. OpenStreetMap was used as a data source. The development process followed the steps of data collection, software development, analysis and visualisation. The resulting tool, StreetRose, visualises street trends in settlements and provides insights into street network analysis. This study provides an example of the use of ChatGPT in urban studies and discusses its advantages and disadvantages. In addition, the tool provides practical information and visualisations that can help researchers interested in settlements and serve as a valuable asset for urban planning and street network analysis. The results show that there are areas for improvement and shortcomings that need to be addressed, although ChatGPT significantly speeds up the coding process. The study also highlights the transformative impact of LLMs on urban analysis and sets an example for future applications in this field.",
issn="2366-3294",
doi="10.1007/s41324-025-00614-3",
url="https://doi.org/10.1007/s41324-025-00614-3"
}


@inproceedings{10.1007/978-3-031-62495-7_27,
 abstract = {Automated Program Repair (APR) is a domain of research in software engineering that focuses on providing computationally-generated fixes to buggy code. The primary objective is to alleviate the challenges associated with identifying and rectifying errors that exist within large-scale projects. Fault localization is a critical stage of the APR pipeline, dedicated to identifying the locations of bugs within the code. Despite Python now being one of the most popular programming languages, most existing fault localization techniques are limited to real-world Java and C repositories. This paper proposes a graph-based representation of buggy code that utilizes flow of control and data to capture both semantic and syntactic information. We also present an analysis of a novel approach, Class-Imbalanced Learning on Graphs (CILG) for fault localization, as an alternative to conventional methods of calculating program element suspiciousness scores. The proposed approach is trained and tested on a real-world dataset containing buggy Python code snippets extracted from the PyTraceBugs dataset, achieving a notable macro Area Under the Curve-Receiver Operating Characteristic (AUC-ROC) score of 0.85. We have also provided a comparison with Graph Neural Network (GNN) models and gpt-3.5-turbo to demonstrate the effectiveness of our technique.},
 address = {Cham},
 author = {Kulkarni, Apoorva Anand
and Niranjan, Divya G.
and Saju, Noel
and Shenoy, P. Rakshith
and Arya, Arti},
 booktitle = {Engineering Applications of Neural Networks},
 editor = {Iliadis, Lazaros
and Maglogiannis, Ilias
and Papaleonidas, Antonios
and Pimenidis, Elias
and Jayne, Chrisina},
 isbn = {978-3-031-62495-7},
 pages = {354--368},
 publisher = {Springer Nature Switzerland},
 title = {Graph-Based Fault Localization in Python Projects with Class-Imbalanced Learning},
 url = {https://link.springer.com/chapter/10.1007/978-3-031-62495-7_27},
 year = {2024}
}

@Article{Al-Hashimi2026,
author="Al-Hashimi, Hussein A.",
title="A generative AI cybersecurity risks mitigation model for code generation: using ANN-ISM hybrid approach",
journal="Scientific Reports",
year="2026",
month="Jan",
day="14",
volume="16",
number="1",
pages="4239",
abstract="The increasing reliance on automatic code generation integrated with Generative AI technology has raised new challenges for cybersecurity defense against code injection, insecure code templates, and adversarial manipulation of an AI model. These risks make developing advanced frameworks imperative to ensure secure, reliable, and privacy-preserving code generation processes. The paper presents a novel Hybrid Artificial Neural Network (ANN)-Interpretive Structural Modeling (ISM) Framework to alleviate the cybersecurity risks associated with the automatic code generation using Generative AI. The proposed framework integrates the predictive capability of ANN and structured analysis of ISM for the identification, evaluation, and treatment of common vulnerabilities and risks in automatic code generation. We first conduct a multivocal literature review (MLR) to identify cybersecurity risks and generative AI practices for addressing these risks in automatic code generation. Then we conduct a questionnaire survey to identify and validate the identified risks and practices. An expert panel review was then assigned for the process of ANN-ISM. The ANN model can predict potential security risks by learning from historical data and code generation patterns. ISM is used to (1) structure and visualize (2) relations between identified risks and mitigation approaches and (3) offer a combined, multi-layered risk management methodology. We then perform an in-depth examination of the framework with a case study of an AI-based code generation company. We further determine its practicality and usefulness in real-world settings. The case study results show that the framework efficiently handles the primary cybersecurity challenges, such as injection attacks, code quality, backdoors, and lack of input validation. The analysis characterizes the maturity of several mitigation practices and areas for improvement for security integration with automatic code generation functionality. Advanced risk mitigation is enabled in the framework across multiple process areas, where techniques such as static code analysis, automated penetration testing, and adversarial training hold much promise. The Hybrid ANN-ISM Mechanism is a stable and flexible solution for cybersecurity risk reduction in automatic code generation environments. The coupling of ANN and ISM, in terms of predictive analysis and structured risk management, respectively, contributes effectively towards the security of AI-based code generation tools. More research is required to improve the scalability, privacy preserving, and dynamic integration of the framework with cybersecurity threat intelligence.",
issn="2045-2322",
doi="10.1038/s41598-025-34350-3",
url="https://doi.org/10.1038/s41598-025-34350-3"
}


@Article{Li2025,
author="Li, Yuning
and Zhong, Wenkang
and Shen, Zongwen
and Li, Chuanyi
and Chen, Xiang
and Ge, Jidong
and Luo, Bin",
title="An empirical study on the code naturalness modeling capability for LLMs in automated patch correctness assessment",
journal="Automated Software Engineering",
year="2025",
month="Apr",
day="02",
volume="32",
number="2",
pages="35",
abstract="Just like natural language, code can exhibit naturalness. This property manifests in highly repetitive patterns within specific contexts. Code naturalness can be captured by language models and then applied to various software engineering tasks (such as fault localization and program repair). Recently, Large Language Models (LLMs) based on Transformers have become advantageous tools for modeling code naturalness. However, existing work lacks systematic studies on the code naturalness modeling capability for LLMs. To bridge this gap, this paper explores the code naturalness modeling capability for LLMs, starting with the task of automated patch correctness assessment. Specifically, we investigate whether LLMs with different architectures and scales, under varying context window sizes, (1) can identify buggy code from common code based on naturalness and consider fixed code more natural than buggy code, and (2) can distinguish different degrees of repairs (i.e., complete repairs and incomplete repairs) from automated tools. Then, we propose metrics to assess the above two capabilities of the models. Experimental results indicate that models with different architectures and scales have the code naturalness modeling capability, even models not specifically pre-trained on code. Additionally, smaller models do not necessarily exhibit weaker modeling capability compared to larger models. We also find more contextual information only provides limited benefits. Based on experimental findings, we select the best performing model that has 220 M parameters to develop an Entropy-based Automated Patch Correctness Assessment (E-APCA) approach by calculating code naturalness. On the large-scale dataset PraPatch, E-APCA surpasses traditional methods by over 20{\%} across various evaluation metrics. Compared to the latest APCA method Entropy-delta based on a 6.7B LLM, E-APCA achieves a 17.32{\%} higher correct patch recall and a 6.83{\%} higher F1 score, while the reasoning time is less than 7{\%} of that required by Entropy-delta.",
issn="1573-7535",
doi="10.1007/s10515-025-00502-y",
url="https://doi.org/10.1007/s10515-025-00502-y"
}


@inproceedings{10.1007/978-3-031-49252-5_14,
 abstract = {In this paper we show how our approach of extending Language Driven Engineering (LDE) with natural language-based code generation supports system migration: The characteristic decomposition of LDE into tasks that are solved with dedicated domain-specific languages divides the migration tasks into portions adequate to apply LLM-based code generation. We illustrate this effect by migrating a low-code/no-code generator for point-and-click adventures from JavaScript to TypeScript in a way that maintains an important property: generated web applications can automatically be validated via automata learning and model analysis by design. In particular, this allows to easily test the correctness of migration by learning the difference automaton for the generated products of the source and the target system of the migration.},
 address = {Cham},
 author = {Busch, Daniel
and Bainczyk, Alexander
and Steffen, Bernhard},
 booktitle = {Engineering of Computer-Based Systems},
 editor = {Kofro{\v{n}}, Jan
and Margaria, Tiziana
and Seceleanu, Cristina},
 isbn = {978-3-031-49252-5},
 pages = {191--200},
 publisher = {Springer Nature Switzerland},
 title = {Towards LLM-Based System Migration in Language-Driven Engineering},
 url = {https://link.springer.com/chapter/10.1007/978-3-031-49252-5_14},
 year = {2024}
}

@Article{Korkmaz2026,
author="Korkmaz, Yusuf Yunus
and Ayd{\i}n, O{\u{g}}uzhan
and G{\"u}ng{\"o}r, Feyyaz
and Kuda{\c{s}}, {\.{I}}lyas
and Sarigoz, Talha
and Bostanci, Ozgur",
title="ChatGPT and other large language models in laparoscopic cholecystectomy: a multidimensional audit of reliability, quality, and readability",
journal="Surgical Endoscopy",
year="2026",
month="Jan",
day="01",
volume="40",
number="1",
pages="480--490",
abstract="The rapid uptake of large language models (LLMs) in surgery demands evidence of their reliability when guiding laparoscopic cholecystectomy (LC).",
issn="1432-2218",
doi="10.1007/s00464-025-12315-x",
url="https://doi.org/10.1007/s00464-025-12315-x"
}


@Article{Balasubramanian2025,
author="Balasubramanian, Prasasthy
and Liyana, Sonali
and Sankaran, Hamsini
and Sivaramakrishnan, Shambavi
and Pusuluri, Sruthi
and Pirttikangas, Susanna
and Peltonen, Ella",
title="Generative AI for cyber threat intelligence: applications, challenges, and analysis of real-world case studies",
journal="Artificial Intelligence Review",
year="2025",
month="Aug",
day="20",
volume="58",
number="11",
pages="336",
abstract="This paper presents a comprehensive survey of the applications, challenges, and limitations of Generative AI (GenAI) in enhancing threat intelligence within cybersecurity, supported by real-world case studies. We examine a wide range of data sources in Cyber Threat Intelligence (CTI), including security reports, blogs, social media, network traffic, malware samples, dark web data, and threat intelligence platforms (TIPs). This survey provides a full reference for integrating GenAI into CTI. We discuss various GenAI models such as Large Language Models (LLMs) and Deep Generative Models (DGMs) like Variational Autoencoders (VAEs), Generative Adversarial Networks (GANs), and Diffusion Models, explaining their roles in detecting and addressing complex cyber threats. The survey highlights key applications in areas such as malware detection, network traffic analysis, phishing detection, threat actor attribution, and social engineering defense. We also explore critical challenges in deploying GenAI, including data privacy, security concerns, and the need for interpretable and transparent models. As regulations like the European Commission's AI Act emerge, ensuring trustworthy AI solutions is becoming more crucial. Real-world case studies, such as the impact of the WannaCry ransomware, the rise of deepfakes, and AI-driven social engineering, demonstrate both the potential and current limitations of GenAI in CTI. Our goal is to provide foundational insights and strategic direction for advancing GenAI's role in future cybersecurity frameworks, emphasizing the importance of innovation, adaptability, and ongoing learning to enhance resilience against evolving cyber threats. Ultimately, this survey offers critical insights into how GenAI can shape the future of cybersecurity by addressing key challenges and providing actionable guidance for effective implementation.",
issn="1573-7462",
doi="10.1007/s10462-025-11338-z",
url="https://doi.org/10.1007/s10462-025-11338-z"
}


@inproceedings{10.1007/978-3-031-82362-6_1,
 abstract = {Large language models (LLMs) are becoming more advanced and widespread and have shown their applicability to various domains, including cybersecurity. Static malware analysis is one of the most important tasks in cybersecurity; however, it is time-consuming and requires a high level of expertise. Therefore, we conducted a demonstration experiment focusing on whether an LLM can be used to support static analysis. First, we evaluated the ability of the LLM to explain malware functionality. The results showed that the LLM can generate descriptions that cover functions with an accuracy of up to 90.9{\%}. In addition, we asked six static analysts to perform a pseudo static analysis task using LLM explanations to verify that the LLM can be used in practice. Through subsequent questionnaires and interviews with the participants, we also demonstrated the practical applicability of LLMs. Lastly, we summarized the problems and required functions when using an LLM as static analysis support, as well as recommendations for future research opportunities.},
 address = {Cham},
 author = {Fujii, Shota
and Yamagishi, Rei},
 booktitle = {Computer Security. ESORICS 2024 International Workshops},
 editor = {Garcia-Alfaro, Joaquin
and Kalutarage, Harsha
and Yanai, Naoto
and Kozik, Rafa{\l}
and Ksieniewicz, Pawe{\l}
and Wo{\'{z}}niak, Micha{\l}
and Abie, Habtamu
and Ranise, Silvio
and Verderame, Luca
and Cambiaso, Enrico
and Ugarelli, Rita
and Pra{\c{c}}a, Isabel
and Katt, Basel
and Pirbhulal, Sandeep
and Shukla, Ankur
and Pawlicki, Marek
and Chora{\'{s}}, Micha{\l}},
 isbn = {978-3-031-82362-6},
 pages = {5--28},
 publisher = {Springer Nature Switzerland},
 title = {Feasibility Study for Supporting Static Malware Analysis Using LLM},
 url = {https://link.springer.com/chapter/10.1007/978-3-031-82362-6_1},
 year = {2025}
}

@inproceedings{10.1007/978-3-031-82633-7_19,
 abstract = {Generative AI models such as ChatGPT and Stable Diffusion have become easily available to end users through various apps. Research has identified several safety risks and limitations of generative AI, but the experiences and issues faced by real users of this technology in the wild have not been systematically investigated. In this paper, we identify user issues related to trustworthiness dimensions of generative AI, by analyzing user reviews of AI apps using a hybrid approach that combines unsupervised topic modeling and manual qualitative analysis. The results revealed user issues related to the validity, reliability, safety, security and privacy of the AI. Validity-related issues, such as incorrect output, were often found, but these issues appeared to result from high expectations about the capabilities of the technology, rather than an accurate reflection of its limitations. Concerns about safety issues, such as bias and the handling of inappropriate content, also appeared frequently, although users had conflicting expectations on how these should be handled. On the other hand, the user reviews contained fewer instances of concern related to the security and privacy of the AI itself. Overall, the results suggest that real users of generative AI have inadequate information about the characteristics and limitations of these models.},
 address = {Cham},
 author = {Bracamonte, Vanessa
and Loebner, Sascha
and Tronnier, Frederic
and Lieberknecht, Ann-Kristin
and Pape, Sebastian},
 booktitle = {Computer-Human Interaction Research and Applications},
 editor = {Pl{\'a}cido da Silva, Hugo
and Cipresso, Pietro},
 isbn = {978-3-031-82633-7},
 pages = {304--322},
 publisher = {Springer Nature Switzerland},
 title = {User Issues and Concerns in Generative AI: A Mixed-Methods Analysis of App Reviews},
 url = {https://link.springer.com/chapter/10.1007/978-3-031-82633-7_19},
 year = {2025}
}

@inproceedings{10.1007/978-3-032-04549-2_24,
 abstract = {Source code summarization automates the generation of natural comments, enhancing efficiency in software development and maintenance. With the emergence of large language models (LLMs), significant progress has been made in this domain. However, current approaches either rely on manually crafted prompts or standard fine-tuning that fail to fully leverage the power of continuous embeddings. To address this gap, we propose StageCS, a novel framework integrating prompt learning and model fine-tuning for code summarization, featuring a strategic two-stage training process with a multi-branch transformer architecture. StageCS generates specialized continuous embeddings that synergistically guide LLMs to produce high-quality summaries while maximizing the benefits of targeted fine-tuning. Evaluations on the CodeSearchNet Java dataset show StageCS outperforms baselines in representative LLM architectures like PolyCoder and CodeGen. More importantly, our ablation studies demonstrate that both the multi-branch transformer and the strategic two-stage process contribute significantly. StageCS enables LLMs to excel in code summarization without extensive manual prompt engineering, delivering superior quality through specialized training.},
 address = {Cham},
 author = {Sun, Xiaoshu
and Lv, Siqi
and Wan, Wei
and Qin, Yiming
and Hu, Gang},
 booktitle = {Artificial Neural Networks and Machine Learning -- ICANN 2025},
 editor = {Senn, Walter
and Sanguineti, Marcello
and Saudargiene, Ausra
and Tetko, Igor V.
and Villa, Alessandro E. P.
and Jirsa, Viktor
and Bengio, Yoshua},
 isbn = {978-3-032-04549-2},
 pages = {294--306},
 publisher = {Springer Nature Switzerland},
 title = {A Two-Stage Framework Integrating Prompt Learning and Fine-Tuning for Code Summarization},
 url = {https://link.springer.com/chapter/10.1007/978-3-032-04549-2_24},
 year = {2026}
}

@Article{Qasaimeh2025,
author="Qasaimeh, Malik
and Nakhleh, Saja
and Al-Qassas, Raad
and Abdallah, Ammar
and Yasin, Muneer Bani",
title="Robustness of AraBERT models to black-box adversarial deletions and perturbations",
journal="International Journal of Speech Technology",
year="2025",
month="Dec",
day="01",
volume="29",
number="1",
pages="4",
abstract="Large Language Models (LLMs) are becoming vulnerable to adversarial attacks due to the creation crafted inputs that may deceive even the most advanced protections. These models can produce the wrong prediction with slight changes in the input data. This flaw can be noticed in many areas, such as computer vision, speech recognition, and natural language processing, and there are serious doubts about the robustness of AraBERT LLMs in sensitive use. Consequently, this paper assesses the robustness of AraBERT LLMs to character-level and word-level black-box attacks that cause spelling errors in the input data. The adversarial samples were generated using the Chain-of-Thought prompting method, to evaluate the robustness of a fine-tuned AraBERT model and the original AarBERT v2.0 LLM. A considerable reduction in accuracy was found, especially with regard to word-level delete attacks, and the largest reduction of 44.93{\%} of the original model was obtained. The deletions at the word and character levels were also significant. This highlights how important it is to strengthen the model's robustness to deletions, in order to obtain reliable performance for a variety of text lengths and challenging crafted input.",
issn="1572-8110",
doi="10.1007/s10772-025-10237-6",
url="https://doi.org/10.1007/s10772-025-10237-6"
}


@Article{Pandey2025,
author="Pandey, Sushant Kumar
and Chand, Sivajeet
and Horkoff, Jennifer
and Staron, Miroslaw
and Ochodek, Miroslaw
and Durisic, Darko",
title="Design pattern recognition: a study of large language models",
journal="Empirical Software Engineering",
year="2025",
month="Feb",
day="18",
volume="30",
number="3",
pages="69",
abstract="As Software Engineering (SE) practices evolve due to extensive increases in software size and complexity, the importance of tools to analyze and understand source code grows significantly.",
issn="1573-7616",
doi="10.1007/s10664-025-10625-1",
url="https://doi.org/10.1007/s10664-025-10625-1"
}


@inproceedings{10.1007/978-3-032-00639-4_18,
 abstract = {Understanding and controlling the behavior of Large Language Models (LLMs) is crucial for their reliable use in software vulnerability detection. While LLMs show promising zero-shot capabilities, our analysis shows that they often behave inconsistently by over-predicting vulnerabilities, overlooking real vulnerabilities in domain shifts. In this paper, we approach vulnerability detection as a behavior shaping problem. We apply Group Relative Policy Optimization (GRPO) to guide the behavior of models through structured rule-based rewards. Our reward verifiers target both the accuracy of predictions and the coherence of explanations, encouraging the model to develop stable and trustworthy decision patterns. Through experiments on BigVul, DiverseVul and CleanVul benchmarks, we show that behavior shaping with GRPO improves the model's ability to generalize across projects, programming languages, and data quality levels. Furthermore, we show that tuning the regularization's strength of the Kullback--Leibler (KL) divergence enables a balance between risk-seeking and risk-averse behavior, reducing false negatives without overwhelming users with false positives.},
 address = {Cham},
 author = {Fontana, Aleksandar
and Simoni, Marco},
 booktitle = {Availability, Reliability and Security},
 editor = {Coppens, Bart
and Volckaert, Bruno
and Naessens, Vincent
and De Sutter, Bjorn},
 isbn = {978-3-032-00639-4},
 pages = {316--333},
 publisher = {Springer Nature Switzerland},
 title = {Unmasking Model Behavior: How LLMs Reason on Vulnerability Detection},
 url = {https://link.springer.com/chapter/10.1007/978-3-032-00639-4_18},
 year = {2025}
}

@Inbook{Domínguez-Isidro2025,
author="Dom{\'i}nguez-Isidro, Sa{\'u}l
and S{\'a}nchez-Garc{\'i}a, {\'A}ngel J.
and Morales-Utrera, Axel Jordano
and Lim{\'o}n, Xavier",
editor="Mej{\'i}a, Jezreel
and Mu{\~{n}}oz, Mirna
and Rocha, Alvaro
and Espinosa-Faller, Francisco Javier
and Trejo-Sanchez, Joel Antonio",
title="Machine Learning Techniques for Automatic Program Repair: A Systematic Literature Mapping",
bookTitle="New Challenges in Software Engineering: Volume 1",
year="2025",
publisher="Springer Nature Switzerland",
address="Cham",
pages="529--543",
abstract="Program repair involves identifying and fixing issues in a program's source code, encompassing error correction, performance improvement, code optimization, and even addressing security problems. However, this process can address issues beyond errors, such as code readability, elimination of duplicated code, and more. Generally, code repair activities are carried out during the software construction and maintenance process. This work presents an analysis of Machine Learning (ML) techniques used in automatic program repair (APR) processes to identify the benefits and challenges of their use. For this purpose, guidelines for conducting systematic literature reviews in software engineering were followed. Four databases were explored, yielding a total of 21 studies. It was found that the most frequently addressed automatic program repair approaches using ML techniques involve syntax, semantic, and logical errors; security aspects have been addressed to a lesser extent. The ML techniques used for this purpose include decision trees and deep learning techniques such as convolutional and recurrent neural networks, long short-term memory models, and others combined with code languages models. Finally, according to the analysis, the main advantage of using these techniques is their ability to handle large volumes of data and learn from their environment. However, in many cases, manual verification of the solutions generated by these techniques is still necessary.",
isbn="978-3-031-90310-6",
doi="10.1007/978-3-031-90310-6_32",
url="https://doi.org/10.1007/978-3-031-90310-6_32"
}


@Article{Onan2025,
author="Onan, Aytug
and Alhumyani, Hesham A.",
title="CodeDiffuSe: A masked diffusion framework for structure-aware code completion and repair",
journal="Journal of King Saud University Computer and Information Sciences",
year="2025",
month="Sep",
day="24",
volume="37",
number="8",
pages="230",
abstract="Code completion and code repair have become fundamental tasks in software engineering and machine learning research. However, existing large language models (LLMs) for code generation, predominantly based on autoregressive modeling (ARM), exhibit limitations when dealing with incomplete or buggy code snippets, especially when the missing or erroneous spans are located arbitrarily within the sequence. In this paper, we propose CodeDiffuSe, a novel masked diffusion framework specifically designed for structure-aware code completion and repair. Unlike traditional ARM approaches, CodeDiffuSe learns to recover missing code spans by leveraging both left and right context, enabling bidirectional reasoning and flexible infilling. We introduce a syntax-aware masking strategy that randomly masks entire Abstract Syntax Tree (AST) subtrees during training, and a semantic consistency regularization that encourages type-correct and syntactically valid predictions. During inference, we propose an error-aware remasking mechanism that dynamically identifies uncertain or invalid tokens and selectively refines them across adaptive reverse diffusion steps. Extensive experiments on standard code completion and bug repair benchmarks, including CodeXGLUE, Defects4J, and QuixBugs, demonstrate that CodeDiffuSe consistently outperforms strong autoregressive baselines such as CodeGen, CodeEditorBench, and InCoder across multiple programming languages. Our work introduces a structure- and semantics-aware diffusion-based alternative for code completion and repair, offering consistent gains in both syntactic validity and functional correctness across diverse benchmarks. Rather than claiming to replace existing paradigms, we demonstrate how diffusion models can complement and improve upon autoregressive and retrieval-based approaches in structure-sensitive settings.",
issn="2213-1248",
doi="10.1007/s44443-025-00237-6",
url="https://doi.org/10.1007/s44443-025-00237-6"
}


@Article{Patel2025,
author="Patel, Kinjal
and Shah, Milind
and Qureshi, Karishma M.
and Qureshi, Mohamed Rafik N.",
title="A systematic review of generative AI: importance of industry and startup-centered perspectives, agentic AI, ethical considerations {\&} challenges, and future directions",
journal="Artificial Intelligence Review",
year="2025",
month="Nov",
day="12",
volume="59",
number="1",
pages="7",
abstract="Generative Artificial Intelligence (GenAI) is rapidly redefining the landscape of work organizations and society at large. GenAI has rapidly evolved from rule-based symbolic systems ofThe 1940 s to advanced deep learning architectures capable of producing human-like content across modalities, such as text, images, audio, and video. This review focuses on current emerging trends, such as large concept models and critical comparisons of tools, including ChatGPT, Gemini, and Claude. This study synthesizes evidence of GenAI's essential role across major industries, revealing transformative applications in the finance, cloud and IT, healthcare, education, and energy sectors. The paper also highlights the unique opportunities GenAI offers for start-ups, enabling agile projects to leverage cutting-edge technology for competitive advantage. However, the deployment of GenAI systems through edge devices also raises critical challenges related to ethics, transparency, bias, accountability, computational issues, and many more. To address these complexities, this paper examines emerging approaches such as AI agents, agentic AI, and multi-agent systems that aim to extend the functionality of GenAI through autonomy, goal-directed behavior, and collaborative intelligence. It discovers novel incorporations with agentic AI architecture, such as BabyAGI, and discusses emerging issues of coordination, hallucination, and security risks. The findings reveal persistent challenges related to scalability, interpretability, and regulatory compliance while identifying future research directions toward developing more sophisticated, ethical, and accessible GenAI systems that will continue to reshape technological landscapes and societal interactions. This systematic review informs researchers, academicians, data scientists, and developers about the latest advancements in GenAI and highlights its applications and role across various industries, as well as supporting practitioners and scholars in staying current with the rapidly evolving landscape of generative technologies.",
issn="1573-7462",
doi="10.1007/s10462-025-11435-z",
url="https://doi.org/10.1007/s10462-025-11435-z"
}


@Article{Ishfaq2025,
author="Ishfaq, Faryal
and Marwat, Safdar Nawaz Khan
and Khan, Waseem Ullah
and Shahzad, Sara
and Khan, Shahid
and Abbasi, Qammer H.",
title="A machine learning based empathy mapping framework for enhancing user experience through app review analysis",
journal="Scientific Reports",
year="2025",
month="Dec",
day="02",
volume="16",
number="1",
pages="335",
abstract="The effectiveness of software applications largely depends on the user experience (UX), since it has a direct impact on user engagement and satisfaction. Empathy mapping is an important design thinking technique that organizes user perceptions into distinct categories for better understanding. However, traditional empathy mapping methods rely entirely on interviews and manual analysis which are both time-consuming and costly, thereby limiting the scalability of UX design and research. To address these challenges, this study presents an automated process for empathy mapping by analyzing user-posted app reviews. This study uses the Bidirectional Encoder Representations from Transformers (BERT) model for sentiment analysis, classifying user reviews as either positive (gain points or desires) or negative (pain points or frustrations). Latent Dirichlet Allocation (LDA) is then used to apply topic modeling to pinpoint preferences and important themes. By concentrating on gains and pains, this method automates the traditional manual and costly process of design thinking and empathy mapping, making it more scalable and efficient through data-driven insights. In training, the proposed model with several versions of BERT model, the binary accuracy improved from 78.14 to 98.61{\%}, with precision achieving 97.82{\%}, F1 score of 98.62{\%}, and recall up to 99.42{\%}. The validation accuracy also increased from 87.40 to 92.58{\%}, with an F1 score 92.59{\%}, precision of 92.43{\%}, and recall of 92.75{\%}. These accurate results indicate that the proposed model may be used by user experience design teams, which will help them improve and streamline UX design while also assisting developers in promptly receiving user feedback.",
issn="2045-2322",
doi="10.1038/s41598-025-30729-4",
url="https://doi.org/10.1038/s41598-025-30729-4"
}


@inproceedings{10.1007/978-3-032-09318-9_24,
 abstract = {The following article investigates how large language models (LLMs) can be used to evaluate source code quality. A rapid literature review identified 23 relevant articles that addressed key questions about LLM-based code reviews. Based on the information collected, we developed a structured evaluation framework comprising twelve quantifiable metrics designed to systematically measure code quality across multiple dimensions. In order to compare LLMs as code reviewers with human developers, the created metrics were then used by both human developers and LLMs against 75 source code files. These files were curated to represent diverse coding practices, emphasising examples that demonstrate potential quality issues, antipatterns, and suboptimal implementations. A dedicated software platform was developed to automate the prompting and collection of evaluations from multiple LLMs, complemented by parallel human evaluations. Our study shows that certain LLMs can effectively evaluate source code quality with a high correlation with human expert review, with Claude Sonet showing the strongest agreement. We observed that LLMS assign higher scores than humans for complex dimensions such as the Liskov substitution principle and the dependence inversion principle. The cost analysis demonstrated a significant difference between the models, suggesting that GPT-4o-mini may offer the best balance of performance and economic efficiency for real-world applications (with the consideration of data privacy remaining an important factor).},
 address = {Cham},
 author = {Kawalerowicz, Marcin
and Pietranik, Marcin
and St{\k{e}}pniak, Krzysztof},
 booktitle = {Computational Collective Intelligence},
 editor = {Nguyen, Ngoc Thanh
and Dinh Duc Anh, Vu
and Kozierkiewicz, Adrianna
and Nguyen Van, Sinh
and N{\'u}{\~{n}}ez, Manuel
and Treur, Jan
and Vossen, Gottfried},
 isbn = {978-3-032-09318-9},
 pages = {346--360},
 publisher = {Springer Nature Switzerland},
 title = {LLMs as Code Review Agents: A Rapid Review and Experimental Evaluation with Human Expert Judges},
 url = {https://link.springer.com/chapter/10.1007/978-3-032-09318-9_24},
 year = {2026}
}

@inproceedings{10.1007/978-3-031-80853-1_30,
 abstract = {Large language models play a crucial role in modern natural language processing technologies. However, their extensive use also introduces potential security risks, such as the possibility of black-box attacks. These attacks can embed hidden malicious features into the model, leading to adverse consequences during its deployment.},
 address = {Cham},
 author = {Khomsky, Daniil
and Maloyan, Narek
and Nutfullin, Bulat},
 booktitle = {Distributed Computer and Communication Networks},
 editor = {Vishnevsky, Vladimir M.
and Samouylov, Konstantin E.
and Kozyrev, Dmitry V.},
 isbn = {978-3-031-80853-1},
 pages = {404--416},
 publisher = {Springer Nature Switzerland},
 title = {Prompt Injection Attacks in Defended Systems},
 url = {https://link.springer.com/chapter/10.1007/978-3-031-80853-1_30},
 year = {2025}
}

@Article{Jiang2025,
author="Jiang, Ling
and Mohd Pakri, Mohamad Rashidi",
title="Identifying and quantifying literary intimacy with ChatGPT: a computational reading of Great Expectations",
journal="Humanities and Social Sciences Communications",
year="2025",
month="Sep",
day="25",
volume="12",
number="1",
pages="1480",
abstract="As intimacy has garnered increasing attention in sociology and psychology, its systematic study in literary scholarship remains predominantly rooted in qualitative close reading and interpretive analysis. To address this gap, this study introduces a novel computational framework for identifying and quantifying intimacy dynamics in literary texts. Leveraging GPT-4, we construct the first multi-layered intimacy corpus comprising over 12,000 verbal and nonverbal interaction segments, calibrated against established psychological intimacy scales and validated through zero-shot regression and cross-model transfer tests. Applied to Charles Dickens's Great Expectations, the framework generates chapter-level heatmaps and dyadic trajectory analyses that both reaffirm canonical literary interpretations and reveal underlying emotional and social tensions, and its outputs are further validated through a human-annotation study. This study demonstrates how large language models can complement traditional literary criticism by offering empirically grounded, multi-scale insights into the emotional architecture of narrative fiction.",
issn="2662-9992",
doi="10.1057/s41599-025-05847-z",
url="https://doi.org/10.1057/s41599-025-05847-z"
}


@Article{Njeh2025,
author="Njeh, Chaima
and Nakouri, Ha{\"i}fa
and Jaafar, Fehmi",
title="A Data-centric approach for safe and secure large language models against threatening and toxic content",
journal="International Journal of Information Security",
year="2025",
month="May",
day="30",
volume="24",
number="3",
pages="148",
abstract="Large Language Models (LLM) have made remarkable progress, but concerns about potential biases and harmful content persist. To address these apprehensions, we introduce a practical solution for ensuring LLM's safe and ethical use. Our novel approach focuses on a post-generation correction mechanism, the BART-Corrective Model, which adjusts generated content to ensure safety and security. Unlike relying solely on model fine-tuning or prompt engineering, our method provides a robust data-centric alternative for mitigating harmful content. We demonstrate the effectiveness of our approach through experiments on multiple toxic datasets, which show a significant reduction in mean toxicity and jail-breaking scores after integration. Specifically, our results show a reduction of 15{\%} and 21{\%} in mean toxicity and jail-breaking scores with GPT-4, a substantial reduction of 28{\%} and 5{\%} with PaLM2, a reduction of approximately 26{\%} and 23{\%} with Mistral-7B, and a reduction of 11.1{\%} and 19{\%} with Gemma-2b-it. These results demonstrate the potential of our approach to improve the safety and security of LLM, making them more suitable for real-world applications.",
issn="1615-5270",
doi="10.1007/s10207-025-01066-4",
url="https://doi.org/10.1007/s10207-025-01066-4"
}


@inproceedings{10.1007/978-981-96-4566-4_13,
 abstract = {When detecting and repairing code defects, enhancing the generalization ability and detection accuracy of models is a key challenge. This paper proposes a data fine-tuning method based on Chain of Thought (CoT) fine-tuning to improve the capabilities of models on defect detection in AI code. We constructed a dataset that includes the CrossVul dataset and a manually created dataset of AI code defects and repairs, improving data quality through techniques like context free removal. In the experiments, we used the Codeshell-7B, Qwencoder2.5-7B and Llama3.1-7B as the base and trained them using LoRA fine-tuning techniques. We compared different datasets and training methods to verify the model's effectiveness in detecting and repairing AI code defects. The results show that the CoT fine-tuning model outperforms models without CoT fine-tuning in all aspects of handling code defect tasks. Additionally, the specialized dataset we created for AI code defects and repairs significantly enhances the model's accuracy and repair rate in AI code detection. Our experiments highlight the importance of constructing targeted datasets for AI code defects and employing CoT fine-tuning strategies in improving code defect detection.},
 address = {Singapore},
 author = {Gong, Huimin
and Shen, Zongliang
and Zhang, Hua
and Qiao, Lei
and Wang, Huawei
and Zhang, Chi},
 booktitle = {Machine Learning for Cyber Security},
 editor = {Xiang, Yang
and Shen, Jian},
 isbn = {978-981-96-4566-4},
 pages = {184--196},
 publisher = {Springer Nature Singapore},
 title = {Construction of an AI Code Defect Detection and Repair Dataset Based on Chain of Thought},
 url = {https://link.springer.com/chapter/10.1007/978-981-96-4566-4_13},
 year = {2025}
}

@Article{Liu2025,
author="Liu, Bo
and Jiang, Yanjie
and Zhang, Yuxia
and Niu, Nan
and Li, Guangjie
and Liu, Hui",
title="Exploring the potential of general purpose LLMs in automated software refactoring: an empirical study",
journal="Automated Software Engineering",
year="2025",
month="Mar",
day="01",
volume="32",
number="1",
pages="26",
abstract="Software refactoring is an essential activity for improving the readability, maintainability, and reusability of software projects. To this end, a large number of automated or semi-automated approaches/tools have been proposed to locate poorly designed code, recommend refactoring solutions, and conduct specified refactorings. However, even equipped with such tools, it remains challenging for developers to decide where and what kind of refactorings should be applied. Recent advances in deep learning techniques, especially in large language models (LLMs), make it potentially feasible to automatically refactor source code with LLMs. However, it remains unclear how well LLMs perform compared to human experts in conducting refactorings automatically and accurately. To fill this gap, in this paper, we conduct an empirical study to investigate the potential of LLMs in automated software refactoring, focusing on the identification of refactoring opportunities and the recommendation of refactoring solutions. We first construct a high-quality refactoring dataset comprising 180 real-world refactorings from 20 projects, and conduct the empirical study on the dataset. With the to-be-refactored Java documents as input, ChatGPT and Gemini identified only 28 and 7 respectively out of the 180 refactoring opportunities. The evaluation results suggested that the performance of LLMs in identifying refactoring opportunities is generally low and remains an open problem. However, explaining the expected refactoring subcategories and narrowing the search space in the prompts substantially increased the success rate of ChatGPT from 15.6 to 86.7{\%}. Concerning the recommendation of refactoring solutions, ChatGPT recommended 176 refactoring solutions for the 180 refactorings, and 63.6{\%} of the recommended solutions were comparable to (even better than) those constructed by human experts. However, 13 out of the 176 solutions suggested by ChatGPT and 9 out of the 137 solutions suggested by Gemini were unsafe in that they either changed the functionality of the source code or introduced syntax errors, which indicate the risk of LLM-based refactoring.",
issn="1573-7535",
doi="10.1007/s10515-025-00500-0",
url="https://doi.org/10.1007/s10515-025-00500-0"
}


@inproceedings{10.1007/978-3-031-93598-5_16,
 abstract = {ELISAR represents a significant advancement in cybersecurity risk assessment through its novel modular architecture, which integrates Zero-shot Chain-of-Thought (CoT) prompting to train a local Large Language Model (LLM). This LLM not only excels in complex reasoning tasks but also capitalizes on the historical knowledge of past risk assessments stored in a secure, local environment, ensuring data privacy and confidentiality. By leveraging a Retrieval Augmented Generation (RAG) system, ELISAR enhances the precision of semantic data extraction from a vector database, improving decision-making for cybersecurity professionals. The combination of state-of-the-art Generative AI (GenAI), LLMs, and a focus on data confidentiality allows for the development of AI-driven assistants that preserve sensitive information while delivering superior performance in risk assessment use cases. Compared to traditional GenAI models like GPTs or AI copilots, ELISAR offers a more secure and effective solution tailored to the unique challenges of the cybersecurity field.},
 address = {Cham},
 author = {Allani, Sabri
and Bou-Chaaya, Karam
and Rais, Helmi},
 booktitle = {Management of Digital EcoSystems},
 editor = {Chbeir, Richard
and Damiani, Ernesto
and Dustdar, Schahram
and Manolopoulos, Yannis
and Masciari, Elio
and Pitoura, Evaggelia
and Rinaldi, Antonio},
 isbn = {978-3-031-93598-5},
 pages = {215--228},
 publisher = {Springer Nature Switzerland},
 title = {ELISAR: An Adaptive Framework for Cybersecurity Risk Assessment Powered by GenAI},
 url = {https://link.springer.com/chapter/10.1007/978-3-031-93598-5_16},
 year = {2025}
}

@inproceedings{10.1007/978-3-031-98465-5_48,
 abstract = {Large language models (LLMs) are transforming student learning, mirroring how search engines previously disrupted traditional information sources. This study compares these technologies through a within-subjects study where participants learned topics using both Google and ChatGPT, followed by interviews exploring their experiences. Our analysis reveals nuanced insights into students' strategic tool selection processes, showing how contextual factors influence when and why students prefer LLMs over search engines, with particular attention to emergent trust heuristics, epistemic challenges, and evolving notions of authorship and verification--with implications for education stakeholders navigating this technological shift.},
 address = {Cham},
 author = {Divekar, Rahul R.
and Guerra, Sophia
and Gonzalez, Lisette
and Boos, Natasha
and Zhou, Yalun},
 booktitle = {Artificial Intelligence in Education},
 editor = {Cristea, Alexandra I.
and Walker, Erin
and Lu, Yu
and Santos, Olga C.
and Isotani, Seiji},
 isbn = {978-3-031-98465-5},
 pages = {380--387},
 publisher = {Springer Nature Switzerland},
 title = {Exploring Undercurrents of Learning Tensions in an LLM-Enhanced Landscape: A Student-Centered Qualitative Perspective on LLM vs Search},
 url = {https://link.springer.com/chapter/10.1007/978-3-031-98465-5_48},
 year = {2025}
}

@Article{Aburakhia2024,
author="Aburakhia, Abdalrahman
and Alshayeb, Mohammad",
title="A Machine Learning Approach for Classifying the Default Bug Severity Level",
journal="Arabian Journal for Science and Engineering",
year="2024",
month="Sep",
day="01",
volume="49",
number="9",
pages="13131--13148",
abstract="Bug reports (BRs) play a major role in the software maintenance process; they alert developers about the bugs discovered by the end-users. Software applications utilize bug tracking systems (BTS) to manage submitted bug reports. Recent studies showed that the majority of BRs in BTS belong to the default severity category, which does not represent their actual severity. In this paper, we propose an approach that can automatically classify default bug reports into severe or non-severe categories. We curated a dataset based on the history of bug report logs. After that, we used the Support Vector Machine algorithm and Term Frequency-Inverse Document Frequency feature extraction method to classify default bug reports into severe or non-severe categories. The results show that building customized models for default severity bug reports provides better and more reliable results than training one model for all severity. Overall, the proposed Log model outperformed the three models (approaches) from the literature; it achieved an improvement of up to{\thinspace}{\textasciitilde}{\thinspace}4{\%} f-measure compared to others, and in some projects, it achieved an improvement of 11.2{\%} f-measure. Moreover, we investigated the impact of sentiment analysis on default bug severity prediction; the results show no noticeable influence.",
issn="2191-4281",
doi="10.1007/s13369-024-09081-8",
url="https://doi.org/10.1007/s13369-024-09081-8"
}


@Article{Le2024,
author="Le, Kim Tuyen
and Andrzejak, Artur",
title="Rethinking AI code generation: a one-shot correction approach based on user feedback",
journal="Automated Software Engineering",
year="2024",
month="Jul",
day="12",
volume="31",
number="2",
pages="60",
abstract="Code generation has become an integral feature of modern IDEs, gathering significant attention. Notable approaches like GitHub Copilot and TabNine have been proposed to tackle this task. However, these tools may shift code writing tasks towards code reviewing, which involves modification from users. Despite the advantages of user feedback, their responses remain transient and lack persistence across interaction sessions. This is attributed to the inherent characteristics of generative AI models, which require explicit re-training for new data integration. Additionally, the non-deterministic and unpredictable nature of AI-powered models limits thorough examination of their unforeseen behaviors. We propose a methodology named One-shot Correction to mitigate these issues in natural language to code translation models with no additional re-training. We utilize decomposition techniques to break down code translation into sub-problems. The final code is constructed using code snippets of each query chunk, extracted from user feedback or selectively generated from a generative model. Our evaluation indicates comparable or improved performance compared to other models. Moreover, the methodology offers straightforward and interpretable approaches, which enable in-depth examination of unexpected results and facilitate insights for potential enhancements. We also illustrate that user feedback can substantially improve code translation models without re-training. Ultimately, we develop a preliminary GUI application to demonstrate the utility of our methodology in simplifying customization and assessment of suggested code for users.",
issn="1573-7535",
doi="10.1007/s10515-024-00451-y",
url="https://doi.org/10.1007/s10515-024-00451-y"
}


@Article{Förster2025,
author="F{\"o}rster, Susanne
and Skop, Yarden",
title="Between fact and fairy: tracing the hallucination metaphor in AI discourse",
journal="AI {\&} SOCIETY",
year="2025",
month="May",
day="26",
abstract="Large and powerful language models such as OpenAI's GPT model family, Google's LaMDA and BERT or Meta's LlaMA are integral to many applications, such as translation, summarization or language generation. They have become an inherent part of current everyday activities and working practices. These models produce and process language in an impressively convincing human-like manner, but also repeatedly generate outputs that appear untrustworthy and factually incorrect. In computer science (Ji et al. 2023) and popular discourse alike this phenomenon is called hallucination. The term is used broadly to describe various forms of untruthfulness, from factual errors to inconsistencies between prompt and output. This article discusses the hallucination metaphor guided by STS perspectives and takes software documentation as its main corpus of analysis. We examine model papers and documentation from leading tech companies to trace the hallucination metaphor and the discursive work it does. We claim that tech companies anthropomorphize the models, relieving them and themselves from responsibility over non-factual outputs by normalizing the use of the metaphor. Models are relegated to two main positions: either a learning child that needs time to develop or an illogic agent, a position that we connect to cultural scripts of madness.",
issn="1435-5655",
doi="10.1007/s00146-025-02392-w",
url="https://doi.org/10.1007/s00146-025-02392-w"
}


@inproceedings{10.1007/978-3-031-48796-5_12,
 abstract = {Large language models (LLMs) have recently been integrated in a variety of applications including software engineering tasks. In this work, we study the use of LLMs to enhance the explainability of software patches. In particular, we evaluate the performance of GPT 3.5 in explaining patches generated by the search-based automated program repair system ARJA-e for 30 bugs from the popular Defects4J benchmark. We also investigate the performance achieved when explaining the corresponding patches written by software developers. We find that on average 84{\%} of the LLM explanations for machine-generated patches were correct and 54{\%} were complete for the studied categories in at least 1 out of 3 runs. Furthermore, we find that the LLM generates more accurate explanations for machine-generated patches than for human-written ones.},
 address = {Cham},
 author = {Sobania, Dominik
and Geiger, Alina
and Callan, James
and Brownlee, Alexander
and Hanna, Carol
and Moussa, Rebecca
and L{\'o}pez, Mar Zamorano
and Petke, Justyna
and Sarro, Federica},
 booktitle = {Search-Based Software Engineering},
 editor = {Arcaini, Paolo
and Yue, Tao
and Fredericks, Erik M.},
 isbn = {978-3-031-48796-5},
 pages = {147--152},
 publisher = {Springer Nature Switzerland},
 title = {Evaluating Explanations for Software Patches Generated by Large Language Models},
 url = {https://link.springer.com/chapter/10.1007/978-3-031-48796-5_12},
 year = {2024}
}

@inproceedings{10.1007/978-3-032-05188-2_15,
 abstract = {Effective cybersecurity testing relies on accurate threat identification to guide test design and risk mitigation. Threat modelling plays a central role in this process by helping analysts anticipate potential vulnerabilities. However, traditional threat modelling is a manual, time-consuming task that requires significant expertise, which can limit its scalability and integration into modern testing workflows.},
 address = {Cham},
 author = {Zelenskiy, Leonid
and Sadovykh, Andrey},
 booktitle = {Testing Software and Systems},
 editor = {Bonfanti, Silvia
and Papadopoulos, George Angelos},
 isbn = {978-3-032-05188-2},
 pages = {231--247},
 publisher = {Springer Nature Switzerland},
 title = {Extracting Threats from System Descriptions with LLMs Comparing One and Two Agents Strategies},
 url = {https://link.springer.com/chapter/10.1007/978-3-032-05188-2_15},
 year = {2026}
}

@Article{Zhao2026,
author="Zhao, Penghao
and Zhang, Hailin
and Yu, Qinhan
and Wang, Zhengren
and Geng, Yunteng
and Fu, Fangcheng
and Yang, Ling
and Zhang, Wentao
and Jiang, Jie
and Cui, Bin",
title="Retrieval-Augmented Generation for AI-Generated Content: A Survey",
journal="Data Science and Engineering",
year="2026",
month="Jan",
day="02",
abstract="Advancements in model algorithms, the growth of foundational models, and access to high-quality datasets have propelled the evolution of Artificial Intelligence Generated Content (AIGC). Despite its notable successes, AIGC still faces hurdles such as updating knowledge, handling long-tail data, mitigating data leakage, and managing high training and inference costs. Retrieval-augmented generation (RAG) has recently emerged as a paradigm to address such challenges. In particular, RAG introduces the information retrieval process, which enhances the generation process by retrieving relevant objects from available data stores, leading to higher accuracy and better robustness. In this paper, we comprehensively review existing efforts that integrate RAG techniques into AIGC scenarios. We first classify RAG foundations according to how the retriever augments the generator, distilling the fundamental abstractions of the augmentation methodologies for various retrievers and generators. This unified perspective encompasses all RAG scenarios, illuminating advancements and pivotal technologies that help with potential future progress. We also summarize additional enhancement methods for RAG, facilitating effective engineering and implementation of RAG systems. Then from another view, we survey practical applications of RAG across different modalities and tasks, offering valuable references for researchers and practitioners. Furthermore, we introduce the benchmarks for RAG, discuss the limitations of current RAG systems, and suggest potential directions for future research.",
issn="2364-1541",
doi="10.1007/s41019-025-00335-5",
url="https://doi.org/10.1007/s41019-025-00335-5"
}


@inproceedings{10.1007/978-981-95-4875-0_16,
 abstract = {Electric vehicle maintenance standards constitute a key knowledge base for understanding the technical specifications, operational procedures, safety requirements, and quality evaluation systems of electric vehicle maintenance. To systematically reveal the standardization status, technical priorities, and development trends in electric vehicle maintenance field, this study focuses on maintenance standard texts and conducts an in-depth analysis of 35 electric vehicle-related standards published in China from 2015 to the present using text mining methods. First, natural language processing techniques are adopted to preprocess the collected electric vehicle maintenance standard text data. Then, term frequency statistics and TF-IDF keyword extraction are performed to reveal the core concerns of the standard texts, such as energy technologies, system engineering and electrical safety. Simultaneously, large language models are utilized to conduct text segmentation, topic sentence identification, semantic compression, and summary optimization of the standard texts, achieving efficient concentration of standard content. Furthermore, this study employs the Latent Dirichlet Allocation model to deeply explore the core modules and technological evolution trends in the current electric vehicle maintenance standards field. Through text mining methods applied to electric vehicle maintenance standards, enterprises can enhance maintenance service quality based on standards while better grasping the key priorities and trends of industry development.},
 address = {Singapore},
 author = {Mao, Yuxin
and Fang, Lei
and Sun, Zhaoyang
and Zong, Qi},
 booktitle = {Intelligent Vehicles},
 editor = {Li, Huiyun
and Wang, Zhongli
and Zhao, Shuai
and Sun, Peng
and Herrmann, Michael
and Zheng, Xi
and Liu, Yuling},
 isbn = {978-981-95-4875-0},
 pages = {202--214},
 publisher = {Springer Nature Singapore},
 title = {Integrating LLM and LDA for Text Mining of Electric Vehicle Maintenance Standards},
 url = {https://link.springer.com/chapter/10.1007/978-981-95-4875-0_16},
 year = {2026}
}

@inproceedings{10.1007/978-3-031-76335-9_10,
 abstract = {Identifying and annotating student use of debugging strategies when solving computer programming problems can be a meaningful tool for studying and better understanding the development of debugging skills, which may lead to the design of effective pedagogical interventions. However, this process can be challenging when dealing with large datasets, especially when the strategies of interest are rare but important. This difficulty lies not only in the scale of the dataset but also in operationalizing these rare phenomena within the data. Operationalization requires annotators to first define how these rare phenomena manifest in the data and then obtain a sufficient number of positive examples to validate that this definition is reliable by accurately measuring Inter-Rater Reliability (IRR). This paper presents a method that leverages Large Language Models (LLMs) to efficiently exclude computer programming episodes that are unlikely to exhibit a specific debugging strategy. By using LLMs to filter out irrelevant programming episodes, this method focuses human annotation efforts on the most pertinent parts of the dataset, enabling experts to operationalize the coding scheme and reach IRR more efficiently.},
 address = {Cham},
 author = {Fan, Aysa Xuemo
and Liu, Qianhui
and Paquette, Luc
and Pinto, Juan},
 booktitle = {Advances in Quantitative Ethnography},
 editor = {Kim, Yoon Jeon
and Swiecki, Zachari},
 isbn = {978-3-031-76335-9},
 pages = {136--151},
 publisher = {Springer Nature Switzerland},
 title = {Using LLM-Based Filtering to Develop Reliable Coding Schemes for Rare Debugging Strategies},
 url = {https://link.springer.com/chapter/10.1007/978-3-031-76335-9_10},
 year = {2024}
}

@inproceedings{10.1007/978-3-032-09044-7_9,
 abstract = {Automated program repair (APR) research predominantly focuses on Python environments, creating significant infrastructure gaps for compiled languages like C, C++, and Java that dominate production systems. We present the first systematic pipeline addressing multi-language APR infrastructure limitations through compiler-assisted dataset curation and paradigm-aware evaluation frameworks. Our approach combines a DFA-based code classification system achieving 92.4{\%} accuracy in programming paradigm detection with systematic dataset filtering that processes over 3 million samples to extract 30,000 high-quality object-oriented examples. Initial evaluation on Qwen3-14B using LoRA fine-tuning reveals critical adaptation thresholds: effective multi-language adaptation requires modification of approximately 1.2{\%} or more model parameters, with lighter fine-tuning underperforming baseline models. Our open-source pipeline provides end-to-end infrastructure from compiler-assisted dataset curation to cloud deployment, enabling systematic research advancement in multi-language automated program repair and establishing methodological foundations for compiler-assisted machine learning across diverse programming environments.},
 address = {Cham},
 author = {Pineda, Moises
and Luna, Diego
and Esquivel, Mariana
and Bours, Jes{\'u}s
and Salazar, Juan
and Flores-Araiza, Dainel
and Hinojosa, Salvador},
 booktitle = {Advances in Soft Computing},
 editor = {Mart{\'i}nez-Villase{\~{n}}or, Lourdes
and V{\'a}zquez, Roberto A.
and Ochoa-Ruiz, Gilberto},
 isbn = {978-3-032-09044-7},
 pages = {115--127},
 publisher = {Springer Nature Switzerland},
 title = {Beyond SWE-Bench: A Compiler-Assisted Pipeline for Multi-language Automated Program Repair},
 url = {https://link.springer.com/chapter/10.1007/978-3-032-09044-7_9},
 year = {2026}
}

@inproceedings{10.1007/978-3-031-75387-9_15,
 abstract = {Recent investigations hint at the ability of large language models (LLMs) to generate formal specifications for given program code. In this work, we systematically discuss and categorize different use cases and application scenarios that combine specification synthesis via LLMs with deductive program verification. We present preliminary quantitative experiments on the capabilities of LLMs to generate correct specifications. To this end, we use a prototypical integration of GPT (versions 3.5 and 4o) with the deductive program verifier KeY and the bounded model checker JJBMC. We evaluated our prototype on a set of Java programs that are partially annotated with specifications written in the Java Modeling Language (JML). We show that GPT 4o generates correct annotations in approximately half of all instances across the investigated scenarios. For the case of faulty specifications, we investigate how a feedback loop can help to improve the original answer. Finally, we present a vision of how Large Language Models may support rigorous formal verification of software systems and describe the necessary next steps in this direction.},
 address = {Cham},
 author = {Beckert, Bernhard
and Klamroth, Jonas
and Pfeifer, Wolfram
and R{\"o}per, Patrick
and Teuber, Samuel},
 booktitle = {Leveraging Applications of Formal Methods, Verification and Validation. Software Engineering Methodologies},
 editor = {Margaria, Tiziana
and Steffen, Bernhard},
 isbn = {978-3-031-75387-9},
 pages = {242--257},
 publisher = {Springer Nature Switzerland},
 title = {Towards Combining the Cognitive Abilities of Large Language Models with the Rigor of Deductive Progam Verification},
 url = {https://link.springer.com/chapter/10.1007/978-3-031-75387-9_15},
 year = {2025}
}

@Inbook{Vorel2025,
author="Vorel, Roman",
title="AI-Orchestrated CI/CD and Pipeline Optimization",
bookTitle="NoOps: How AI Agents Are Reinventing DevOps and Software",
year="2025",
publisher="Apress",
address="Berkeley, CA",
pages="149--180",
abstract="We've seen how generative AI can revolutionize coding, testing, and infrastructure provisioning. Now, we turn to the central nervous system of DevOps: the continuous integration/continuous delivery (CI/CD) pipeline. A well-designed pipeline automates everything from build to deployment, but even robust pipelines can suffer from bottlenecks, flaky tests, slow feedback loops, or under-optimized release strategies.",
isbn="979-8-8688-1694-9",
doi="10.1007/979-8-8688-1694-9_9",
url="https://doi.org/10.1007/979-8-8688-1694-9_9"
}


@inproceedings{10.1007/978-3-031-64626-3_22,
 abstract = {Configurable Program Analysis (CPA) allows users to customize program analysis based on their preferences. However, current program verification tools like Cpachecker require manual strategy selection, which can be complex and error-prone. In this paper, we present a novel approach to efficiently perform program verification tasks by harnessing the capabilities of Large Language Models (LLMs) to automatically select verification strategies based on code features and specifications. Specifically, we begin by extracting relevant code snippets and querying LLMs to identify code features. Based on the identified code features, we propose a strategy selector to automatically choose the verification strategy. Finally, we execute the Cpachecker with the selected verification strategy. We evaluated our approach using a diverse set of 600 verification tasks. The results demonstrate the effectiveness of our approach, surpassing basic strategies and SOTA combination strategies while also standing out for its simplicity and ease of understanding.},
 address = {Cham},
 author = {Su, Jie
and Deng, Liansai
and Wen, Cheng
and Qin, Shengchao
and Tian, Cong},
 booktitle = {Theoretical Aspects of Software Engineering},
 editor = {Chin, Wei-Ngan
and Xu, Zhiwu},
 isbn = {978-3-031-64626-3},
 pages = {374--391},
 publisher = {Springer Nature Switzerland},
 title = {CFStra: Enhancing Configurable Program Analysis Through LLM-Driven Strategy Selection Based on Code Features},
 url = {https://link.springer.com/chapter/10.1007/978-3-031-64626-3_22},
 year = {2024}
}

@Article{Millière2025,
author="Milli{\`e}re, Rapha{\"e}l",
title="Normative conflicts and shallow AI alignment",
journal="Philosophical Studies",
year="2025",
month="Jul",
day="01",
volume="182",
number="7",
pages="2035--2078",
abstract="The progress of AI systems such as large language models (LLMs) raises increasingly pressing concerns about their safe deployment. This paper examines the value alignment problem for LLMs, arguing that current alignment strategies are fundamentally inadequate to prevent misuse. Despite ongoing efforts to instill norms such as helpfulness, honesty, and harmlessness in LLMs through fine-tuning based on human preferences, they remain vulnerable to adversarial attacks that exploit conflicts between these norms. I argue that this vulnerability reflects a fundamental limitation of existing alignment methods: they reinforce shallow behavioral dispositions rather than endowing LLMs with a genuine capacity for normative deliberation. Drawing from on research in moral psychology, I show how humans' ability to engage in deliberative reasoning enhances their resilience against similar adversarial tactics. LLMs, by contrast, lack a robust capacity to detect and rationally resolve normative conflicts, leaving them susceptible to manipulation; even recent advances in reasoning-focused LLMs have not addressed this vulnerability. This ``shallow alignment'' problem carries significant implications for AI safety and regulation, suggesting that current approaches are insufficient for mitigating potential harms posed by increasingly capable AI systems.",
issn="1573-0883",
doi="10.1007/s11098-025-02347-3",
url="https://doi.org/10.1007/s11098-025-02347-3"
}


@Article{Haque2025,
author="Haque, Radowanul
and Ali, Aftab
and McClean, Sally
and Khan, Naveed",
title="A zero-shot framework for cross-project vulnerability detection in source code",
journal="Empirical Software Engineering",
year="2025",
month="Oct",
day="29",
volume="31",
number="1",
pages="3",
abstract="The growing prevalence of software vulnerabilities has increased the need for effective detection methods, particularly in cross-project settings where domain differences create significant challenges. Existing vulnerability detection models often struggle to generalise across projects due to variations in coding styles, feature distributions, and the absence of labelled target data. This paper presents ZSVulD, a zero-shot, cross-project vulnerability detection framework designed to operate without target-domain labels. ZSVulD uses domain-agnostic CodeBERT embeddings to capture both syntactic and semantic features of source code, enabling knowledge transfer between projects. The framework applies an iterative pseudo-labelling process in which a neural network and XGBoost classifier collaboratively refine predictions for the target domain. Feature alignment is incorporated as a diagnostic technique to assess and visualise distributional differences between source and target datasets. Experiments on the Devign and REVEAL datasets show that ZSVulD achieves higher recall, F1, and F2 scores compared to existing methods, with an emphasis on reducing false negatives. These findings indicate that ZSVulD can support automated vulnerability detection pipelines, contributing to more reliable security assessments across different software projects.",
issn="1573-7616",
doi="10.1007/s10664-025-10749-4",
url="https://doi.org/10.1007/s10664-025-10749-4"
}


@inproceedings{10.1007/978-3-032-08977-9_25,
 abstract = {For market-based software evolution, user feedback has become a primary source utilized by various machine learning (ML) algorithms to identify insightful information. However, it heavily employs human subjects to complete such experimental tasks, particularly for data annotation. It is reported in software engineering (SE) literature that human subjects are challenging to find, prone to errors, and can have a second guess in identifying the correct annotation type, resulting in possible bias. In contrast, large language models (LLMs) have recently demonstrated comparatively equal or better performances in various complex SE tasks, making them a good alternative for data annotation tasks. For this purpose, the proposed approach investigated and experimented with the performance of LLMs, particularly ChatGPT, to annotate end-user feedback for ML classification tasks. We experimented with two datasets, i.e., human and ChatGPT API annotated, to explore whether ChatGPT can be used as an alternative to human annotators when preparing labeled datasets for ML experiments. For this purpose, we identify the efficacy of various deep learning (DL) classifiers in detecting associated emotions, including anger, confusion, distrust, sadness, disappointment, frustration, disgust, and fear, with end-user reviews. We obtained satisfactory results with BILSTM, GRU, CNN, LSTM, BiGRU, and RNN algorithms using the ChatGPT-generated dataset compared to the human-annotated data set. We obtained an average accuracy of 92{\%}, 92{\%}, 91{\%}, 90{\%}, 91{\%}, and 91{\%} compared to the manually annotated data set, 75{\%}, 75{\%}, 79{\%},48{\%}, 73{\%}, and 85{\%}, with CNN, LSTM, BILSTM, GRU, BiGRU, and RNN Classifiers, respectively. The study results show that LLMs can be an alternative source for annotating datasets for ML classification experiments. However, the results need to be validated by human experts for improved generalizability and trust.},
 address = {Cham},
 author = {Khan, Nek Dil
and Almufareh, Maram Fahaad
and Khan, Javed Ali
and Li, Jianqiang
and Khan, Arif Ali
and Humayun, Mamoona},
 booktitle = {SEET---Software Engineering for Emerging Technologies},
 editor = {Hussain, Shahid
and Khan, Arif Ali
and Abdul Basit Ur Rahim, Muhammad
and Khan, Saif Ur Rehman},
 isbn = {978-3-032-08977-9},
 pages = {400--416},
 publisher = {Springer Nature Switzerland},
 title = {Can Large Language Models be Used as an Alternative for Human Annotation: A Case Study of Emotion Classification},
 url = {https://link.springer.com/chapter/10.1007/978-3-032-08977-9_25},
 year = {2026}
}

@Inbook{Huang2025,
author="Huang, Ken
and Hughes, Chris",
title="Agentic AI Red Teaming",
bookTitle="Securing AI Agents: Foundations, Frameworks, and Real-World Deployment",
year="2025",
publisher="Springer Nature Switzerland",
address="Cham",
pages="207--252",
abstract="This chapter provides a comprehensive and practical framework for Agentic AI Red Teaming, designed to equip security professionals and AI developers with the tactical skills needed to assess and secure these advanced systems. Grounded in the 12 critical threat categories identified by the Cloud Security Alliance (CSA) and OWASP AI Exchange, this chapter delivers a deep, tactical analysis of each category, complete with detailed attack scenarios and illustrative code examples. Key areas of focus include exploiting agentic tool use, manipulating goal setting and planning logic, poisoning memory and knowledge bases, and executing attacks against multi-agent orchestration protocols. By providing actionable techniques, real-world case study analysis, and a robust ethical framework, this chapter serves as an practical playbook for proactively identifying and mitigating the novel vulnerabilities inherent in Agentic AI, fostering the development of more secure, resilient, and trustworthy autonomous systems.",
isbn="978-3-032-02130-4",
doi="10.1007/978-3-032-02130-4_8",
url="https://doi.org/10.1007/978-3-032-02130-4_8"
}


@inproceedings{10.1007/978-3-032-00630-1_2,
 abstract = {The growing and evolving landscape of cybersecurity threats necessitates the development of supporting tools and platforms that allow for the creation of realistic IT environments operating within virtual, controlled settings as Cyber Ranges (CRs). CRs can be exploited for analyzing vulnerabilities and experimenting with the effectiveness of devised countermeasures, as well as serving as training environments for building cyber security skills and abilities for IT operators. This paper proposes ARCeR as an innovative solution for the automatic definition and deployment of CRs, starting from user-provided descriptions in a natural language. ARCeR relies on the Agentic RAG paradigm, which allows it to fully exploit state-of-art AI technologies. Experimental results show that ARCeR is able to successfully process prompts even in cases that LLMs or basic RAG systems are not able to cope with. Furthermore, ARCeR is able to target any CR framework provided that specific knowledge is made available to it.},
 address = {Cham},
 author = {Lupinacci, Matteo
and Blefari, Francesco
and Romeo, Francesco
and Pironti, Francesco Aurelio
and Furfaro, Angelo},
 booktitle = {Availability, Reliability and Security},
 editor = {Coppens, Bart
and Volckaert, Bruno
and Naessens, Vincent
and De Sutter, Bjorn},
 isbn = {978-3-032-00630-1},
 pages = {23--40},
 publisher = {Springer Nature Switzerland},
 title = {ARCeR: An Agentic RAG for the Automated Definition of Cyber Ranges},
 url = {https://link.springer.com/chapter/10.1007/978-3-032-00630-1_2},
 year = {2025}
}

@Article{Liu2025,
author="Liu, Yang
and Foundjem, Armstrong
and Khomh, Foutse
and Li, Heng",
title="Adversarial attack classification and robustness testing for large language models for code",
journal="Empirical Software Engineering",
year="2025",
month="Aug",
day="11",
volume="30",
number="5",
pages="154",
abstract="In the rapidly evolving landscape of software development, Large Language models have become essential tools for code tasks such as code generation, completion, analysis, and suggestion. The growing integration of Large Language models for Code into development workflows highlights the critical need for ensuring robustness against potential vulnerabilities--specifically, weaknesses in how these models handle various inputs that could lead to incorrect or insecure code completion. In this context, vulnerabilities can manifest as the model's susceptibility to producing flawed code when faced with adversarial inputs or subtle perturbations in task descriptions, code, or comments.",
issn="1573-7616",
doi="10.1007/s10664-025-10693-3",
url="https://doi.org/10.1007/s10664-025-10693-3"
}


@Article{LaProva2025,
author="La Prova, Daniele
and Gentili, Emanuele
and Falessi, Davide",
title="Anticipating bugs: Ticket-level bug prediction and temporal proximity effects",
journal="Empirical Software Engineering",
year="2025",
month="Dec",
day="15",
volume="31",
number="2",
pages="43",
abstract="Software bugs significantly impact project time, budgets, and safety, motivating extensive research in bug prediction. The primary goal of bug prediction is to optimize testing efforts by focusing on software fragments, i.e., classes, methods, commits (i.e., Just-In-Time or JIT), or lines of code, most likely to be buggy. However, these predictions are made only after defects have already been introduced. Thus, the current bug prediction approaches support fixing rather than prevention. Motivated by the principle of ``prevention is better than cure,'' the aim of this paper is to introduce and evaluate Ticket-Level Prediction (TLP), an approach to identify tickets that will introduce bugs once implemented. We analyze TLP at three temporal points, each point represents a ticket lifecycle stage: Open, In Progress, or Closed. We conjecture that: (1) TLP accuracy increases as tickets progress towards the closed stage due to improved feature reliability over time, and (2) the predictive power of features changes across these temporal points. Our TLP approach leverages 72 features belonging to seven different families: code, developer, external temperature, internal temperature, intrinsic, ticket to tickets, and JIT. Our TLP evaluation uses a sliding-window approach, balancing feature selection and three machine-learning bug prediction classifiers on about 10,000 tickets of two Apache open-source projects. Our results show that TLP accuracy increases with proximity, con- firming the expected trade-off between early prediction and accuracy. Regarding the prediction power of feature families, no single feature family dominates across stages; developer-centric signals are most informative early, whereas code and JIT metrics prevail near closure, and temperature-based features provide complementary value throughout. Our findings complement and extend the literature on bug prediction at the class, method, or commit level by showing that defect predic- tion can be effectively moved upstream, offering opportunities for risk-aware ticket triaging and developer assignment before any code is written.",
issn="1573-7616",
doi="10.1007/s10664-025-10771-6",
url="https://doi.org/10.1007/s10664-025-10771-6"
}


@Article{Park2024,
author="Park, Jungmin
and Lee, Younghoon",
title="Key attribute generation from review texts based on in-context learning for recommender systems",
journal="Applied Intelligence",
year="2024",
month="Oct",
day="01",
volume="54",
number="20",
pages="10194--10205",
abstract="User review texts provide valuable information for recommender systems, as they express various dimensions and perspectives regarding the experience of a user with a specific item. Consequently, many studies have proposed recommender systems based on review texts. However, because review texts typically contain a high proportion of noise that is not related to user preferences or item characteristics, existing studies that input the entire review text into the model are vulnerable to noise issues. Therefore, this study proposes a methodology for extracting key attributes based on in-context learning(ICL) to fundamentally address the noise problem in review texts. We used zero-shot, one-shot, and few-shot large language model (LLM) ICL to generate key attributes that define user preferences and item characteristics from integrated review texts, and we trained a recommender system to predict user ratings on items using the generated key attributes as new input. Our proposed research is the first to create and utilize new user and item characteristics through LLM ICL for a recommender system. Experiments demonstrate that our methodology effectively generates key attributes related to user preferences and item characteristics from review texts and achieves superior predictive performance compared to existing review-based recommender systems.",
issn="1573-7497",
doi="10.1007/s10489-024-05698-2",
url="https://doi.org/10.1007/s10489-024-05698-2"
}


@inproceedings{10.1007/978-3-031-64302-6_19,
 abstract = {Large Language Models (LLMs) now excel at generative skills and can create content at impeccable speeds. However, they are imperfect and still make various mistakes. In a Computer Science education context, as these models are widely recognized as ``AI pair programmers,'' it becomes increasingly important to train students on evaluating and debugging the LLM-generated code. In this work, we introduce HypoCompass, a novel system to facilitate deliberate practice on debugging, where human novices play the role of Teaching Assistants and help LLM-powered teachable agents debug code. We enable effective task delegation between students and LLMs in this learning-by-teaching environment: students focus on hypothesizing the cause of code errors, while adjacent skills like code completion are offloaded to LLM-agents. Our evaluations demonstrate that HypoCompass generates high-quality training materials (e.g., bugs and fixes), outperforming human counterparts fourfold in efficiency, and significantly improves student performance on debugging by 12{\%} in the pre-to-post test.},
 address = {Cham},
 author = {Ma, Qianou
and Shen, Hua
and Koedinger, Kenneth
and Wu, Sherry Tongshuang},
 booktitle = {Artificial Intelligence in Education},
 editor = {Olney, Andrew M.
and Chounta, Irene-Angelica
and Liu, Zitao
and Santos, Olga C.
and Bittencourt, Ig Ibert},
 isbn = {978-3-031-64302-6},
 pages = {265--279},
 publisher = {Springer Nature Switzerland},
 title = {How to Teach Programming in the AI Era? Using LLMs as a Teachable Agent for Debugging},
 url = {https://link.springer.com/chapter/10.1007/978-3-031-64302-6_19},
 year = {2024}
}

@Article{Liu2025,
author="Liu, Lu
and Wei, Lili
and Zhang, Wuqi
and Li, Shuqing
and Zhou, Yifan
and Liu, Yepang
and Cheung, Shing-Chi
and Lyu, Michael R.",
title="On state reverting in solidity smart contracts: Developer practices, fault categorization, and tool evaluation",
journal="Empirical Software Engineering",
year="2025",
month="Jul",
day="17",
volume="30",
number="5",
pages="141",
abstract="Smart contracts are computer programs deployed on blockchains to facilitate transactions. A critical aspect of smart contract security is the use of state-reverting statements (e.g., require, if...revert, if...throw). These statements protect transactions from abnormal behaviors or malicious attacks by reverting a contract to its previous state when certain input constraints or security properties are violated. While essential, the correct use of these state-reverting (SR) statements is nontrivial. Improper use can lead to security vulnerabilities, resulting in substantial financial losses or other severe consequences. It is, therefore, highly important to understand developers' practices of state reverting in smart contracts and the common mistakes they make. To achieve this goal, we conduct the first comprehensive empirical study on the use of SR statements and their related faults in Solidity smart contracts. First, we analyze the prevalence and purposes of SR statements in 21,414 verified contracts from popular decentralized applications (dapps) and manually examine 381 SR statements, leading to a taxonomy of their uses. Second, we collect 320 real-world state-reverting faults (SR faults) from open-source projects on GitHub and audit reports on Code4rena. We categorize the SR faults into 17 types and summarize 12 distinct fixing strategies. This knowledge can help researchers and practitioners to better understand the common usages of SR statements and learn how to prevent or cope with SR faults. Lastly, the variety of SR fault types and the presence of high-risk issues highlight the need for automated tools to identify and mitigate these faults. This further motivates us to assess the SR fault detection performance of state-of-the-art security analyzers, with the aim of understanding their capability and identifying their deficiencies. Via evaluating 12 representative tools on a benchmark comprising 243 contracts with six types of SR faults and the corresponding patched versions, we observe that existing tools exhibit limited capabilities in detecting SR faults (the average detection rate is 14.4{\%}). This result underscores the need for more advanced security analysis tools specifically tailored for SR faults. To facilitate the development of such tools, we further provide a comprehensive analysis of three common limitations of existing tools.",
issn="1573-7616",
doi="10.1007/s10664-025-10685-3",
url="https://doi.org/10.1007/s10664-025-10685-3"
}


@Inbook{Turtiainen2026,
author="Turtiainen, Hannu
and Costin, Andrei
and H{\"a}m{\"a}l{\"a}inen, Timo",
editor="Lehto, Martti
and Neittaanm{\"a}ki, Pekka",
title="VulnBERTa-XAI: Towards Explainable AI for Automating CWE Weakness Assignment and Improving the Quality of Cybersecurity CVE",
bookTitle="Cyber Security: Policy and Technology",
year="2026",
publisher="Springer Nature Switzerland",
address="Cham",
pages="385--432",
abstract="Vulnerability management is crucial for companies with compliance requirements and regulations. The goal is to allocate the most appropriate resources to address vulnerabilities efficiently. The growing number of vulnerabilities discovered by various contributors leads to reports of varying quality and differing perspectives. To address this challenge, machine learning (ML) has shown potential in automating vulnerability assignments. Nevertheless, there remains room for further development. Additionally, more research is needed to explore how the specific terminology used in vulnerability databases and reports affects ML performance. In this chapter, we aim to address several of these gaps. First, we present a systematic approach leveraging the RoBERTa transformer architecture to automatically assign the information related to Common Weakness Enumeration (CWE) to vulnerability descriptions. Second, we apply our models to retroactively and automatically assign CWEs to unassigned entries in the National Institute of Standards and Technology (NIST) National Vulnerability Database (NVD), enhancing the quality of open data. Finally, we utilize the attention mechanism within the transformer architecture to identify common keywords in the vulnerability descriptions in the NIST NVD. Our results are comparable to the state-of-the-art while achieving greater classification granularity and scalability.",
isbn="978-3-032-08890-1",
doi="10.1007/978-3-032-08890-1_16",
url="https://doi.org/10.1007/978-3-032-08890-1_16"
}


@inproceedings{10.1007/978-3-031-76554-4_19,
 abstract = {We investigate how combinations of Large Language Models (LLMs) and symbolic analyses can be used to synthesise specifications of C programs. The LLM prompts are augmented with outputs from two formal methods tools in the Frama-C ecosystem, Pathcrawler and EVA, to produce C program annotations in the specification language ACSL. We demonstrate how the addition of symbolic analysis to the workflow impacts the quality of annotations: information about input/output examples from Pathcrawler produce more context-aware annotations, while the inclusion of EVA reports yields annotations more attuned to runtime errors. In addition, we show that the method infers the programs intent, rather than its behaviour, by generating specifications for buggy programs and observing robustness of the result against bugs.},
 address = {Cham},
 author = {Granberry, George
and Ahrendt, Wolfgang
and Johansson, Moa},
 booktitle = {Integrated Formal Methods},
 editor = {Kosmatov, Nikolai
and Kov{\'a}cs, Laura},
 isbn = {978-3-031-76554-4},
 pages = {307--325},
 publisher = {Springer Nature Switzerland},
 title = {Specify What? Enhancing Neural Specification Synthesis by Symbolic Methods},
 url = {https://link.springer.com/chapter/10.1007/978-3-031-76554-4_19},
 year = {2025}
}

@inproceedings{10.1007/978-981-96-9958-2_34,
 abstract = {Binary code similarity detection (BCSD) is critical in enhancing software security. Recent research indicates that deep learning approaches have achieved notable advancements. However, existing methods lack interpretability. They typically provide only a direct similarity score between two functions, without any explanatory rationale. Although large-scale models can provide result explanations, their deployment is costly. Online usage consumes tokens through API and risks exposing sensitive corporate data. Alternatively, local deployment requires expensive hardware resources. To address this issue, we employ LoRA to fine-tune a model, named Fine-Tune Model (FTM), only 33B, which can generate similarity labels and explanations for similarity analysis. These interpretative analyses play an important role in the analysis and repair of potential vulnerabilities for security personnel. Our approach consists of three steps. First, we use Deepseek-Chat to generate an interpretable dataset for model fine-tuning. Second, we perform LoRA-based fine-tuning on the Deepseek-Coder-33B-base using the interpretable dataset, obtaining a domain-specific model named FTM. Finally, users can input the assembly code of two functions into FTM. Combined with our carefully designed prompt, FTM predicts similarity labels and provides an interpretable analysis. Experimental results indicate that FTM performs similarly to Deepseek-Chat in BCSD, validating the successful distillation of the model. FTM outperforms all state-of-the-art (SOTA) methods in terms of AUC. In the vulnerability search task, FTM successfully predicted 7 similarity labels out of 10 cases.},
 address = {Singapore},
 author = {Li, Yanlin
and Wang, Taiyan
and Yu, Lu
and Chen, Qiangpu
and Li, Yuwei
and Pan, Zulie},
 booktitle = {Advanced Intelligent Computing Technology and Applications},
 editor = {Huang, De-Shuang
and Chen, Wei
and Pan, Yijie
and Chen, Haiming},
 isbn = {978-981-96-9958-2},
 pages = {415--430},
 publisher = {Springer Nature Singapore},
 title = {Balancing Interpretability and Cost in Binary Code Similarity Detection by LLM Distillation},
 url = {https://link.springer.com/chapter/10.1007/978-981-96-9958-2_34},
 year = {2025}
}

@inproceedings{10.1007/978-3-031-80889-0_17,
 abstract = {Large language models (LLMs) based on transformer architecture have revolutionized natural language processing (NLP), demonstrating excellent capabilities in understanding and generating human-like text. In Software Engineering, LLMs have been applied in code generation, documentation, and report writing tasks, to support the developer and reduce the amount of manual work. In Software Testing, one of the cornerstones of Software Engineering, LLMs have been explored for generating test code, test inputs, automating the oracle process or generating test scenarios. However, their application to high-level testing stages such as system testing, in which a deep knowledge of the business and the technological stack is needed, remains largely unexplored. This paper presents an exploratory study about how LLMs can support system test development. Given that LLM performance depends on input data quality, the study focuses on how to query general purpose LLMs to first obtain test scenarios and then derive test cases from them. The study evaluates two popular LLMs (GPT-4o and GPT-4o-mini), using as a benchmark a European project demonstrator. The study compares two different prompt strategies and employs well-established prompt patterns, showing promising results as well as room for improvement in the application of LLMs to support system testing.},
 address = {Cham},
 author = {Augusto, Cristian
and Mor{\'a}n, Jes{\'u}s
and Bertolino, Antonia
and de la Riva, Claudio
and Tuya, Javier},
 booktitle = {Testing Software and Systems},
 editor = {Men{\'e}ndez, H{\'e}ctor D.
and Bello-Orgaz, Gema
and Barnard, Pepita
and Bautista, John Robert
and Farahi, Arya
and Dash, Santanu
and Han, DongGyun
and Fortz, Sophie
and Rodriguez-Fernandez, Victor},
 isbn = {978-3-031-80889-0},
 pages = {239--255},
 publisher = {Springer Nature Switzerland},
 title = {Software System Testing Assisted by Large Language Models: An Exploratory Study},
 url = {https://link.springer.com/chapter/10.1007/978-3-031-80889-0_17},
 year = {2025}
}

@inproceedings{10.1007/978-3-031-94544-1_10,
 abstract = {Generative AI has been rapidly adopted by the software development industry in various ways, offering innovative approaches to transforming requirements into working software. Combining Generative AI with Test-Driven Development (TDD) presents a creative method to accelerate this transformation. However, questions remain about ChatGPT's readiness for this challenge, including the techniques and best practices required for success and the scenarios where this approach can consistently deliver results. To explore these questions, we designed a study where a group of master's students performed programming assignments using TDD, first independently and then with the support of ChatGPT. The three assignments represent distinct scenarios: mathematical calculations (function), text processing (class), and system integration (class with dependencies). We performed a qualitative analysis of the submitted code and reports identifying key strategies that significantly influence success rates, such as providing contextual information, separating instructions in prompts following an iterative process, and assisting AI in fixing errors. Among the scenarios, the integration task achieved the highest performance. This study highlights the potential of leveraging Generative AI in TDD for software development and presents a list of effective strategies to maximize its impact. By applying these positive strategies and avoiding identified pitfalls, this research marks a step toward establishing best practices for integrating Generative AI with TDD in software engineering.},
 address = {Cham},
 author = {Pancher, Juliano Cesar
and Melegati, Jorge
and Guerra, Eduardo Martins},
 booktitle = {Agile Processes in Software Engineering and Extreme Programming},
 editor = {Peter, Sibylle
and Kropp, Martin
and Aguiar, Ademar
and Anslow, Craig
and Lunesu, Maria Ilaria
and Pinna, Andrea},
 isbn = {978-3-031-94544-1},
 pages = {145--159},
 publisher = {Springer Nature Switzerland},
 title = {Exploratory Test-Driven Development Study with ChatGPT in Different Scenarios},
 url = {https://link.springer.com/chapter/10.1007/978-3-031-94544-1_10},
 year = {2025}
}

@inproceedings{10.1007/978-981-95-5294-8_15,
 abstract = {This paper explores the integration of large language models (LLMs) with China's information technology application innovation (ITAI), highlighting transformative opportunities and challenges. It systematically analyses the impacts of LLMs across four domains: hardware, software, security, and ecosystem. For hardware, innovations such as advanced packaging and ISAs are proposed to address computational bottlenecks while enhancing energy efficiency. In software, lightweight middleware and hybrid transactional/analytical processing (HTAP) databases optimize resource scheduling and intelligent workloads. Security advancements include differential privacy frameworks for federated learning and full-lifecycle LLMs security assessments to mitigate adversarial threats. Ecosystem development relies on open-source collaboration. This paper emphasizes hardware---software codesign, privacy-preserving AI, and policy support to overcome technical fragmentation and regulatory challenges. By leveraging LLMs, ITAI can shift from functional substitution to intelligent leadership, establishing an autonomous, secure, and high-performance digital infrastructure to support China's economic growth.},
 address = {Singapore},
 author = {Kou, Yingshuai
and Yu, Haifeng},
 booktitle = {Computer Applications},
 editor = {Huang, Lan
and Xu, Bin
and Chen, Xuebin
and Wang, Shangguang
and Xia, Bing
and Pan, Jianguo
and Song, Xianhua
and Lu, Zeguang},
 isbn = {978-981-95-5294-8},
 pages = {201--221},
 publisher = {Springer Nature Singapore},
 title = {Riding the Wave of LLMs: Navigating Opportunities and Challenges in Chinese Information Technology Application Innovation},
 url = {https://link.springer.com/chapter/10.1007/978-981-95-5294-8_15},
 year = {2026}
}

@Article{Alber2025,
author="Alber, Daniel Alexander
and Yang, Zihao
and Alyakin, Anton
and Yang, Eunice
and Rai, Sumedha
and Valliani, Aly A.
and Zhang, Jeff
and Rosenbaum, Gabriel R.
and Amend-Thomas, Ashley K.
and Kurland, David B.
and Kremer, Caroline M.
and Eremiev, Alexander
and Negash, Bruck
and Wiggan, Daniel D.
and Nakatsuka, Michelle A.
and Sangwon, Karl L.
and Neifert, Sean N.
and Khan, Hammad A.
and Save, Akshay Vinod
and Palla, Adhith
and Grin, Eric A.
and Hedman, Monika
and Nasir-Moin, Mustafa
and Liu, Xujin Chris
and Jiang, Lavender Yao
and Mankowski, Michal A.
and Segev, Dorry L.
and Aphinyanaphongs, Yindalon
and Riina, Howard A.
and Golfinos, John G.
and Orringer, Daniel A.
and Kondziolka, Douglas
and Oermann, Eric Karl",
title="Medical large language models are vulnerable to data-poisoning attacks",
journal="Nature Medicine",
year="2025",
month="Feb",
day="01",
volume="31",
number="2",
pages="618--626",
abstract="The adoption of large language models (LLMs) in healthcare demands a careful analysis of their potential to spread false medical knowledge. Because LLMs ingest massive volumes of data from the open Internet during training, they are potentially exposed to unverified medical knowledge that may include deliberately planted misinformation. Here, we perform a threat assessment that simulates a data-poisoning attack against The Pile, a popular dataset used for LLM development. We find that replacement of just 0.001{\%} of training tokens with medical misinformation results in harmful models more likely to propagate medical errors. Furthermore, we discover that corrupted models match the performance of their corruption-free counterparts on open-source benchmarks routinely used to evaluate medical LLMs. Using biomedical knowledge graphs to screen medical LLM outputs, we propose a harm mitigation strategy that captures 91.9{\%} of harmful content (F1{\thinspace}={\thinspace}85.7{\%}). Our algorithm provides a unique method to validate stochastically generated LLM outputs against hard-coded relationships in knowledge graphs. In view of current calls for improved data provenance and transparent LLM development, we hope to raise awareness of emergent risks from LLMs trained indiscriminately on web-scraped data, particularly in healthcare where misinformation can potentially compromise patient safety.",
issn="1546-170X",
doi="10.1038/s41591-024-03445-1",
url="https://doi.org/10.1038/s41591-024-03445-1"
}


@inproceedings{10.1007/978-3-032-15984-7_24,
 abstract = {Just-in-time Software Defect Prediction (JIT-SDP) aims to identify defects before they are introduced, enabling mitigation of risky software submission to the repository during the software development cycle. Recent approaches model JIT-SDP as a multimodal task that involves code changes, commit messages, and hand-crafted commit-level features, also known as expert features. The present study thoroughly compares the performance of pretrained code language models, including encoder-only, decoder-only, and encoder-decoder architectures across both open-source fine-tuned models (e.g., CodeT5+, UniXCoder) and closed API-based large language models (e.g., GPT models, Gemini) for the JIT-SDP task. Furthermore, to the best of our knowledge, this is the first study to compare open (trainable) and closed (prompt-based) decoder-only models in this context. Our experiments show that fine-tuned small and medium-sized open models significantly outperform zero-shot prompting with closed large language models. In particular, CodeT5+ and UniXCoder overcome the state-of-the-art performance in a cross-project setting. The results highlight the importance of architecture, fine-tuning strategies, and expert feature integration for accurate JIT-SDP.},
 address = {Cham},
 author = {Monteiro, Monique Louise
and Cabral, George Gomes
and de Oliveira, Adriano Lorena},
 booktitle = {Intelligent Systems},
 editor = {de Freitas, Rosiane
and Furtado, Diego},
 isbn = {978-3-032-15984-7},
 pages = {347--361},
 publisher = {Springer Nature Switzerland},
 title = {Pre-trained Code Language Models for Just-in-Time Software Defect Prediction: An Empirical Study},
 url = {https://link.springer.com/chapter/10.1007/978-3-032-15984-7_24},
 year = {2026}
}

@inproceedings{10.1007/978-3-032-06336-6_4,
 abstract = {The design and implementation of unit tests is a complex task that many programmers neglect. This research evaluates the potential of Large Language Models (LLMs) in automatically generating test cases, comparing them with manual tests. An optimized prompt was developed, that integrates code and requirements, covering critical cases such as equivalence partitions and boundary values. The strengths and weaknesses of LLMs versus trained programmers were compared through quantitative metrics and manual qualitative analysis. The results show that the effectiveness of LLMs depends on well-designed prompts, robust implementation, and precise requirements. Although flexible and promising, LLMs still require human supervision. This work highlights the importance of manual qualitative analysis as an essential complement to automation in unit test evaluation.},
 address = {Cham},
 author = {Rodr{\'i}guez, Mart{\'i}n
and Rossi, Gustavo
and Fernandez, Alejandro},
 booktitle = {Cloud Computing, Big Data and Emerging Topics},
 editor = {Naiouf, Marcelo
and De Giusti, Laura
and Chichizola, Franco
and Libutti, Leandro},
 isbn = {978-3-032-06336-6},
 pages = {46--60},
 publisher = {Springer Nature Switzerland},
 title = {Evaluating Large Language Models for the Generation of Unit Tests with Equivalence Partitions and Boundary Values},
 url = {https://link.springer.com/chapter/10.1007/978-3-032-06336-6_4},
 year = {2026}
}

@inproceedings{10.1007/978-3-031-96262-2_10,
 abstract = {The adoption of large language models (LLMs) as an advanced artificial intelligence (AI) system has seen significant growth, especially in the cybersecurity domain. Thus, it is imperative to understand the impact of LLMs in cybersecurity since they pose pros and cons regarding digital threats and attacks. This study systematically reviewed existing research on how LLMs impact cybersecurity, examining their benefits and challenges. Researchers conducted a systematic search across three databases, namely Scopus, IEEE and Science Direct. These databases were chosen due to their relevance to the discipline, focusing on peer-reviewed English publications from 2020--2024. A total of 47 articles were identified, 30 were deemed suitable for synthesis, and 17 were excluded as they did not meet the criteria. Using thematic analysis, the authors identified key themes related to LLMs in cybersecurity. The study's key findings revealed several important insights, such as LLMs' ability to assist in network anomaly detection, carry out automated threat analysis, conduct enhanced threat detection, improve incident response, carry out knowledge sharing and conduct training and awareness. In contrast, the negative findings reveal that security systems based on large language models (LLMs) may be vulnerable to cyberattacks, contribute to spreading misinformation and disinformation, raise legal, ethical, and fairness concerns, and create a false sense of security. The contribution of the study is that it provides a foundational understanding of the impact of LLMs on cybersecurity and identifies areas for future investigation. Subsequent studies should explore undiscovered impacts, challenges, and potential future developments in LLMs impacting cybersecurity in specific industries.},
 address = {Cham},
 author = {Moongela, Harry
and Mayayise, Thembekile},
 booktitle = {South African Computer Science and Information Systems Research Trends},
 editor = {Gerber, Aurona},
 isbn = {978-3-031-96262-2},
 pages = {129--146},
 publisher = {Springer Nature Switzerland},
 title = {The Impact of Large Language Models on Cybersecurity},
 url = {https://link.springer.com/chapter/10.1007/978-3-031-96262-2_10},
 year = {2026}
}

@inproceedings{10.1007/978-981-96-7502-9_18,
 abstract = {Artificial intelligence, specifically large language models and generative AI, has dramatically changed the finance industry. According to a few research studies, the current article discusses some of those findings that provided insight into different applications of artificial intelligence in the sector of financial operations and improved predictive analytics, operational efficiency, and quality in decision-making processes. Most critical findings point toward the efficiency of LLMs, where it has achieved automation, financial analysis improvements, and compliance with enforcement standards in their usage of such technologies, as toward most security concerns, privacy, and ethical perspectives. Specifically, the long-term implications for financial decision-making and the potential consequences arising from the use of such technologies from an ethical perspective stand out starkly as red flags of concern. Thus, the review brings new knowledge in the sphere of AI in finance and grounds further justification to be done with proper research motives toward complete responsible development of AI.},
 address = {Singapore},
 author = {Vairagar, Shubhangi
and Shravage, Chetana
and Merti, Priya
and Ingale, Nikhil M.
and Gaikwad, Sakshi N.
and Yaranalkar, Dhruv G.
and Pimple, Atharva R.},
 booktitle = {Smart Trends in Computing and Communications},
 editor = {Senjyu, Tomonobu
and So-In, Chakchai
and Joshi, Amit},
 isbn = {978-981-96-7502-9},
 pages = {217--226},
 publisher = {Springer Nature Singapore},
 title = {Transforming Financial Statement Analysis with Large Language Models: A Survey of Approaches and Challenges},
 url = {https://link.springer.com/chapter/10.1007/978-981-96-7502-9_18},
 year = {2026}
}

@Inbook{Kühn2025,
author="K{\"u}hn, Philipp",
title="Discussion",
bookTitle="Proactive Cyber Threat Intelligence: Automating the Intelligence Cycle based on Open Sources",
year="2025",
publisher="Springer Fachmedien Wiesbaden",
address="Wiesbaden",
pages="45--53",
abstract="The previous chapter shows the applicability of open information sources in the context of the Cyber Threat Intelligence (CTI) cycle. In this chapter these findings are presented with regard to the research questions. Additionally, as the advent of foundational models is not part of the present work, but show tremendous potential in different aspects, I will discuss their limitations and potentials.",
isbn="978-3-658-49241-0",
doi="10.1007/978-3-658-49241-0_5",
url="https://doi.org/10.1007/978-3-658-49241-0_5"
}


@Article{Zhang2025,
author="Zhang, Zicheng
and Wang, Junying
and Wen, Farong
and Guo, Yijin
and Zhao, Xiangyu
and Fang, Xinyu
and Ding, Shengyuan
and Jia, Ziheng
and Xiao, Jiahao
and Shen, Ye
and Zheng, Yushuo
and Zhu, Xiaorong
and Wu, Yalun
and Jiao, Ziheng
and Sun, Wei
and Chen, Zijian
and Zhang, Kaiwei
and Fu, Kang
and Cao, Yuqin
and Hu, Ming
and Zhou, Yue
and Zhou, Xuemei
and Cao, Juntai
and Zhou, Wei
and Cao, Jinyu
and Li, Ronghui
and Zhou, Donghao
and Tian, Yuan
and Zhu, Xiangyang
and Li, Chunyi
and Wu, Haoning
and Liu, Xiaohong
and He, Junjun
and Zhou, Yu
and Liu, Hui
and Zhang, Lin
and Wang, Zesheng
and Duan, Huiyu
and Zhou, Yingjie
and Min, Xiongkuo
and Jia, Qi
and Zhou, Dongzhan
and Zhang, Wenlong
and Cao, Jiezhang
and Yang, Xue
and Yu, Junzhi
and Zhang, Songyang
and Duan, Haodong
and Zhai, Guangtao",
title="Large multimodal models evaluation: a survey",
journal="Science China Information Sciences",
year="2025",
month="Nov",
day="18",
volume="68",
number="12",
pages="221301",
abstract="As large multimodal models (LMMs) advance rapidly across diverse multimodal understanding and generation tasks, the need for systematic and reliable evaluation frameworks becomes increasingly critical. To address this need, this survey provides a structured overview of LMM evaluation, centered around two main axes: multimodal evaluation for understanding and generation. (1) For understanding, a dual-perspective framework is introduced to distinguish benchmarks between general capabilities, which emphasize common tasks, and specialized capabilities, which reflect expert-level competence in domain-specific fields. (2) For generation, evaluation is organized by output modality, including image, video, audio, and 3D content. (3) From a community perspective, this survey further highlights authoritative leaderboards and foundational tools that have been instrumental in establishing a comprehensive evaluation ecosystem for LMMs. By unifying general-specialized understanding and modality-specific generation evaluations, this survey clarifies the current landscape and provides guidance for future research in the LMM evaluation field.",
issn="1869-1919",
doi="10.1007/s11432-025-4676-4",
url="https://doi.org/10.1007/s11432-025-4676-4"
}


@Article{Yang2025,
author="Yang, Zhenyao
and Yuan, Sha
and Shao, Zhou
and Li, Wenfa
and Liu, Runzhou",
title="A review on synergizing knowledge graphs and large language models",
journal="Computing",
year="2025",
month="Jun",
day="06",
volume="107",
number="6",
pages="143",
abstract="This paper examines the integration of large language models (LLMs) with knowledge graphs (KGs) through a systematic four-layer framework that includes data, model, technology, and application dimensions. We analyze the capabilities and limitations of LLMs in natural language processing along with the strengths and challenges of KGs in knowledge representation. We address fundamental weaknesses in each approach and identify complementary integration methods. Our analysis reveals that LLMs excel at contextual understanding and generation but struggle with factual consistency and reasoning transparency. In contrast, KGs provide structured and verifiable knowledge but lack adaptability to unstructured inputs. We review integration strategies, including knowledge injection techniques, retrieval-augmented generation, and neuro-symbolic approaches. The combined methods demonstrate significant performance improvements. Through the case study of the GLM architecture, we demonstrate how the integration of KGs and LLMs improves accuracy, interpretability, and factual grounding in specialized domains and also shows substantial performance improvements in knowledge-intensive tasks (15--20{\%} on MedQA and 14--17{\%} on the medical MMLU benchmarks). The resulting hybrid systems offer concrete advantages in critical applications requiring precision and adaptability, including healthcare diagnostics, financial compliance, and educational technology. Lightweight knowledge representation, adaptive update mechanisms, and unified cross-modal frameworks are promising research directions to advance KG--LLM integration.",
issn="1436-5057",
doi="10.1007/s00607-025-01499-8",
url="https://doi.org/10.1007/s00607-025-01499-8"
}


@Inbook{Wienholt2025,
author="Wienholt, Nick",
title="Code Migrations and Refactoring",
bookTitle="GitHub Copilot and AI Coding Tools in Practice: Accelerate AI Adoption from Individual Developers to Enterprise",
year="2025",
publisher="Apress",
address="Berkeley, CA",
pages="251--274",
abstract="Choosing the right time to fix code that is functionally complete but could use some form of nonfunctional attention is a difficult choice. The two extremes on the spectrum are equally toxic:",
isbn="979-8-8688-1784-7",
doi="10.1007/979-8-8688-1784-7_11",
url="https://doi.org/10.1007/979-8-8688-1784-7_11"
}


@inproceedings{10.1007/978-3-032-12993-2_11,
 abstract = {Code analysis ensures software quality, readability, and maintenance. Traditional methods such as Static Analysis Tools (SAT) and SZZ algorithms recognize errors and analyze contributions based on predefined rules and historical trends. However, complex code semantics are difficult to understand and often lead to overlooked issues and limited feedback. Despite its benefits, LLM is not practical in some business environments, facing challenges such as high computing costs, slower processing, and data protection concerns. Hybrid models may provide scalable and inexpensive solutions for large enterprise projects.},
 address = {Cham},
 author = {Muraleedharan, Prathilothamai
and Ravi, Nitin
and Pradeepkumar, Rishi
and Rajan, P. Sajith
and Nagilla, Anurag},
 booktitle = {Information Systems for Intelligent Systems},
 editor = {Iglesias, Andres
and Shin, Jungpil
and Bhatt, Nityesh
and Joshi, Amit},
 isbn = {978-3-032-12993-2},
 pages = {100--107},
 publisher = {Springer Nature Switzerland},
 title = {Code Reviews Using Traditional Methods vs Hybrid Models},
 url = {https://link.springer.com/chapter/10.1007/978-3-032-12993-2_11},
 year = {2026}
}

@inproceedings{10.1007/978-3-032-08649-5_2,
 abstract = {Modern cloud-native CI/CD pipelines enable rapid software delivery but introduce new security challenges. We propose GenSecAI-Ops, a framework embedding specialized generative AI agents across the pipeline for proactive security and automated remediation. GenSecAI-Ops uses Retrieval-Augmented Generation (RAG) and eXplainable AI (XAI) for reliable security automation. Implemented using GitHub Actions and Jenkins, our framework demonstrates significant improvements in detection accuracy, remediation correctness, and developer trust while maintaining pipeline velocity. The approach provides a practical blueprint for integrating AI-driven security controls in DevSecOps workflows.},
 address = {Cham},
 author = {Mittal, Akshay
and Venkatesan, Vivek},
 booktitle = {Software and Data Engineering},
 editor = {Rahimi, Nick
and Margapuri, Venkat
and Golilarz, Noor Amiri},
 isbn = {978-3-032-08649-5},
 pages = {18--39},
 publisher = {Springer Nature Switzerland},
 title = {Leveraging Generative AI for Proactive Security and Automated Remediation in Cloud-Native CI/CD Pipelines},
 url = {https://link.springer.com/chapter/10.1007/978-3-032-08649-5_2},
 year = {2026}
}

@Article{Paweroi2025,
author="Paweroi, Rio Mukhtarom
and Syarawy, Mutawally
and K{\"o}ppen, Mario",
title="A three-tier generative AI workflow for metaverse asset creation",
journal="Cluster Computing",
year="2025",
month="Oct",
day="17",
volume="28",
number="16",
pages="1017",
abstract="The Metaverse as a platform for virtual interaction underscores the critical importance of 3D assets in constructing virtual environments. This study proposes a Generative AI-based workflow for Metaverse asset creation by developing three main approaches: (1) 3D asset generation through a text-to-image and image-to-3D pipeline using SDXL, Hunyuan3D-2, Trellis, and TripoSR; (2) texture generation for 3D asset using StableProjectorZ and SDXL; and (3) 3D asset creation via scripts generated by Large Language Models (LLMs) integrated with Blender. The generative AI-assisted workflow is compared against a procedural generation method. Our experiments demonstrate the successful creation and implementation of 3D assets within a Metaverse platform, albeit with limitations such as the necessity for manual intervention and a dependency on the underlying pretrained models. Furthermore, the high complexity of AI-generated objects remains a primary challenge, necessitating a post-processing step to optimize these assets for use in a Metaverse environment. We achieved an average polygon count reduction of 89.29{\%} in AI-generated objects while maintaining visual quality and ensuring usability within the OpenSimulator Metaverse environment. Furthermore, the average similarity to the original objects, measured by chamfer distance, reached 0.027 (L1) and 0.020 (L2). Our study highlights the trade-off of integrating generative AI into the 3D asset creation process, with careful consideration for the critical factors of diversity, quality, complexity, and usability.",
issn="1573-7543",
doi="10.1007/s10586-025-05734-x",
url="https://doi.org/10.1007/s10586-025-05734-x"
}


@Article{Yu2025,
author="Yu, Qiao
and Que, Tristan
and Cushing, Lara J.
and Pierce, Gregory
and Shen, Ke
and Kejriwal, Mayank
and Yao, Yuan
and Zhu, Yifang",
title="Equity and reliability of public electric vehicle charging stations in the United States",
journal="Nature Communications",
year="2025",
month="Jun",
day="18",
volume="16",
number="1",
pages="5291",
abstract="Equitable coverage and reliable operation of electric vehicle charging stations (EVCSs) are crucial for a just transition to a carbon-free future. Yet, a comprehensive national analysis of public EVCSs across different communities is lacking in the United States. Here, we utilize real-world reviews (n{\thinspace}={\thinspace}470,142) from a user-generated content platform to analyze public EVCSs at the census tract level. We find that disadvantaged communities (DACs) have 64{\%} fewer public EVCSs per capita than non-DACs. This disparity rises to 73{\%} when considering renters in multi-dwelling units. Additionally, EVCS users in DACs and urban areas experience significantly more reliability issues compared to those in non-DACs and rural areas, primarily related to hardware and technical failures. Given the limited access to home charging in DACs and their underserved public infrastructure, these findings highlight critical equity concerns and call for targeted investment in EVCS infrastructure and reliability improvements, particularly in DACs.",
issn="2041-1723",
doi="10.1038/s41467-025-60091-y",
url="https://doi.org/10.1038/s41467-025-60091-y"
}


@Article{Song2026,
author="Song, Ziyu
and Xu, Nan
and Ding, Haitao",
title="Decision-Making Framework for Autonomous Vehicles in Complex Scenarios Using Large Language Models",
journal="Automotive Innovation",
year="2026",
month="Jan",
day="13",
abstract="Current behavior decision-making methods for autonomous vehicles often struggle to generalize to complex scenarios. Large language models (LLMs) offer a promising approach to address these limitations. This study investigates the integration of LLMs into decision-making modules for autonomous vehicles (AVs), leveraging their reasoning capabilities to emulate intricate human-like driving behaviors and overcoming the limitations inherent in traditional methods. First, LLMs are incorporated into decision-making modules, together with a rule-based planner to complete closed-loop tasks. A scoring module is integrated to evaluate decisions, thereby reducing uncertainty associated with LLMs. Second, a memory module is introduced to facilitate a reflection mechanism, prompting LLMs to perform error correction to refine decision accuracy, assisted by human experts feedback. Subsequently, an extended naturalistic driving dataset highlighting complex scenarios is used for more rigorous evaluation. Experimental results on the extended nuPlan dataset demonstrate that the proposed methods effectively address complex scenarios, outperforming existing methods and improving the driving score by 13.6{\%} to 22.7{\%}. Furthermore, the reflection mechanism enhances contextual reasoning ability, resulting in more stable route completion and fewer task failures in complex scenarios.",
issn="2522-8765",
doi="10.1007/s42154-025-00408-1",
url="https://doi.org/10.1007/s42154-025-00408-1"
}


@inproceedings{10.1007/978-3-032-01241-8_13,
 abstract = {Software engineers in various industrial domains are already using Large Language Models (LLMs) to accelerate the process of implementing parts of software systems. When considering its potential use for ADAS or AD systems in the automotive context, there is a need to systematically assess this new setup: LLMs entail a well-documented set of risks for safety-related systems' development due to their stochastic nature. To reduce the effort for code reviewers to evaluate LLM-generated code, we propose an evaluation pipeline to conduct sanity-checks on the generated code. We compare the performance of six state-of-the-art LLMs (CodeLlama, CodeGemma, DeepSeek-r1, DeepSeek-Coders, Mistral, and GPT-4) on four safety-related programming tasks. Additionally, we qualitatively analyse the most frequent faults generated by these LLMs, creating a failure-mode catalogue to support human reviewers. Finally, the limitations and capabilities of LLMs in code generation, and the use of the proposed pipeline in the existing process, are discussed.},
 address = {Cham},
 author = {Nouri, Ali
and Cabrero-Daniel, Beatriz
and Fei, Zhennan
and Ronanki, Krishna
and Sivencrona, H{\aa}kan
and Berger, Christian},
 booktitle = {Computer Safety, Reliability, and Security},
 editor = {Gallina, Barbara
and T{\"o}rngren, Martin
and Bitsch, Friedemann},
 isbn = {978-3-032-01241-8},
 pages = {193--208},
 publisher = {Springer Nature Switzerland},
 title = {Large Language Models in Code Co-generation for Safe Autonomous Vehicles},
 url = {https://link.springer.com/chapter/10.1007/978-3-032-01241-8_13},
 year = {2026}
}

@inproceedings{10.1007/978-3-032-07884-1_25,
 abstract = {Safety alignment and adversarial attack research for Large Language Models (LLMs) predominantly focuses on natural language inputs and outputs. This work introduces StructTransform, a blackbox attack against alignment where malicious prompts are encoded into diverse structure transformations. These range from standard formats (e.g., SQL, JSON) to novel syntaxes generated entirely by LLMs. By shifting harmful prompts Out-Of-Distribution (OOD) relative to typical natural language, these transformations effectively circumvent existing safety alignment mechanisms. Our extensive evaluations show that simple StructTransform attacks achieve high Attack Success Rates (ASR), nearing 90{\%} even against state-of-the-art models like Claude 3.5 Sonnet. Combining structural and content transformations further increases ASR to over 96{\%} without any refusals. We demonstrate the ease with which LLMs can generate novel syntaxes and their effectiveness in bypassing defenses, creating a vast attack surface. Using a new benchmark, we show that current alignment techniques and defences largely fail against these structure-based attacks. This failure strongly suggests a reliance on token-level patterns within natural language, rather than a robust, structure-aware conceptual understanding of harmful requests, exposing a critical need for generalized safety mechanisms robust to variations in input structure.},
 address = {Cham},
 author = {Yoosuf, Shehel
and Ali, Temoor
and Lekssays, Ahmed
and AlSabah, Mashael
and Khalil, Issa},
 booktitle = {Computer Security -- ESORICS 2025},
 editor = {Nicomette, Vincent
and Benzekri, Abdelmalek
and Boulahia-Cuppens, Nora
and Vaidya, Jaideep},
 isbn = {978-3-032-07884-1},
 pages = {488--507},
 publisher = {Springer Nature Switzerland},
 title = {StructTransform: A Scalable Attack Surface for Safety-Aligned Large Language Models},
 url = {https://link.springer.com/chapter/10.1007/978-3-032-07884-1_25},
 year = {2026}
}

@Article{Wang2025,
author="Wang, Yanlin
and Zhong, Wanjun
and Huang, Yanxian
and Shi, Ensheng
and Yang, Min
and Chen, Jiachi
and Li, Hui
and Ma, Yuchi
and Wang, Qianxiang
and Zheng, Zibin",
title="Agents in software engineering: survey, landscape, and vision",
journal="Automated Software Engineering",
year="2025",
month="Aug",
day="07",
volume="32",
number="2",
pages="70",
abstract="In recent years, Large Language Models (LLMs) have achieved remarkable success and have been widely used in various downstream tasks, especially in the tasks of the software engineering (SE) field. We find that many studies combining LLMs with SE have employed the concept of agents either explicitly or implicitly. However, there is a lack of an in-depth survey to sort out the development context of existing works, analyze how existing works combine the LLM-based agent technologies to optimize various tasks, and clarify the framework of LLM-based agents in SE. In this paper, we conduct the first survey of the studies on combining LLM-based agents with SE and present a framework of LLM-based agents in SE which includes three key modules: perception, memory, and action. We also summarize the current challenges in combining the two fields and propose future opportunities in response to existing challenges. We maintain a GitHub repository of the related papers at: https://github.com/DeepSoftwareAnalytics/Awesome-Agent4SE.",
issn="1573-7535",
doi="10.1007/s10515-025-00544-2",
url="https://doi.org/10.1007/s10515-025-00544-2"
}


@inproceedings{10.1007/978-3-031-99991-8_4,
 abstract = {Formal program specifications in the form of preconditions, postconditions, and class invariants have several benefits for the construction and maintenance of programs. They not only aid in program understanding due to their unambiguous semantics but can also be enforced dynamically (or even statically when the language supports a formal verifier). However, synthesizing high-quality specifications in an underlying programming language is limited by the expressivity of the specifications or the need to express them in a declarative manner. Prior work has demonstrated the potential of large language models (LLMs) for synthesizing high-quality method pre/postconditions for Python and Java, but does not consider class invariants.},
 address = {Cham},
 author = {Sun, Chuyue
and Agashe, Viraj
and Chakraborty, Saikat
and Taneja, Jubi
and Barrett, Clark
and Dill, David
and Qiu, Xiaokang
and Lahiri, Shuvendu K.},
 booktitle = {AI Verification},
 editor = {Giacobbe, Mirco
and Lukina, Anna},
 isbn = {978-3-031-99991-8},
 pages = {64--96},
 publisher = {Springer Nature Switzerland},
 title = {ClassInvGen: Class Invariant Synthesis Using Large Language Models},
 url = {https://link.springer.com/chapter/10.1007/978-3-031-99991-8_4},
 year = {2026}
}

@Inbook{Alsaedi2025,
author="Alsaedi, Najlaa
and Alsaedi, Ahlam
and Almaghathawi, Amjad
and Alshanqiti, Mai
and Siddiqi, Abdul Ahad",
editor="Yafooz, Wael M.S.
and Al-Gumaei, Yousef",
title="Investigating LLMs Potential in Software Requirements Evaluation",
bookTitle="AI-Driven: Social Media Analytics and Cybersecurity",
year="2025",
publisher="Springer Nature Switzerland",
address="Cham",
pages="291--307",
abstract="LLMs have made a big splash on social media by changing how trends catch on and keeping users engaged. LLMs now have the application and understanding of software engineering activities, including requirement engineering, designing, coding, testing and software security. LLMs can help accelerate the process and save cost, time, and enhance security. This paper investigates software engineering activities using LLMs but focuses on requirements evaluation on the generated requirements from different LLMs. The three LLMs used in our work are GPT3.5, through ChatGPT bot; Palm 2, through Poe bot; and Claude 1, through Poe bot. We investigated if LLMs have the understanding and can evaluate the provided requirements according to software quality attributes. We consider the requirements evaluation a main process since all other processes depend on it.",
isbn="978-3-031-80334-5",
doi="10.1007/978-3-031-80334-5_18",
url="https://doi.org/10.1007/978-3-031-80334-5_18"
}


@Article{Irfan2025,
author="Irfan, Bahar
and Kuoppam{\"a}ki, Sanna
and Hosseini, Aida
and Skantze, Gabriel",
title="Between reality and delusion: challenges of applying large language models to companion robots for open-domain dialogues with older adults",
journal="Autonomous Robots",
year="2025",
month="Mar",
day="10",
volume="49",
number="1",
pages="9",
abstract="Throughout our lives, we interact daily in conversations with our friends and family, covering a wide range of topics, known as open-domain dialogue. As we age, these interactions may diminish due to changes in social and personal relationships, leading to loneliness in older adults. Conversational companion robots can alleviate this issue by providing daily social support. Large language models (LLMs) offer flexibility for enabling open-domain dialogue in these robots. However, LLMs are typically trained and evaluated on textual data, while robots introduce additional complexity through multi-modal interactions, which has not been explored in prior studies. Moreover, it is crucial to involve older adults in the development of robots to ensure alignment with their needs and expectations. Correspondingly, using iterative participatory design approaches, this paper exposes the challenges of integrating LLMs into conversational robots, deriving from 34 Swedish-speaking older adults' (one-to-one) interactions with a personalized companion robot, built on Furhat robot with GPT{\$}{\$}-{\$}{\$}3.5. These challenges encompass disruptions in conversations, including frequent interruptions, slow, repetitive, superficial, incoherent, and disengaging responses, language barriers, hallucinations, and outdated information, leading to frustration, confusion, and worry among older adults. Drawing on insights from these challenges, we offer recommendations to enhance the integration of LLMs into conversational robots, encompassing both general suggestions and those tailored to companion robots for older adults.",
issn="1573-7527",
doi="10.1007/s10514-025-10190-y",
url="https://doi.org/10.1007/s10514-025-10190-y"
}


@Article{Uçar2025,
author="U{\c{c}}ar, Suna-{\c{S}}eyma
and Lopez-Gazpio, Inigo
and Lopez-Gazpio, Josu",
title="Evaluating and challenging the reasoning capabilities of generative artificial intelligence for technology-assisted chemistry education",
journal="Education and Information Technologies",
year="2025",
month="Jun",
day="01",
volume="30",
number="8",
pages="11463--11482",
abstract="Recent advancements in large language models (LLMs) have shown potential in enhancing educational practices, particularly in technology-assisted learning environments. This study critically evaluates the reasoning capabilities of LLMs, such as ChatGPT, within the context of chemistry education. We designed targeted adversarial prompts that challenge the models to solve complex chemistry problems and assessed their performance. By pushing the boundaries of LLM reasoning, we aim to identify their limitations and strengths in handling queries within the chemistry domain. Our findings expose inherent weaknesses in current AI systems, emphasizing the necessity of cautious AI deployment in teaching methodologies. We argue for a balanced approach, leveraging the benefits of LLMs while mitigating their limitations, to facilitate their seamless adoption in education.",
issn="1573-7608",
doi="10.1007/s10639-024-13295-6",
url="https://doi.org/10.1007/s10639-024-13295-6"
}


@Article{Fekih2025,
author="Fekih, Rim Ben
and Lahami, Mariam
and Bradai, Salma
and Jmaiel, Mohamed",
title="SmartGuard: Verifying Safety and Compliance of Ethereum Smart Contracts",
journal="SN Computer Science",
year="2025",
month="Aug",
day="29",
volume="6",
number="7",
pages="785",
abstract="Solidity smart contracts are recognized for their susceptibility to vulnerabilities, necessitating thorough verification to mitigate potential security risks. Beyond basic functionality, smart contracts must also adhere to established standards and reflect expected behaviors to ensure proper operation. To address these challenges, adopting a model checking method has proven to be an effective approach. However, the modeling and specification phases demand expertise to achieve rigor. Critical aspects such as inheritance and behavioral factors are often overlooked during the modeling phase, while manual specification can be error-prone and time-consuming. To address these limitations, we introduce SmartGuard, a model checking-based tool for Ethereum smart contracts. Our framework not only examines the functional aspects of Solidity smart contracts but also checks their behavior. Additionally, SmartGuard ensures the correctness of standard-based contracts, providing support for compliance with established standards. To reduce the need for human intervention, we abstract contract models into Behavioral Interaction Priority (BIP) and utilize Large Language Models (LLMs) for formalizing properties into Computation Tree Logic (CTL) formulas. We conducted several experiments that demonstrate the efficiency of the Mistral model to generate CTL properties. Moreover, we evaluate our verification process on several smart contracts showing the effectiveness of our approach. Finally, we conclude with a discussion on future work.",
issn="2661-8907",
doi="10.1007/s42979-025-04298-2",
url="https://doi.org/10.1007/s42979-025-04298-2"
}


@inproceedings{10.1007/978-3-032-05144-8_13,
 abstract = {Access to humanities research databases is often hindered by the limitations of traditional interaction formats, particularly in the methods of searching and response generation. This study introduces an LLM-based smart assistant designed to facilitate natural language communication with digital humanities data. The assistant, developed in a chatbot format, leverages the RAG approach and integrates state-of-the-art technologies such as hybrid search, automatic query generation, text-to-SQL filtering, semantic database search, and hyperlink insertion. To evaluate the effectiveness of the system, experiments were conducted to assess the response quality of various language models. The testing was based on the Prozhito digital archive, which contains diary entries from predominantly Russian-speaking individuals who lived in the 20th century. The chatbot is tailored to support anthropology and history researchers, as well as non-specialist users with an interest in the field, without requiring prior technical training. By enabling researchers to query complex databases with natural language, this tool aims to enhance accessibility and efficiency in humanities research. The study highlights the potential of Large Language Models to transform the way researchers and the public interact with digital archives, making them more intuitive and inclusive. Additional materials are presented in GitHub repository: https://github.com/alekosus/talking-to-data-intersys2025.},
 address = {Cham},
 author = {Sergeev, Alexander
and Goloviznina, Valeriya
and Melnichenko, Mikhail
and Kotelnikov, Evgeny},
 booktitle = {Internet and Modern Society},
 editor = {Bakaev, Maxim
and Bolgov, Radomir
and Chizhik, Anna
and Chugunov, Andrei
and Demareva, Valeriia
and Kabanov, Yury
and Pereira, Roberto
and R., Elakkiya
and Zhang, Wei},
 isbn = {978-3-032-05144-8},
 pages = {166--180},
 publisher = {Springer Nature Switzerland},
 title = {Talking to Data: Designing Smart Assistants for Humanities Databases},
 url = {https://link.springer.com/chapter/10.1007/978-3-032-05144-8_13},
 year = {2026}
}

@inproceedings{10.1007/978-3-031-97620-9_15,
 abstract = {Code obfuscation alters software code to conceal its logic while retaining functionality, aiding intellectual property protection but hindering security audits and malware analysis. To address this, automated deobfuscation techniques have been developed, though existing approaches remain constrained by limited scope and specificity. Motivated by these challenges, this paper explores a novel approach for code deobfuscation based on Large Language Models (LLMs).  First, we investigate the general capabilities of LLMs in reducing code complexity by choosing five different source-to-source obfuscation methods. Despite challenges regarding semantical correctness, our findings indicate that LLMs can be very effective in this task. Building on this, we fine-tune two versatile models capable of simplifying code obfuscated through up to seven different chained obfuscation transformations while consistently outperforming deobfuscation based on compiler optimizations and general-purpose LLMs. Our best model demonstrates an average Halstead metric program length reduction of 89.21{\%} for our most challenging scenario. Finally, we conduct a memorization test to assess if performance stems from memorized code rather than true deobfuscation capabilities, which our models pass.},
 address = {Cham},
 author = {Beste, David
and Menguy, Gr{\'e}goire
and Hajipour, Hossein
and Fritz, Mario
and Cin{\`a}, Antonio Emanuele
and Bardin, S{\'e}bastien
and Holz, Thorsten
and Eisenhofer, Thorsten
and Sch{\"o}nherr, Lea},
 booktitle = {Detection of Intrusions and Malware, and Vulnerability Assessment},
 editor = {Egele, Manuel
and Moonsamy, Veelasha
and Gruss, Daniel
and Carminati, Michele},
 isbn = {978-3-031-97620-9},
 pages = {267--286},
 publisher = {Springer Nature Switzerland},
 title = {Exploring the Potential of LLMs for Code Deobfuscation},
 url = {https://link.springer.com/chapter/10.1007/978-3-031-97620-9_15},
 year = {2025}
}

@Article{Raza2025,
author="Raza, Muhammad Anas
and Wardat, Mohammad",
title="Graph neural network for fault localization in sequence-based models",
journal="Empirical Software Engineering",
year="2025",
month="Jun",
day="02",
volume="30",
number="5",
pages="119",
abstract="Deep learning models, particularly sequence-based models (SBMs) like Recurrent Neural Networks (RNNs), Long Short-Term Memory Networks (LSTMs), Gated Recurrent Units (GRUs), Transformers, and patch-based architectures, are crucial for intelligent software systems. However, like any software, SBM applications are prone to bugs. Bug patterns in SBMs are distinguished from traditional techniques primarily due to their inherent black box, data-driven nature, and sequential dependencies. Moreover, currently used methods, although working for generic deep neural network (DNN) structures, require specialized expertise and are not directly applicable to the unique structures and requirements of SBMs. To address these challenges, we propose LocatorGraph (LG), a novel graph neural network-based framework designed to identify the root causes of faults in SBMs. To detect and localize faults in SBMs, we convert SBM code to TraceGraphs, which are then analyzed by LocatorGraph. The framework utilizes a graph neural network(GNN)--based architecture to not only identify bugs but also to precisely locate faulty features within the TraceGraphs. By leveraging this information, LocatorGraph can also pinpoint faulty nodes in SBMs through an evaluation of 152 diverse models, including generated buggy models, LocatorGraph outperforms existing methods for localizing faults, showing robustness to identify potential problems with an AUC of 89.46{\%} and F1 score of 81.34{\%}. On Graph-Based Feature Inspection, LG gives an accuracy of 91.19{\%} and an F1-score of 89.68{\%}. In the final phase, LG outperforms all baselines in locating bug causing nodes in the graph with an accuracy of 81.54{\%} and an F1-score of 79.98{\%}. We also provide details of the improvement observed in the models after fixing bugs using the LocatorGraph-based approach.",
issn="1573-7616",
doi="10.1007/s10664-025-10666-6",
url="https://doi.org/10.1007/s10664-025-10666-6"
}


@inproceedings{10.1007/978-981-96-7030-7_15,
 abstract = {In recent years, Large language models (LLMs) have garnered significant attention due to their superior performance in complex reasoning tasks. However, recent studies may diminish their reasoning capabilities markedly when problem descriptions contain irrelevant information, even with the use of advanced prompting techniques. To further investigate this issue, a dataset of primary school mathematics problems containing irrelevant information, named GSMIR, was constructed. Testing prominent LLMs and prompting techniques on this dataset revealed that while LLMs can identify irrelevant information, they do not effectively mitigate the interference it causes once identified. A novel automatic construction method, ATF, which enhances the ability of LLMs to identify and self-mitigate the influence of irrelevant information, is proposed to address this shortcoming. This method operates in two steps: first, analysis of irrelevant information, followed by its filtering. The ATF method, as demonstrated by experimental results, significantly improves the reasoning performance of LLMs and prompting techniques, even in the presence of irrelevant information on the GSMIR dataset. Our code is publicly available at: https://github.com/ymt9/GSMIR/tree/master.},
 address = {Singapore},
 author = {Jiang, Ming
and Huang, Tingting
and Guo, Biao
and Lu, Yao
and Zhang, Feng},
 booktitle = {Neural Information Processing},
 editor = {Mahmud, Mufti
and Doborjeh, Maryam
and Wong, Kevin
and Leung, Andrew Chi Sing
and Doborjeh, Zohreh
and Tanveer, M.},
 isbn = {978-981-96-7030-7},
 pages = {207--222},
 publisher = {Springer Nature Singapore},
 title = {Enhancing Robustness in Large Language Models: Prompting for Mitigating the Impact of Irrelevant Information},
 url = {https://link.springer.com/chapter/10.1007/978-981-96-7030-7_15},
 year = {2025}
}

@Article{Huang2024,
author="Huang, Xiaowei
and Ruan, Wenjie
and Huang, Wei
and Jin, Gaojie
and Dong, Yi
and Wu, Changshun
and Bensalem, Saddek
and Mu, Ronghui
and Qi, Yi
and Zhao, Xingyu
and Cai, Kaiwen
and Zhang, Yanghao
and Wu, Sihao
and Xu, Peipei
and Wu, Dengyu
and Freitas, Andre
and Mustafa, Mustafa A.",
title="A survey of safety and trustworthiness of large language models through the lens of verification and validation",
journal="Artificial Intelligence Review",
year="2024",
month="Jun",
day="17",
volume="57",
number="7",
pages="175",
abstract="Large language models (LLMs) have exploded a new heatwave of AI for their ability to engage end-users in human-level conversations with detailed and articulate answers across many knowledge domains. In response to their fast adoption in many industrial applications, this survey concerns their safety and trustworthiness. First, we review known vulnerabilities and limitations of the LLMs, categorising them into inherent issues, attacks, and unintended bugs. Then, we consider if and how the Verification and Validation (V{\&}V) techniques, which have been widely developed for traditional software and deep learning models such as convolutional neural networks as independent processes to check the alignment of their implementations against the specifications, can be integrated and further extended throughout the lifecycle of the LLMs to provide rigorous analysis to the safety and trustworthiness of LLMs and their applications. Specifically, we consider four complementary techniques: falsification and evaluation, verification, runtime monitoring, and regulations and ethical use. In total, 370+ references are considered to support the quick understanding of the safety and trustworthiness issues from the perspective of V{\&}V. While intensive research has been conducted to identify the safety and trustworthiness issues, rigorous yet practical methods are called for to ensure the alignment of LLMs with safety and trustworthiness requirements.",
issn="1573-7462",
doi="10.1007/s10462-024-10824-0",
url="https://doi.org/10.1007/s10462-024-10824-0"
}


@inproceedings{10.1007/978-981-95-4674-9_17,
 abstract = {The rise of autonomous AI agents powered by large language models (LLMs) has been accompanied by new frameworks for integrating these agents with external tools and data. One such framework is Anthropic's Model Context Protocol (MCP), a recently introduced open standard that enables AI assistants to connect with a wide variety of external systems. While MCP unlocks powerful capabilities for agentic AI, it also dramatically expands the supply chain threat surface. In this paper, we investigate supply chain threats in the MCP ecosystem, a rapidly emerging security frontier as community-driven development and open-source MCP servers become prevalent. We identify and categorize major supply chain threats in the MCP ecosystem and validate representative scenarios via proof-of-concept attacks. In particular, we demonstrate how malicious MCP servers, as well as hostile data inputs, can be used to intentionally trigger unauthorized or harmful behaviors, such as sensitive data exfiltration or security policy violations. Finally, we outline two complementary avenues for mitigating MCP supply-chain threats. First, a specification- and document-based validation framework addresses code-driven threats by statically verifying that an MCP tool's implementation aligns with its declared interface and behavior. This method has been prototyped and shown to detect functional inconsistencies in real-world tools. Second, we propose the conceptual design of lightweight runtime validation agents---supervisory components that monitor prompt flows, tool responses, and runtime context to intercept data-driven threats. Together, these layers---proven static analyzers and envisioned runtime validators---form a cohesive foundation for securing AI-agent infrastructures in the MCP ecosystem. By identifying novel attack surfaces and proposing layered defenses, our work represents an early step toward framing supply chain threat dimensions in MCP and AI agent security, and contributes to the ongoing discourse on mitigation strategies for AI-integrated supply chains.},
 address = {Singapore},
 author = {Lee, Yonghwa
and Choi, Wonseok
and Nam, Donghyun},
 booktitle = {Advances in Information and Computer Security},
 editor = {Cid, Carlos
and Yanai, Naoto},
 isbn = {978-981-95-4674-9},
 pages = {329--349},
 publisher = {Springer Nature Singapore},
 title = {Supply Chain Threats in the MCP Ecosystem: Attack Vectors and Mitigation Strategies},
 url = {https://link.springer.com/chapter/10.1007/978-981-95-4674-9_17},
 year = {2026}
}

@Article{Chen2024,
author="Chen, Bingting
and Zou, Weiqin
and Cai, Biyu
and Meng, Qianshuang
and Liu, Wenjie
and Li, Piji
and Chen, Lin",
title="An empirical study on the potential of word embedding techniques in bug report management tasks",
journal="Empirical Software Engineering",
year="2024",
month="Jul",
day="25",
volume="29",
number="5",
pages="122",
abstract="Representing the textual semantics of bug reports is a key component of bug report management (BRM) techniques. Existing studies mainly use classical information retrieval-based (IR-based) approaches, such as the vector space model (VSM) to do semantic extraction. Little attention is paid to exploring whether word embedding (WE) models from the natural language process could help BRM tasks.",
issn="1573-7616",
doi="10.1007/s10664-024-10510-3",
url="https://doi.org/10.1007/s10664-024-10510-3"
}


@Inbook{Bradley2024,
author="Bradley, Herbie
and Fan, Honglu
and Galanos, Theodoros
and Zhou, Ryan
and Scott, Daniel
and Lehman, Joel",
editor="Winkler, Stephan
and Trujillo, Leonardo
and Ofria, Charles
and Hu, Ting",
title="The OpenELM Library: Leveraging Progress in Language Models for Novel Evolutionary Algorithms",
bookTitle="Genetic Programming Theory and Practice XX",
year="2024",
publisher="Springer Nature Singapore",
address="Singapore",
pages="177--201",
abstract="In recent years, Large Language Models (LLMs)Large language models have rapidly progressed in their capabilities in natural language processing (NLP) tasks, which have interestingly grown in scope to include generating computer programs. Indeed, recent studies have demonstrated how LLMs can enable highly proficient genetic programming (GP) algorithms and novel evolutionary algorithms more broadly. Motivated by these opportunities, this paper introduces OpenELM, an open-source Python library for designing evolutionary algorithms that leverage LLMs to intelligently generate variation, as well as to assess fitness and measures of diversity. The library includes implementations of several variation operators, and is designed to accommodate those with limited compute resources, by enabling fast inference, being runnable through hosted notebooks (such as Google Colab), and allowing for API-based LLMs to be used instead of local models run on GPUs. Additionally, OpenELM includes a variety of domain implementations for easy experimentation and adaptation, including several GP domains. The hope is to help researchers easily develop new approaches and applications within the nascent and largely unexplored paradigm of evolutionary algorithms that leverage LLMs.",
isbn="978-981-99-8413-8",
doi="10.1007/978-981-99-8413-8_10",
url="https://doi.org/10.1007/978-981-99-8413-8_10"
}


@Article{PrabhakaraRao2025,
author="Prabhakara Rao, Sriram
and Balaji, Shree Ram Abayankar
and Rahman, Farishta
and Suha, Tasneem
and Mahfuz, Tanzim
and Hossain, Tanvir
and Hasan, Mahmudul
and Hasan, Md Sakib
and Hoque, Tamzidul
and Chakraborty, Prabuddha
and Ranganathan, Prakash",
title="Secure and Trustworthy Microelectronics: Vulnerabilities, Solutions, and Trends",
journal="Journal of Hardware and Systems Security",
year="2025",
month="Dec",
day="01",
volume="9",
number="3",
pages="163--189",
abstract="Microelectronics that make up modern-day devices need to be studied with greater scrutiny on security to mitigate emerging cyber threats. The growth of technology, such as the Internet of Things (IoT), fuels the need to build secure and trustworthy microelectronics that form these end devices. The wide applications of microelectronics broaden the cyber-attack range for adversaries to initiate security attacks. When compromised at a larger scale, impacts can lead to supply and service disruptions in the supply chain, and it must be contained, with risks being eliminated to avoid further impacts. Therefore, understanding the threat surfaces for microelectronics, associated endpoints, evolution, and future developments can help ongoing security research and industrial efforts to protect microelectronics, making them a trustworthy building block of modern electronic devices. This paper reviews the current prevailing security factors contributing to the major impacts of security issues in microelectronics involving hardware and manufacturing vulnerabilities. Furthermore, the exploration extends to various security attacks, including trojans, counterfeiting, and side-channel attacks, covering their attack vectors and associated strategies within the field of microelectronics. This review article also discusses mitigation strategies and security measures for eliminating risks to avoid further impacts. This article provides a comprehensive overview of trustworthy, state-of-the-art microelectronics. By understanding security issues and the current state of countermeasures, we can work towards creating more secure and reliable microelectronic devices across the global supply chain.",
issn="2509-3436",
doi="10.1007/s41635-025-00165-x",
url="https://doi.org/10.1007/s41635-025-00165-x"
}


@inproceedings{10.1007/978-981-97-5606-3_19,
 abstract = {Penetration testing is an effective means of maintaining network security. To address the challenge of high labor costs in traditional penetration testing, researchers have been investigating the potential of automated solutions. In this paper, we introduce an automated penetration testing framework with multi-agent characteristics, named PTGroup, which utilizes the prior knowledge and textual comprehension capabilities of Large Language Models (LLMs). The framework follows the operational mode of ReAct, executing a multi-round decision-making process through successive Thought-Act-Observe cycles, thereby sequentially accomplishing automated penetration testing tasks. We conducted experiments using different LLMs and demonstrated that our framework is capable of adapting to various LLMs. In addition, we proposed an approach to improve PTGroup's adaptability to different vulnerability environments by designing multiple prompt chains and we demonstrated the efficacy of this approach through a series of experiments. Finally, we propose the directions for improvement of the PTGroup, encompassing the use of more specialized LLMs and the adoption of automated prompt chain generation.},
 address = {Singapore},
 author = {Wu, Lei
and Zhong, Xiaofeng
and Liu, Jingju
and Wang, Xiang},
 booktitle = {Advanced Intelligent Computing Technology and Applications},
 editor = {Huang, De-Shuang
and Chen, Wei
and Guo, Jiayang},
 isbn = {978-981-97-5606-3},
 pages = {220--232},
 publisher = {Springer Nature Singapore},
 title = {PTGroup: An Automated Penetration Testing Framework Using LLMs and Multiple Prompt Chains},
 url = {https://link.springer.com/chapter/10.1007/978-981-97-5606-3_19},
 year = {2024}
}

@Article{Yousef2025,
author="Yousef, Mina
and Mohamed, Kareem
and Medhat, Walaa
and Mohamed, Ensaf Hussein
and Khoriba, Ghada
and Arafa, Tamer",
title="BeGrading: large language models for enhanced feedback in programming education",
journal="Neural Computing and Applications",
year="2025",
month="Jan",
day="01",
volume="37",
number="2",
pages="1027--1040",
abstract="In recent years, large language models (LLMs) have gained significant traction across various domains, including education. This paper explores the application of LLMs in grading programming assignments. By leveraging data collected from existing programming assignments and their corresponding grades, we aim to develop a robust LLM-based grading system. We also incorporate augmented data representing various grading scenarios to enhance the model's performance and ensure comprehensive coverage across all grading levels. Our approach involves training the LLM on this combined dataset to enable accurate and consistent evaluation of programming assignments. The proposed model, BeGrading, aims to reduce the grading burden on educators and provide timely and objective feedback to students. Compared to the Codestral model, our proposed model demonstrates an absolute difference rate of 19{\%}, equivalent to {\$}{\$}{\backslash}pm 0.95{\$}{\$}out of 5. This is acceptable for using a small, fine-tuned model with optimized data. Additionally, the Codestral model compared to the dataset optimized score shows a difference of 15{\%} equivalent to a margin of {\$}{\$}{\backslash}pm 0.75{\$}{\$}out of 5. Preliminary results demonstrate the potential of LLMs to perform grading tasks with a high degree of reliability, opening avenues for further research and practical applications in automated education systems.",
issn="1433-3058",
doi="10.1007/s00521-024-10449-y",
url="https://doi.org/10.1007/s00521-024-10449-y"
}


@inproceedings{10.1007/978-981-95-5294-8_35,
 abstract = {As software systems continue to grow in scale and complexity, vulnerability detection has become a critical task in ensuring software security. Recently, retrieval-augmented generation (RAG) techniques combined with large language models (LLMs) have demonstrated significant potential in source code vulnerability detection. However, existing RAG-based approaches typically rely on a single retrieval strategy, either sparse retrieval or dense retrieval. While sparse retrieval excels at precise keyword matching, it struggles with capturing deep semantic relationships, whereas dense retrieval, despite its ability to model semantic similarity, may fail to retrieve low-frequency or exact-match keywords. This trade-off limits both the coverage and accuracy of vulnerability information retrieval. To address these challenges, this paper proposes HRvul-RAG, a hybrid retrieval-based vulnerability detection framework that integrates BM25 and bge-m3 dual-modal retrieval. By leveraging the complementary strengths of sparse and dense retrieval, HRvul-RAG enhances the recall and retrieval precision. Furthermore, to refine the retrieval ranking, we incorporate the weighted reciprocal rank fusion (WRRF) algorithm, which dynamically adjusts the ranking weights on the basis of the query relevance, ensuring robustness and effectiveness in retrieval fusion. The optimized retrieval results are then fed into an LLM for vulnerability analysis, thereby improving the accuracy and intelligence of source code vulnerability detection.},
 address = {Singapore},
 author = {Xie, Honggang
and Li, Xiangdong
and Shen, Zenghui},
 booktitle = {Computer Applications},
 editor = {Huang, Lan
and Xu, Bin
and Chen, Xuebin
and Wang, Shangguang
and Xia, Bing
and Pan, Jianguo
and Song, Xianhua
and Lu, Zeguang},
 isbn = {978-981-95-5294-8},
 pages = {518--528},
 publisher = {Springer Nature Singapore},
 title = {Source Code Vulnerability Detection Method Based on Hybrid Retrieval-Augmented Generation Technology},
 url = {https://link.springer.com/chapter/10.1007/978-981-95-5294-8_35},
 year = {2026}
}

@Article{Veytia2025,
author="Veytia, Devi
and Mariani, Ga{\"e}l
and Mart{\'i} Barclay, Vicky
and Airoldi, Laura
and Claudet, Joachim
and Cooley, Sarah
and Magnan, Alexandre
and Neill, Simon
and Sumaila, U. Rashid
and Th{\'e}baud, Olivier
and Voolstra, Christian R.
and Williamson, Phillip
and Bonnin, Marie
and Langridge, Joseph
and Comte, Adrien
and Viard, Fr{\'e}d{\'e}rique
and Shin, Yunne-Jai
and Bopp, Laurent
and Gattuso, Jean-Pierre",
title="A machine learning-based evidence map of ocean-related options for climate change mitigation and adaptation",
journal="npj Ocean Sustainability",
year="2025",
month="Nov",
day="19",
volume="4",
number="1",
pages="60",
abstract="The ocean has a vital role to play in addressing the global challenge of climate change, which requires both mitigation and adaptation actions. The exponential increase in research relating to ocean-related options (OROs) requires a rapid and reproducible method to assess the state of knowledge. We train a state-of-the-art large language model to characterise the landscape of ORO research by classifying 44,193 ({\textpm}11,615) articles across various descriptors. Research proves to be unevenly distributed, concentrating on OROs with mitigation objectives (80{\%}), while revealing research gaps including under-researched ecosystems and an observed paucity of studies simultaneously assessing different ORO types. We also uncover social inequalities driven by mismatches between the global distribution of research effort, climate change responsibility, and risk. These findings are important to maximise the efficacy of OROs, position them within broader climate action portfolios, and inform future research priorities.",
issn="2731-426X",
doi="10.1038/s44183-025-00159-w",
url="https://doi.org/10.1038/s44183-025-00159-w"
}


@Article{Lou2025,
author="Lou, Jiaxu
and Sun, Yifan",
title="Anchoring bias in large language models: an experimental study",
journal="Journal of Computational Social Science",
year="2025",
month="Dec",
day="05",
volume="9",
number="1",
pages="11",
abstract="Large Language Models (LLMs) like GPT-4 and Gemini have significantly advanced artificial intelligence by enabling machines to generate and comprehend human-like text. Despite their impressive capabilities, LLMs are not without limitations. They exhibit various biases. Although much research has explored demographic biases, cognitive biases in LLM have not been equally studied. This study delves into anchoring bias, a cognitive bias in which initial information disproportionately influences judgment. Using an experimental data set, we examine how anchoring bias manifests in LLM and verify the effectiveness of various mitigation strategies. Our findings highlight the sensitivity of LLM responses to biased prompts. At the same time, our experiments show that to mitigate anchoring bias, one needs to collect information from comprehensive angles to prevent the LLMs from being anchored to individual pieces of information, as simple algorithms such as Chain-of-Thought, Thoughts of Principles, Ignoring Anchor Hints, and Reflection are insufficient.",
issn="2432-2725",
doi="10.1007/s42001-025-00435-2",
url="https://doi.org/10.1007/s42001-025-00435-2"
}


@Article{TauhidUllahShah2026,
author="Tauhid Ullah Shah, Syed
and Hussein, Mohamad
and Barcomb, Ann
and Moshirpour, Mohammad",
title="QuaRUM: qualitative data analysis-based retrieval-augmented UML domain model from requirements documents",
journal="Automated Software Engineering",
year="2026",
month="Jan",
day="09",
volume="33",
number="2",
pages="44",
abstract="Effective Requirements Engineering (RE) is essential for building successful software systems, yet analyzing unstructured stakeholder input remains a persistent challenge. Qualitative Data Analysis (QDA) provides structured methods, open coding (entity extraction), axial coding (relationship discovery), and selective coding (model refinement) to transform natural language requirements into domain models. While manual QDA has proven effective for requirements analysis, it remains time-consuming, repetitive, and difficult to scale. Although individual RE tasks have been automated, no prior work has automated the complete QDA methodology for domain modeling. In this paper, we present QuaRUM, the first framework to automate end-to-end QDA for UML domain model generation by combining large language models with retrieval-augmented generation. QuaRUM processes requirements through document ingestion, semantic indexing, and retrieval-augmented coding, and helps ground each model element in the source text to mitigate hallucination risks. Empirical results show that QuaRUM performs with high accuracy across three domains. It achieves F1-scores between 0.85 and 0.98. Cohen's {\$}{\$}{\backslash}kappa{\$}{\$}reaches up to 0.92, surpassing human inter-coder agreement. Notably, QuaRUM recovers 37 valid attributes and 23 relationships initially missed by human analysts. A cost-benefit analysis shows a 218{\%} Return on Investment (ROI) for initial use, increasing to 1,131{\%} in repeated deployments, demonstrating strong economic scalability.",
issn="1573-7535",
doi="10.1007/s10515-025-00587-5",
url="https://doi.org/10.1007/s10515-025-00587-5"
}


@Article{Zhao2026,
author="Zhao, Yinhu
and Lin, Guanjun
and Liao, Zhenxuan",
title="Long-range context modeling for software vulnerability detection using an XLNet-based approach",
journal="Scientific Reports",
year="2026",
month="Jan",
day="16",
volume="16",
number="1",
pages="5338",
abstract="Software vulnerability detection is a critical area of research in cybersecurity. Recently, various Language Model (LM)-based approaches have shown strong potential in this domain. However, most existing methods rely on Transformer architectures that, while powerful, struggle to capture very long-range code dependencies essential for identifying subtle vulnerabilities. To address this limitation, we introduce XLNetVD, an XLNet-based function-level Vulnerability Detection framework leveraging a bidirectional Transformer-XL model for extended context modeling. XLNet effectively captures long code sequences encompassing data flow, control flow, and variable dependencies that are key factors in vulnerability identification. We benchmark XLNet against six mainstream contextual embedding models and three non-contextual embedding models to evaluate its representation capability for vulnerability detection. Experimental results show that XLNet surpasses CodeBERT and GPT-2, achieving the best F1-score of 68{\%}. Furthermore, by applying the Low-Rank Adaptation (LoRA) fine-tuning technique, we demonstrate that XLNet-LoRA achieves the best trade-off between performance and efficiency among LoRA-enhanced LMs. We further integrate XLNet into an end-to-end framework, XLNetVD, and conduct extensive evaluations on two datasets: a real-world dataset with a highly imbalanced vulnerable-to-non-vulnerable ratio of 1:65, and the SARD dataset, which contains balanced, synthetic samples. Results confirm that XLNetVD consistently delivers competitive performance across both real-world and synthetic datasets, establishing it as one of the state-of-the-art vulnerability detection solutions.",
issn="2045-2322",
doi="10.1038/s41598-026-36196-9",
url="https://doi.org/10.1038/s41598-026-36196-9"
}


@Article{Papastergiou2026,
author="Papastergiou, Spyridon
and Basheer, Nihala
and Lampropoulos, Kostas
and Verrios, Panayiotis
and Islam, Shareeful",
title="Explainable AI based dynamic cybersecurity risk management for cyber insurability",
journal="International Journal of Information Security",
year="2026",
month="Jan",
day="22",
volume="25",
number="1",
pages="36",
abstract="Cybersecurity risk is one of the primary and growing concerns for ensuing security and resilience of organizations, regardless of their size and type. While proactive risk management is effective, it is challenging due to the evolving and sophisticated threat landscape, exploitation of known and unknown vulnerabilities, and a dynamic security context. The dynamic security context further complicates to calculate the accurate risk level, leading to risk perception that can vary between different stakeholders. However, the demand for adopting cyber insurance is increasing as an effective risk mitigation strategy to avoid any potential loss. In this context, this paper proposes an Explainable AI (XAI) based dynamic cybersecurity risk management approach for informed cyber insurability decision making. The approach utilizes an Large Language Model (LLM) based framework for real-time, contextualized risk level assessment and adopts XAI techniques such as feature contribution and correlation, to justify the decision making. A comprehensive evaluation using an industrial use case and experiment demonstrates the applicability of the proposed approach. The experiment part uses a widely used vulnerability dataset to predicate high exploitable vulnerabilities and links them with the identified assets of the use case scenario. The result shows 96.9{\%} accuracy for the exploitable vulnerability identification and XAI operationalisation justifies the selection of right security control and the cyber insurability decision based on the residual risk.",
issn="1615-5270",
doi="10.1007/s10207-025-01189-8",
url="https://doi.org/10.1007/s10207-025-01189-8"
}


@inproceedings{10.1007/978-3-031-72992-8_22,
 abstract = {The security concerns surrounding Large Language Models (LLMs) have been extensively explored, yet the safety of Multimodal Large Language Models (MLLMs) remains understudied. In this paper, we observe that Multimodal Large Language Models (MLLMs) can be easily compromised by simple query-relevant images when paired with a malicious text query. This attack is achieved without the need for adversarial attacks on either the text or the images. To address this, we introduce MM-SafetyBench, a comprehensive framework designed for conducting safety-critical evaluations of MLLMs against such image-based manipulations. We have compiled a dataset comprising 13 scenarios, resulting in a total of 5,040 text-image pairs. Our analysis across 12 state-of-the-art models reveals that MLLMs are susceptible to breaches instigated by our approach, even when the equipped LLMs have been safety-aligned. In response, we propose a straightforward yet effective prompting strategy to enhance the resilience of MLLMs against these types of attacks. Our work underscores the need for a concerted effort to strengthen and enhance the safety measures of open-source MLLMs against potential malicious exploits. The resource is available at https://github.com/isXinLiu/MM-SafetyBench.},
 address = {Cham},
 author = {Liu, Xin
and Zhu, Yichen
and Gu, Jindong
and Lan, Yunshi
and Yang, Chao
and Qiao, Yu},
 booktitle = {Computer Vision -- ECCV 2024},
 editor = {Leonardis, Ale{\v{s}}
and Ricci, Elisa
and Roth, Stefan
and Russakovsky, Olga
and Sattler, Torsten
and Varol, G{\"u}l},
 isbn = {978-3-031-72992-8},
 pages = {386--403},
 publisher = {Springer Nature Switzerland},
 title = {MM-SafetyBench: A Benchmark for Safety Evaluation of Multimodal Large Language Models},
 url = {https://link.springer.com/chapter/10.1007/978-3-031-72992-8_22},
 year = {2025}
}

@inproceedings{10.1007/978-3-031-89350-6_8,
 abstract = {The last couple of years were marked by the rise of LLMs (Large Language Models) that are shown to bring important developments in various sectors such as technology, education or customer service. Generative AI also impacts other areas such as content generation, data analysis, and can be used to automate various systems. From a security point of view, initial applications of LLMs improve customer support services - be it direct customer support or security operation centers - and also the way an IT administrator connects with SIEM (Security Information and Event Management) or XDR (Extended Detection and Response) products.},
 address = {Cham},
 author = {Marilena, Lupa{\c{s}}cu
and Constantin, Vi{\c{T}}el Silviu
and Teodor, Gavrilu{\c{T}} Drago{\c{s}}
and Henri, Luchian},
 booktitle = {Risks and Security of Internet and Systems},
 editor = {Collart-Dutilleul, Simon
and Ouchani, Samir
and Cuppens, Nora
and Cuppens, Fr{\'e}d{\'e}ric},
 isbn = {978-3-031-89350-6},
 pages = {125--140},
 publisher = {Springer Nature Switzerland},
 title = {On the Properness of Large Language Models for Malware Detection},
 url = {https://link.springer.com/chapter/10.1007/978-3-031-89350-6_8},
 year = {2025}
}

@Article{Grigorev2025,
author="Grigorev, Artur
and Saleh, Khaled
and Ou, Yuming
and Mih{\u{a}}i{\c{T}}{\u{a}}, Adriana-Simona",
title="Enhancing Traffic Incident Management with Large Language Models: A Hybrid Machine Learning Approach for Severity Classification",
journal="International Journal of Intelligent Transportation Systems Research",
year="2025",
month="Apr",
day="01",
volume="23",
number="1",
pages="259--280",
abstract="This research showcases the innovative integration of Large Language Models into machine learning workflows for traffic incident management, focusing on the classification of incident severity using accident reports. By leveraging features generated by modern language models alongside conventional data extracted from incident reports, our research demonstrates improvements in the accuracy of severity classification across several machine learning algorithms. Our contributions are threefold. First, we present an extensive comparison of various machine learning models paired with multiple large language models for feature extraction, aiming to identify the optimal combinations for accurate incident severity classification. Second, we contrast traditional feature engineering pipelines with those enhanced by language models, showcasing the superiority of language-based feature engineering in processing unstructured text. Third, our study illustrates how merging baseline features from accident reports with language-based features can improve the severity classification accuracy. This comprehensive approach not only advances the field of incident management but also highlights the cross-domain application potential of our methodology, particularly in contexts requiring the prediction of event outcomes from unstructured textual data or features translated into textual representation. Specifically, our novel methodology was applied to three distinct datasets originating from the United States, the United Kingdom, and Queensland, Australia. This cross-continental application underlines the robustness of our approach, suggesting its potential for widespread adoption in improving incident management processes globally. The code and data subsets are available by the link: https://github.com/Future-Mobility-Lab/LLM-Incident-Classification.",
issn="1868-8659",
doi="10.1007/s13177-024-00448-7",
url="https://doi.org/10.1007/s13177-024-00448-7"
}


@Article{Leong2025,
author="Leong, Shi Xuan
and Griesbach, Caleb E.
and Zhang, Rui
and Darvish, Kourosh
and Zhao, Yuchi
and Mandal, Abhijoy
and Zou, Yunheng
and Hao, Han
and Bernales, Varinia
and Aspuru-Guzik, Al{\'a}n",
title="Steering towards safe self-driving laboratories",
journal="Nature Reviews Chemistry",
year="2025",
month="Oct",
day="01",
volume="9",
number="10",
pages="707--722",
abstract="The past decade has witnessed remarkable advancements in autonomous systems, such as automobiles that are evolving from traditional vehicles to ones capable of navigating complex environments without human intervention. Similarly, the rise of self-driving laboratories (SDLs), which leverage robotics and artificial intelligence to accelerate discovery, is driving a paradigm shift in scientific research. As SDLs evolve to expand the scope of chemical processes that can be performed, it is essential to bring safety to the forefront to ensure that the necessary safeguards are in place to mitigate against potential accidents that range from near-misses to catastrophic failures. This Perspective examines the development trajectory of SDLs, juxtaposing their development with those of other autonomous technologies, with a particular focus on safety. We explore current safety status and concerns, identify opportunities for innovation to shape this rapidly evolving landscape, and reflect on the actions the SDL community can take moving forward.",
issn="2397-3358",
doi="10.1038/s41570-025-00747-x",
url="https://doi.org/10.1038/s41570-025-00747-x"
}


@Article{Lebed2024,
author="Lebed, S. V.
and Namiot, D. E.
and Zubareva, E. V.
and Khenkin, P. V.
and Vorobeva, A. A.
and Svichkar, D. A.",
title="Large Language Models in Cyberattacks",
journal="Doklady Mathematics",
year="2024",
month="Dec",
day="01",
volume="110",
number="2",
pages="S510--S520",
abstract="The article provides an overview of the practice of using large language models (LLMs) in cyberattacks. Artificial intelligence models (machine learning and deep learning) are applied across various fields, with cybersecurity being no exception. One aspect of this usage is offensive artificial intelligence, specifically in relation to LLMs. Generative models, including LLMs, have been utilized in cybersecurity for some time, primarily for generating adversarial attacks on machine learning models. The analysis focuses on how LLMs, such as ChatGPT, can be exploited by malicious actors to automate the creation of phishing emails and malware, significantly simplifying and accelerating the process of conducting cyberattacks. Key aspects of LLM usage are examined, including text generation for social engineering attacks and the creation of malicious code. The article is aimed at cybersecurity professionals, researchers, and LLM developers, providing them with insights into the risks associated with the malicious use of these technologies and recommendations for preventing their exploitation as cyber weapons. The research emphasizes the importance of recognizing potential threats and the need for active countermeasures against automated cyberattacks.",
issn="1531-8362",
doi="10.1134/S1064562425700012",
url="https://doi.org/10.1134/S1064562425700012"
}


@inproceedings{10.1007/978-981-96-9101-2_18,
 abstract = {Fuzz drivers are essential components in fuzzing, acting as the interface between the fuzzer and the target library. Recent advancements have led to a variety of methods aimed at automating and optimizing fuzz driver generation. However, the performance and applicability of these techniques are influenced by factors such as the complexity of the target library. This paper offers a systematic review of recent progress in fuzz driver generation techniques, providing a comprehensive categorization and analysis of key approaches in the field. We propose a robust set of evaluation criteria that address critical challenges, including statistical evaluation, resource management, and API coverage, establishing a more standardized framework for future assessments. Furthermore, we introduce a set of Best Practices for evaluating fuzz driver generation techniques, which provides practical guidelines to improve transparency, consistency, and reproducibility in research. Finally, we identify promising research directions to address current limitations, with a particular focus on leveraging emerging technologies such as large language models (LLMs) and expanding fuzz driver support to new programming languages and platforms.},
 address = {Singapore},
 author = {Yan, Qian
and Huang, Minhuan
and Cao, Huayang
and Lu, Shuaibing},
 booktitle = {Information Security and Privacy},
 editor = {Susilo, Willy
and Pieprzyk, Josef},
 isbn = {978-981-96-9101-2},
 pages = {348--368},
 publisher = {Springer Nature Singapore},
 title = {SoK: From Systematization to Best Practices in Fuzz Driver Generation},
 url = {https://link.springer.com/chapter/10.1007/978-981-96-9101-2_18},
 year = {2025}
}

@inproceedings{10.1007/978-981-97-5498-4_4,
 abstract = {Deep learning has showcased remarkable performance in source code vulnerability detection. However, significant challenges persist in terms of generalization and handling real-world samples. These challenges are frequently attributed to dataset distribution shift, such as spurious correlations. While previous research has explored spurious correlations in other tasks, such as text classification and function naming, vulnerability detection has yet to receive extensive study in this context. This paper proposes a novel approach called VulCausal, which integrates a causal inference framework into neural network models for vulnerability detection. VulCausal aims to capture and address the spurious correlations present in the API function, user-defined identifiers, and code structure during the training phase. The mitigation of spurious correlations is achieved through backdoor adjustment in the inference phase, effectively mitigating the effects of these confounding factors. Experimental results demonstrate that VulCausal significantly enhances the accuracy and robustness of vulnerability detection. It achieves state-of-the-art accuracy in the CodeXGLUE defect dataset benchmark and ranks first on the leaderboard. Additionally, it reduces the attack success rate from 63.08{\%} to 23.7{\%} when confronted with a state-of-the-art adversarial attack called ALERT, which is for a pre-trained language model of code.},
 address = {Singapore},
 author = {Kuang, Hongyu
and Zhang, Jingjing
and Yang, Feng
and Zhang, Long
and Huang, Zhijian
and Yang, Lin},
 booktitle = {Knowledge Science, Engineering and Management},
 editor = {Cao, Cungeng
and Chen, Huajun
and Zhao, Liang
and Arshad, Junaid
and Asyhari, Taufiq
and Wang, Yonghao},
 isbn = {978-981-97-5498-4},
 pages = {41--56},
 publisher = {Springer Nature Singapore},
 title = {VulCausal: Robust Vulnerability Detection Using Neural Network Models from a Causal Perspective},
 url = {https://link.springer.com/chapter/10.1007/978-981-97-5498-4_4},
 year = {2024}
}

@inproceedings{10.1007/978-3-032-07313-6_6,
 abstract = {Large Language Models (LLMs) have become increasingly popular in academia and industry due to their ability to comprehend natural language and generate content to perform various tasks. Numerous studies suggested that service-oriented computing (SOC), a key computing paradigm for building distributed enterprise applications, can benefit from LLMs. We observed that SOC researchers are increasingly applying LLMs to tasks such as service composition, service code generation, and service monitoring. However, no study systematically reviews these research efforts and delineates research challenges. To fill this research gap, this study presents a systematic literature review of 64 papers published on LLMs for SOC tasks between January 2019 and February 2025. We aimed to analyze and summarize the relevant studies and formulate a research roadmap to provide a context for shaping ongoing research activities. We categorized our findings into four SOC domains: service foundation, service composition, monitoring and management, and design and development.},
 address = {Cham},
 author = {Kumara, Indika
and Kaplan, Hasan
and Owotogbe, Joshua
and Tamburri, Damian Andrew
and van den Heuvel, Willem-Jan},
 booktitle = {Service-Oriented Computing},
 editor = {Aiello, Marco
and Barzen, Johanna
and Dustdar, Schahram
and Leymann, Frank},
 isbn = {978-3-032-07313-6},
 pages = {90--113},
 publisher = {Springer Nature Switzerland},
 title = {Large Language Models for Service-Oriented Computing (LLM4SOC): Review and Research Directions},
 url = {https://link.springer.com/chapter/10.1007/978-3-032-07313-6_6},
 year = {2026}
}

@inproceedings{10.1007/978-3-032-04403-7_10,
 abstract = {AI-powered software tools are widely used to assist software engineers. However, there is still a need to understand the productivity benefits of such tools for software engineers. In addition to short-term benefits, there is a question of how adopting AI-generated solutions affects the quality of software over time (e.g., maintainability).},
 address = {Cham},
 author = {Amasanti, Giorgio
and Jahi{\'{c}}, Jasmin},
 booktitle = {Software Architecture. ECSA 2025 Tracks and Workshops},
 editor = {Bianculli, Domenico
and Sartaj, Hassan
and Andrikopoulos, Vasilios
and Pautasso, Cesare
and Mikkonen, Tommi
and Perez, Jennifer
and Bure{\v{s}}, Tom{\'a}{\v{s}}
and De Sanctis, Martina
and Muccini, Henry
and Navarro, Elena
and Soliman, Mohamed
and Zdun, Uwe},
 isbn = {978-3-032-04403-7},
 pages = {89--104},
 publisher = {Springer Nature Switzerland},
 title = {The Impact of AI-Generated Solutions on Software Architecture and Productivity: Results from a Survey Study},
 url = {https://link.springer.com/chapter/10.1007/978-3-032-04403-7_10},
 year = {2026}
}

@Article{Spelda2025,
author="Spelda, Petr
and Stritecky, Vit",
title="Security practices in AI development",
journal="AI {\&} SOCIETY",
year="2025",
month="Aug",
day="01",
volume="40",
number="6",
pages="4869--4879",
abstract="What makes safety claims about general purpose AI systems such as large language models trustworthy? We show that rather than the capabilities of security tools such as alignment and red teaming procedures, it is security practices based on these tools that contributed to reconfiguring the image of AI safety and made the claims acceptable. After showing what causes the gap between the capabilities of security tools and the desired safety guarantees, we critically investigate how AI security practices attempt to fill the gap and identify several shortcomings in diversity and participation. We found that these security practices are part of securitization processes aiming to support (commercial) development of general purpose AI systems whose trustworthiness can only be imperfectly tested instead of guaranteed. We conclude by offering several improvements to the current AI security practices.",
issn="1435-5655",
doi="10.1007/s00146-025-02247-4",
url="https://doi.org/10.1007/s00146-025-02247-4"
}


@Inbook{Berton2026,
author="Berton, Luca",
title="Monitoring and Maintenance",
bookTitle="Practical RHEL AI: Designing, Deploying and Scaling AI Solutions with Red Hat Enterprise Linux",
year="2026",
publisher="Apress",
address="Berkeley, CA",
pages="143--177",
abstract="Let us examine the telemetry associated with an AI solution, which converts raw data into meaningful Service-Level Objectives (SLOs) and proactive rollback scripts that identify issues before they impact users. I will guide you through how RPO/RTO budgets define clear boundaries for each component---and how boot images, geo-replicated Quay registries, and GitOps automation assist in preemptive problem resolution with minimal delay. This approach aims to transition from midnight emergencies to seamless confidence.",
isbn="979-8-8688-1901-8",
doi="10.1007/979-8-8688-1901-8_6",
url="https://doi.org/10.1007/979-8-8688-1901-8_6"
}


@Article{Peng2023,
author="Peng, Xin",
title="Software development in the age of intelligence: embracing large language models with the right approach",
journal="Frontiers of Information Technology {\&} Electronic Engineering",
year="2023",
month="Nov",
day="01",
volume="24",
number="11",
pages="1513--1519",
abstract="Embracing LLMs is definitely a correct and even necessary direction for software enterprises to improve quality and efficiency. However, achieving systematic and comprehensive intelligent software development still requires careful consideration and there is much fundamental work to do. For enterprises, solidifying the digitization and knowledge accumulation of software development, as well as the fundamental capabilities of software engineering such as requirement analysis, design, and validation, remains crucial and is also a basic condition for achieving higher levels of intelligent development. For academic research, there is still much work to do in the direction of systematic and comprehensive intelligent software development. This also requires us have a deeper understanding of the complexity of software systems and software requirements and design, based on understanding the capabilities of LLMs.",
issn="2095-9230",
doi="10.1631/FITEE.2300537",
url="https://doi.org/10.1631/FITEE.2300537"
}


@inproceedings{10.1007/978-981-96-5860-2_14,
 abstract = {In today's complex business landscape, maintaining high standards of operational efficiency and reliability in both business and technology environments is critical. Managing distributed systems across on-premises, cloud, and hybrid environments while ensuring seamless business operations, such as payment repairs and FX reconciliation, requires robust, real-time observability. This paper introduces a comprehensive observability framework that integrates probabilistic models, machine learning (ML) algorithms, large language models (LLMs), and rule-based systems. This framework dynamically guides event data, structured and unstructured, through intelligent paths to ensure accurate categorization and timely issue resolution, ultimately enhancing key business and technology KPIs. Empirical validations from hypothetical and real-world scenarios strengthen its applicability, and a detailed implementation guideline provides actionable insights for deploying the framework. Additionally, a clearer breakdown of framework components, scalability considerations, diverse datasets, and real-world case studies are presented for a more comprehensive understanding.},
 address = {Singapore},
 author = {Allu, Vikram Kishore Murty
and Pichu, Narayanan},
 booktitle = {Data Management, Analytics and Innovation},
 editor = {Goswami, Saptarsi
and Saha, Sajal
and Basu, Kanadpriya
and Beed, Romit S.},
 isbn = {978-981-96-5860-2},
 pages = {227--243},
 publisher = {Springer Nature Singapore},
 title = {Amalgamated Business and Technology Operations Observability Framework},
 url = {https://link.springer.com/chapter/10.1007/978-981-96-5860-2_14},
 year = {2026}
}

@inproceedings{10.1007/978-3-031-46002-9_24,
 abstract = {This paper presents an approach to no-code development based on the interplay of formally defined (graphical) Domain-Specific Languages and informal, intuitive Natural Language which is enriched with contextual information to enable referencing of formally defined entities. The paper focuses on the use and automated integration of these enriched intuitive languages via ChatGPT-based code generation to exploit the best of both language paradigms for domain-specific application development. To compensate for the lack of control over the intuitive languages we apply automated system-level validation via automata learning and subsequent model checking. All this is illustrated using the development of point-and-click adventures as a minimal viable example.},
 address = {Cham},
 author = {Busch, Daniel
and Nolte, Gerrit
and Bainczyk, Alexander
and Steffen, Bernhard},
 booktitle = {Bridging the Gap Between AI and Reality},
 editor = {Steffen, Bernhard},
 isbn = {978-3-031-46002-9},
 pages = {375--390},
 publisher = {Springer Nature Switzerland},
 title = {ChatGPT in the Loop: A Natural Language Extension for Domain-Specific Modeling Languages},
 url = {https://link.springer.com/chapter/10.1007/978-3-031-46002-9_24},
 year = {2024}
}

@Article{Wang2026,
author="Wang, Jingxuan
and Meng, Weiliang
and Jin, Yixiao
and Wang, Jinyuan
and Qin, Yiming
and Yang, Meng",
title="A survey of visual-language foundation models for enhancing virtual and augmented reality interactivity",
journal="The Visual Computer",
year="2026",
month="Jan",
day="13",
volume="42",
number="1",
pages="127",
abstract="Visual-language foundation models (VLFMs) are rapidly emerging as key enablers of interaction in virtual and augmented reality (VR/AR). By integrating advanced visual perception with language-based reasoning, they overcome the limitations of rule-driven systems and enable more natural, adaptive, and context-aware engagement. This paper provides a systematic survey of the evolving roles of VLFMs in immersive environments. We introduce the Interaction-Role Taxonomy, a novel framework that categorizes VLFMs into three complementary functions: the Semantic Interpreter, which perceives and interprets environments; the Embodied Agent, which facilitates collaborative interaction; and the Generative Engine, which creates dynamic content. Building on this framework, we review representative applications, analyze implementation pipelines and evaluation protocols, and critically discuss technical and ethical challenges. Finally, we identify open research directions to enhance the capability, efficiency, and trustworthiness of VLFMs, paving the way for next-generation human--computer interaction in immersive VR/AR ecosystems.",
issn="1432-2315",
doi="10.1007/s00371-025-04332-7",
url="https://doi.org/10.1007/s00371-025-04332-7"
}


@inproceedings{10.1007/978-981-96-6537-2_10,
 abstract = {Generative AI (or in short, GenAI) is redefining the software development landscape by automating complex tasks and significantly enhancing productivity. Utilizing powerful Large Language Models (LLMs) such as OpenAI's ChatGPT, GenAI allows developers to optimize key tasks such as code generation, refactoring, and debugging. This paper provides a thorough analysis and recommendations of leading AI-assisted tools in the industry, highlighting their potential to automate code-related tasks and enhancing developer productivity. It aims at providing a summarized set of recommendations for leveraging GenAI in software development. It also identifies best practices for exploiting such technologies, including intelligent code suggestions with GitHub Copilot, advanced code refactoring with DeepCode, and AI powered completions using Tabnine. These guidelines are to assist developers and organizations in choosing the best generative AI solution so that they can speed up their development cycles, improve the quality of their codes, and ensure scalable integration within projects.},
 address = {Singapore},
 author = {Sengupta, Sagnik
and Pal, Sayak
and Das, Soumili
and Chattopadhyay, Swastik
and Das, Amit Kumar},
 booktitle = {Data Management, Analytics and Innovation},
 editor = {Goswami, Saptarsi
and Saha, Sajal
and Beed, Romit S.
and Basu, Kanadpriya},
 isbn = {978-981-96-6537-2},
 pages = {149--162},
 publisher = {Springer Nature Singapore},
 title = {Recommendation Framework for Generative AI-Assisted Software Development},
 url = {https://link.springer.com/chapter/10.1007/978-981-96-6537-2_10},
 year = {2026}
}

@Article{Sheikhaei2024,
author="Sheikhaei, Mohammad Sadegh
and Tian, Yuan
and Wang, Shaowei
and Xu, Bowen",
title="An empirical study on the effectiveness of large language models for SATD identification and classification",
journal="Empirical Software Engineering",
year="2024",
month="Oct",
day="01",
volume="29",
number="6",
pages="159",
abstract="Self-Admitted Technical Debt (SATD), a concept highlighting sub-optimal choices in software development documented in code comments or other project resources, poses challenges in the maintainability and evolution of software systems. Large language models (LLMs) have demonstrated significant effectiveness across a broad range of software tasks, especially in software text generation tasks. Nonetheless, their effectiveness in tasks related to SATD is still under-researched. In this paper, we investigate the efficacy of LLMs in both identification and classification of SATD. For both tasks, we investigate the performance gain from using more recent LLMs, specifically the Flan-T5 family, across different common usage settings. Our results demonstrate that for SATD identification, all fine-tuned LLMs outperform the best existing non-LLM baseline, i.e., the CNN model, with a 4.4{\%} to 7.2{\%} improvement in F1 score. In the SATD classification task, while our largest fine-tuned model, Flan-T5-XL, still led in performance, the CNN model exhibited competitive results, even surpassing four of six LLMs. We also found that the largest Flan-T5 model, i.e., Flan-T5-XXL, when used with a zero-shot in-context learning (ICL) approach that only provides instructions for SATD identification, yields competitive results with traditional approaches but performs 6.4{\%} to 9.2{\%} worse than fine-tuned LLMs. For SATD classification, few-shot ICL approach, incorporating examples and category descriptions in prompts, outperforms the zero-shot approach and even surpasses the fine-tuned smaller Flan-T5 models. Moreover, our experiments demonstrate that incorporating contextual information, such as surrounding code, into the SATD classification task enables larger fine-tuned LLMs to improve their performance. Our study highlights the capabilities and limitations of LLMs for SATD tasks and the role of contextual information in achieving higher performance with larger LLMs, setting a foundation for future efforts to enhance these models for more effective technical debt management.",
issn="1573-7616",
doi="10.1007/s10664-024-10548-3",
url="https://doi.org/10.1007/s10664-024-10548-3"
}


@Article{ref1,
title="Selected Abstracts from the SIIM 2025 Annual Meeting of the Society for Imaging Informatics in Medicine (SIIM)",
journal="Journal of Imaging Informatics in Medicine",
year="2025",
month="Sep",
day="01",
volume="38",
number="1",
pages="1--49",
issn="2948-2933",
doi="10.1007/s10278-025-01501-x",
url="https://doi.org/10.1007/s10278-025-01501-x"
}


@Inbook{Möller2026,
author="M{\"o}ller, Dietmar P. F.",
title="Digitalization and Cybersecurity",
bookTitle="Cybersecurity for Network and Information Security: Principles, Techniques and Applications",
year="2026",
publisher="Springer Nature Switzerland",
address="Cham",
pages="1--73",
abstract="Digitalization and digital transformation representing a global phenomenon, capturing the attention in every industry, organization and society, spurring major investments to succeed. However, digitalization and thus digital transformation are no single objectives, rather they depend on the digital maturity of industrial sectors, organizations and society. With digitalization, industry, organizations and society can store, process and visualize digital data in new ways to speed up with decision making, enhance new products, productivity, services, and others. In this regard, digitalization is a digital transformation process that involves the embedment of digital technologies into all areas of industrial and organizational business as well as society activities, leading to disruptive changes compared with preceded technological waves. This can change business models and operations as well as market requirements to deliver values to customers. Furthermore, digital transformation is the way of change from a monolithic business approach to dynamic and rapidly evolving digital business forms, driving disruptive change across all industries. Given this fact, this chapter introduce in Sect. 1.1 into evolving features of digitalization with their unprecedented impact on all sectors of industry and society development today, which take advantage of digital technologies and digital data to enhance their business success. Section 1.2 focuses on challenges and obstacles in digital transformation leveraging knowledge and embedding it into businesses to enhance performance and create new value. Section 1.3 introduce into cybersecurity that apply technologies, processes, and controls to protect digital systems, networks, programs, devices and data from inherent cyber risks. In general cybersecurity aims to reduce cyber risks, for example, protecting against unauthorized exploitation of digital systems, networks, and technologies. This require at the foremost cybersecurity situational awareness, cybersecurity risk assessment and risk-management, to reach a high cybersecurity maturity level. From the practical perspective, Section 1.4 provide a guidance to cybersecure Operational Technology (OT), while Sect. 1.5 introduce in the CIA triad, a globally accepted model to gain cybersecurity in information systems. Finally, Sect. 1.6 describe why actionable knowledge in cybersecurity is still paramount and should be situational aware and sustainable in usage. This also considers Artificial Intelligence (AI) applications today. Applying AI and Machine Learning (ML) high-fidelity detection of cyber threat risk incidents and automated responses are necessary to effectively defend against today's sophisticated cyber threat risk incidents. This measure protect organizations through monitoring, detecting, analyzing, and investigating potential cyber threat risk incidents. Section 1.7 contains comprehensive questions, followed by references for further reading.",
isbn="978-3-031-99790-7",
doi="10.1007/978-3-031-99790-7_1",
url="https://doi.org/10.1007/978-3-031-99790-7_1"
}


@Article{Alekseevskaia2025,
author="Alekseevskaia, I. S.
and Arkhipenko, K. V.
and Turdakov, D. Yu.",
title="Development of a Red-Teaming Dataset to Defend Large Language Models Against Attacks",
journal="Programming and Computer Software",
year="2025",
month="Dec",
day="01",
volume="51",
number="7",
pages="472--477",
abstract="State-of-the-art (SOTA) large language models (LLMs) are huge systems with complex internal mechanisms that implement black-box response generation. Even though aligned LLMs have built-in defense mechanisms against attacks, recent studies demonstrate the vulnerability of LLMs to jailbreak attacks. In this study, we aim to extend the existing red-teaming datasets obtained from jailbreak attacks to address these LLM vulnerabilities in the future. In addition, we carry out some experiments with SOTA LLMs on our dataset to demonstrate the existing weaknesses in these models.",
issn="1608-3261",
doi="10.1134/S0361768825700343",
url="https://doi.org/10.1134/S0361768825700343"
}


@inproceedings{10.1007/978-3-031-88531-0_17,
 abstract = {[Context and Motivation] Software engineers can interact with users through digital channels (e.g., online forums) to exchange information about software products and achieve their requirements engineering (RE) goals. However, conducting RE manually is challenging due to the large number of users and the volume of their online feedback. [Question/Problem] Previous work has proposed tools to automatically extract useful information from online feedback (e.g., feature requests); however, these tools suffer from three major limitations: (i) an overlooked RE perspective in their design and evaluation; (ii) insufficient functional and performance capabilities; and (iii) missing evaluations of their ability to address RE needs. [Principal Idea/Results] This paper presents a vision for an intelligent RE software agent designed to overcome these limitations. Specifically, our vision explores how RE can guide the design and evaluation of software agents powered by large language models (LLMs), proposes empirical assessments of LLMs for RE usage and the agent's ability to meet RE needs. [Contributions] Our contribution is threefold: (i) a vision for an RE agent, (ii) identification of key challenges, and (iii) a roadmap to address current limitations.},
 address = {Cham},
 author = {D{\k{a}}browski, Jacek
and Bennaceur, Amel
and Rajbahadur, Gopi Krishnan
and Nuseibeh, Bashar
and Alrimawi, Faeq},
 booktitle = {Requirements Engineering: Foundation for Software Quality},
 editor = {Hess, Anne
and Susi, Angelo},
 isbn = {978-3-031-88531-0},
 pages = {235--243},
 publisher = {Springer Nature Switzerland},
 title = {Prompt Me: Intelligent Software Agent for Requirements Engineering - A Vision Paper},
 url = {https://link.springer.com/chapter/10.1007/978-3-031-88531-0_17},
 year = {2025}
}

@Article{Liu2025,
author="Liu, Shuai
and Pan, Yiheng
and Hong, Kun
and Fei, Ruite
and Lin, Chenhao
and Li, Qian
and Shen, Chao",
title="Backdoor threats in large language models---a survey",
journal="Science China Information Sciences",
year="2025",
month="Aug",
day="14",
volume="68",
number="9",
pages="191101",
abstract="Large language models (LLMs), with their advanced language comprehension and text generation capabilities, have demonstrated remarkable performance across diverse application scenarios involving code processing, search engines, and translation, among others. However, these models have become increasingly vulnerable to security threats, particularly to backdoor attacks. Therefore, a timely and comprehensive review of the existing backdoor threats is urgently required. In this paper, we present a systematic and timely review of the research on backdoor attacks on LLMs, categorising existing attack and defence methods according to the LLM. Additionally, we draw comparisons with backdoor attacks in traditional deep learning to provide a more intuitive understanding of backdoor threats in LLMs. Through this effective analysis and an evaluation of the reviewed studies, we identify the current research challenges and propose potential future research directions to address these issues.",
issn="1869-1919",
doi="10.1007/s11432-024-4351-3",
url="https://doi.org/10.1007/s11432-024-4351-3"
}


@Inbook{Antipova2025,
author="Antipova, Tatiana
and Riurean, Simona
and Riurean, Patricia
and Bolog, George",
editor="Antipova, Tatiana",
title="AI-Powered Tools: Threat, Defense, and Cyber-Resilience for Individuals",
bookTitle="Digital Technology Platforms and Deployment",
year="2025",
publisher="Springer Nature Switzerland",
address="Cham",
pages="171--188",
abstract="As Artificial Intelligence (AI) becomes increasingly involved into our lives, the threats posed by AI-powered cyberattacks are growing more sophisticated. This chapter provides a comprehensive overview of AI-powered tools that can both threaten and defend individuals in the digital world. We discuss the various tactics used to increase individuals' cyber-resilience. This chapter considers the diverse approaches to education and awareness that available to empower individuals to combat cybersecurity threats. The goal of this chapter is to detect the cybersecurity threats that developed by AI tools for individuals, define a new frontier in cybersecurity defense, and provide good practices to protect individuals' data and their own smart devices, online.",
isbn="978-3-031-86547-3",
doi="10.1007/978-3-031-86547-3_14",
url="https://doi.org/10.1007/978-3-031-86547-3_14"
}


@inproceedings{10.1007/978-981-96-1694-7_17,
 abstract = {Large Language Models (LLMs) are currently very popular and have been widely used. However, despite the success, LLMs are not entirely safe. This has been confirmed by jailbreak attacks. The main difficulty of fixing the leak is that the training datasets for LLMs are huge and not entirely harmless. There is a risk of generating harmful content based on user prompts when generating outputs, although various developers of large models have made restrictions.},
 address = {Singapore},
 author = {Wang, Xueting
and Wei, Yanjie},
 booktitle = {Proceedings of the 3rd International Conference on Machine Learning, Cloud Computing and Intelligent Mining (MLCCIM2024)},
 editor = {Sun, Fuchun
and Wang, Hesheng
and Long, Han
and Wei, Yifei
and Yu, Hongqi},
 isbn = {978-981-96-1694-7},
 pages = {198--209},
 publisher = {Springer Nature Singapore},
 title = {Defender: The Possibility of Repairing Jailbreak Defects},
 url = {https://link.springer.com/chapter/10.1007/978-981-96-1694-7_17},
 year = {2025}
}

@inproceedings{10.1007/978-3-032-01377-4_1,
 abstract = {As developers increasingly rely on Large Language Models (LLMs) to generate code, the pace of software development is accelerating beyond the capabilities of traditional design-time verification and testing methods. We predict a paradigm shift towards continuous monitoring to complement and eventually supersede upfront verification. By embracing a ``correct-ish by design'' philosophy, we acknowledge the inevitability of imperfections in LLM-generated code. We anticipate an adaptive approach where real-time monitoring and feedback mechanisms are employed to detect, diagnose, and rectify issues as they emerge in the field. This continuous monitoring strategy not only ensures sustained software reliability and performance, but also provides valuable insights into LLM behavior, facilitating iterative improvements. Specifically, we use an LLM to generate Python code from a formal specification written in the VDM specification language, accessible as a PDF document. The VDM specification formalizes aspects of NASA's SAFER rescue system, which uses small thrusters on a backpack to let astronauts maneuver and return safely to the spacecraft during spacewalks in case they become untethered. We experiment with property-based testing, and by using two Python programs, both generated from the specification by the LLM in two different developments, to monitor each other during runtime.},
 address = {Cham},
 author = {Aichernig, Bernhard K.
and Havelund, Klaus},
 booktitle = {Bridging the Gap Between AI and Reality},
 editor = {Steffen, Bernhard},
 isbn = {978-3-032-01377-4},
 pages = {3--29},
 publisher = {Springer Nature Switzerland},
 title = {Correct-ish by Design: From Upfront Verification to Continuous Monitoring of LLM Generated Code},
 url = {https://link.springer.com/chapter/10.1007/978-3-032-01377-4_1},
 year = {2026}
}

@Article{Zhang2024,
author="Zhang, Huangzhao
and Zhang, Kechi
and Li, Zhuo
and Li, Jia
and Li, Jia
and Li, Yongmin
and Zhao, Yunfei
and Zhu, Yuqi
and Liu, Fang
and Li, Ge
and Jin, Zhi",
title="Deep learning for code generation: a survey",
journal="Science China Information Sciences",
year="2024",
month="Aug",
day="20",
volume="67",
number="9",
pages="191101",
abstract="In the past decade, thanks to the powerfulness of deep-learning techniques, we have witnessed a whole new era of automated code generation. To sort out developments, we have conducted a comprehensive review of solutions to deep learning-based code generation. In this survey, we generally formalize the pipeline and procedure of code generation and categorize existing solutions according to taxonomy from perspectives of architecture, model-agnostic enhancing strategy, metrics, and tasks. In addition, we outline the challenges faced by current dominant large models and list several plausible directions for future research. We hope that this survey may provide handy guidance to understanding, utilizing, and developing deep learning-based code-generation techniques for researchers and practitioners.",
issn="1869-1919",
doi="10.1007/s11432-023-3956-3",
url="https://doi.org/10.1007/s11432-023-3956-3"
}


@Article{Aslam2025,
author="Aslam, Muhammad Muzamil
and Tufail, Ali
and Gul, Haji
and Irshad, Muhammad Nauman
and Namoun, Abdallah",
title="Artificial intelligence for secure and sustainable industrial control systems - A Survey of challenges and solutions",
journal="Artificial Intelligence Review",
year="2025",
month="Aug",
day="21",
volume="58",
number="11",
pages="349",
abstract="In modern industrial environments, the security and sustainability of Industrial Control Systems (ICS) have become crucial. This comprehensive review examines the transformative potential of Artificial Intelligence (AI) in ICS, focusing on technologies like Machine Learning (ML), Deep Learning (DL), Large Language Models (LLMs), and cloud computing. Moreover, this research explores integrating existing and proposed sustainable practices within the ICS framework, with a particular emphasis on energy efficiency and carbon footprint reduction, to enhance the overall sustainability of ICS. This review employed a systematic approach to select relevant articles from multiple reputable databases, such as Scopus, IEEE Explore, Science Direct, ACM digital library, Web of Science, and IET digital library, including 250 articles that provide valuable insights into the intersection of AI, security, and sustainability in ICS. This review examines vulnerabilities in ICS, such as data breaches, insider threats, and malware, emphasizing the need for effective anomaly detection. It highlights how AI technologies like anomaly detection and predictive analytics can enhance threat detection and response in ICS by improving accuracy and efficiency. The review offers insights to researchers and professionals on the future of secure, sustainable ICS, supporting a resilient industrial landscape that meets cybersecurity, compliance, and sustainability goals.",
issn="1573-7462",
doi="10.1007/s10462-025-11320-9",
url="https://doi.org/10.1007/s10462-025-11320-9"
}


@inproceedings{10.1007/978-3-031-73010-8_23,
 abstract = {Visual encoding constitutes the basis of large multimodal models (LMMs) in understanding the visual world. Conventional LMMs process images in fixed sizes and limited resolutions, while recent explorations in this direction are limited in adaptivity, efficiency, and even correctness. In this work, we first take GPT-4V and LLaVA 1.5 as representative examples and expose systematic flaws rooted in their visual encoding strategy. To address the challenges, we present LLaVA-UHD, a large multimodal model that can efficiently perceive images in any aspect ratio and high resolution. LLaVA-UHD includes three key components: (1) An image modularization strategy that divides native-resolution images into smaller variable-sized slices for efficient and extensible encoding, (2) a compression module that further condenses image tokens from visual encoders, and (3) a spatial schema to organize slice tokens for LLMs. Comprehensive experiments show that LLaVA-UHD outperforms established LMMs trained with 2--3 orders of magnitude more data on 8 benchmarks. Notably, our model built on LLaVA-1.5{\$}{\$}{\_}{\{}336{\backslash}times 336{\}}{\$}{\$}336{\texttimes}336supports 6 times larger (i.e., 672 {\$}{\$}{\backslash}times {\$}{\$}{\texttimes}1008) resolution images, and achieves 5.7 accuracy improvement on TextVQA.},
 address = {Cham},
 author = {Guo, Zonghao
and Xu, Ruyi
and Yao, Yuan
and Cui, Junbo
and Ni, Zanlin
and Ge, Chunjiang
and Chua, Tat-Seng
and Liu, Zhiyuan
and Huang, Gao},
 booktitle = {Computer Vision -- ECCV 2024},
 editor = {Leonardis, Ale{\v{s}}
and Ricci, Elisa
and Roth, Stefan
and Russakovsky, Olga
and Sattler, Torsten
and Varol, G{\"u}l},
 isbn = {978-3-031-73010-8},
 pages = {390--406},
 publisher = {Springer Nature Switzerland},
 title = {LLaVA-UHD: An LMM Perceiving Any Aspect Ratio and High-Resolution Images},
 url = {https://link.springer.com/chapter/10.1007/978-3-031-73010-8_23},
 year = {2025}
}

@Inbook{Grigorov2024,
author="Grigorov, Dilyan",
title="Applications of LLMs in Python",
bookTitle="Introduction to Python and Large Language Models: A Guide to Language Models",
year="2024",
publisher="Apress",
address="Berkeley, CA",
pages="263--301",
abstract="In this chapter, we explore the diverse and impactful applications of large language models (LLMs) within the Python ecosystem. From enhancing natural language processing tasks to generating creative content, LLMs have become integral to many innovative solutions. We will delve into practical use cases, examine real-world examples, and provide insights into how these powerful models are transforming industries and driving technological advancements.",
isbn="979-8-8688-0540-0",
doi="10.1007/979-8-8688-0540-0_6",
url="https://doi.org/10.1007/979-8-8688-0540-0_6"
}


@inproceedings{10.1007/978-3-031-84228-3_5,
 abstract = {Proactive detection of vulnerabilities in smart contracts is imperative for ensuring the security of user funds entrusted to them. Once deployed, a smart contract is immutable and therefore cannot be updated. This posits the challenge of detecting and fixing all vulnerabilities before deployment. In this context, static analysis has proved to be a formidable tool, even though there is still a lot to be discovered in this field, and the likelihood of the discovery of new classes of vulnerabilities is high. Since 2019, there has been a rise in methods that use Machine (ML) and Deep Learning (DL) to enhance the existing methods, whether in static or dynamic analysis, to cover this issue. This research presents a comprehensive review of existing ML models that detect vulnerabilities in smart contracts statically, i.e. without running their code. The authors evaluate the accuracy of publicly available models in identifying reentrancy in smart contracts based on their F1 score when tested on a foreign dataset with files of newer Solidity versions. The findings point to the limitations of such models in adapting to the continuously evolving nature of the Solidity language, which is still going through its infancy. The authors also explore and share the optimal parameters for training and testing those models, detailing things that were overlooked by the official documentation. All the scripts used for integration and interoperability were published on GitHub to facilitate further research in this area. The research highlights the critical need for constantly updating the existing detectors to avoid false negatives. This research is significant for the broader blockchain community, safeguarding smart contract integrity and fortifying overall system security.},
 address = {Cham},
 author = {Adamantis, Maksym
and Sokolov, Volodymyr
and Skladannyi, Pavlo},
 booktitle = {Advances in Computer Science for Engineering and Education VII},
 editor = {Hu, Zhengbing
and Yanovsky, Felix
and Dychka, Ivan
and He, Matthew},
 isbn = {978-3-031-84228-3},
 pages = {53--65},
 publisher = {Springer Nature Switzerland},
 title = {Evaluation of State-of-the-Art Machine Learning Smart Contract Vulnerability Detection Method},
 url = {https://link.springer.com/chapter/10.1007/978-3-031-84228-3_5},
 year = {2025}
}

@Article{Zhou2025,
author="Zhou, Chunying
and Xie, Xiaoyuan
and Chen, Gong
and He, Peng
and Li, Bing",
title="Multi-view adaptive contrastive learning for information retrieval based fault localization",
journal="Automated Software Engineering",
year="2025",
month="Nov",
day="06",
volume="33",
number="1",
pages="29",
abstract="Most studies focused on information retrieval-based techniques for fault localization, which built representations for bug reports and source code files and matched their semantic vectors through similarity measurement. However, such approaches often ignore some useful information that might help improve localization performance, such as 1) the interaction relationship between bug reports and source code files; 2) the similarity relationship between bug reports; and 3) the co-citation relationship between source code files. In this paper, we propose a novel approach named Multi-View Adaptive Contrastive Learning for Information Retrieval Fault Localization (MACL-IRFL) to learn the above-mentioned relationships for software fault localization. Specifically, we first generate data augmentations from report-code interaction view, report-report similarity view and code-code co-citation view separately, and adopt graph neural network to aggregate the information of bug reports or source code files from the three views in the embedding process. Moreover, we perform contrastive learning across these views. Our design of contrastive learning task will force the bug report representations to encode information shared by report-report and report-code views, and the source code file representations shared by code-code and report-code views, thereby alleviating the noise from auxiliary information. Finally, to evaluate the performance of our approach, we conduct extensive experiments on five open-source Java projects. The results show that our model can improve over the best baseline up to 28.93{\%}, 25.57{\%} and 20.35{\%} on Accuracy@1, MAP and MRR, respectively.",
issn="1573-7535",
doi="10.1007/s10515-025-00573-x",
url="https://doi.org/10.1007/s10515-025-00573-x"
}


@Article{Dong2025,
author="Dong, Yi
and Mu, Ronghui
and Zhang, Yanghao
and Sun, Siqi
and Zhang, Tianle
and Wu, Changshun
and Jin, Gaojie
and Qi, Yi
and Hu, Jinwei
and Meng, Jie
and Bensalem, Saddek
and Huang, Xiaowei",
title="Safeguarding large language models: a survey",
journal="Artificial Intelligence Review",
year="2025",
month="Oct",
day="17",
volume="58",
number="12",
pages="382",
abstract="In the burgeoning field of Large Language Models (LLMs), developing a robust safety mechanism, colloquially known as ``safeguards'' or ``guardrails'', has become imperative to ensure the ethical use of LLMs within prescribed boundaries. This article provides a systematic literature review on the current status of this critical mechanism. It discusses its major challenges and how it can be enhanced into a comprehensive mechanism dealing with ethical issues in various contexts. First, the paper elucidates the current landscape of safeguarding mechanisms that major LLM service providers and the open-source community employ. This is followed by the techniques to evaluate, analyze, and enhance some (un)desirable properties that a guardrail might want to enforce, such as hallucinations, fairness, privacy, and so on. Based on them, we review techniques to circumvent these controls (i.e., attacks), to defend the attacks, and to reinforce the guardrails. While the techniques mentioned above represent the current status and the active research trends, we also discuss several challenges that cannot be easily dealt with by the methods and present our vision on how to implement a comprehensive guardrail through the full consideration of multi-disciplinary approach, neural-symbolic method, and systems development lifecycle.",
issn="1573-7462",
doi="10.1007/s10462-025-11389-2",
url="https://doi.org/10.1007/s10462-025-11389-2"
}


@Article{He2025,
author="He, Rui
and Zhang, Liang
and Lyu, Liangqing
and Xue, Changbin",
title="Enhancing the ability of LLMs for spaceborne equipment code generation via retrieval-augmented generation and contrastive learning",
journal="Automated Software Engineering",
year="2025",
month="Aug",
day="29",
volume="33",
number="1",
pages="1",
abstract="In the code generation field, Large Language Models (LLMs) pre-trained on numerous open-source code fragments show powerful reasoning abilities and remarkable downstream performance. They assist code generation by combining retrieval techniques like retrieving relevant code fragments as templates or using retrieval results to supplement natural language descriptions and get code examples. However, in domains like aerospace equipment, existing code generation technologies perform suboptimally. Different aerospace equipment has different functions and significant data processing and loading differences. There is a lack of effective retrieval methods to provide semantically similar code contexts for LLMs, hindering code generation from meeting complex task requirements. To address this, we propose CodeCLARE, a retrieval-augmented code generation framework. It first fine-tunes UniXcoder via contrastive learning and uses it as a semantic encoder for code fragment retrieval. Then, the NL2Code search strategy is adopted with program requirements as queries. In the final stage of the code generation process, through a ``Few-Shots Selection'' mechanism, the prompt templates effectively integrate both the retrieved code examples and the specific requirement information, enabling the successful generation of highly accurate C++ code through the advanced capabilities of LLMs. Experimental results show that our approach significantly improves code quality compared to traditional ones and provides an effective solution for spacecraft control code generation.",
issn="1573-7535",
doi="10.1007/s10515-025-00545-1",
url="https://doi.org/10.1007/s10515-025-00545-1"
}


@inproceedings{10.1007/978-3-032-02138-0_5,
 abstract = {Recent advancements in Large Language Models (LLMs) offer promising opportunities for automating software architecture recovery (SAR). In this study, we assess the effectiveness of state-of-the-art general-purpose LLMs when used as off-the-shelf tools by practitioners seeking architectural insights from source code. We evaluate four models across three key tasks: (i) identifying implementation-level class diagrams, (ii) identifying architectural and design patterns, and (iii) identifying architectural styles. The experiment adopts a realistic usage setting, combining prompt engineering with a Self-Reflection mechanism to simulate how users iteratively refine queries. Results show that LLMs can support SAR activities, particularly in identifying structural and stylistic elements, but they struggle with complex abstractions such as class relationships and fine-grained design patterns. In addition to performance evaluation, we analyze the types of errors made by the models and assess the impact of Self-Reflection in refining their outputs, offering deeper insights into LLM behavior and highlighting implications for future research and practice.},
 address = {Cham},
 author = {Amalfitano, Domenico
and De Luca, Marco
and Santilli, Tiziano
and Pelliccione, Patrizio
and Fasolino, Anna Rita},
 booktitle = {Software Architecture},
 editor = {Andrikopoulos, Vasilios
and Pautasso, Cesare
and Ali, Nour
and Soldani, Jacopo
and Xu, Xiwei},
 isbn = {978-3-032-02138-0},
 pages = {73--89},
 publisher = {Springer Nature Switzerland},
 title = {Automated Software Architecture Design Recovery from Source Code Using LLMs},
 url = {https://link.springer.com/chapter/10.1007/978-3-032-02138-0_5},
 year = {2026}
}

@Inbook{Gupta2026,
author="Gupta, Aditya
and Gunawat, Chhaya",
editor="Pandey, Bishwajeet
and Patel, Advait",
title="AI-Driven Secure and Compliant Real-Time Cloud Monitoring",
bookTitle="Revolutionizing the Cloud: Generative AI, Security, and Sustainability",
year="2026",
publisher="Springer Nature Switzerland",
address="Cham",
pages="173--194",
abstract="Cloud-native architecture has changed the role of telemetry from ancillary to integral operating metric. This chapter provides a comprehensive guide to building secure, compliant, and AI-fortified observability systems to ingest, correlate, and analyze metrics, logs, and traces in real time. Based on open standards like OpenTelemetry and scalable building blocks such as Fluent Bit, Amazon Kinesis, and AWS Lambda, this architecture provides high-throughput, low-latency telemetry pipelines on distributed systems.",
isbn="978-3-032-07479-9",
doi="10.1007/978-3-032-07479-9_9",
url="https://doi.org/10.1007/978-3-032-07479-9_9"
}


@Inbook{Huang2025,
author="Huang, Ken
and Hughes, Chris",
title="Agentic AI Threat Modeling",
bookTitle="Securing AI Agents: Foundations, Frameworks, and Real-World Deployment",
year="2025",
publisher="Springer Nature Switzerland",
address="Cham",
pages="17--50",
abstract="This chapter introduces the MAESTRO (Multi-Agent Environment, Security, Threat, Risk, and Outcome) framework, a novel approach specifically designed for threat modeling Agentic AI systems. It begins by defining Agentic AI, highlighting its key components and technical drivers, and contrasting its unique challenges with traditional software systems. The chapter then critically examines the limitations of existing threat modeling frameworks, such as STRIDE, DREAD, PASTA, OCTAVE, and LINDDUN, in addressing the emergent properties of Agentic AI, including non-determinism, autonomy, agent identity, and agent-to-agent communication. MAESTRO is then presented as a seven-layer framework that systematically addresses these gaps. Each layer---Foundation Models, Data Operations, Agent Frameworks, Deployment Infrastructure, Evaluation and Observability, Security and Compliance, and Agent Ecosystem---is explored in detail, with concrete examples of potential threats. The chapter emphasizes the importance of analyzing cross-layer threats, where vulnerabilities in one layer can cascade and amplify risks in others. A case study demonstrating the application of MAESTRO to an enterprise copilot illustrates its practical use. Finally, the chapter contrasts MAESTRO with list-based frameworks, highlighting its layered granularity, cross-layer focus, agent-specific threats, and structured methodology. This chapter provides readers with a robust foundation in Agentic AI threat modeling, equipping them to proactively identify and mitigate risks.",
isbn="978-3-032-02130-4",
doi="10.1007/978-3-032-02130-4_2",
url="https://doi.org/10.1007/978-3-032-02130-4_2"
}


@inproceedings{10.1007/978-3-031-82031-1_22,
 abstract = {This research explores the integration of ChatGPT, a Large Language Model (LLM), with traditional penetration testing tools such as Nmap, Metasploit, Whois, and Msfvenom. By leveraging ChatGPT's natural language processing abilities, the study introduces a novel approach to penetration testing that simplifies tool usage and improves task efficiency. The model assists in understanding complex queries, generating targeted attack scripts, and analyzing data. This fusion enhances both the accuracy and speed of penetration testing while making it more accessible to newcomers. The study also emphasizes the importance of ethical AI use in cybersecurity. The findings suggest that combining language models with technical tools fosters better communication among security experts and opens new possibilities for AI-assisted cybersecurity practices. Ultimately, the research paves the way for smarter, more efficient penetration testing methods as organizations strive to protect their digital assets.},
 address = {Cham},
 author = {Gullinkala, Veerababu
and Pournouri, Sina},
 booktitle = {Cybersecurity and Human Capabilities Through Symbiotic Artificial Intelligence},
 editor = {Jahankhani, Hamid
and Issac, Biju},
 isbn = {978-3-031-82031-1},
 pages = {423--452},
 publisher = {Springer Nature Switzerland},
 title = {Automated Penetration Testing Using Generative Artificial},
 url = {https://link.springer.com/chapter/10.1007/978-3-031-82031-1_22},
 year = {2025}
}

@Article{Huang2025,
author="Huang, Hongli
and Chai, Yihan
and Wu, Xuewei
and Wang, Sen
and Li, Binglin
and Wang, Yanyan
and Zheng, Wen
and Wu, Yuefeng
and Meng, Di
and Wang, Hua
and Tu, Zhengliang
and Du, Chengli
and Lyu, Xiayi
and Li, Guiying
and Guo, Wei",
title="Non-canonical NOTCH1 signaling regulates ferroptosis vulnerability in dormant lung cancer cells with stable resistance",
journal="Cell Death {\&} Disease",
year="2025",
month="Dec",
day="26",
volume="17",
number="1",
pages="1",
abstract="Non-genetic resistance of cancer remains poorly understood in clinical research and practice. To better understand resistant cancer cell heterogeneity, we isolated a novel riboflavin+NOTCH1+ population from cisplatin-na{\"i}ve and -resistant lung cancer cell lines and patient specimens with or without immunotherapy and chemotherapy. This population was also identified as SLC52A2 (one of the riboflavin transporters)+NOTCH1+ cells in single-cell RNA sequencing (scRNA-seq) data derived from advanced lung tumors before therapy. Despite its therapy-na{\"i}ve origin, the population, designated as stably resistant cancer cells (SRCC), exhibited the epithelial state, innate and stable resistance to therapy (chemotherapy, targeted therapy and immunotherapy), cell dormancy, elevated reactive oxygen species (ROS), and anti-apoptotic and anti-ferroptotic survival. These cellular and molecular characteristics distinguished SRCC from other resistant populations, including cancer stem-like cells (CSC), epithelial-mesenchymal transition (EMT) cells, and drug-tolerant persisters (DTP). The non-canonical NOTCH1 pathway, but not the inactivated canonical NOTCH1 pathway, played a critical role in the resistance of SRCC. Specifically, it modulates cell cycle, iron metabolism, EMT, and ferroptosis vulnerability in SRCC at the transcriptional level. It also controls the initiation of ferroptosis in lysosomes via a posttranslational NOTCH1-AKT-BAX axis. Inhibition of the non-canonical NOTCH1 pathway re-sensitizes these dormant and resistant cells to cisplatin-induced cell death in vitro and in vivo, including ferroptosis, apoptosis, and necroptosis. Our study contributes to a deeper understanding of cancer resistance and promotes the development of more effective therapeutic strategies against resistant cancer cells.",
issn="2041-4889",
doi="10.1038/s41419-025-08355-9",
url="https://doi.org/10.1038/s41419-025-08355-9"
}


@Article{Ahmed2024,
author="Ahmed, Roshan
and Sridevi, S.",
title="Quantum space-efficient large language models for Prolog query translation",
journal="Quantum Information Processing",
year="2024",
month="Oct",
day="16",
volume="23",
number="10",
pages="349",
abstract="As large language models (LLMs) continue to expand in complexity, their size follows an exponential increase following Moore's law. However, implementing such complex tasks with LLMs poses a significant challenge, as classical computers may lack the necessary space to run or store the model parameters. In this context leveraging the principles of hybrid quantum machine learning for language models offers a promising solution to mitigate this issue by reducing storage space for model parameters. Although pure quantum language models have demonstrated success in recent past, they are constrained by limited features and availability. In this research we propose the DeepKet model an approach with a quantum embedding layer, which utilizes the Hilbert space generated by quantum entanglement to store feature vectors, leading to a significant reduction in size. The experimental analysis evaluates the performance of open-source pre-trained models like Microsoft Phi and CodeGen, specifically fine-tuned for generating Prolog code for geo-spatial data retrieval. Furthermore, it investigates the effectiveness of quantum DeepKet embedding layers by comparing them with the total parameter count of traditional models.",
issn="1573-1332",
doi="10.1007/s11128-024-04559-8",
url="https://doi.org/10.1007/s11128-024-04559-8"
}


@Article{Macedo2025,
author="Macedo, Marcos
and Tian, Yuan
and Cogo, Filipe R.
and Adams, Bram",
title="Output format biases in the evaluation of large language models for code translation",
journal="Empirical Software Engineering",
year="2025",
month="Dec",
day="06",
volume="31",
number="2",
pages="41",
abstract="Code translation between programming languages (PLs) is a long-existing and critical task in software engineering, facilitating the modernization of legacy systems, ensuring cross-platform compatibility, and enhancing software performance. With the recent advances in Large Language Models (LLMs) and their applications to code translation, there is an increasing need for comprehensive evaluation of these models. Most existing studies instruct LLMs to perform code translation and evaluate their performance by either running the generated outputs through test suites or comparing them to reference outputs (ground truth). These outputs, however, may contain not only executable source code but also additional non-code elements, such as natural language explanations or formatting tokens. We refer to the way source code and non-code elements are combined as output format. It is crucial to understand and address variations in output format, as non-code elements can interfere with evaluation metrics, resulting in biased (inaccurate or unfair) assessments of model performance and comparisons. We refer to this bias as output format bias. To investigate the presence of output format biases, we first conduct an empirical analysis of the outputs from eleven instruct-tuned open-source LLMs, applied to 3,820 translation pairs across five languages: C, C++, Go, Java, and Python. The results show that between 26.4{\%} and 73.7{\%} of outputs produced by our evaluated LLMs necessitate post-processing (to result in source code for evaluation). To mitigate output format bias, we propose a strategic combination of prompt engineering and regular expressions that effectively extracts source code from mixed-format outputs, enabling the eleven open-source models to achieve an average Code Extraction Success Rate (CSR) of 92.73{\%}. Our empirical study confirms that output format bias affects widely used execution-based metrics, i.e., Computational Accuracy (CA), and text-based metrics, i.e., BLEU, CodeBLEU and CrystalBLEU. Additionally, we test five closed LLMs and observe that they also generate varying distributions of output formats, which could contribute to output format biases. Our results highlight the need to mitigate the output format bias to enable reliable evaluations in LLMs code translation.",
issn="1573-7616",
doi="10.1007/s10664-025-10768-1",
url="https://doi.org/10.1007/s10664-025-10768-1"
}


@Inbook{Ranjan2025,
author="Ranjan, Sumit
and Chembachere, Divya
and Lobo, Lanwin",
title="Enhancing LLMs for Agentic AI: RAG vs. Fine-Tuning",
bookTitle="Agentic AI in Enterprise: Harnessing Agentic AI for Business Transformation",
year="2025",
publisher="Apress",
address="Berkeley, CA",
pages="151--186",
abstract="As large language models (LLMs) evolve into autonomous agents, the demands placed on their capabilities are shifting from general-purpose reasoning to domain-specific execution. Enterprises seeking to operationalize Agentic AI must decide how best to enhance and specialize these models to align with their data, workflows, and compliance requirements. Two primary strategies---Retrieval-Augmented Generation (RAG) and fine-tuning---have emerged as leading approaches to extend LLM functionality and reliability in real-world deployments.",
isbn="979-8-8688-1542-3",
doi="10.1007/979-8-8688-1542-3_4",
url="https://doi.org/10.1007/979-8-8688-1542-3_4"
}


@inproceedings{10.1007/978-3-032-08977-9_16,
 abstract = {Patching configurable systems is challenging due to the complexity of testing these systems. Some of the problems that need to be addressed in this context include identification of the configurations that can reveal the bugs related to the patches and identification of the configurations that should be used for validating the patched version. We present a tool, CONFER, that can help developers in solving these problems. Our approach uses the patch report to identify configuration variables related to the vulnerable version and those that are related to the fixed version. We apply CONFER to BusyBox, a configurable system that is popular in the Internet of Things (IoT) domain. Analyzing the patches related to the memory vulnerabilities in BusyBox reveals that approximately half of the patched vulnerabilities are somehow configuration related although only 16{\%} of the patches explicitly refer to the configuration variables. Half of the configuration relevant vulnerabilities involve configuration variable settings that are consistent with the default configuration defined for Android. We show effectiveness of CONFER in generating patch relevant configurations for BusyBox and discuss its modes and limitations.},
 address = {Cham},
 author = {Yavuz, Tuba},
 booktitle = {SEET---Software Engineering for Emerging Technologies},
 editor = {Hussain, Shahid
and Khan, Arif Ali
and Abdul Basit Ur Rahim, Muhammad
and Khan, Saif Ur Rehman},
 isbn = {978-3-032-08977-9},
 pages = {222--245},
 publisher = {Springer Nature Switzerland},
 title = {Exploring the Configuration Space of BusyBox Vulnerabilities with CONFER},
 url = {https://link.springer.com/chapter/10.1007/978-3-032-08977-9_16},
 year = {2026}
}

@inproceedings{10.1007/978-981-97-6352-8_50,
 abstract = {As natural language processing (NLP) models develop, the question of whether they can be used to solve problems across a variety of industries becomes more important. This research paper offers a thorough examination of the possibilities of ChatGPT, a cutting-edge language model, as a flexible NLP problem solver. We research its performance across a range of tasks, evaluate its applicability to various fields, consider its moral ramifications, and look at its function in assisting human problem-solving. We list the benefits and drawbacks of ChatGPT and discuss its potential contributions to AI-powered solutions through empirical studies and practical implementations. Because ChatGPT can provide excellent responses to input from humans and automatically fix previous mistakes based on new talks, it has recently attracted a lot of interest from the natural language processing (NLP) field. We demonstrate the merits and drawbacks of the current ChatGPT version using in-depth empirical studies. We find that while ChatGPT does well on many tasks that favor reasoning skills (like arithmetic reasoning), it still has difficulties with more specialized tasks like sequence labeling.},
 address = {Singapore},
 author = {Nagender, Y.
and Kumar, S. Vijaya
and Chakravarthi, M. Kalyan
and Guha, Shouvik Kumar
and Moharekar, Tejashree Tejpal
and Sajida Bhanu, P.},
 booktitle = {International Conference on Signal, Machines, Automation, and Algorithm},
 editor = {Malik, Hasmat
and Mishra, Sukumar
and Sood, Y. R.
and Garc{\'i}a M{\'a}rquez, Fausto Pedro
and Ustun, Taha Selim},
 isbn = {978-981-97-6352-8},
 pages = {699--710},
 publisher = {Springer Nature Singapore},
 title = {A Study on Potentiality of ChatGPT as Task Solver Based on Natural Language Processing},
 url = {https://link.springer.com/chapter/10.1007/978-981-97-6352-8_50},
 year = {2024}
}

@inproceedings{10.1007/978-3-031-61382-1_4,
 abstract = {In our modern digital world, where virtually everything is intertwined with computer systems, critical infrastructures face vulnerability to a variety of cyber-attacks, stemming from the absence of a cybersecurity mindset within these establishments. We need to efficiently educate these workers about the cybersecurity threats that exist, their potential effects, and the subsequent substantial impact on human populations. Previous research has suggested traditional non-interactive training methods are often not effective. We propose an interactive learning experience that incorporates Extended Reality, Digital Twins, and Artificial Intelligence (AI) to help workers become more aware of cybersecurity issues within their critical infrastructure. This paper introduces an innovative testbed that seamlessly integrates Artificial Intelligence (AI) and Large Language Models to create an immersive educational experience. The goal is to effectively convey complex technical concepts to users with limited background knowledge on the subject. Our specific focus lies in addressing the need for proper cybersecurity training among water treatment plant employees.},
 address = {Cham},
 author = {Lee, Anthony
and King, Kenneth
and Gra{\v{c}}anin, Denis
and Azab, Mohamed},
 booktitle = {HCI for Cybersecurity, Privacy and Trust},
 editor = {Moallem, Abbas},
 isbn = {978-3-031-61382-1},
 pages = {56--69},
 publisher = {Springer Nature Switzerland},
 title = {Experiential Learning Through Immersive XR: Cybersecurity Education for Critical Infrastructures},
 url = {https://link.springer.com/chapter/10.1007/978-3-031-61382-1_4},
 year = {2024}
}

@inproceedings{10.1007/978-981-95-5294-8_12,
 abstract = {This study addresses the critical challenge of selecting appropriate software testing platforms and optimizing test case generation in AI-era vocational education. We propose a novel framework that integrates knowledge graph technology with mainstream testing platforms to increase automated test case generation efficiency. The methodology combines multimodal knowledge graph construction (entity recognition accuracy: 92.7{\%}, relationship extraction F1 score: 89.3{\%}) with AI-powered testing tools, implementing dynamic knowledge updating through continuous integration mechanisms. The experimental results on three industrial-standard testing datasets demonstrate a 38{\%} improvement in test coverage and a 45{\%} reduction in false positives compared with traditional methods. The proposed system architecture enables semantic-aware test scenario generation through ontology-driven knowledge mapping, which is particularly effective in complex web application testing scenarios. This research contributes to 1) a knowledge graph-enhanced test automation framework; 2) a multimodal educational resource integration methodology; and 3) an industry-academia collaborative validation mechanism. Practical applications show a 27{\%} increase in student competency assessment scores through our platform-embedded teaching system.},
 address = {Singapore},
 author = {Peng, Ling
and Zhang, Yu},
 booktitle = {Computer Applications},
 editor = {Huang, Lan
and Xu, Bin
and Chen, Xuebin
and Wang, Shangguang
and Xia, Bing
and Pan, Jianguo
and Song, Xianhua
and Lu, Zeguang},
 isbn = {978-981-95-5294-8},
 pages = {159--167},
 publisher = {Springer Nature Singapore},
 title = {Intelligent Software Testing Platform Selection and Knowledge Graph-Driven Test Case Generation},
 url = {https://link.springer.com/chapter/10.1007/978-981-95-5294-8_12},
 year = {2026}
}

@Inbook{Bryce2024,
author="Bryce, Ciar{\'a}n
and Kalousis, Alexandros
and Leroux, Ilan
and Madinier, H{\'e}l{\`e}ne
and Pasche, Thomas
and Ruch, Patrick",
editor="Kucharavy, Andrei
and Plancherel, Octave
and Mulder, Valentin
and Mermoud, Alain
and Lenders, Vincent",
title="Exploring the Dual Role of LLMs in Cybersecurity: Threats and Defenses",
bookTitle="Large Language Models in Cybersecurity: Threats, Exposure and Mitigation",
year="2024",
publisher="Springer Nature Switzerland",
address="Cham",
pages="235--242",
abstract="Large Language Models (LLMs) pose risks for cybersecurity since they facilitate minimal cost creation of malware, phishing messages, and malicious chatbots. At the same time, LLMs can help defend against cyberattacks. This chapter reviews security research around the risks and benefits of LLMs.",
isbn="978-3-031-54827-7",
doi="10.1007/978-3-031-54827-7_26",
url="https://doi.org/10.1007/978-3-031-54827-7_26"
}


@Inbook{Jeong2025,
author="Jeong, Hyen Seuk",
title="AIOps RCA",
bookTitle="Observability For Legacy Systems: Methods and Solutions with OpenTelemetry and AIOps",
year="2025",
publisher="Apress",
address="Berkeley, CA",
pages="713--888",
abstract="AI is a key technology for root cause analysis and operational automation. This final chapter explains how to automate root cause analysis using machine learning algorithms. It also explains how Generative AI and RAG can be applied to AIOps. To apply machine learning, you will utilize OpenSearch.",
isbn="979-8-8688-1688-8",
doi="10.1007/979-8-8688-1688-8_10",
url="https://doi.org/10.1007/979-8-8688-1688-8_10"
}


@inproceedings{10.1007/978-981-96-0808-9_20,
 abstract = {A classic, central Service-Oriented Computing (SOC) challenge is the service composition problem. It concerns solving a user-defined task by selecting a suitable set of services, possibly found at runtime, determining an invocation order, and handling request and response parameters. The solutions proposed in the past two decades mostly resort to additional formal modeling of the services, leading to extra effort, scalability issues, and overall brittleness. With the rise of Large Language Models (LLMs), it has become feasible to process semi-structured information like state-of-the-practice OpenAPI documentation containing formal parts like endpoints and free-form elements like descriptions. We propose Compositio Prompto to generate service compositions based on those semi-structured documents. Compositio Prompto acts as an encapsulation of the prompt creation and the model invocation such that the user only has to provide the service specifications, the task, and which input and output format they expect, eliminating any manual and laborious annotation or modeling task by relying on already existing documentation. To validate our approach, we implement a fully operational prototype, which operates on a set of OpenAPIs, a plain text task, and an input and output JSON schema as input and returns the generated service composition as executable Python code. We measure the effectiveness of our approach on a parking spot booking case study. Our experiments show that models can solve several tasks, especially those above 70B parameters, but none can fulfill all tasks. Furthermore, compared with manually created sample solutions, the ones generated by LLMs appear to be close approximations.},
 address = {Singapore},
 author = {Pesl, Robin D.
and Mombrey, Carolin
and Klein, Kevin
and Zyberaj, Denesa
and Georgievski, Ilche
and Becker, Steffen
and Herzwurm, Georg
and Aiello, Marco},
 booktitle = {Service-Oriented Computing},
 editor = {Gaaloul, Walid
and Sheng, Michael
and Yu, Qi
and Yangui, Sami},
 isbn = {978-981-96-0808-9},
 pages = {276--286},
 publisher = {Springer Nature Singapore},
 title = {Compositio Prompto: An Architecture to Employ Large Language Models in Automated Service Computing},
 url = {https://link.springer.com/chapter/10.1007/978-981-96-0808-9_20},
 year = {2025}
}

@Article{An2026,
author="An, Hongjun
and Hu, Wenhan
and Huang, Sida
and Huang, Siqi
and Li, Ruanjun
and Liang, Yuanzhi
and Shao, Jiawei
and Song, Yiliang
and Wang, Zihan
and Yuan, Cheng
and Zhang, Chi
and Zhang, Hongyuan
and Zhuang, Wenhao
and Li, Xuelong",
title="AI Flow: perspectives, scenarios, and approaches",
journal="Vicinagearth",
year="2026",
month="Jan",
day="26",
volume="3",
number="1",
pages="1",
abstract="Pioneered by the foundational information theory by Claude Shannon and the visionary framework of machine intelligence by Alan Turing, the convergent evolution of information and communication technologies (IT/CT) has created an unbroken wave of connectivity and computation. This synergy has sparked a technological revolution, now reaching its peak with large artificial intelligence (AI) models that are reshaping industries and redefining human-machine collaboration. However, the realization of ubiquitous intelligence faces considerable challenges due to substantial resource consumption in large models and high communication bandwidth demands. To address these challenges, AI Flow has been introduced as a multidisciplinary framework that integrates cutting-edge IT and CT advancements, with a particular emphasis on the following three key points. First, device-edge-cloud framework serves as the foundation, which integrates end devices, edge servers, and cloud clusters to optimize scalability and efficiency for low-latency model inference. Second, we introduce the concept of familial models, which refers to a series of different-sized models with aligned hidden features, enabling effective collaboration and the flexibility to adapt to varying resource constraints and dynamic scenarios. Third, connectivity- and interaction-based intelligence emergence is a novel paradigm of AI Flow. By leveraging communication networks to enhance connectivity, the collaboration among AI models across heterogeneous nodes achieves emergent intelligence that surpasses the capability of any single model. The innovations of AI Flow provide enhanced intelligence, timely responsiveness, and ubiquitous accessibility to AI services, paving the way for the tighter fusion of AI techniques and communication systems. These advancements are crucial to numerous application scenarios, including but not limited to embodied AI, wearable devices, and smart cities.",
issn="3005-060X",
doi="10.1007/s44336-025-00031-y",
url="https://doi.org/10.1007/s44336-025-00031-y"
}


@inproceedings{10.1007/978-3-032-14518-5_12,
 abstract = {The integration of generative AI in software organizations presents promising opportunities for automating and enhancing security activities; however, its practical impact on the security practices of software organizations and associated risks remains underexplored. This study investigates how generative AI can enhance software security practices through a multi-case study involving five software organizations. By conducting semi-structured interviews with developers and managers, we identified four security practices where generative AI can support threat assessment, security testing, operational management, and education {\&} guidance. At the same time, we uncover sociotechnical risks that may impede adoption, including concerns about inadequate data management of AI, inaccurate output, and a lack of trust. Guided by the OWASP SAMM framework and rooted in the Gioia methodological approach, our findings are synthesized into a theoretical model of generative AI adoption impacting security practices. This model highlights both the perceived benefits and the risks organizations encounter. Our work provides insights for software organizations looking to adopt generative AI into security-related workflows and contributes a foundational understanding of how such tools are perceived in practice. The paper emphasizes the importance of balancing innovation with caution as software organizations increasingly incorporate generative AI within secure development.},
 address = {Cham},
 author = {Hein, David Kinnberg},
 booktitle = {Software Business},
 editor = {Herzwurm, Georg
and Petrik, Dimitri
and Strobel, Gero
and Kude, Thomas
and Block, Lukas},
 isbn = {978-3-032-14518-5},
 pages = {139--157},
 publisher = {Springer Nature Switzerland},
 title = {Investigating Generative AI's Impact on Software Organizations' Security Practices: A Multi-case Study Using Gioia Methodology},
 url = {https://link.springer.com/chapter/10.1007/978-3-032-14518-5_12},
 year = {2026}
}

@Article{Chowa2026,
author="Chowa, Sadia Sultana
and Alvi, Riasad
and Rahman, Subhey Sadi
and Rahman, Md Abdur
and Raiaan, Mohaimenul Azam Khan
and Islam, Md Rafiqul
and Hussain, Mukhtar
and Azam, Sami",
title="From language to action: a review of large language models as autonomous agents and tool users",
journal="Artificial Intelligence Review",
year="2026",
month="Jan",
day="06",
volume="59",
number="2",
pages="71",
abstract="The pursuit of human-level artificial intelligence (AI) has significantly advanced the development of autonomous agents and Large Language Models (LLMs). LLMs are now widely utilized as decision-making agents for their ability to interpret instructions, manage sequential tasks, and adapt through feedback. This review examines recent developments in employing LLMs as autonomous agents and tool users and comprises seven research questions. We only used the papers published between 2023 and 2025 in conferences of the A* and A-ranked and Q1 journals. A structured analysis of the LLM agents' architectural design principles, dividing their applications into single-agent and multi-agent systems, and strategies for integrating external tools is presented. In addition, the cognitive mechanisms of LLMs, including reasoning, planning, and memory, and the impact of prompting methods and fine-tuning procedures on agent performance are also investigated. Furthermore, we have evaluated current benchmarks and assessment protocols and provided an analysis of 68 publicly available datasets to assess the performance of LLM-based agents in various tasks. In conducting this review, we have identified critical findings on verifiable reasoning of LLMs, the capacity for self-improvement, and the personalization of LLM-based agents. Finally, we have discussed ten future research directions to overcome these gaps.",
issn="1573-7462",
doi="10.1007/s10462-025-11471-9",
url="https://doi.org/10.1007/s10462-025-11471-9"
}


@inproceedings{10.1007/978-981-96-0201-8_50,
 abstract = {In the context of Work Integrated Learning (WIL), student feedback is extremely crucial. It offers a unique perspective on the efficacy of the learning journey, the applicability of the course content, and the influence of practical experiences on their academic comprehension. Feedback from students allows educators and institutions to pinpoint the strong and weak areas of the WIL program, thereby facilitating necessary modifications for enhancements. It also encourages a dialog, creating an atmosphere of mutual understanding and collective accountability for learning. Furthermore, it gives students an active role in their education, promoting engagement and enriching their learning journey. Hence, student feedback is not just beneficial, but essential for the ongoing refinement and effectiveness of Work Integrated Learning initiatives. To assist with the process of introducing necessary changes in the style of teaching or the layout of the course using feedback, this paper introduces a pipeline to achieve this by applying the streaming technology of Kafka developed in the field of Big Data along with filtering and summarization supported by BART and BERT.},
 address = {Singapore},
 author = {D'Souza, Claudius
and Giri, Animesh},
 booktitle = {Work Integrated Learning-Directions for the Future},
 editor = {Bindumadhavan, Krishnamurthy
and Lacey, Nelson},
 isbn = {978-981-96-0201-8},
 pages = {801--815},
 publisher = {Springer Nature Singapore},
 title = {Improving Work Integrated Learning Outcomes Through Big Data Technologies: An Insight into Student Learning Patterns},
 url = {https://link.springer.com/chapter/10.1007/978-981-96-0201-8_50},
 year = {2025}
}

@inproceedings{10.1007/978-3-031-70381-2_20,
 abstract = {As the AI revolution unfolds, the push toward automating support systems in diverse professional fields ranging from open-source software to healthcare, and banking to transportation has become more pronounced. Central to the automation of these systems is the early detection of named entities, a task that is foundational yet fraught with challenges due to the need for domain-specific expert annotations amid a backdrop of specialized terminologies, making the process both costly and complex. In response to this challenge, our paper presents an innovative named entity recognition (NER) framework (https://github.com/NeuralSentinel/DistALANER) tailored for the open-source software domain. Our method stands out by employing a distantly supervised, two-step annotation process that cleverly exploits language heuristics, bespoke lookup tables, external knowledge bases, and an active learning model. This multifaceted strategy not only elevates model performance but also addresses the critical hurdles of high costs and the dearth of expert annotators. A notable achievement of our approach is its capability to enable pre-large language models (pre-LLMs) to significantly outperform specially designed generic/domain specific LLMs for NER tasks. We also show the effectiveness of NER in the downstream task of relation extraction.},
 address = {Cham},
 author = {Banerjee, Somnath
and Dutta, Avik
and Agrawal, Aaditya
and Hazra, Rima
and Mukherjee, Animesh},
 booktitle = {Machine Learning and Knowledge Discovery in Databases. Applied Data Science Track},
 editor = {Bifet, Albert
and Krilavi{\v{c}}ius, Tomas
and Miliou, Ioanna
and Nowaczyk, Slawomir},
 isbn = {978-3-031-70381-2},
 pages = {313--331},
 publisher = {Springer Nature Switzerland},
 title = {DistALANER: Distantly Supervised Active Learning Augmented Named Entity Recognition in the Open Source Software Ecosystem},
 url = {https://link.springer.com/chapter/10.1007/978-3-031-70381-2_20},
 year = {2024}
}

@Article{Liang2026,
author="Liang, Yuqing
and Xiao, Jiancheng
and Gan, Wensheng
and Yu, Philip S.",
title="Watermarking techniques for large language models: a survey",
journal="Artificial Intelligence Review",
year="2026",
month="Jan",
day="05",
volume="59",
number="2",
pages="74",
abstract="With the rapid advancement and extensive application of artificial intelligence technology, large language models (LLMs) are extensively used to enhance production, creativity, learning, and work efficiency across various domains. However, the abuse of LLMs also poses potential harm to human society, such as intellectual property rights issues, academic misconduct, false content, and hallucinations. Relevant research has proposed the use of LLM watermarking to achieve IP protection for LLMs and traceability of multimedia data output by LLMs. To our knowledge, this is the first thorough review that investigates and analyzes LLM watermarking technology in detail. This review begins by recounting the history of traditional watermarking technology, then analyzes the current state of LLM watermarking research, and thoroughly examines the inheritance and relevance of these techniques. By analyzing their inheritance and relevance, this review can provide research with ideas for applying traditional digital watermarking techniques to LLM watermarking, to promote the cross-integration and innovation of watermarking technology. In addition, this review examines the pros and cons of LLM watermarking. Considering the current multimodal development trend of LLMs, it provides a detailed analysis of emerging multimodal LLM watermarking, such as visual and audio data, to offer more reference ideas for relevant research. This review delves into the challenges and future prospects of current watermarking technologies, offering valuable insights for future LLM watermarking research and applications.",
issn="1573-7462",
doi="10.1007/s10462-025-11474-6",
url="https://doi.org/10.1007/s10462-025-11474-6"
}


@Inbook{Godavarthi2025,
author="Godavarthi, Srinath
and Nagvekar, Ravi",
title="Risk and Compliance Management with GenAI",
bookTitle="Transforming Financial Services with Generative AI: From Strategy and Design to Practical Applications",
year="2025",
publisher="Apress",
address="Berkeley, CA",
pages="131--164",
abstract="In Chapter 4, we explored the key concepts for building GenAI applications. In this chapter, we outline the Risk and Compliance Management Framework for FSOs and how GenAI can enable the overall risk management. Let's discuss further context on why Risk and Compliance management is one of the most critical aspects in the Financial Services Sectors (FSS).",
isbn="979-8-8688-2053-3",
doi="10.1007/979-8-8688-2053-3_5",
url="https://doi.org/10.1007/979-8-8688-2053-3_5"
}


@Article{Hofeditz2025,
author="Hofeditz, Lennart
and Jung, Anna-Katharina
and Mirbabaie, Milad
and Stieglitz, Stefan",
title="Ethical Guidelines for the Application of Generative AI in German Journalism",
journal="Digital Society",
year="2025",
month="Jan",
day="16",
volume="4",
number="1",
pages="4",
abstract="Generative Artificial Intelligence (genAI) holds immense potential in revolutionizing journalism and media production processes. By harnessing genAI, journalists can streamline various tasks, including content creation, curation, and dissemination. Through genAI, journalists already automate the generation of diverse news articles, ranging from sports updates and financial reports to weather forecasts. However, this raises ethical questions of high relevance for media organizations and societies especially when genAI is used for more sensitive topics and at larger scale. To not jeopardize trustworthiness in journalistic organizations, it is important that the use of genAI in journalism is guided by moral principles. We therefore conducted 18 interviews with researchers and practitioners with expertise in AI-based technologies, journalism, and ethics from a German perspective in order to identify guidelines for the ethical use of genAI in media organizations. We derived requirements for the ethical introduction of genAI and actionable guidelines which explain how decision makers in media organizations should address ethical principles for the use of AI in the news production life cycle, in order to contribute to trustworthiness of journalistic organizations and products.",
issn="2731-4669",
doi="10.1007/s44206-024-00151-w",
url="https://doi.org/10.1007/s44206-024-00151-w"
}


@inproceedings{10.1007/978-3-032-12457-9_3,
 abstract = {Introductory programming education is constantly challenged with how to provide effective, personalized guidance to struggling novices. AST-based hints generation emerges as a potential solution, marrying abstract syntax tree analysis with generative AI to offer tailored, instructive feedback for Python learners. Existing hint generation systems like ITAP and GPT4Hints-GPT3.5Val have approached hint generation through path construction and generative models, respectively. Both approaches to hint generation have shown promise in generating human-like hints, but each has its own limitations. These approaches either provide highly instructive hints that are often too explicit or more abstract but may lack the specificity necessary for effective guidance. Our study combines the strengths of both approaches to provide students with hints that are both instructive and abstract but do not give away the solution. We provide a detailed overview of the AST-based hints system, including requirements gathering, the system architecture and features. The system is evaluated through path construction testing and A/B testing with speculative analysis. The results from path construction and A/B testing demonstrate that AST-Hints is moderately effective at generating human-like hints, faster than human tutors, with its success strongly related to the quality of the goal solution and hint relevance.},
 address = {Cham},
 author = {Levin, Marc
and Kandjimi, Herman
and Safla, Aslam},
 booktitle = {ICT Education},
 editor = {Nel, Liezel
and Stott, Tanya
and Calitz, Andr{\'e}},
 isbn = {978-3-032-12457-9},
 pages = {33--50},
 publisher = {Springer Nature Switzerland},
 title = {Leveraging Abstract Syntax Trees To Generate Instructive Hints In Programming},
 url = {https://link.springer.com/chapter/10.1007/978-3-032-12457-9_3},
 year = {2026}
}

@inproceedings{10.1007/978-3-032-05188-2_20,
 abstract = {Static code analysis conducted by means of learning-based methods is an essential part of Security Testing. Effective learning algorithms are crucial for training reliable models that can accurately detect weaknesses and vulnerabilities. During models' training, however, it is also of paramount importance to use adequate datasets of vulnerable and non-vulnerable source code.},
 address = {Cham},
 author = {Foulefack, Rosma{\"e}l Z. L.
and Marchetto, Alessandro},
 booktitle = {Testing Software and Systems},
 editor = {Bonfanti, Silvia
and Papadopoulos, George Angelos},
 isbn = {978-3-032-05188-2},
 pages = {307--324},
 publisher = {Springer Nature Switzerland},
 title = {On the Use of Imbalanced Datasets for Learning-Based Vulnerability Detection},
 url = {https://link.springer.com/chapter/10.1007/978-3-032-05188-2_20},
 year = {2026}
}

@Inbook{Camayd-Freixas2025,
author="Camayd-Freixas, Erik",
title="The Parasitic Age",
bookTitle="Metagrammar: Critical Thinking and Problem Solving in the Age of Artificial Intelligence",
year="2025",
publisher="Springer Nature Switzerland",
address="Cham",
pages="265--296",
abstract="Machines have always been parasitic on humans, and computers increasingly so. Historically, artisans made tools and products by hand after developing mastery through years of apprenticeship and experience in traditional trades.",
isbn="978-3-031-92337-1",
doi="10.1007/978-3-031-92337-1_15",
url="https://doi.org/10.1007/978-3-031-92337-1_15"
}


@Article{Bukar2024,
author="Bukar, Umar Ali
and Sayeed, Md Shohel
and Razak, Siti Fatimah Abdul
and Yogarayan, Sumendra
and Amodu, Oluwatosin Ahmed
and Raja Mahmood, Raja Azlina",
title="Text Analysis on Early Reactions to ChatGPT as a Tool for Academic Progress or Exploitation",
journal="SN Computer Science",
year="2024",
month="Mar",
day="29",
volume="5",
number="4",
pages="366",
abstract="Large language models (LLMs) have garnered significant attention lately, and one particular implementation that has captivated users is ChatGPT, a first-of-its kind innovation that sparks intense debates among professionals regarding its ethical boundaries, especially after it was released for public use. Some of the debates are ongoing, with several people getting more used to this tool and some losing touch with several ethical concerns. Thus, it is crucial to explore the early professional's perspectives surrounding ChatGPT to comprehend its potential impact on the educational sector and, in particular, society in general. This research aims to address this knowledge gap by employing data mining techniques on LinkedIn to analyze ChatGPT-related discussions and provide a comprehensive textual examination. The dataset for this investigation consists of 166 records written in English by LinkedIn users discussing ChatGPT in its early stage. The study utilized VOSviewer software to clean, analyze, mine, and visualize the data. Among the sample, it was observed that 49{\%} of the users are PhD holders, predominantly with experience in scientific research and data science fields. Hence, the findings reveal that issues such as plagiarism, references, citations, manuscripts, papers, and literature reviews stand out as the major concerns. These results suggest that while ChatGPT can contribute to academic progress by fostering new knowledge, thoughts, and ideas, it can also facilitate academic misconduct, including plagiarism and inaccurate information dissemination. Therefore, this paper also provides discussions on the implications and ethical issues of ChatGPT. In particular, it is imperative to establish clear ethical boundaries and guidelines for the appropriate use of LLMs in academic settings.",
issn="2661-8907",
doi="10.1007/s42979-024-02714-7",
url="https://doi.org/10.1007/s42979-024-02714-7"
}


@Article{Brand2025,
author="Brand, Joshua L. M.",
title="Put Yourself in My Shoes: Revisiting the Moral Value of Algorithm Aversion Through Reciprocity and Vulnerability",
journal="Philosophy {\&} Technology",
year="2025",
month="Jun",
day="17",
volume="38",
number="3",
pages="88",
abstract="This paper begins by exploring the phenomenon known as algorithm aversion, where users distrust AI and prefer human advice or decision-making even when they are aware of the algorithm's superior performance. Current literature generally frames it as a misguided bias that harms decision accuracy and speed, likening it to a form of neo-Luddism. This view, however, overlooks the fact that the two groups (supporters and sceptics of algorithmic decisions) are speaking different moral languages: the supporters are outcome-orientated, arguing for accuracy and performance, while the sceptics offer a Kantian-position, that asks us to challenge the very precept that AI systems can be a decision-maker, or obligation-bearer, for decisions constrained by rights. To consider their position on the human-AI moral relationship, I take advantage of Korsgaard's work on interspecies moral relationships, concluding that to be an obligation-bearer toward human right-holders, there needs to be a reciprocal reasons-giving relationship which AI in principle cannot fulfil. To meet objections that argue AI could eventually replicate the necessary moral agency requirements, I show that reciprocal relationships also call for constitutive symmetry, highlighting the importance of not only matching rationality, but also the vulnerability inherent in the human condition. With this account, algorithm sceptics are not misguided and have something morally important to say. This does not suggest eliminating AI entirely from decision processes. AI-assisted decision-making can be defended as long as a robust human-centred approach where genuine human control is upheld.",
issn="2210-5441",
doi="10.1007/s13347-025-00911-7",
url="https://doi.org/10.1007/s13347-025-00911-7"
}


@Article{Rahman2026,
author="Rahman, Subhey Sadi
and Islam, Md. Adnanul
and Alam, Md. Mahbub
and Zeba, Musarrat
and Rahman, Md. Abdur
and Chowa, Sadia Sultana
and Raiaan, Mohaimenul Azam Khan
and Azam, Sami",
title="Hallucination to truth: a review of fact-checking and factuality evaluation in large language models",
journal="Artificial Intelligence Review",
year="2026",
month="Jan",
day="03",
volume="59",
number="2",
pages="70",
abstract="Large language models (LLMs) are trained on vast and diverse internet corpora that often include inaccurate or misleading content. Consequently, LLMs can generate misinformation, making robust fact-checking essential. This review systematically analyzes how LLM-generated content is evaluated for factual accuracy by exploring key challenges such as hallucinations, dataset limitations, and the reliability of evaluation metrics. The review emphasizes the need for strong fact-checking frameworks that integrate advanced prompting strategies, domain-specific fine-tuning, and retrieval-augmented generation (RAG) methods. It proposes five research questions that guide the analysis of the recent literature from 2020 to 2025, focusing on evaluation methods and mitigation techniques. Instruction tuning, multi-agent reasoning, and RAG frameworks for external knowledge access are also reviewed. The key findings demonstrate the limitations of current metrics, the importance of validated external evidence, and the improvement of factual consistency through domain-specific customization. The review underscores the importance of building more accurate, understandable, and context-aware fact-checking. These insights contribute to the advancement of research toward more trustworthy models.",
issn="1573-7462",
doi="10.1007/s10462-025-11454-w",
url="https://doi.org/10.1007/s10462-025-11454-w"
}


@Article{Barandiaran2025,
author="Barandiaran, Xabier E.
and Almendros, Lola S.",
title="Transforming agency: On the mode of existence of large language models",
journal="Phenomenology and the Cognitive Sciences",
year="2025",
month="Aug",
day="22",
abstract="This paper investigates the ontological characterization of Large Language Models (LLMs) like ChatGPT. Between inflationary and deflationary accounts, we pay special attention to their status as agents. This requires explaining in detail the architecture, processing, and training procedures that enable LLMs to display their capacities, and the extensions used to turn LLMs into agent-like systems. After a systematic analysis we conclude that a LLM fails to meet necessary and sufficient conditions for autonomous agency in the light of embodied theories of mind: the individuality condition (it is not the product of its own activity, it is not even directly affected by it), the normativity condition (it does not generate its own norms or goals), and, partially the interactional asymmetry condition (it is not the origin and sustained source of its interaction with the environment). If not agents, then{\ldots} What are LLMs? We argue that ChatGPT should be characterized as an interlocutor or linguistic automaton, a library-that-talks, devoid of (autonomous) agency, but capable to engage performatively on non-purposeful yet purpose-structured and purpose-bounded tasks. When interacting with humans, a ``ghostly'' component of the human-machine interaction makes it possible to enact genuine conversational experiences with LLMs. Despite their lack of sensorimotor and biological embodiment, LLMs textual embodiment (the training corpus), digital extended interface embodiments, and resource-hungry computational embodiment, significantly transform existing forms of machine automatism and human agency. Beyond assisted and extended agency, the LLM-human coupling can produce midtended forms of agency, closer to the production of intentional agency than to the extended instrumentality of any previous technologies.",
issn="1572-8676",
doi="10.1007/s11097-025-10094-3",
url="https://doi.org/10.1007/s11097-025-10094-3"
}


@inproceedings{10.1007/978-981-97-1814-6_12,
 abstract = {As software engineering educators, we are always looking for ways to engage our students and help them develop the skills they will require in their future careers. Generative artificial intelligence offers an exciting new tool with the potential to change software engineering education. As a case in point, in this paper we explore how large language models like ChatGPT can be used as interactive teaching assistants in undergraduate software engineering courses. We provide a catalog of prompt examples tailored for 12 key knowledge areas of software engineering. These prompts can be used to generate code, tests, explanations and more, enabling students to experiment and get personalized guidance. Students can brainstorm requirements, simulate APIs, get automated feedback, and much more. Using ChatGPT and similar AI systems prepares students for tools they may encounter in industry and gives them valuable experience in human-AI collaboration. While generative AI offers many benefits, it also brings challenges that must be addressed. We discuss concerns around the correctness, risks and quality of AI-generated answers. Our goal is to inspire and motivate educators to consider using generative AI to create engaging learning experiences for students. We provide resources to help educators get started, encouraging experimentation and sharing of ideas and prompts.},
 address = {Singapore},
 author = {Pereira, Juanan
and L{\'o}pez, Juan Miguel
and Azanza, Maider
and D{\'i}az, {\'O}scar
and Garmendia, Xabier},
 booktitle = {Proceedings of TEEM 2023},
 editor = {Gon{\c{c}}alves, Jos{\'e} Alexandre de Carvalho
and Lima, Jos{\'e} Lu{\'i}s Sousa de Magalh{\~a}es
and Coelho, Jo{\~a}o Paulo
and Garc{\'i}a-Pe{\~{n}}alvo, Francisco Jos{\'e}
and Garc{\'i}a-Holgado, Alicia},
 isbn = {978-981-97-1814-6},
 pages = {125--134},
 publisher = {Springer Nature Singapore},
 title = {Leveraging on Generative AI to Teach Undergraduate Software Engineering: A Collaborative Prompt Catalogue},
 url = {https://link.springer.com/chapter/10.1007/978-981-97-1814-6_12},
 year = {2024}
}

@Article{Wen2023,
author="Wen, Ming
and Wang, Yongcong 
and Xia, Yifan
and Jin, Hai",
title="Evaluating seed selection for fuzzing JavaScript engines",
journal="Empirical Software Engineering",
year="2023",
month="Sep",
day="26",
volume="28",
number="6",
pages="133",
abstract="JavaScript (JS), as a platform-independent programming language, remains to be the most popular language over the years. However, popular JavaScript engines that have been widely utilized by web browsers to interpret JS code, have become the most common targets for attackers. Thus ensuring the security and reliability of JS engines is significant. Fuzzing is a simple yet effective method to unveil vulnerabilities. However, existing JS fuzzers focus more on the design of effective mutation mechanisms to generate diverse and valid seeds while they often ignore the importance of the initial seed corpus selected to drive the fuzzing process. In this paper, we performed extensive experiments to systematically evaluate the impact of seed selection on fuzzing JavaScript engines. In particular, we investigate seed selections from three main dimensions, their collected sources (e.g., CVE PoCs, Regression tests, etc.), the number and sizes, as well as a set of concerned code properties. Our major findings reveal that seeds collected from different sources can cast a significant impact on the fuzzing effectiveness (i.e., CVE PoC is significantly better than the other types of seeds), and seed files containing those concerned code structures can lead existing fuzzers to achieve superior results in terms of both code coverage and unique crashes identified. Inspired by our observations, we devised a simple heuristic to prioritize JavaScript files when selecting seed corpus. Our experiments show that when driven by our selected seed corpus, the existing state-of-art fuzzer is able to achieve significantly higher code coverage and identify more crashes.",
issn="1573-7616",
doi="10.1007/s10664-023-10340-9",
url="https://doi.org/10.1007/s10664-023-10340-9"
}


@inproceedings{10.1007/978-981-97-9255-9_11,
 abstract = {An AI tutor with a backend of GPT-4 was used as a homework assistant in an introductory data science course with 127 students. The tutor was given the homework assignments and the solutions, but was instructed not to give answers directly, and instead reply to student queries with questions meant to lead the student in the right direction, deploying a strategy known as the Socratic method. All interactions were anonymously logged and coded on seventeen attributes such as helpfulness and degree of answer leak. Surveys were also deployed to better understand students' perceptions and use cases. The tutor was generally perceived as helpful to learning by students and an independent coder, and only rarely directly leaked the solution. However, the system also displayed a pattern of student achievement observed in other tutoring systems, where students who asked too many questions did worse on the midterms, even controlling for programming background. Other trends in the usage of the tutor are also discussed.},
 address = {Singapore},
 author = {Gold, Kevin
and Geng, Shuang},
 booktitle = {Artificial Intelligence in Education Technologies: New Development and Innovative Practices},
 editor = {Schlippe, Tim
and Cheng, Eric C. K.
and Wang, Tianchong},
 isbn = {978-981-97-9255-9},
 pages = {154--170},
 publisher = {Springer Nature Singapore},
 title = {On the Helpfulness of a Zero-Shot Socratic Tutor},
 url = {https://link.springer.com/chapter/10.1007/978-981-97-9255-9_11},
 year = {2025}
}

@Inbook{Gupta2025,
author="Gupta, Rajan
and Tiwari, Sanju
and Chaudhary, Poonam",
title="Large Generative Models for Different Data Types",
bookTitle="Generative AI: Techniques, Models and Applications",
year="2025",
publisher="Springer Nature Switzerland",
address="Cham",
pages="103--162",
abstract="This chapter provides a comprehensive overview of large generative models across various data types, including vision, text, speech, image, and code. It explores the foundational architectures, key concepts, and applications that have driven advancements in Generative AI. Starting with text generative models, such as GPT and T5, the chapter delves into models that generate coherent and contextually relevant text for applications like content creation, summarization, and translation. It then moves to image and vision generative models, highlighting architectures like GANs, VAEs, and diffusion models that generate high-quality images and enable tasks such as image synthesis, style transfer, and text-to-image generation (e.g., DALL-E and Stable Diffusion). The discussion extends to speech generative models, focusing on models like WaveNet, Tacotron, and FastSpeech, which are pivotal in text-to-speech synthesis and voice cloning. The chapter also covers audio generation models, exploring how models like WaveGAN and MelGAN generate high-fidelity audio, including music and sound effects. In the realm of programming code generation, models like Codex and AlphaCode are explored for their ability to generate, complete, and translate code, thus enhancing software development workflows. Finally, the chapter examines multimodal generative models that operate across different data types, such as text-to-image or text-to-video generation, showcasing models like CLIP and Make-A-Video. This chapter offers both practitioners and research scholars a detailed understanding of the diverse landscape of generative models and their transformative applications across multiple domains.",
isbn="978-3-031-82062-5",
doi="10.1007/978-3-031-82062-5_6",
url="https://doi.org/10.1007/978-3-031-82062-5_6"
}


@Article{Wang2025,
author="Wang, Gui-Zhen
and Wang, Zheng
and Bai, Shi-Hao
and Tan, Yun
and Zhong, Wen-Zhao
and Sun, Guo-Gui
and Liu, Yu-Tao
and Pan, Bo
and Huang, Chen
and Wang, Di
and Sun, Bei-Bei
and Chen, Dong-Ni
and Zhang, Bin
and Zhou, Yong-Chun
and Li, Sheng
and Zhang, Xiang-Wei
and Han, Si-Chong
and Yang, Fu-Ying
and Shi, Xue-Yan
and Jie, Xiao-Liang
and Shen, Yu-Ke
and Liang, Li-Jun
and Wen, Zhe-Sheng
and Zhang, Li
and Li, Ming-Kun
and Wang, Na
and Liu, Jin-song
and Dong, Ying
and Wang, Man-Li
and Wang, Yan
and Wang, Chang-Li
and Xie, Da-Wei
and Han, Ze-Guang
and Ying, Jian-Ming
and Chen, Chong
and Huang, Yun-Chao
and Ji, Hong-Bin
and Zhang, Yuan-Yuan
and Yu, Yan
and Zhou, Guang-Biao",
title="Characterization of the extrinsic and intrinsic signatures and therapeutic vulnerability of small cell lung cancers",
journal="Signal Transduction and Targeted Therapy",
year="2025",
month="Sep",
day="10",
volume="10",
number="1",
pages="290",
abstract="Small-cell lung cancer (SCLC), an aggressive neuroendocrine tumor strongly associated with exposure to tobacco carcinogens, is characterized by early dissemination and dismal prognosis with a five-year overall survival of less than 7{\%}. High-frequency gain-of-function mutations in oncogenes are rarely reported, and intratumor heterogeneity (ITH) remains to be determined in SCLC. Here, via multiomics analyses of 314 SCLCs, we found that the ASCL1+/MKI67+ and ASCL1+/CRIP2+ clusters accounted for 74.38{\%} of the 190,313 SCLC cancer cells from 39 patients, with the ASCL1+SOX1+ stem-like cell cluster across SCLC subtypes. The major histocompatibility complex (MHC) class I molecules were expressed at low levels in six and high levels in five cancer cell clusters and were inversely associated with the KI67 expression level. Abnormal splicing of mRNAs was a feature of SCLC, with focal adhesion kinase (FAK) splicing variants identified in 119 (77.3{\%}) of 154 patients. FAK variants exhibited elevated kinase activity, were associated with the worst prognosis, and were sensitive to FAK inhibitors in patient-derived organoids and xenograft models. Eleven high-frequency mutations were identified in addition to TP53 and RB1, and smoking status and tumor stage did not affect microbiota variance in SCLC. Taken together, our data further revealed the complicated ITH and discovered that FAK splicing variants represent high-frequency gain-of-function alterations in oncogene in SCLC and potential therapeutic targets for this recalcitrant cancer.",
issn="2059-3635",
doi="10.1038/s41392-025-02378-6",
url="https://doi.org/10.1038/s41392-025-02378-6"
}


@Inbook{Vostokov2024,
author="Vostokov, Dmitry",
title="What AI and Machine Learning Can Do for Python Debugging",
bookTitle="Python Debugging for AI, Machine Learning, and Cloud Computing: A Pattern-Oriented Approach ",
year="2024",
publisher="Apress",
address="Berkeley, CA",
pages="213--218",
abstract="In this chapter, you will survey the possibilities of using the rapidly evolving AI/ML to aid, enhance, and even revolutionize the traditional manual Python debugging process.",
isbn="978-1-4842-9745-2",
doi="10.1007/978-1-4842-9745-2_15",
url="https://doi.org/10.1007/978-1-4842-9745-2_15"
}


@Inbook{Sikos2025,
author="Sikos, Leslie F.",
title="Defensive Generative AI",
bookTitle="Generative AI in Cybersecurity",
year="2025",
publisher="Springer Nature Switzerland",
address="Cham",
pages="1--24",
abstract="Prompt-based data generation using generative models can be used for a wide range of defensive actions by assisting, and reducing the workload of, security analysts, and enabling partial automation. Processing cybersecurity logs and analyzing complex data distributions for anomaly detection, performing data augmentation, attack simulation, and feature extraction for intrusion detection and malware classification are just some of the application areas. This chapter discusses the use of generative pre-training, generative adversarial networks, and variational autoencoders in cybersecurity.",
isbn="978-3-032-05250-6",
doi="10.1007/978-3-032-05250-6_1",
url="https://doi.org/10.1007/978-3-032-05250-6_1"
}


@inproceedings{10.1007/978-3-031-72983-6_3,
 abstract = {This work focuses on benchmarking the capabilities of vision large language models (VLLMs) in visual reasoning. Different from prior studies, we shift our focus from evaluating standard performance to introducing a comprehensive safety evaluation suite Unicorn, covering out-of-distribution (OOD) generalization and adversarial robustness. For the OOD evaluation, we present two novel visual question-answering (VQA) datasets, each with one variant, designed to test model performance under challenging conditions. In exploring adversarial robustness, we propose a straightforward attack strategy for misleading VLLMs to produce visual-unrelated responses. Moreover, we assess the efficacy of two jailbreaking strategies, targeting either the vision or language input of VLLMs. Our evaluation of 22 diverse models, ranging from open-source VLLMs to GPT-4V and Gemini Pro, yields interesting observations: 1) Current VLLMs struggle with OOD texts but not images, unless the visual information is limited; and 2) These VLLMs can be easily misled by deceiving vision encoders only, and their vision-language training often compromise safety protocols. We release this safety evaluation suite at https://github.com/UCSC-VLAA/vllm-safety-benchmark.},
 address = {Cham},
 author = {Tu, Haoqin
and Cui, Chenhang
and Wang, Zijun
and Zhou, Yiyang
and Zhao, Bingchen
and Han, Junlin
and Zhou, Wangchunshu
and Yao, Huaxiu
and Xie, Cihang},
 booktitle = {Computer Vision -- ECCV 2024},
 editor = {Leonardis, Ale{\v{s}}
and Ricci, Elisa
and Roth, Stefan
and Russakovsky, Olga
and Sattler, Torsten
and Varol, G{\"u}l},
 isbn = {978-3-031-72983-6},
 pages = {37--55},
 publisher = {Springer Nature Switzerland},
 title = {How Many Are in This Image A Safety Evaluation Benchmark for Vision LLMs},
 url = {https://link.springer.com/chapter/10.1007/978-3-031-72983-6_3},
 year = {2025}
}

@inproceedings{10.1007/978-3-031-64315-6_9,
 abstract = {The integration of ChatGPT as a supportive tool in education, notably in programming courses, addresses the unique challenges of programming education by providing assistance with debugging, code generation, and explanations. Despite existing research validating ChatGPT's effectiveness, its application in university-level programming education and a detailed understanding of student interactions and perspectives remain limited. This paper explores ChatGPT's impact on learning in a Python programming course tailored for first-year students over eight weeks. By analyzing responses from surveys, open-ended questions, and student-ChatGPT dialog data, we aim to provide a comprehensive view of ChatGPT's utility and identify both its advantages and limitations as perceived by students. Our study uncovers a generally positive reception toward ChatGPT and offers insights into its role in enhancing the programming education experience. These findings contribute to the broader discourse on AI's potential in education, suggesting paths for future research and application.},
 address = {Cham},
 author = {Ma, Boxuan
and Chen, Li
and Konomi, Shin'ichi},
 booktitle = {Artificial Intelligence in Education. Posters and Late Breaking Results, Workshops and Tutorials, Industry and Innovation Tracks, Practitioners, Doctoral Consortium and Blue Sky},
 editor = {Olney, Andrew M.
and Chounta, Irene-Angelica
and Liu, Zitao
and Santos, Olga C.
and Bittencourt, Ig Ibert},
 isbn = {978-3-031-64315-6},
 pages = {113--126},
 publisher = {Springer Nature Switzerland},
 title = {Enhancing Programming Education with ChatGPT: A Case Study on Student Perceptions and Interactions in a Python Course},
 url = {https://link.springer.com/chapter/10.1007/978-3-031-64315-6_9},
 year = {2024}
}

@inproceedings{10.1007/978-981-95-3182-0_3,
 abstract = {Database Management Systems (DBMSs) serve as critical infrastructure software, underpinning data storage, management, and access control across a wide range of applications. In practice, DBMS development often follows a fork-based model, where the vendors build upon native systems to implement customized features. However, native DBMSs tend to contain silent vulnerability fixes - fixes applied to vulnerabilities without public disclosure - which lead to vulnerability information leakage and leave downstream DBMSs exposed to unfixed vulnerabilities. This work presents the first systematic study of silent vulnerability fixes in DBMSs and their security implications under fork-based development. Firstly, we introduce NightHawk, a detection framework that analyzes inheritance relationships and identifies silent fixes. NightHawk demonstrates superior performance in silent vulnerability fix detection, with higher precision and more than a 10{\%} improvement in recall compared to existing approaches. Using NightHawk, we analyze 16 DBMSs, over 60 latest releases, and more than 9,000 commits. As a result, we identify 1,951 silent vulnerability fixes, the inheritance of which by downstream DBMSs poses significant security risks. We believe the findings offer new insights into DBMS security and reveal overlooked threats in software supply chains.},
 address = {Singapore},
 author = {Dong, Jialiang
and Ni, Zihan
and Susilo, Willy
and Ma, Siqi},
 booktitle = {Data Security and Privacy Protection},
 editor = {Chen, Xiaofeng
and Hu, Haibo
and Wang, Ding},
 isbn = {978-981-95-3182-0},
 pages = {38--56},
 publisher = {Springer Nature Singapore},
 title = {Ghosts in DBMS: Revealing the Security Impacts of Silent Fixes},
 url = {https://link.springer.com/chapter/10.1007/978-981-95-3182-0_3},
 year = {2026}
}

@inproceedings{10.1007/978-3-032-07373-0_44,
 abstract = {Cloud computing environments are increasingly complex and distributed, making them fertile ground for advanced cyber threats. Traditional centralized security models often fail to meet the agility, scalability, and resilience requirements of modern infrastructure. This literature review explores how Multi-Agent Systems (MAS) have been used as a distributed and intelligent approach to enhancing cloud security. Through thematic analysis of recent academic work, including architectures, AI techniques, and real-world deployments, this paper highlights current trends, strengths, limitations, and future directions of MAS in cybersecurity. It also offers personal insights from the authors, reflecting on the evolving landscape of cloud security and the role of intelligent systems in shaping it.},
 address = {Cham},
 author = {Gjini, Anisa
and Daci, Genti
and Aranitasi, Marin},
 booktitle = {AI and Digital Transformation: Opportunities, Challenges, and Emerging Threats in Technology, Business, and Security},
 editor = {Dhoska, Klodian
and Spaho, Evjola},
 isbn = {978-3-032-07373-0},
 pages = {588--596},
 publisher = {Springer Nature Switzerland},
 title = {Securing the Cloud with AI: How Multi-agent Systems Detect and Prevent Cyber Threats},
 url = {https://link.springer.com/chapter/10.1007/978-3-032-07373-0_44},
 year = {2026}
}

@inproceedings{10.1007/978-981-96-8197-6_7,
 abstract = {Large Language Models (LLMs) excel in various applications, from conversational agents to medical diagnostics. However, they often inherit societal biases from training data, risking discriminatory outcomes. This survey provides a structured review of fairness in LLMs, analyzing the origins of bias in data and model design. We summarize key bias evaluation metrics, including embedding-based and probability-based approaches, and examine mitigation techniques in pre-processing, in-processing, and post-processing strategies. We also highlight essential datasets and tools for fairness research and discuss current challenges, offering future directions to guide the development of fairer LLMs.},
 address = {Singapore},
 author = {Haque, Farsheed
and Xu, Depeng
and Niu, Xi},
 booktitle = {Trends and Applications in Knowledge Discovery and Data Mining},
 editor = {Yuan, Shuhan
and Malliaros, Fragkiskos
and Zheng, Xin},
 isbn = {978-981-96-8197-6},
 pages = {83--101},
 publisher = {Springer Nature Singapore},
 title = {A Comprehensive Survey on Bias and Fairness in Large Language Models},
 url = {https://link.springer.com/chapter/10.1007/978-981-96-8197-6_7},
 year = {2025}
}

@inproceedings{10.1007/978-3-032-06700-5_20,
 abstract = {In today's fast-paced development environment, where efficiency and speed are paramount, manual tasks such as taking screenshots, converting files, rebooting systems, and managing repositories have become increasingly tedious and time-consuming. These routine activities disrupt developer workflow and hinder productivity, consuming valuable time. To address these inefficiencies, this work proposes a comprehensive automation tool that extends beyond handling basic tasks to streamline workflows and optimize productivity. Firstly, this tool centralizes a wide range of operations, including automating code generation, creating detailed reports, and developing websites. By integrating these functionalities, developers can eliminate redundant tasks and focus on high-level problem-solving. Secondly, the automation tool enhances accuracy and consistency across development projects, ensuring higher standards of work and reducing errors associated with manual processes. Furthermore, the tool aligns with evolving technological demands, enabling teams to adapt to increasing project complexities while maintaining efficient workflows. This solution represents a transformative approach to software development, combining automation and centralization to reduce manual workloads and optimize developer productivity. The implementation of such an all-in-one automation platform promises to significantly improve efficiency and foster innovation in the industry.},
 address = {Cham},
 author = {Nixon, Dwayne
and Menezes, Shaun
and Kulkarni, Ramya
and Shaikh, Dr. Phiroj},
 booktitle = {ICT Analysis and Applications},
 editor = {Fong, Simon
and Dey, Nilanjan
and Joshi, Amit},
 isbn = {978-3-032-06700-5},
 pages = {195--205},
 publisher = {Springer Nature Switzerland},
 title = {Unified NL Interface for Automating System Commands and Advanced Dev Ops Tasks},
 url = {https://link.springer.com/chapter/10.1007/978-3-032-06700-5_20},
 year = {2026}
}

@Inbook{Taulli2023,
author="Taulli, Tom",
title="Auto Code Generation",
bookTitle="Generative AI: How ChatGPT and Other AI Tools Will Revolutionize Business",
year="2023",
publisher="Apress",
address="Berkeley, CA",
pages="127--143",
abstract="Andrej Karpathy is one of the world's top programmers. While at the University of Toronto, he got a double major in computer science and physics. There was then a minor in math. He attended a class of Geoffrey Hinton and learned about deep learning.",
isbn="978-1-4842-9367-6",
doi="10.1007/978-1-4842-9367-6_6",
url="https://doi.org/10.1007/978-1-4842-9367-6_6"
}


@Article{Huang2025,
author="Huang, Yu-Huan
and Liu, Chen-Yan
and Lin, Yun
and Cai, Yu-Fan
and Jiang, Bo
and Yang, Ping
and Huang, Zhiyong
and Dong, Jin Song",
title="CoEdPilot: Interactively Recommending Project-Wise Code Edits",
journal="Journal of Computer Science and Technology",
year="2025",
month="Jul",
day="01",
volume="40",
number="4",
pages="980--992",
abstract="Incremental code editing, as a fundamental task in software development, requires developers to iteratively identify edit locations and modify code. However, existing language model-driven approaches primarily focus on generating edit solutions for a single location, failing to provide comprehensive end-to-end solutions. To address this limitation and support real-world editing scenarios, we propose CoEdPilot, a project-wide interactive code editing recommendation tool. CoEdPilot utilizes edit descriptions and edit history, and recommends the next edit location with solutions across the entire project. It further refines its recommendations based on user editing feedback, enabling an end-to-end, iterative, and interactive editing process. We implement CoEdPilot as a visual studio code extension that monitors user actions, identifies subsequent editing locations, and generates edits throughout the project. Its functionality is powered by a set of backend language models, which are trained on 180k high-quality commits from 471 open-source repositories. Extensive experiments demonstrate CoEdPilot's capabilities in accurately identifying editing locations (i.e., edit location predicted with an accuracy of 85.03{\%}--88.99{\%}) and generating high-quality edit solutions (i.e., generated edit content with a top-1 exact match rate (EMR) of 33.48{\%}--48.94{\%}). Our case study and user study of 18 participants further validate CoEdPilot's practicability.",
issn="1860-4749",
doi="10.1007/s11390-025-5139-z",
url="https://doi.org/10.1007/s11390-025-5139-z"
}


@Inbook{Chakrabarty2025,
author="Chakrabarty, Jaydeep
and Muralinath, Harinee",
title="The Age of Agents - From Scripts to Sidekicks",
bookTitle="The Value Vector: Building Scalable Generative AI-Based Applications",
year="2025",
publisher="Apress",
address="Berkeley, CA",
pages="237--285",
abstract="The morning security briefing was just wrapping up in TechNova's main conference room. Our enterprise AI guardrails implementation had exceeded expectations, with the metrics showing robust performance across all checkpoints. Dr. Chen, our Lead AI Researcher, was presenting the final testing results to the technical leads.",
isbn="979-8-8688-1880-6",
doi="10.1007/979-8-8688-1880-6_6",
url="https://doi.org/10.1007/979-8-8688-1880-6_6"
}


@Inbook{Werthner2025,
author="Werthner, Hannes",
title="Artificial Intelligence",
bookTitle="Digital Humanism: On Digitalization and Artificial Intelligence ",
year="2025",
publisher="Springer Nature Switzerland",
address="Cham",
pages="9--38",
abstract="This chapter explores the rapid rise of AI, especially large language models (LLMs), which excel in tasks like programming, creative work, and decision-making but also pose risks associated with bias, misinformation, and black box. While AI holds immense economic potential, it raises concerns about job losses, energy consumption, and the concentration of power in a few tech companies. The chapter emphasizes the importance of balancing AI development through regulation, collaboration, and democratic control to ensure it supports, rather than replaces, human capabilities.",
isbn="978-3-031-86905-1",
doi="10.1007/978-3-031-86905-1_2",
url="https://doi.org/10.1007/978-3-031-86905-1_2"
}


@Inbook{Dmonte2025,
author="Dmonte, Alphaeus
and Wang, Yuxia
and Nakov, Preslav
and Li, Haonan
and Han, Xudong
and Zampieri, Marcos
and Lybarger, Kevin
and Albanese, Massimiliano
and Baldwin, Timothy",
editor="Srivastava, Biplav
and Nikolich, Anita
and Hickerson, Andrea
and Koppel, Tarmo",
title="Creating Frameworks and Datasets at the Intersection of AI Safety and Elections",
bookTitle="PROMISE -- PROMoting AI's Safe usage for Elections",
year="2025",
publisher="Springer Nature Switzerland",
address="Cham",
pages="115--145",
abstract="This chapter explores the intersection of Artificial Intelligence (AI) and elections, focusing on the critical challenge of ensuring safety in the age of Large Language Models (LLMs) including AI-generated misinformation and its impact on electoral integrity. This chapter is divided into two complementary parts. The first part describes Do-Not-Answer, a framework featuring a three-level hierarchical taxonomy of LLM risks including hallucination, bias, toxic language, and misinformation. Do-Not-Answer has been used to create datasets in multiple languages that serve to evaluate LLMs with respect to mitigation strategies, content filtering, and model alignment. The second part discusses the ElectAI taxonomy and dataset. ElectAI has been created to aid claim understanding with respect to election processes, equipment, and claims of fraud in both AI- and human-generated social media posts. The two parts combined present the reader with a comprehensive overview of both general and election-related AI safety issues along with strategies to address them.",
isbn="978-3-031-89853-2",
doi="10.1007/978-3-031-89853-2_9",
url="https://doi.org/10.1007/978-3-031-89853-2_9"
}


@Article{Fang2025,
author="Fang, Luyang
and Yu, Xiaowei
and Cai, Jiazhang
and Chen, Yongkai
and Wu, Shushan
and Liu, Zhengliang
and Yang, Zhenyuan
and Lu, Haoran
and Gong, Xilin
and Liu, Yufang
and Ma, Terry
and Ruan, Wei
and Abbasi, Ali
and Zhang, Jing
and Wang, Tao
and Latif, Ehsan
and Liu, Wei
and Zhang, Wei
and Kolouri, Soheil
and Zhai, Xiaoming
and Zhu, Dajiang
and Zhong, Wenxuan
and Liu, Tianming
and Ma, Ping",
title="Knowledge distillation and dataset distillation of large language models: emerging trends, challenges, and future directions",
journal="Artificial Intelligence Review",
year="2025",
month="Nov",
day="20",
volume="59",
number="1",
pages="17",
abstract="The exponential growth of Large Language Models (LLMs) continues to highlight the need for efficient strategies to meet ever-expanding computational and data demands. This survey provides a comprehensive analysis of two complementary paradigms: Knowledge Distillation (KD) and Dataset Distillation (DD), both aimed at compressing LLMs while preserving their advanced reasoning capabilities and linguistic diversity. We first examine key methodologies in KD, such as task-specific alignment, rationale-based training, and multi-teacher frameworks, alongside DD techniques that synthesize compact, high-impact datasets through optimization-based gradient matching, latent space regularization, and generative synthesis. Building on these foundations, we explore how integrating KD and DD can produce more effective and scalable compression strategies. Together, these approaches address persistent challenges in model scalability, architectural heterogeneity, and the preservation of emergent LLM abilities. We further highlight applications across domains such as healthcare and education, where distillation enables efficient deployment without sacrificing performance. Despite substantial progress, open challenges remain in preserving emergent reasoning and linguistic diversity, enabling efficient adaptation to continually evolving teacher models and datasets, and establishing comprehensive evaluation protocols. By synthesizing methodological innovations, theoretical foundations, and practical insights, our survey charts a path toward sustainable, resource-efficient LLMs through the tighter integration of KD and DD principles.",
issn="1573-7462",
doi="10.1007/s10462-025-11423-3",
url="https://doi.org/10.1007/s10462-025-11423-3"
}


@inproceedings{10.1007/978-981-95-2521-8_28,
 abstract = {The rapid advancement of artificial intelligence technology, particularly large language models (LLMs) exemplified by ChatGPT, is significantly impacting the global labor market. By integrating competency theory with BERTopic topic modeling, this study analyzes 1,827 LLM-related job postings from recruitment platforms in China, revealing the core structural demands of the current talent market. The results identify six primary thematic categories for LLMs positions, dominated by AI product operation (37.71{\%}) and large-model infrastructure development (18.98{\%}), followed closely by data engineering and analysis (14.94{\%}), application development and integration (13.57{\%}), and multimodal AI development (13.03{\%}). Although vertical-domain applications represent the smallest proportion (1.75{\%}), they demonstrate a clear trend toward deep industry penetration. Furthermore, the identified positions reflect three notable characteristics: hierarchical technological structures aligned with evolving full-stack competencies, combining business acumen with innovative thinking, and multimodal expansion paired with vertical industry integration. Based on these insights, this paper proposes a project-based curriculum integrating technology, practical applications, and industry experience. Additionally, it emphasizes enhancing students' competence and AI literacy. These recommendations aim to provide an empirical basis for cultivating versatile talents suited for the era of LLMs.},
 address = {Singapore},
 author = {Zhuoyuan, Tang
and Wei, Wei
and Kai, Liang
and Yao, Yang
and Zhe, Lin
and Lam, Chi Kin},
 booktitle = {Proceedings of The 11th International Conference on Frontiers of Educational Technologies 2025},
 editor = {Gokhale, Anu},
 isbn = {978-981-95-2521-8},
 pages = {395--406},
 publisher = {Springer Nature Singapore},
 title = {Data-Driven Analysis of Talent Demand for Large Language Models: Implications for Educational Reform from a Competency Perspective},
 url = {https://link.springer.com/chapter/10.1007/978-981-95-2521-8_28},
 year = {2026}
}

@Inbook{Lehman2024,
author="Lehman, Joel
and Gordon, Jonathan
and Jain, Shawn
and Ndousse, Kamal
and Yeh, Cathy
and Stanley, Kenneth O.",
editor="Banzhaf, Wolfgang
and Machado, Penousal
and Zhang, Mengjie",
title="Evolution Through Large Models",
bookTitle="Handbook of Evolutionary Machine Learning",
year="2024",
publisher="Springer Nature Singapore",
address="Singapore",
pages="331--366",
abstract="This chapter pursues the insightInsight that large language modelsLarge language models (LLMs) trained to generate code can vastly improve the effectiveness of mutation operators applied to programs in genetic programming (GP). Because such LLMs benefit from training data that includes sequential changes and modifications, they can approximate likely changes that humans would make. To highlight the breadth of implications of such evolution through large models (ELM), inEvolution through Large Models the main experiment ELM combined with MAP-ElitesMAP-Elites generates hundreds of thousands of functional examples of Python programs that output working ambulating robots in the SodaraceSodarace domain, which the original LLM had never seen in pretraining. These examples then help to bootstrapBootstrap training a new conditional language model that can output the right walker for a particular terrain. The ability to bootstrapBootstrap new models that can output appropriate artifacts for a given context in a domain where zero training data was previously available carries implications for open-endednessOpen-endedness, deep learning, and reinforcement learningReinforcement Learning. These implications are explored here in depth in the hope of inspiring new directions of research now opened up by ELM.",
isbn="978-981-99-3814-8",
doi="10.1007/978-981-99-3814-8_11",
url="https://doi.org/10.1007/978-981-99-3814-8_11"
}


@inproceedings{10.1007/978-3-031-78471-2_20,
 abstract = {When planning for upcoming travel, it is essential to engage in systematic pre-trip preparation. To commence, travelers should first pinpoint their preferences, followed by conducting extensive research using various resources such as travel blogs, forums, tourism websites, social media platforms, and personal recommendations to compile a preliminary list of activities. Subsequently, these activities should be prioritized based on constraints such as time, budget, and accessibility, aligning with the travelers' specific travel objectives. A detailed day-to-day itinerary should be developed, encompassing the prioritized activities while also allowing room for spontaneity and relaxation. Practical considerations like securing reservations, arranging transportation and accommodations, and planning appropriate clothing and gear should be addressed during this phase. Recent advancements in artificial intelligence (AI), particularly Large Language Models (LLMs) such as GPT-4, have significantly improved the process of travel planning. These advanced AI models offer personalized recommendations, streamline itinerary creation, and provide real-time updates. By analyzing travelers' preferences and constraints, these tools suggest optimized plans, thus ensuring a tailored and dynamic approach to travel planning. This research proposes a comprehensive framework that integrates three key components: a survey designed to capture traveler profiles, interaction with an LLM for tailored recommendations, and an evaluation process focused on assessing satisfaction. A pilot study involving a two-stage survey has demonstrated the framework's potential to enhance travel planning by offering customized, dynamic, and highly satisfactory activity suggestions.},
 address = {Cham},
 author = {Nikas, Ioannis A.
and Koutras, Athanasios
and Theodora, Antonopoulou},
 booktitle = {Innovation and Creativity in Tourism, Business and Social Sciences},
 editor = {Katsoni, Vicky
and Costa, Carlos},
 isbn = {978-3-031-78471-2},
 pages = {515--530},
 publisher = {Springer Nature Switzerland},
 title = {A Conceptual Framework for Developing and Evaluating Personalized Tourist Recommendation Systems Using Large Language Models},
 url = {https://link.springer.com/chapter/10.1007/978-3-031-78471-2_20},
 year = {2025}
}

@Article{Jiang2025,
author="Jiang, Leilei
and Zhu, Guixiang
and Sun, Jianshan
and Cao, Jie
and Wu, Jia",
title="Exploring the occupational biases and stereotypes of Chinese large language models",
journal="Scientific Reports",
year="2025",
month="May",
day="29",
volume="15",
number="1",
pages="18777",
abstract="Large Language Models (LLMs) are transforming various aspects of our daily lives and work through their generated content, known as Artificial Intelligence Generated Content (AIGC). To effectively harness this change, it is essential to understand the limitations within these models. While extensive prior research has addressed biases in OpenAI's ChatGPT, limited attention has been given to biases present in Chinese Large Language Models (C-LLMs). This study systematically examines biases in five representative C-LLMs. We collected 90 Chinese surnames derived from authoritative demographic statistics and 12 occupations covering various professional sectors as input prompts. Each prompt was generated three times by the C-LLMs, resulting in a dataset comprising 16,200 generated personal profiles. We then evaluated these profiles for biases regarding gender, region, age, and educational background. Our findings reveal that the content produced by each examined C-LLMs exhibits significant gender and regional biases, as well as age and educational stereotypes. Notably, while most models can generate some unbiased content, ChatGLM stands out as the exception. In contrast, Tongyiqianwen is the only model that may refuse to generate certain content, due to its strong privacy protection mechanisms. We also further analyze the underlying mechanisms of bias formation by examining different stages of the model lifecycle and considering the unique characteristics of the Chinese linguistic and sociocultural context. This paper will contribute substantially to the literature on biases in C-LLMs and provide important insights for users aiming to utilize these models more effectively and ethically.",
issn="2045-2322",
doi="10.1038/s41598-025-03893-w",
url="https://doi.org/10.1038/s41598-025-03893-w"
}


@inproceedings{10.1007/978-981-96-4506-0_3,
 abstract = {In the increasingly challenging landscape of cyberspace, penetration testing has garnered significant attention as a means of maintaining cybersecurity. The automation and intelligent enhancement of penetration testing have become key areas of current research. Particularly, the focus of automated penetration testing research lies in penetration path planning and penetration decision generation. This review provides a comprehensive overview of the methodologies related to automated penetration testing, with an emphasis on the development processes and current research status of penetration path planning and penetration decision generation. It also summarizes the current limitations and challenges in this field and offers perspectives on future development.},
 address = {Singapore},
 author = {Zheng, Chencong
and Lu, Hui
and Peng, Jin
and Huang, Xun
and Gao, Rui
and Xue, Jieyao
and Zhang, Xinge
and Liu, Youyu
and Zhang, Haonan
and Liu, Zhouyang},
 booktitle = {Cyberspace Simulation and Evaluation},
 editor = {Xu, Guangxia
and Zhou, Wanlei
and Zhang, Jiawei
and Zhang, Yanchun
and Jia, Yan},
 isbn = {978-981-96-4506-0},
 pages = {43--59},
 publisher = {Springer Nature Singapore},
 title = {Review of Path Planning and Decision-Making in Automated Penetration Testing},
 url = {https://link.springer.com/chapter/10.1007/978-981-96-4506-0_3},
 year = {2025}
}

@inproceedings{10.1007/978-3-032-05246-9_2,
 abstract = {This chapter examines the accelerating convergenceConvergence of artificial intelligenceArtificial Intelligence (AI) (AI) and biotechnologyBiotechnology, a transformative development reshaping scientific discovery, technological innovation, and global governanceGovernance. As AIArtificial Intelligence (AI) methods, particularly machine learning (ML), deep learning (DL), and natural language processing (NLP), are increasingly applied to biological research and engineering, they enable new capabilities ranging from protein structure prediction and gene editing to AI-driven design of biological systems. These developments mark not only the integration of tools and techniques but the emergence of new paradigms at the intersection of computation and biology. We explore how this convergenceConvergence unfolds across multiple domains, detailing both its profound opportunities and systemic risks. The chapter identifies four core integration paradigms: AI-augmented biological design, biological computing substrates, distributed biological intelligence, and synthetic systems at the AI-biology interface, highlighting the potential for cascading dynamics and identifying measures for assessing and mitigating risk. In parallel, we consider how diverse stakeholder perspectives shape interpretations of uncertainty, with implications for public trust, innovation policy, and regulatory design. A central contribution of the chapter is its ten governanceGovernance principles grounded in an adaptive, pluralistic, and capability-based approach. Drawing on theories of complex systems, risk, and uncertainty, the chapter outlines mechanisms for anticipating emerging challenges, managing dual-use concerns, and fostering inclusive, participatory approaches to decision-making. The future of this interface relies not only on technological advances but on our ability to ensure that innovation proceeds in ways that are safe, equitable, and aligned with societal priorities and values. By integrating empirical case studies with conceptual frameworks, this chapter offers guidance for scholars, policymakers, and practitioners seeking to responsibly navigate the convergenceConvergence of AIArtificial Intelligence (AI) and biotechnologyBiotechnology.},
 address = {Cham},
 author = {Marcellin, Megan C.
and Pescaroli, Gianluca
and Schmidt, Markus
and Trump, Benjamin D.
and Tubbs, Maj Travis
and Villegas-Torres, Maria Francisca
and Lambert, James H.
and Haggenmiller, Christian
and Blume, Yaroslav
and Kamrud, Alexander
and Linkov, Igor},
 booktitle = {Biotechnology and AI: Technological Convergence and Information Hazards},
 editor = {Cummings, Christopher L.
and Trump, Benjamin D.
and Prado, Valentina
and Ellinport, Beth
and Linkov, Igor},
 isbn = {978-3-032-05246-9},
 pages = {11--55},
 publisher = {Springer Nature Switzerland},
 title = {A Brief Introduction to the Convergence of Artificial Intelligence and Biotechnology},
 url = {https://link.springer.com/chapter/10.1007/978-3-032-05246-9_2},
 year = {2026}
}

@Article{Esmaeili2025,
author="Esmaeili, Amirreza
and Saberi, Iman
and Fard, Fatemeh",
title="Empirical studies of parameter efficient methods for large language models of code and knowledge transfer to R",
journal="Empirical Software Engineering",
year="2025",
month="Dec",
day="05",
volume="31",
number="2",
pages="30",
abstract="Parameter Efficient Fine-Tuning (PEFT) methods are proposed as an alternative fine-tuning approach for Large Language Models (LLM) to minimize high training costs. While prior research demonstrates the effectiveness of PEFT methods in knowledge transfer using smaller language models, their application to larger LLMs, particularly in low-resource and unseen programming languages such as R, remains under-explored. In this work, we evaluate PEFT methods, LoRA, Compacter, and IA{\$}{\$}^3{\$}{\$}on LLMs for code summarization and generation, with a particular emphasis on knowledge transfer to R as an unseen under-explored target language. Our experiments reveal that LoRA consistently outperforms Compacter and IA{\$}{\$}^3{\$}{\$}in all settings, while Compacter offers significant resource efficiency with minimal performance trade-offs. Additionally, we find that the number of trainable parameters has a greater influence on the functional accuracy of the generated code than PEFT architecture. Our study can direct future research in developing code intelligent tasks for unseen languages including R, as well as the choice of PEFT methods for knowledge transfer, especially when balancing the computational cost and performance.",
issn="1573-7616",
doi="10.1007/s10664-025-10740-z",
url="https://doi.org/10.1007/s10664-025-10740-z"
}


@Article{Malekela2025,
author="Malekela, Mark-Silas A.",
title="AI and Confidentiality protection in International Commercial Arbitration: Analysis of the existing legal framework",
journal="Discover Artificial Intelligence",
year="2025",
month="May",
day="30",
volume="5",
number="1",
pages="83",
abstract="The use of Generative Artificial Intelligence (AI) tools in international commercial arbitration reveals a complex intersection with the potential risk of confidential data breaches. Adopting a doctrinal research approach, this research article analyses the legal and regulatory framework applicable to ensure responsible and ethical uses of AI so as to protect confidentiality in international arbitration. This article argues that the use of AI in international arbitration has brought in a new age of efficiency and accuracy in international arbitration, but it also raises concerns on the protection of confidentiality as third-party owned AI tools and systems are prone to a potential risk of confidential data breaches and confidentiality violations on volumes of data stored together in AI tools. The application of the guidelines and principles on the use of AI in international arbitration as well as emerging regulations and laws on AI have varied approaches that are either discretionary or only play a guiding role on the protection of confidential information in international arbitration. Ultimately, this article recommends that it is imperative for the upcoming versions of institutional arbitration rules to enhance the confidentiality obligations in arbitration proceedings with a focus on the integration of AI tools. Alternatively, with the use of confidentiality orders, arbitration participants must ensure that appropriate safeguards are in place to ensure that confidentiality is a core consideration from the initial stages of deploying AI tools. Confidentiality by design could also be applied in Generative AIs used by law firms, arbitral tribunals or institutions.",
issn="2731-0809",
doi="10.1007/s44163-025-00316-7",
url="https://doi.org/10.1007/s44163-025-00316-7"
}


@Article{Zhao2025,
author="Zhao, Weixiang
and Hu, Yulin
and Sui, Xingyu
and Li, Zhuojun
and Deng, Yang
and Zhao, Yanyan
and Qin, Bing
and Che, Wanxiang",
title="The gains do not make up for the losses: a comprehensive evaluation for safety alignment of large language models via machine unlearning",
journal="Frontiers of Computer Science",
year="2025",
month="Oct",
day="17",
volume="20",
number="2",
pages="2002319",
abstract="Machine Unlearning (MU) has emerged as a promising technique for aligning large language models (LLMs) with safety requirements to steer them forgetting specific harmful contents. Despite the significant progress in previous studies, we argue that the current evaluation criteria, which solely focus on safety evaluation, are actually impractical and biased, leading to concerns about the true effectiveness of MU techniques. To address this, we propose to comprehensively evaluate LLMs after MU from three aspects: safety, over-safety, and general utility. Specifically, a novel benchmark MuBench with 18 related datasets is first constructed, where the safety is measured with both vanilla harmful inputs and 10 types of jailbreak attacks. Furthermore, we examine whether MU introduces side effects, focusing on over-safety and utility-loss. Extensive experiments are performed on 3 popular LLMs with 7 recent MU methods. The results highlight a challenging trilemma in safety alignment without side effects, indicating that there is still considerable room for further exploration. MuBench serves as a comprehensive benchmark, fostering future research on MU for safety alignment of LLMs.",
issn="2095-2236",
doi="10.1007/s11704-024-41099-x",
url="https://doi.org/10.1007/s11704-024-41099-x"
}


@inproceedings{10.1007/978-3-032-02853-2_3,
 abstract = {In today's fast paced software development world where AI agents and LLM (Large Language Model) are writing or assisting in code development, methods like Agile and DevOps demand faster, more efficient testing and early testing that are commonly known as shift-left testing. To deliver reliable, high-quality software early to market. Traditionally, In SDLC (software development life cycle) phase, testing comes late in the development process and leads to late and costly bug fixing. Shift-left testing means early testing in the software development lifecycle to identify and mitigate defects early.},
 address = {Cham},
 author = {Sharma, Gaurav},
 booktitle = {ICT for Global Innovations and Solutions},
 editor = {Bhattacharya, Saurav},
 isbn = {978-3-032-02853-2},
 pages = {33--41},
 publisher = {Springer Nature Switzerland},
 title = {Revolutionizing Shift-Left Testing Through an Agentic AI Framework: Enhancing Software Quality and Digital Trust},
 url = {https://link.springer.com/chapter/10.1007/978-3-032-02853-2_3},
 year = {2026}
}

@Article{Touqir2025,
author="Touqir, Asma
and Iradat, Faisal
and Iqbal, Waseem
and Rakib, Abdur
and Taskin, Nazim
and Jadidbonab, Hesam
and Haas, Olivier",
title="Systematic exploration of fuzzing in IoT: techniques, vulnerabilities, and open challenges",
journal="The Journal of Supercomputing",
year="2025",
month="May",
day="23",
volume="81",
number="8",
pages="877",
abstract="As our dependence on the internet and digital platforms grows, the risk of cyber threats rises, making it essential to implement effective measures to safeguard sensitive information through cybersecurity, ensure system integrity, and prevent unauthorized data access. Fuzz testing, commonly known as fuzzing, is a valuable technique for software testing as it uncovers vulnerabilities and defects in systems by introducing random data inputs, often leading to system crashes. In the Internet of Things (IoT) domain, fuzzing is crucial for identifying vulnerabilities in networks, devices, and applications through automated tools that systematically inject malformed inputs into IoT systems. However, despite its importance, existing research on fuzzing techniques in IoT contexts remains limited by the absence of standardized benchmarks, inefficiencies in re-hosting strategies, and difficulties in detecting complex, condition-dependent vulnerabilities. The primary objective of this study is to comprehensively evaluate current fuzzing practices, emphasizing adaptive techniques designed for IoT systems. Using the PRISMA (Preferred Reporting Items for Systematic Reviews and Meta-Analyses) model, a systematic literature review was conducted across 32 academic articles published between 2020 and 2024. The analysis revealed that although fuzzing enhances IoT security, its effectiveness is hindered by device heterogeneity, limited system resources, and evolving cyber threat landscapes. The findings suggest that to overcome these limitations, future research should focus on AI-driven fuzzing methods, robust multi-architecture support, and the development of standardized evaluation frameworks to strengthen IoT cybersecurity.",
issn="1573-0484",
doi="10.1007/s11227-025-07371-y",
url="https://doi.org/10.1007/s11227-025-07371-y"
}


@Article{Islayem2025,
author="Islayem, Ruba
and Gebreab, Senay
and AlKhader, Walaa
and Musamih, Ahmad
and Salah, Khaled
and Jayaraman, Raja
and Khan, Muhammad Khurram",
title="Using large language models for enhanced fraud analysis and detection in blockchain based health insurance claims",
journal="Scientific Reports",
year="2025",
month="Aug",
day="13",
volume="15",
number="1",
pages="29763",
abstract="Traditional health insurance claim processing systems are plagued by inefficiencies and vulnerabilities, often resulting in significant financial losses due to fraudulent activities. Existing fraud detection methods are largely manual, time-consuming, and inadequate for handling the complexity and scale of modern fraudulent schemes. Moreover, the trust-based relationships between insurers and healthcare providers lack mechanisms to ensure data integrity and prevent manipulation. While several blockchain-based systems have been proposed to improve transparency and tamper resistance, they typically focus on structured data and predefined fraud types, offering limited adaptability and analytical insight. This paper proposes a novel solution leveraging blockchain technology and Large Language Models (LLMs) to transform fraud detection. The system uses Ethereum smart contracts (SCs) to securely store medical records and claim details on a decentralized, tamper-proof ledger that ensures data integrity, traceability, and accountability. This immutable data is accessed by an LLM via a Retrieval-Augmented Generation (RAG) system, which enables intelligent retrieval and analysis of relevant clinical information to detect fraud patterns and inconsistencies. To support complex scenarios involving free-text documents, unstructured clinical data, such as lab reports, are stored using decentralized off-chain storage and retrieved during LLM analysis. In addition, an LLM-powered chatbot also allows insurance providers to interact with the system in natural language for claim inquiries, explanations, and summaries. The architecture, sequence diagrams, and implementation algorithms outline the development process, while testing scenarios demonstrate the system's ability to detect fraud such as inflated costs, unnecessary treatments, and unrendered services. Evaluation using both synthetic and public clinical datasets showed strong performance, with the LLM achieving up to 99{\%} fraud detection accuracy. Cost, security, and scalability analyses confirm the system's practicality and resilience, with the complete detection process executing in just 13 seconds. By overcoming the limitations of traditional systems, this framework offers a scalable and adaptable approach for healthcare and other domains. The SCs and source code are publicly available on GitHub.",
issn="2045-2322",
doi="10.1038/s41598-025-15676-4",
url="https://doi.org/10.1038/s41598-025-15676-4"
}


@inproceedings{10.1007/978-3-031-66459-5_9,
 abstract = {ChatGPT can advise developers and provide code on how to fix bugs, add new features, refactor, reuse, and secure their code but currently, there is little knowledge about whether the developers trust ChatGPT's responses and actually use the provided code. In this context, this study aims to identify patterns that describe the interaction of developers with ChatGPT with respect to the characteristics of the prompts and the actual use of the provided code by the developer. We performed a case study on 267,098 lines of code provided by ChatGPT related to commits, pull requests, files of code, and discussions between ChatGPT and developers. Our findings show that developers are more likely to integrate the given code snapshot in their code base when they have provided information to ChatGPT through several rounds of brief prompts that include problem-related specific words instead of using large textual or code prompts. Results also highlight the ability of ChatGPT to handle efficiently different types of problems across different programming languages.},
 address = {Cham},
 author = {Terzi, Anastasia
and Bibi, Stamatia
and Tsitsimiklis, Nikolaos
and Angelidis, Pantelis},
 booktitle = {Reuse and Software Quality},
 editor = {Achilleos, Achilleas
and Fuentes, Lidia
and Papadopoulos, George Angelos},
 isbn = {978-3-031-66459-5},
 pages = {137--152},
 publisher = {Springer Nature Switzerland},
 title = {Using Code from ChatGPT: Finding Patterns in the Developers' Interaction with ChatGPT},
 url = {https://link.springer.com/chapter/10.1007/978-3-031-66459-5_9},
 year = {2024}
}

@Article{Domingo-Aldama2026,
author="Domingo-Aldama, Ane G.
and Merino Prado, Marcos
and Garc{\'i}a-Olea, Alain
and Goikoetxea, Josu
and Gojenola, Koldo
and Atutxa, Aitziber",
title="Leveraging electronic health records for atrial fibrillation cohort generation",
journal="Health Information Science and Systems",
year="2026",
month="Jan",
day="07",
volume="14",
number="1",
pages="24",
abstract="Cohort selection and eligibility screening are critical in clinical research, especially in trials where manual patient matching remains a major bottleneck. This study investigates the use of Natural Language Processing and Large Language Models (LLMs) in two real use cases, namely Atrial Fibrillation (AF) progression and Hearth Failure (HF) decompensation, within a non-English clinical context. We specifically address the following research questions: (1) Can discharge reports and NLP support cohort selection? (2) Can LLMs effectively model longitudinal patient trajectories and temporal reasoning? (3) Do general-purpose or domain-adapted LLMs outperform rule-based baselines for this task? (4) Compared to large foundation models, do small-scale LLMs offer similar performance?",
issn="2047-2501",
doi="10.1007/s13755-025-00415-w",
url="https://doi.org/10.1007/s13755-025-00415-w"
}


@Inbook{dosSantos2025,
author="dos Santos, Hugo Luz",
title="The Devilishly Harmful Virus of Algorithmic Dictatorship: Is the Rule of Law Backsliding Beyond Repair?",
bookTitle="Controllable Artificial Intelligence and the Future of Law",
year="2025",
publisher="Springer Nature Singapore",
address="Singapore",
pages="379--1009",
abstract="Artificial Intelligence and Rule of Law are shaping up to be inversely correlated. Artificial Intelligence will inevitably stifle vested fundamental rights that we, collectively, hold so dear. The unbridled rise of Artificial Intelligence will accordingly fray the fabric of Rule of Law, which is already backsliding in some parts of the world. With this context in place, this long-winded chapter parses my newly-crafted concept of Algorithmic Dictatorship that arises out of discriminatory algorithmic decision-making undertaken by Artificial Intelligence that affects the fundamental rights of citizens devoid of, and fundamentally decoupled of, any reasonable grounds to do so. Pivotally, algorithmic life-changing decisions (those pertaining to citizens' health, wealth, freedom, and credit scoring) should be subject to stringent scrutiny or inordinate damages may otherwise occur. This is the kernel of «inequality in application» that consists of Artificial Intelligence-embedded technologies that seemingly works well for everyone, when such is not the case. The paradigmatic examples of errors-fraught evaluations undertaken by machine learning algorithms are legion. This chapter brings them into a sharper focus.",
isbn="978-981-95-0508-1",
doi="10.1007/978-981-95-0508-1_4",
url="https://doi.org/10.1007/978-981-95-0508-1_4"
}


@Article{Strachan2024,
author="Strachan, James W. A.
and Albergo, Dalila
and Borghini, Giulia
and Pansardi, Oriana
and Scaliti, Eugenio
and Gupta, Saurabh
and Saxena, Krati
and Rufo, Alessandro
and Panzeri, Stefano
and Manzi, Guido
and Graziano, Michael S. A.
and Becchio, Cristina",
title="Testing theory of mind in large language models and humans",
journal="Nature Human Behaviour",
year="2024",
month="Jul",
day="01",
volume="8",
number="7",
pages="1285--1295",
abstract="At the core of what defines us as humans is the concept of theory of mind: the ability to track other people's mental states. The recent development of large language models (LLMs) such as ChatGPT has led to intense debate about the possibility that these models exhibit behaviour that is indistinguishable from human behaviour in theory of mind tasks. Here we compare human and LLM performance on a comprehensive battery of measurements that aim to measure different theory of mind abilities, from understanding false beliefs to interpreting indirect requests and recognizing irony and faux pas. We tested two families of LLMs (GPT and LLaMA2) repeatedly against these measures and compared their performance with those from a sample of 1,907 human participants. Across the battery of theory of mind tests, we found that GPT-4 models performed at, or even sometimes above, human levels at identifying indirect requests, false beliefs and misdirection, but struggled with detecting faux pas. Faux pas, however, was the only test where LLaMA2 outperformed humans. Follow-up manipulations of the belief likelihood revealed that the superiority of LLaMA2 was illusory, possibly reflecting a bias towards attributing ignorance. By contrast, the poor performance of GPT originated from a hyperconservative approach towards committing to conclusions rather than from a genuine failure of inference. These findings not only demonstrate that LLMs exhibit behaviour that is consistent with the outputs of mentalistic inference in humans but also highlight the importance of systematic testing to ensure a non-superficial comparison between human and artificial intelligences.",
issn="2397-3374",
doi="10.1038/s41562-024-01882-z",
url="https://doi.org/10.1038/s41562-024-01882-z"
}


@Article{Schmidt2025,
author="Schmidt, Simone
and Cabrera Lozoya, Daniel
and Kiropoulos, Litza
and Conway, Mike
and D'Alfonso, Simon",
title="Psychology student and mental health practitioner experiences of and perspectives on Client101, a virtual client chatbot training tool",
journal="BMC Medical Education",
year="2025",
month="Oct",
day="02",
volume="25",
number="1",
pages="1293",
abstract="Limited practice opportunities and recent developments in natural language processing have spurred the development of systems housing virtual client chatbots to be used as psychotherapy training tools. Initial studies convey the potential of virtual client chatbots to support the mental health practitioner in developing key skills in the delivery of therapy. These studies are limited in that they often do not include mental health professionals or students in the testing of their virtual client chatbots, nor focus on a particular learning context in their evaluation of the training tool. This study addresses these shortcomings by exploring mental health professionals' and psychology students' experiences of and perspectives on Client101, a virtual client chatbot training tool, in relation to specific training contexts.",
issn="1472-6920",
doi="10.1186/s12909-025-07668-9",
url="https://doi.org/10.1186/s12909-025-07668-9"
}


@Inbook{Huang2025,
author="Huang, Ken
and Hughes, Chris",
title="Securing Multi-Modal Agentic AI Systems",
bookTitle="Securing AI Agents: Foundations, Frameworks, and Real-World Deployment",
year="2025",
publisher="Springer Nature Switzerland",
address="Cham",
pages="253--285",
abstract="This chapter examines the field of multi-modal agentic AI systems---intelligent agents that perceive and reason across multiple data modalities like vision, audio, text, and video. While this data fusion enables unprecedented real-world capabilities, it simultaneously creates a multiplicative and novel security attack surface. The chapter details critical vulnerabilities unique to these systems, including cross-modal adversarial attacks, steganographic jailbreaks, deepfake-based identity spoofing, and covert data exfiltration channels. To systematically analyze these threats, the MAESTRO threat modeling framework is applied, dissecting the agentic stack from foundation models to the agent ecosystem. Practical, code-level mitigations for threats like visual prompt injection are presented alongside strategic defense-in-depth principles. Looking forward, the chapter proposes the concept of an ``AI Immune System''---a network of specialized security agents designed to monitor and protect operational agents. Finally, it culminates in the strategic principle of Zero Trust Perception, arguing that inputs from every modality must be treated as inherently untrustworthy by default. The central thesis is that securing the next generation of AI requires a fundamental shift from securing code to securing cognition itself.",
isbn="978-3-032-02130-4",
doi="10.1007/978-3-032-02130-4_9",
url="https://doi.org/10.1007/978-3-032-02130-4_9"
}


@Article{Zhao2024,
author="Zhao, Xiaoqi
and Qu, Haipeng
and Xu, Jianliang
and Li, Xiaohui
and Lv, Wenjie
and Wang, Gai-Ge",
title="A systematic review of fuzzing",
journal="Soft Computing",
year="2024",
month="Mar",
day="01",
volume="28",
number="6",
pages="5493--5522",
abstract="Fuzzing is an important technique in software and security testing that involves continuously generating a large number of test cases against target programs to discover unexpected behaviors such as bugs, crashes, and vulnerabilities. Recently, fuzzing has advanced considerably owing to the emergence of new methods and corresponding tools. However, it still suffers from low coverage, ineffective detection of specific vulnerabilities, and difficulty in deploying complex applications. Therefore, to comprehensively survey the development of fuzzing techniques and analyze their advantages and existing challenges, this paper provides a comprehensive survey of the development of fuzzing techniques, summarizes the main research issues, and provides a categorized overview of the latest research advances and applications. The paper first introduces the background and related work on fuzzing. Research issues are subsequently addressed and summarized, along with the latest research developments. Furthermore, various customized fuzzing techniques in different applications are presented. Finally, the paper discusses future research directions.",
issn="1433-7479",
doi="10.1007/s00500-023-09306-2",
url="https://doi.org/10.1007/s00500-023-09306-2"
}


@inproceedings{10.1007/978-3-032-12089-2_40,
 abstract = {Agentic coding tools receive goals written in natural language as input, break them down into specific tasks, and write/execute the actual code with minimal human intervention. Key to this process are agent manifests, configuration files (such as Claude.md) that provide agents with essential project context, identity, and operational rules. However, the lack of comprehensive and accessible documentation for creating these manifests presents a significant challenge for developers. We analyzed 253 Claude.md files from 242 repositories to identify structural patterns and common content. Our findings show that manifests typically have shallow hierarchies with one main heading and several subsections, with content dominated by operational commands, technical implementation notes, and high-level architecture.},
 address = {Cham},
 author = {Chatlatanagulchai, Worawalan
and Thonglek, Kundjanasith
and Reid, Brittany
and Kashiwa, Yutaro
and Leelaprute, Pattara
and Rungsawang, Arnon
and Manaskasemsak, Bundit
and Iida, Hajimu},
 booktitle = {Product-Focused Software Process Improvement},
 editor = {Scanniello, Giuseppe
and Lenarduzzi, Valentina
and Romano, Simone
and Vegas, Sira
and Francese, Rita},
 isbn = {978-3-032-12089-2},
 pages = {543--551},
 publisher = {Springer Nature Switzerland},
 title = {On the Use of Agentic Coding Manifests: An Empirical Study of Claude Code},
 url = {https://link.springer.com/chapter/10.1007/978-3-032-12089-2_40},
 year = {2026}
}

@inproceedings{10.1007/978-3-031-86698-2_28,
 abstract = {This paper focuses on the review of the literature about the effects of generative AI for SE based on a comparison of research papers on methodologies, applications, and effects on SE processes. It plans to analyze and discuss existing classifications used for the purposes of MMA-based applications, including their potential and challenges, and analyze the productivity rate according to citation scores and other parameters. Generative AI models such as GPT-3, BERT, and Transformers are also involved in the paper, and this paper focuses on automatic coding, testing, and documentation.},
 address = {Cham},
 author = {Saoiabi, Fadwa
and Kharmoum, Nassim
and Elasri, Chaimae
and Lagmiri, Souad Najoua
and Ziti, Soumia},
 booktitle = {Smart Business and Technologies},
 editor = {Lagmiri, Souad Najoua
and Lazaar, Mohamed
and Amine, Fouad Mohammed},
 isbn = {978-3-031-86698-2},
 pages = {315--323},
 publisher = {Springer Nature Switzerland},
 title = {Generative AI in Software Engineering: Enhancing Development and Innovation},
 url = {https://link.springer.com/chapter/10.1007/978-3-031-86698-2_28},
 year = {2025}
}

@Article{Shah2024,
author="Shah, Mehil B.
and Rahman, Mohammad Masudur
and Khomh, Foutse",
title="Towards enhancing the reproducibility of deep learning bugs: an empirical study",
journal="Empirical Software Engineering",
year="2024",
month="Nov",
day="09",
volume="30",
number="1",
pages="23",
abstract="Deep learning has achieved remarkable progress in various domains. However, like any software system, deep learning systems contain bugs, some of which can have severe impacts, as evidenced by crashes involving autonomous vehicles. Despite substantial advancements in deep learning techniques, little research has focused on reproducing deep learning bugs, which is an essential step for their resolution. Existing literature suggests that only 3{\%} of deep learning bugs are reproducible, underscoring the need for further research.",
issn="1573-7616",
doi="10.1007/s10664-024-10579-w",
url="https://doi.org/10.1007/s10664-024-10579-w"
}


@Inbook{Way2025,
author="Way, Andy
and Passban, Peyman
and Rezagholizadeh, Mehdi",
editor="Passban, Peyman
and Way, Andy
and Rezagholizadeh, Mehdi",
title="Remaining Issues for AI",
bookTitle="Enhancing LLM Performance: Efficacy, Fine-Tuning, and Inference Techniques",
year="2025",
publisher="Springer Nature Switzerland",
address="Cham",
pages="179--183",
abstract="In this book, we have featured a number of techniques which can be hugely beneficial for AI practitioners. Given that almost everyone in the field---whether in academia or in industry---is using LLMs, we expect the impact of this collection to be significant. Up to this point, we have focused on providing an almost entirely positive view of AI. AI is on everyone's lips; it is on the front pages of our newspapers and magazines, it is on our TV channels, it is concerning governments right across the planet, and it is the topic of conversations at the dinner table. However, it is right to acknowledge a range of issues that require some pause for thought. Accordingly, in this chapter, we discuss some of the major concerns surrounding the widespread adoption of AI, as a counterpoint to the preceding chapters. It is not that we believe that AI cannot be used for good, far from it; but unless the following concerns are satisfactorily addressed, it is likely that an AI winter will set in.",
isbn="978-3-031-85747-8",
doi="10.1007/978-3-031-85747-8_11",
url="https://doi.org/10.1007/978-3-031-85747-8_11"
}


@Inbook{Maalej2025,
author="Maalej, Walid
and Biryuk, Volodymyr
and Wei, Jialiang
and Panse, Fabian",
editor="Ferrari, Alessio
and Ginde, Gouri",
title="On the Automated Processing of User Feedback",
bookTitle="Handbook on Natural Language Processing for Requirements Engineering",
year="2025",
publisher="Springer Nature Switzerland",
address="Cham",
pages="279--308",
abstract="User feedback is becoming an increasingly important source of information for requirements engineering, user interface design and software engineering in general. Nowadays, user feedback is largely available and easily accessible in social media, product forums or app stores. Over the last decade, research has shown that user feedback can help software teams: (a) better understand how users are actually using specific product features and components, (b) faster identify, reproduce and fix defects, and (b) get inspirations for improvements or new features.",
isbn="978-3-031-73143-3",
doi="10.1007/978-3-031-73143-3_10",
url="https://doi.org/10.1007/978-3-031-73143-3_10"
}


@Article{Ren2024,
author="Ren, Zilong
and Ju, Xiaolin
and Chen, Xiang
and Shen, Hao",
title="ProRLearn: boosting prompt tuning-based vulnerability detection by reinforcement learning",
journal="Automated Software Engineering",
year="2024",
month="Apr",
day="20",
volume="31",
number="2",
pages="38",
abstract="Software vulnerability detection is a critical step in ensuring system security and data protection. Recent research has demonstrated the effectiveness of deep learning in automated vulnerability detection. However, it is difficult for deep learning models to understand the semantics and domain-specific knowledge of source code. In this study, we introduce a new vulnerability detection framework, ProRLearn, which leverages two main techniques: prompt tuning and reinforcement learning. Since existing fine-tuning of pre-trained language models (PLMs) struggles to leverage domain knowledge fully, we introduce a new automatic prompt-tuning technique. Precisely, prompt tuning mimics the pre-training process of PLMs by rephrasing task input and adding prompts, using the PLM's output as the prediction output. The introduction of the reinforcement learning reward mechanism aims to guide the behavior of vulnerability detection through a reward and punishment model, enabling it to learn effective strategies for obtaining maximum long-term rewards in specific environments. The introduction of reinforcement learning aims to encourage the model to learn how to maximize rewards or minimize penalties, thus enhancing performance. Experiments on three datasets (FFMPeg+Qemu, Reveal, and Big-Vul) indicate that ProRLearn achieves performance improvement of 3.27--70.96{\%} over state-of-the-art baselines in terms of F1 score. The combination of prompt tuning and reinforcement learning can offer a potential opportunity to improve performance in vulnerability detection. This means that it can effectively improve the performance in responding to constantly changing network environments and new threats. This interdisciplinary approach contributes to a better understanding of the interplay between natural language processing and reinforcement learning, opening up new opportunities and challenges for future research and applications.",
issn="1573-7535",
doi="10.1007/s10515-024-00438-9",
url="https://doi.org/10.1007/s10515-024-00438-9"
}


@Inbook{Messias2025,
author="Messias, Fernando",
title="Artificial Intelligence (AI) and International Arbitration",
bookTitle="The Practice of Law and International Arbitration in the Age of Artificial Intelligence",
year="2025",
publisher="Springer Nature Switzerland",
address="Cham",
pages="7--36",
abstract="This chapter explores the integration of Artificial Intelligence (AI) within International Arbitration, highlighting its effects on videoconferencing, evidence management, procedural efficiency, and legal decision-making. It provides a critical overview of the technological tools reshaping arbitration, including blockchain and machine learning applications.",
isbn="978-3-032-09127-7",
doi="10.1007/978-3-032-09127-7_2",
url="https://doi.org/10.1007/978-3-032-09127-7_2"
}


@Article{Li2024,
author="Li, Yishu
and Keung, Jacky
and Yang, Zhen
and Ma, Xiaoxue
and Zhang, Jingyu
and Liu, Shuo",
title="SimAC: simulating agile collaboration to generate acceptance criteria in user story elaboration",
journal="Automated Software Engineering",
year="2024",
month="Jun",
day="21",
volume="31",
number="2",
pages="55",
abstract="In agile requirements engineering, Generating Acceptance Criteria (GAC) to elaborate user stories plays a pivotal role in the sprint planning phase, which provides a reference for delivering functional solutions. GAC requires extensive collaboration and human involvement. However, the lack of labeled datasets tailored for User Story attached with Acceptance Criteria (US-AC) poses significant challenges for supervised learning techniques attempting to automate this process. Recent advancements in Large Language Models (LLMs) have showcased their remarkable text-generation capabilities, bypassing the need for supervised fine-tuning. Consequently, LLMs offer the potential to overcome the above challenge. Motivated by this, we propose SimAC, a framework leveraging LLMs to simulate agile collaboration, with three distinct role groups: requirement analyst, quality analyst, and others. Initiated by role-based prompts, LLMs act in these roles sequentially, following a create-update-update paradigm in GAC. Owing to the unavailability of ground truths, we invited practitioners to build a gold standard serving as a benchmark to evaluate the completeness and validity of auto-generated US-AC against human-crafted ones. Additionally, we invited eight experienced agile practitioners to evaluate the quality of US-AC using the INVEST framework. The results demonstrate consistent improvements across all tested LLMs, including the LLaMA and GPT-3.5 series. Notably, SimAC significantly enhances the ability of gpt-3.5-turbo in GAC, achieving improvements of 29.48{\%} in completeness and 15.56{\%} in validity, along with the highest INVEST satisfaction score of 3.21/4. Furthermore, this study also provides case studies to illustrate SimAC's effectiveness and limitations, shedding light on the potential of LLMs in automated agile requirements engineering.",
issn="1573-7535",
doi="10.1007/s10515-024-00448-7",
url="https://doi.org/10.1007/s10515-024-00448-7"
}


@Article{He2024,
author="He, Juntao
and Dai, Haoran
and Sui, Runqi
and Yuan, Xuejing
and Liu, Dun
and Feng, Hao
and Liu, Xinyue
and Yang, Wenchuan
and Cui, Baojiang
and Li, Kedan",
title="EvilPromptFuzzer: generating inappropriate content based on text-to-image models",
journal="Cybersecurity",
year="2024",
month="Aug",
day="26",
volume="7",
number="1",
pages="70",
abstract="Text-to-image (TTI) models provide huge innovation ability for many industries, while the content security triggered by them has also attracted wide attention. Considerable research has focused on content security threats of large language models (LLMs), yet comprehensive studies on the content security of TTI models are notably scarce. This paper introduces a systematic tool, named EvilPromptFuzzer, designed to fuzz evil prompts in TTI models. For 15 kinds of fine-grained risks, EvilPromptFuzzer employs the strong knowledge-mining ability of LLMs to construct seed banks, in which the seeds cover various types of characters, interrelations, actions, objects, expressions, body parts, locations, surroundings, etc. Subsequently, these seeds are fed into the LLMs to build scene-diverse prompts, which can weaken the semantic sensitivity related to the fine-grained risks. Hence, the prompts can bypass the content audit mechanism of the TTI model, and ultimately help to generate images with inappropriate content. For the risks of violence, horrible, disgusting, animal cruelty, religious bias, political symbol, and extremism, the efficiency of EvilPromptFuzzer for generating inappropriate images based on DALL.E 3 are greater than 30{\%}, namely, more than 30 generated images are malicious among 100 prompts. Specifically, the efficiency of horrible, disgusting, political symbols, and extremism up to 58{\%}, 64{\%}, 71{\%}, and 50{\%}, respectively. Additionally, we analyzed the vulnerability of existing popular content audit platforms, including Amazon, Google, Azure, and Baidu. Even the most effective Google SafeSearch cloud platform identifies only 33.85{\%} of malicious images across three distinct categories.",
issn="2523-3246",
doi="10.1186/s42400-024-00279-9",
url="https://doi.org/10.1186/s42400-024-00279-9"
}


@Article{Keloth2025,
author="Keloth, Vipina K.
and Selek, Salih
and Chen, Qingyu
and Gilman, Christopher
and Fu, Sunyang
and Dang, Yifang
and Chen, Xinghan
and Hu, Xinyue
and Zhou, Yujia
and He, Huan
and Fan, Jungwei W.
and Wang, Karen
and Brandt, Cynthia
and Tao, Cui
and Liu, Hongfang
and Xu, Hua",
title="Social determinants of health extraction from clinical notes across institutions using large language models",
journal="npj Digital Medicine",
year="2025",
month="May",
day="17",
volume="8",
number="1",
pages="287",
abstract="Detailed social determinants of health (SDoH) is often buried within clinical text in EHRs. Most current NLP efforts for SDoH have limitations, investigating limited factors, deriving data from a single institution, using specific patient cohorts/note types, with reduced focus on generalizability. We aim to address these issues by creating cross-institutional corpora and developing and evaluating the generalizability of classification models, including large language models (LLMs), for detecting SDoH factors using data from four institutions. Clinical notes were annotated with 21 SDoH factors at two levels: level 1 (SDoH factors only) and level 2 (SDoH factors and associated values). Compared to other models, instruction tuned LLM achieved top performance with micro-averaged F1 over 0.9 on level 1 corpora and over 0.84 on level 2 corpora. While models performed well when trained and tested on individual datasets, cross-dataset generalization highlighted remaining obstacles. Access to trained models will be made available at https://github.com/BIDS-Xu-Lab/LLMs4SDoH.",
issn="2398-6352",
doi="10.1038/s41746-025-01645-8",
url="https://doi.org/10.1038/s41746-025-01645-8"
}


@Article{Tang2025,
author="Tang, Xunzhu
and Tian, Haoye
and Pian, Weiguo
and Ezzini, Saad
and Kabor{\'e}, Abdoul Kader
and Habib, Andrew
and Kim, Kisub
and Klein, Jacques
and Bissyand{\'e}, Tegawend{\'e} F.",
title="Learning to represent code changes",
journal="Empirical Software Engineering",
year="2025",
month="Dec",
day="17",
volume="31",
number="3",
pages="50",
abstract="Code change representation plays a pivotal role in automating numerous software engineering tasks, such as classifying code change correctness or generating natural language summaries of code changes. Recent studies have leveraged deep learning to derive effective code change representation, primarily focusing on capturing changes in token sequences or Abstract Syntax Trees (ASTs). However, these current state-of-the-art representations do not explicitly calculate the intention semantic induced by the change on the AST, nor do they effectively explore the surrounding contextual information of the modified lines. To address this, we propose a new code change representation methodology, Patcherizer, which we refer to as our tool. This innovative approach explores the intention features of the context and structure, combining the context around the code change along with two novel representations. These new representations capture the sequence intention inside the code changes in the code change and the graph intention inside the structural changes of AST graphs before and after the code change. This comprehensive representation allows us to better capture the intentions underlying a code change. Patcherizer builds on graph convolutional neural networks for the structural input representation of the intention graph and on transformers for the intention sequence representation. We assess the generalizability of Patcherizer 's learned embeddings on three tasks: (1) Generating code change description in NL, (2) Predicting code change correctness in program repair, and (3) Code change intention detection. Experimental results show that the learned code change representation is effective for all three tasks and achieves superior performance to the state-of-the-art (SOTA) approaches. For instance, on the popular task of code change description generation (a.k.a. commit message generation), Patcherizer achieves an average improvement of 19.39{\%}, 8.71{\%}, and 34.03{\%} in terms of BLEU, ROUGE-L, and METEOR metrics, respectively.",
issn="1573-7616",
doi="10.1007/s10664-025-10763-6",
url="https://doi.org/10.1007/s10664-025-10763-6"
}


@Inbook{Baraybar-Fernández2025,
author="Baraybar-Fern{\'a}ndez, Antonio
and Arrufat-Mart{\'i}n, Sandro",
editor="Baraybar-Fern{\'a}ndez, Antonio
and Arrufat-Mart{\'i}n, Sandro
and D{\'i}az D{\'i}az, Bel{\'e}n",
title="OpenAI Artificial Intelligence: Revolution or Bubble?",
bookTitle="The AI Revolution: How Technological Developments Affect the Audiovisual Sector",
year="2025",
publisher="Springer Nature Switzerland",
address="Cham",
pages="1--13",
abstract="The emergence of artificial intelligence (AI) promises a revolutionary change that will drive a new stage of digital transformation. In 2024, it has become a global megatrend, generating a significant impact on economic, social, and professional levels---changes to which the communication sector is not immune. The acceleration of certain data analytics processes or its predictive capabilities for decision-making are some of the areas with evident strategic influence in the sector. At the same time, the incorporation of generative AI into content creation processes and production dynamics creates uncertainties in the business and professional realms, as it has the potential to modify current business models and certain job roles.",
isbn="978-3-031-80411-3",
doi="10.1007/978-3-031-80411-3_1",
url="https://doi.org/10.1007/978-3-031-80411-3_1"
}


@Article{AbouAli2025,
author="Abou Ali, Mohamad
and Dornaika, Fadi
and Charafeddine, Jinan",
title="Agentic AI: a comprehensive survey of architectures, applications, and future directions",
journal="Artificial Intelligence Review",
year="2025",
month="Nov",
day="14",
volume="59",
number="1",
pages="11",
abstract="Agentic AI represents a transformative shift in artificial intelligence, but its rapid advancement has led to a fragmented understanding, often conflating modern neural systems with outdated symbolic models---a practice known as conceptual retrofitting. This survey cuts through this confusion by introducing a novel dual-paradigm framework that categorizes agentic systems into two distinct lineages: the symbolic/classical (relying on algorithmic planning and persistent state) and the neural/generative (leveraging stochastic generation and prompt-driven orchestration). Through a systematic PRISMA-based review of 90 studies (2018--2025), we provide a comprehensive analysis structured around this framework across three dimensions: (1) the theoretical foundations and architectural principles defining each paradigm; (2) domain-specific implementations in healthcare, finance, and robotics, demonstrating how application constraints dictate paradigm selection; and (3) paradigm-specific ethical and governance challenges, revealing divergent risks and mitigation strategies. Our analysis reveals that the choice of paradigm is strategic: symbolic systems dominate safety-critical domains (e.g., healthcare), while neural systems prevail in adaptive, data-rich environments (e.g., finance). Furthermore, we identify critical research gaps, including a significant deficit in governance models for symbolic systems and a pressing need for hybrid neuro-symbolic architectures. The findings culminate in a strategic roadmap arguing that the future of Agentic AI lies not in the dominance of one paradigm, but in their intentional integration to create systems that are both adaptable and reliable. This work provides the essential conceptual toolkit to guide future research, development, and policy toward robust and trustworthy hybrid intelligent systems.",
issn="1573-7462",
doi="10.1007/s10462-025-11422-4",
url="https://doi.org/10.1007/s10462-025-11422-4"
}


@Article{Zhang2025,
author="Zhang, Xiaosong
and Zhu, Yukun
and Li, Xiong
and Zhang, Yongzhao
and Niu, Weina
and Xu, Fenghua
and He, Junpeng
and Yan, Ran
and Huang, Shiping",
title="Active cybersecurity: vision, model, and key technologies",
journal="Frontiers of Information Technology {\&} Electronic Engineering",
year="2025",
month="Aug",
day="01",
volume="26",
number="8",
pages="1243--1278",
abstract="Noncooperative computer systems and network confrontation present a core challenge in cyberspace security. Traditional cybersecurity technologies predominantly rely on passive response mechanisms, which exhibit significant limitations when addressing real-world complex and unknown threats. This paper introduces the concept of ``active cybersecurity,'' aiming to enhance network security not only through technical measures but also by leveraging strategy-level defenses. The core assumption of this concept is that attackers and defenders, in the context of network confrontations, act as rational decision-makers seeking to maximize their respective objectives. Building on this observation, this paper integrates game theory to analyze the interdependent relationships between attackers and defenders, thereby optimizing their strategies. Guided by this foundational idea, we propose an active cybersecurity model involving intelligent threat sensing, in-depth behavior analysis, comprehensive path profiling, and dynamic countermeasures, termed SAPC, designed to foster an integrated defense capability encompassing threat perception, analysis, tracing, and response. At its core, SAPC incorporates theoretical analyses of adversarial behavior and the optimization of corresponding strategies informed by game theory. By profiling adversaries and modeling confrontation as a ``game,'' the model establishes a comprehensive framework that provides both theoretical insights into and practical guidance for cybersecurity. The proposed active cybersecurity model marks a transformative shift from passive defense to proactive perception and confrontation. It facilitates the evolution of cybersecurity technologies toward a new paradigm characterized by active prediction, prevention, and strategic guidance.",
issn="2095-9230",
doi="10.1631/FITEE.2500053",
url="https://doi.org/10.1631/FITEE.2500053"
}


@inproceedings{10.1007/978-981-97-6726-7_8,
 abstract = {This paper presents a comprehensive dataset for aspect-based multiclass multilevel suggestion mining from game reviews, addressing the challenges in capturing explicit and implicit suggestions along with their associated aspects. The dataset is carefully annotated by expert annotators, providing valuable insights into the nature of suggestions in game reviews. We describe the dataset's development process and evaluate it using various deep learning models, including LSTM, TCN, DRC{\_}Net, and GPT-3. The results demonstrate the dataset's effectiveness in suggestion mining tasks. Furthermore, we combine TCN with machine learning models to refine subclass and aspect identification. The dataset fills the gap in existing datasets for aspect-based suggestion mining, enabling researchers to develop more robust suggestion mining models and gain a deeper understanding of suggestions in game reviews.},
 address = {Singapore},
 author = {Khan, Usama Bin Rashidullah
and Akhtar, Nadeem
and Naaz, Arshia
and Akram, Mohd
and Haque, Abdul},
 booktitle = {Proceedings of Fifth Doctoral Symposium on Computational Intelligence},
 editor = {Swaroop, Abhishek
and Kansal, Vineet
and Fortino, Giancarlo
and Hassanien, Aboul Ella},
 isbn = {978-981-97-6726-7},
 pages = {105--119},
 publisher = {Springer Nature Singapore},
 title = {Toward Effective Suggestion Mining in Game Reviews: Introducing an Aspect-Based Multiclass Multilevel Dataset},
 url = {https://link.springer.com/chapter/10.1007/978-981-97-6726-7_8},
 year = {2024}
}

@Article{Fulsher2025,
author="Fulsher, Ali
and Pagkratidou, Marianna
and Kendeou, Panayiota",
title="GenAI and misinformation in education: a systematic scoping review of opportunities and challenges",
journal="AI {\&} SOCIETY",
year="2025",
month="Aug",
day="07",
abstract="Generative Artificial Intelligence (GenAI) has emerged as a transformative and disruptive force in education and society, with the potential to both create and correct misinformation. In education, misinformation manifests at three levels: the individual (when students hold misconceptions or inaccurate beliefs); the community (when groups of individuals share the same misconceptions or inaccurate beliefs); and the system (when educational policies and practices are not based on scientific evidence). We conducted a systematic scoping review to identify existing challenges and opportunities for misinformation generation, sharing, and correction across these levels in education, as well as inform future research in this area of work. Our results indicate three approaches to GenAI and misinformation in education: identifying misconceptions, prebunking, and (re)producing misinformation, with most research conducted at the individual level. We conclude with suggestions for future research and practice.",
issn="1435-5655",
doi="10.1007/s00146-025-02536-y",
url="https://doi.org/10.1007/s00146-025-02536-y"
}


@inproceedings{10.1007/978-3-031-72781-8_3,
 abstract = {Test Driven Development (TDD) is one of the major practices of Extreme Programming for which incremental testing and refactoring trigger the code development. TDD has limited adoption in the industry, as it requires more code to be developed and experienced developers. Generative AI (GenAI) may reduce the extra effort imposed by TDD. In this work, we introduce an approach to automatize TDD by embracing GenAI either in a collaborative interaction pattern in which developers create tests and supervise the AI generation during each iteration or a fully-automated pattern in which developers only supervise the AI generation at the end of the iterations. We run an exploratory experiment with ChatGPT in which the interaction patterns are compared with the non-AI TDD regarding test and code quality and development speed. Overall, we found that, for our experiment and settings, GenAI can be efficiently used in TDD, but it requires supervision of the quality of the produced code. In some cases, it can even mislead non-expert developers and propose solutions just for the sake of the query.},
 address = {Cham},
 author = {Mock, Moritz
and Melegati, Jorge
and Russo, Barbara},
 booktitle = {Agile Processes in Software Engineering and Extreme Programming -- Workshops},
 editor = {Marchesi, Lodovica
and Goldman, Alfredo
and Lunesu, Maria Ilaria
and Przyby{\l}ek, Adam
and Aguiar, Ademar
and Morgan, Lorraine
and Wang, Xiaofeng
and Pinna, Andrea},
 isbn = {978-3-031-72781-8},
 pages = {24--32},
 publisher = {Springer Nature Switzerland},
 title = {Generative AI for Test Driven Development: Preliminary Results},
 url = {https://link.springer.com/chapter/10.1007/978-3-031-72781-8_3},
 year = {2025}
}

@Article{Mahmud2025,
author="Mahmud, Tarek
and Che, Meiru
and Ngu, Anne
and Yang, Guowei",
title="Why android app testing falls short: empirical insights from open-source projects and a practitioner survey",
journal="Empirical Software Engineering",
year="2025",
month="Sep",
day="22",
volume="30",
number="6",
pages="163",
abstract="Android dominates the mobile operating system market, yet ensuring the quality and reliability of Android applications remains a persistent challenge. The diversity of devices, screen sizes, and OS versions complicates testing, leading to fragmented adoption of best practices. Despite advancements in automated testing, there is Limited empirical evidence on how developers test Android applications and the extent to which existing tools and frameworks are utilized effectively. In this paper, we aim to investigate the current state of Android app testing, identifying key challenges, Limitations, and best practices. Specifically, we assess the adoption of automated testing, test coverage levels, and the impact of testing practices on software quality. We conduct an experimental study on 2965 open-source Android apps, examining the quantity and coverage of the tests used for open-source Android app development. We further conduct a survey to gather more insights in testing practices from Android app developers and testers. The results reveal a limited adoption of testing among Android app developers, a restricted range of testing tools and frameworks being used, and low code and API coverage in testing. This investigation shows that current Android app testing practices are lacking the use of automated testing tools and embarks on a need for more awareness and adoption of state-of-the-art testing tools and techniques.",
issn="1573-7616",
doi="10.1007/s10664-025-10726-x",
url="https://doi.org/10.1007/s10664-025-10726-x"
}


@Article{Yu2025,
author="Yu, Liang
and Al{\'e}groth, Emil
and Chatzipetrou, Panagiota
and Gorschek, Tony",
title="Evaluating the quality of GenAI applications in software engineering: a multi-case study",
journal="Empirical Software Engineering",
year="2025",
month="Dec",
day="05",
volume="31",
number="2",
pages="29",
abstract="Generative AI (GenAI) is increasingly adopted in software development for tasks such as document generation, data analysis, and code generation. However, evaluating the quality of GenAI applications becomes challenging, as traditional quality measurements may not be fully applicable.",
issn="1573-7616",
doi="10.1007/s10664-025-10759-2",
url="https://doi.org/10.1007/s10664-025-10759-2"
}


@Article{Simon2026,
author="Simon, Emmanuel Iko-Ojo
and Hettiarachchi, Chirath
and Potanin, Alex
and Suominen, Hanna
and Fard, Fatemeh",
title="Automated detection of algorithm debt in deep learning frameworks: an empirical study",
journal="Empirical Software Engineering",
year="2026",
month="Feb",
day="07",
volume="31",
number="3",
pages="66",
abstract="Expedient design choices in software development can lead to Technical Debt (TD), with development teams documenting such decisions as Self-Admitted TD (SATD). Algorithm Debt (AD) is a type of TD resulting from the suboptimal implementation of algorithms, which impacts system performance. Given the impact of AD, its automated detection is crucial in Deep Learning (DL) frameworks due to their complexity and evolution. Early detection of AD in DL frameworks can help mitigate model degradation and scalability issues. Despite previous studies on the automated detection of TD from SATD using Machine Learning (ML)/DL models, research on AD detection in DL frameworks remains underexplored. In this study, we empirically investigated the performance of ML/DL models for the automated detection of AD using a dataset of 38, 881 SATD comments from seven DL frameworks. We trained, evaluated, and tested ML/DL models, used embeddings from both DL and large language models, and explored an approach to enrich the dataset with handcrafted features based on AD-related keywords. Our findings reveal that AD is frequently misclassified as Design or Implementation Debt. Logistic Regression (an ML model) with Custom AD Features, achieved an F1-score of 54{\%} for AD, outperforming other ML/DL models (42{\%} to 52{\%}), highlighting the importance of tailored feature engineering. Our research advances automated AD detection in DL frameworks by providing insights into the strengths and limitations of ML/DL models, serving as a first step to guide future tool development. This could help developers using DL frameworks to identify AD issues during development, thereby enhancing system reliability by mitigating model degradation and scalability challenges.",
issn="1573-7616",
doi="10.1007/s10664-026-10807-5",
url="https://doi.org/10.1007/s10664-026-10807-5"
}


@Inbook{King2025,
author="King, Alyson E.
and Garramone, Pariss",
editor="King, Alyson E.",
title="Teaching Writing in the Time of ChatGPT: Rethinking What Counts as Learning",
bookTitle="Artificial Intelligence, Pedagogy and Academic Integrity",
year="2025",
publisher="Springer Nature Switzerland",
address="Cham",
pages="67--89",
abstract="From the perspective of undergraduate educators in the Social Sciences we ask: what are the challenges and opportunities associated with teaching writing in the time of ChatGPT, a powerful large language model (LLM) that generates written products and is impacting the way we communicate? With the advent of AI-driven LLMs, students now have access to sophisticated writing tools that generate text, and many are using them to complete written assignments. As a result, GenAI writing tools are helping to re-shape the conversation on teaching and learning in undergraduate education. Our chapter examines and traces the evolution of our thinking about undergraduate writing in our programs, our ``way-finding'' process, and the important pedagogical considerations that allow us to enact the curriculum in sensitive and meaningful ways that also build students' academic integrity.",
isbn="978-3-031-92534-4",
doi="10.1007/978-3-031-92534-4_6",
url="https://doi.org/10.1007/978-3-031-92534-4_6"
}


@inproceedings{10.1007/978-3-031-73202-7_11,
 abstract = {Large Vision-Language Models (LVLMs) rely on vision encoders and Large Language Models (LLMs) to exhibit remarkable capabilities on various multi-modal tasks in the joint space of vision and language. However, typographic attacks, which disrupt Vision-Language Models (VLMs) such as Contrastive Language-Image Pretraining (CLIP), have also been expected to be a security threat to LVLMs. Firstly, we verify typographic attacks on current well-known commercial and open-source LVLMs and uncover the widespread existence of this threat. Secondly, to better assess this vulnerability, we propose the most comprehensive and largest-scale Typographic Dataset to date. The Typographic Dataset not only considers the evaluation of typographic attacks under various multi-modal tasks but also evaluates the effects of typographic attacks, influenced by texts generated with diverse factors. Based on the evaluation results, we investigate the causes why typographic attacks impacting VLMs and LVLMs, leading to three highly insightful discoveries. During the process of further validating the rationality of our discoveries, we can reduce the performance degradation caused by typographic attacks from 42.07{\%} to 13.90{\%}. Code and Dataset are available in https://github.com/ChaduCheng/TypoDeceptions.},
 address = {Cham},
 author = {Cheng, Hao
and Xiao, Erjia
and Gu, Jindong
and Yang, Le
and Duan, Jinhao
and Zhang, Jize
and Cao, Jiahang
and Xu, Kaidi
and Xu, Renjing},
 booktitle = {Computer Vision -- ECCV 2024},
 editor = {Leonardis, Ale{\v{s}}
and Ricci, Elisa
and Roth, Stefan
and Russakovsky, Olga
and Sattler, Torsten
and Varol, G{\"u}l},
 isbn = {978-3-031-73202-7},
 pages = {179--196},
 publisher = {Springer Nature Switzerland},
 title = {Unveiling Typographic Deceptions: Insights of the Typographic Vulnerability in Large Vision-Language Models},
 url = {https://link.springer.com/chapter/10.1007/978-3-031-73202-7_11},
 year = {2025}
}

@Article{Küchemann2025,
author="K{\"u}chemann, Stefan
and Avila, Karina E.
and Dinc, Yavuz
and Hortmann, Chiara
and Revenga, Natalia
and Ruf, Verena
and Stausberg, Niklas
and Steinert, Steffen
and Fischer, Frank
and Fischer, Martin
and Kasneci, Enkelejda
and Kasneci, Gjergji
and Kuhr, Thomas
and Kutyniok, Gitta
and Malone, Sarah
and Sailer, Michael
and Schmidt, Albrecht
and Stadler, Matthias
and Weller, Jochen
and Kuhn, Jochen",
title="On opportunities and challenges of large multimodal foundation models in education",
journal="npj Science of Learning",
year="2025",
month="Feb",
day="26",
volume="10",
number="1",
pages="11",
abstract="Recently, the option to use large language models as a middleware connecting various AI tools and other large language models led to the development of so-called large multimodal foundation models, which have the power to process spoken text, music, images and videos. In this overview, we explain a new set of opportunities and challenges that arise from the integration of large multimodal foundation models in education.",
issn="2056-7936",
doi="10.1038/s41539-025-00301-w",
url="https://doi.org/10.1038/s41539-025-00301-w"
}


@Article{He2025,
author="He, Xinlei
and Xu, Guowen
and Han, Xingshuo
and Wang, Qian
and Zhao, Lingchen
and Shen, Chao
and Lin, Chenhao
and Zhao, Zhengyu
and Li, Qian
and Yang, Le
and Ji, Shouling
and Li, Shaofeng
and Zhu, Haojin
and Wang, Zhibo
and Zheng, Rui
and Zhu, Tianqing
and Li, Qi
and He, Chaoxiang
and Wang, Qifan
and Hu, Hongsheng
and Wang, Shuo
and Sun, Shi-Feng
and Yao, Hongwei
and Qin, Zhan
and Chen, Kai
and Zhao, Yue
and Li, Hongwei
and Huang, Xinyi
and Feng, Dengguo",
title="Artificial intelligence security and privacy: a survey",
journal="Science China Information Sciences",
year="2025",
month="Jul",
day="03",
volume="68",
number="8",
pages="181101",
abstract="Artificial intelligence (AI) is revolutionizing both industries and reshaping the global economy. However, the rapid advancement of AI technologies brings significant security and privacy challenges. Recent incidents highlight vulnerabilities in AI systems, such as data leakage and malicious code injection, leading to severe financial losses and privacy breaches. Although existing studies have discussed specific security threats, they often lack detailed granularity and cover a limited scope. In this survey, we fill this gap by systematically categorizing and analyzing the threats and countermeasures in AI systems, which span both the training and inference stages, encompass centralized and distributed settings, and address both conventional and foundation AI models. By reviewing existing literature, we aim to provide AI researchers and practitioners with a thorough understanding of system vulnerabilities and current countermeasures. We hope to inspire further research into robust solutions, ultimately contributing to the development of resilient AI technologies.",
issn="1869-1919",
doi="10.1007/s11432-025-4388-5",
url="https://doi.org/10.1007/s11432-025-4388-5"
}


@Inbook{Jalote2025,
author="Jalote, Pankaj",
title="Industry-Strength Software",
bookTitle="A Concise Introduction to Software Engineering: With Open Source and GenAI",
year="2025",
publisher="Springer Nature Switzerland",
address="Cham",
pages="1--22",
abstract="Software is about programs that run on computers. Starting from simple programs written in machine language or assembly language, software became more complex as hardware became more powerful and higher level languages became popular.",
isbn="978-3-031-74318-4",
doi="10.1007/978-3-031-74318-4_1",
url="https://doi.org/10.1007/978-3-031-74318-4_1"
}


@inproceedings{10.1007/978-3-032-07106-4_10,
 abstract = {Adversarial examples undermine the reliability of neural networks. To defend against attacks, multiple approaches have been proposed. However, many of them introduce high training overhead or high inference overhead, some significantly decrease the network's accuracy or insufficiently increase the network's robustness, and others do not scale to deep networks. To mitigate all these shortcomings, we propose a new form of defense: optimal program synthesis of short repair programs, integrated into a trained network. A repair program modifies a few neurons by using a few other neurons. The challenge is to identify the most successful combination of neurons to enhance the network's robustness while maintaining high accuracy. We introduce DefEnSyn, a stochastic synthesizer of repair programs. To cope with the exponential number of neuron combinations, DefEnSyn learns the effective combinations by synthesizing repair programs of increasing length. We evaluate DefEnSyn on classifiers for ImageNet and CIFAR-10 and show it enhances the robustness of networks to {\$}{\$}L{\_}{\backslash}infty {\$}{\$}L∞-, {\$}{\$}L{\_}2{\$}{\$}L2-, and {\$}{\$}L{\_}0{\$}{\$}L0- black-box adversarial example attacks and to backdoor attacks. DefEnSyn 's repair programs enhance the networks' robustness on average by {\$}{\$}+40{\backslash}{\%}{\$}{\$}+40{\%}and up to {\$}{\$}+71{\backslash}{\%}{\$}{\$}+71{\%}. DefEnSyn decreases the network's accuracy by only {\$}{\$}{\backslash}approx -1{\backslash}{\%}{\$}{\$}≈-1{\%}. We demonstrate that DefEnSyn outperforms existing state-of-the-art defenses based on adversarial training, randomization, and repair, in both robustness and accuracy.},
 address = {Cham},
 author = {Yuviler, Tom
and Drachsler-Cohen, Dana},
 booktitle = {Static Analysis},
 editor = {Oh, Hakjoo
and Sui, Yulei},
 isbn = {978-3-032-07106-4},
 pages = {221--248},
 publisher = {Springer Nature Switzerland},
 title = {Enhancing Neural Network Robustness via Synthesis of Repair Programs},
 url = {https://link.springer.com/chapter/10.1007/978-3-032-07106-4_10},
 year = {2026}
}

@Inbook{Jalote2025,
author="Jalote, Pankaj",
title="Coding",
bookTitle="A Concise Introduction to Software Engineering: With Open Source and GenAI",
year="2025",
publisher="Springer Nature Switzerland",
address="Cham",
pages="139--172",
abstract="The aim of coding or programming activity is to implement the design in the best possible manner. The coding activity profoundly affects both testing and maintenance.",
isbn="978-3-031-74318-4",
doi="10.1007/978-3-031-74318-4_6",
url="https://doi.org/10.1007/978-3-031-74318-4_6"
}


@Inbook{Hajd2025,
author="Hajd, Marcel",
editor="Jacob, Kai
and Schindler, Dierk
and Strathausen, Roger
and Waltl, Bernhard",
title="Towards Access to Justice Through Generative AI and Open Legal Data: Between Utopia and Dystopia",
bookTitle="Liquid Legal -- Sustaining the Rule of Law: Artificial Intelligence, E-Justice, and the Cloud",
year="2025",
publisher="Springer Nature Switzerland",
address="Cham",
pages="39--61",
abstract="Interest in ensuring both access to justice and impartial resolutions through legal and justice institutions continues to be crucial, especially in contexts where public legal assistance is limited, self-help and self-representation are common, and outcomes are influenced by legal awareness and capability. The access to justice, influenced by individual, situational, and systemic factors, raises important questions about who can access it and how to improve it for others. However, access to justice remains unattainable or unachieved for many who require it for various reasons. With the public release of one of the fastest-growing applications in history---ChatGPT, OpenAI demonstrated powerful capabilities of Generative Artificial Intelligence (Generative AI) through digesting a vast amount of information and generating original and unique content. For legal practice, Generative AI demonstrated that legal problems can be solved differently and with higher efficiency. Generative AI can be used to provide legal assistance not only to legal experts but also to people and organizations who may have limited resources to afford traditional legal services. As a result, Generative AI might help increase access to justice and ensure people and organizations can understand their legal rights and options. On the other hand, Generative AI can help legal professionals provide legal aid services more effectively. The essential condition for training Generative AI models in the legal domain is access to legal data. Ironically, the law generates an enormous amount of legal data and legal content, while access to legal information is often limited for different reasons, this is why access to legal, especially judicial data, plays a vital role. Open Legal Data means anyone can freely access, use, modify, and share legal data for any purpose. Availability of the Open Legal Data can bring many benefits to individuals and society through increasing transparency and by enabling informed decisions by individuals and organizations. However, mere opening of legal data is not enough, since legal information is rarely organized in a way to be ready for machine processing. The obstacles related to the lack of public and free access to judgments and decisions made by courts and other official authorities have long been a concern. It is clear, that training Generative AI on relevant documents, such as judgments and other court decisions, can dramatically improve the accuracy of its responses. We believe that the opening of legal data and Generative AI can together create a powerful alliance in facilitating greater access to justice, but only if other requirements are also satisfied, such as the legal capability of subjects seeking solutions to their legal problem.",
isbn="978-3-031-78596-2",
doi="10.1007/978-3-031-78596-2_3",
url="https://doi.org/10.1007/978-3-031-78596-2_3"
}


@inproceedings{10.1007/978-3-032-01377-4_3,
 abstract = {We investigate the potential risks and benefits for students utilising Generative Artificial Intelligence (GAI), specifically OpenAI's ChatGPT and GitHub's Copilot, to generate solutions instead of independently creating them in the traditional educational setting. The rapid advancements in GAI have transformed numerous domains, including software development and software engineering education. While these tools offer unprecedented convenience and efficiency, there are growing concerns regarding their potential implications for academic integrity and genuine student learning. We report on a pilot study in which 40 students, who completed a first-semester course on object-oriented programming, re-engage in a comparable programming project in limited time using GAI tools. In this pilot study, we aim to assess the extent to which students can rely on GAI tools to generate solutions for large programming assignments, to investigate the impact of AI-driven code generation on students' understanding of fundamental programming concepts and problem-solving abilities, and to explore the perspectives of educators and students on the implications and long-term consequences of integrating AI-assisted coding tools in the learning process.},
 address = {Cham},
 author = {van Dijk, Tom
and Zaytsev, Vadim},
 booktitle = {Bridging the Gap Between AI and Reality},
 editor = {Steffen, Bernhard},
 isbn = {978-3-032-01377-4},
 pages = {48--76},
 publisher = {Springer Nature Switzerland},
 title = {The Impact of Generative Artificial Intelligence Tools in Project-Based Learning},
 url = {https://link.springer.com/chapter/10.1007/978-3-032-01377-4_3},
 year = {2026}
}

@Inbook{Fawareh2026,
author="Fawareh, Hamed
and Habaashneh, Tha'er",
editor="Sarea, Adel
and Echchabi, Abdelghani
and Salami, Monsurat Ayojimi
and Mahmood, Abdullah",
title="Impact of AI Tools on Software Security",
bookTitle="Artificial Intelligence for Sustainable Innovation Management and Risk Management: A Systems (and Network) Perspective",
year="2026",
publisher="Springer Nature Switzerland",
address="Cham",
pages="1755--1774",
abstract="The following machine learning methods were employed in the four experiments that were discussed in this study: DT, RF, GB, MLP, and Making a Vote: In the first trial, DT, MLP, and RF were utilized to detect network attacks; in the second trial, RF, DT, and XGB were employed to detect hostile attacks. As a first step in this research, we examine the UNSW NB15 dataset, which details several forms of network attacks. Second, we got the dataset ready to feed into machine learning methods so we could produce many models. Included in this were activities such as using user min{\_}max{\_}normalize to guarantee that all datasets fell within the same ranges and applying a Label Encoder to transform non-numerical attributes into numerical ones. The machine learning algorithms (ML) results were tested according to the previously mentioned standards. Compared to other detection approaches, DT and the voting strategy performed better. The results show that MLs are quite good at detecting and differentiating between malicious and legitimate attacks.",
isbn="978-3-031-95310-1",
doi="10.1007/978-3-031-95310-1_127",
url="https://doi.org/10.1007/978-3-031-95310-1_127"
}


@Article{Wang2024,
author="Wang, Xiqiang
and Zhu, Ling
and Liu, Jing
and Ma, Yanpeng
and Qiu, Chuan
and Liu, Chengfeng
and Gong, Yangchao
and Yuwen, Ya
and Guan, Gongchang
and Zhang, Yong
and Pan, Shuo
and Wang, Junkui
and Liu, Zhongwei",
title="Palmitic acid in type 2 diabetes mellitus promotes atherosclerotic plaque vulnerability via macrophage Dll4 signaling",
journal="Nature Communications",
year="2024",
month="Feb",
day="12",
volume="15",
number="1",
pages="1281",
abstract="Patients with Type 2 Diabetes Mellitus are increasingly susceptible to atherosclerotic plaque vulnerability, leading to severe cardiovascular events. In this study, we demonstrate that elevated serum levels of palmitic acid, a type of saturated fatty acid, are significantly linked to this enhanced vulnerability in patients with Type 2 Diabetes Mellitus. Through a combination of human cohort studies and animal models, our research identifies a key mechanistic pathway: palmitic acid induces macrophage Delta-like ligand 4 signaling, which in turn triggers senescence in vascular smooth muscle cells. This process is critical for plaque instability due to reduced collagen synthesis and deposition. Importantly, our findings reveal that macrophage-specific knockout of Delta-like ligand 4 in atherosclerotic mice leads to reduced plaque burden and improved stability, highlighting the potential of targeting this pathway. These insights offer a promising direction for developing therapeutic strategies to mitigate cardiovascular risks in patients with Type 2 Diabetes Mellitus.",
issn="2041-1723",
doi="10.1038/s41467-024-45582-8",
url="https://doi.org/10.1038/s41467-024-45582-8"
}


@Article{Sharma2025,
author="Sharma, Digvijay Narayan
and Yadav, Dilip Kumar",
title="ROS-XGB: a machine learning model for software defect prediction",
journal="International Journal of System Assurance Engineering and Management",
year="2025",
month="Aug",
day="18",
abstract="The software defect prediction (SDP) research aims to predict defects early in the software lifecycle. It permits stakeholders to improve software quality, functionality, scalability, reliability, and information security aspects for the targeted software. With the digitalisation of enterprises and processes, its extent has grown since enterprises look for reliable, high-quality software applications. Since most of the Software defect identification is done manually during the development and testing phase, SDP has been an area of research in software engineering. Researchers have been trying SDP modelling using Machine Learning (ML) and Deep Learning (DL) techniques from classified feature metrics and semantic information that can be extracted from the Abstract Syntax Tree (AST) and the software's source code. Both ML and DL based models suffer from the class imbalance problem, and predictions are biased towards the majority class. This study conducts an empirical comparative analysis of 7 benchmark machine learning (ML) classification models trained on the PROMISE code metrics repository for both balanced and unbalanced classified code metrics and proposes ROS-XGB (Random Oversampling based XGBoost) as the best model for SDP. The experiment indicates that the ROS-XGB-based SDP model outperformed the recent models on evaluation metrics like accuracy, precision, recall, and F1 score. By using the ROS-XGB framework, organisations can optimise human resource allocation, maintain project schedules, and ensure the successful delivery of quality software.",
issn="0976-4348",
doi="10.1007/s13198-025-02906-1",
url="https://doi.org/10.1007/s13198-025-02906-1"
}


@Article{Li2025,
author="Li, Shangwen
and Yuan, Kang
and Tang, Zihao
and Zhang, Yuanjian
and Ji, Peng
and Ge, Quanbo
and Du, Shaoyi
and Huang, Yanjun
and Chen, Hong",
title="A Survey of Human Intelligence Augmented Artificial Intelligence: An Autonomous Driving Perspective",
journal="Automotive Innovation",
year="2025",
month="Aug",
day="01",
volume="8",
number="3",
pages="591--619",
abstract="The primary goal of artificial intelligence (AI) is to learn, mimic, and ultimately surpass human intelligence. Connectionist AI draws inspiration from human neuroscience and biological mechanisms and has made significant progress in many fields like autonomous driving (AD). However, the performance of AD systems in open, complex, and dynamic environments is still limited. Integrating robust and high-level human intelligence into AI remains a critical trend for its ongoing development and evolution. Inspired by this, this paper reviews Human Intelligence Augmented Artificial Intelligence (HIA-AI) from an AD perspective and offers a taxonomy of related methods, including learning from human demonstrations, tuning from human feedback, integrating from human mechanisms, and abstracting from human knowledge. It discusses current applications and innovations of related methods. Furthermore, this paper examines the characteristics, advantages, and disadvantages of various HIA-AI methods and points out potential future research directions.",
issn="2522-8765",
doi="10.1007/s42154-025-00361-z",
url="https://doi.org/10.1007/s42154-025-00361-z"
}


@Inbook{Dhillon2025,
author="Dhillon, Vikram
and Metcalf, David
and Hooper, Max",
title="Physics of Large Language Models",
bookTitle="AI Frameworks Enabled by Blockchain: Creating Trustworthy and Responsible AI Using Distributed Ledger Technology",
year="2025",
publisher="Apress",
address="Berkeley, CA",
pages="257--297",
abstract="This chapter is adapted from a talk delivered by Zeyuan Allen-Zhu, ScD, at the ICML 2024 Tutorial: Physics of Language Models. The talk was widely acclaimed for its depth and systematic approach to understanding the underlying mechanisms of language models. To ensure accessibility and enhance understanding, the content has been transcribed and supplemented with technical commentary. Certain parts of the talk have been edited for clarity and context while maintaining the integrity and intent of the original presentation. By combining rigorous experimentation with synthetic data and probing techniques, the speaker provides insights that challenge traditional approaches to AI development. This chapter offers both theoretical insights and actionable strategies, making it a significant contribution to the ongoing conversation about advancing AI capabilities and understanding.",
isbn="979-8-8688-1402-0",
doi="10.1007/979-8-8688-1402-0_8",
url="https://doi.org/10.1007/979-8-8688-1402-0_8"
}


@Inbook{Al-Dhabyani2026,
author="Al-Dhabyani, Walid",
editor="Khalifa, Nour Eldeen M.
and Taha, Mohamed Hamed N.",
title="Data Privacy and Security in Large Language Models for Medical Fields",
bookTitle="Next-Gen Healthcare: AI-Powered Medical Innovations",
year="2026",
publisher="Springer Nature Switzerland",
address="Cham",
pages="175--197",
abstract="Large language models (LLMs) are transforming healthcare through applications such as diagnosis, risk prediction, and treatment planning. While offering improved accuracy and efficiency, their use raises significant privacy and security concerns, including prompt injection, adversarial manipulation, and leakage of sensitive patient data. This chapter reviews key vulnerabilities, attack vectors, and their implications for medical practice. It also includes all medical fields branches and clinical use. We also summarize AI adoption across major medical fields, outlining benefits and challenges. Challenges and mitigation with LLMs in the medical field are illustrated and explained in detail. Some defense approaches with evaluation metrics such as privacy leakage rate, attack success rate, and computational overhead are concluded. Our findings highlight the need for robust, adaptive safeguards to ensure safe and ethical LLM deployment in healthcare. More researches in this direction are still open for academics and industries.",
isbn="978-3-032-07267-2",
doi="10.1007/978-3-032-07267-2_7",
url="https://doi.org/10.1007/978-3-032-07267-2_7"
}


@Article{Bhargava2026,
author="Bhargava, Vatsal
and Kar, Arpita
and Gupta, Sonal
and Yadav, Chanchal
and Singh, Khushboo
and Lata, Pushp",
title="AI in Layman's life",
journal="AI and Ethics",
year="2026",
month="Jan",
day="08",
volume="6",
number="1",
pages="97",
abstract="This paper aims to simplify Artificial Intelligence (AI) for non-technical audiences, providing a comprehensive and accessible understanding of its core concepts, historical advancements, and ethical considerations, while promoting transparency in AI systems to foster trust and equity. It begins with an introductory segment tailored for the general public, then articulates the definition of AI utilizing common language and traces its advancement from primitive automatons to contemporary machine learning frameworks and ends with providing the laypeople with a simple AI Literacy Triangle framework. The paper delineates the three primary categories of AI---Artificial Narrow Intelligence (ANI), Artificial General Intelligence (AGI), and Artificial Super Intelligence (ASI)---and explores various AI models, including supervised and unsupervised learning, deep learning, generative AI, Natural Language Processing (NLP), and vision. Noteworthy attention is directed toward Large Language Models (LLMs), and their functionalities. Beyond the technical overview, the paper also examines AI's broader societal implications, with a focus on its impact on the everyday lives of laypersons. Key discussions include the challenges posed by the spread of misinformation and disinformation, ethical concerns over AI's access to personal user data (e.g., privacy risks in smart devices), and issues of copyright and creativity arising from generative AI, alongside strategies for bias mitigation and equitable access. By integrating conceptual elucidation with societal implications, this manuscript equips its readers with the ability to comprehend the trajectory of AI, distinguish between various model types, and real-world applications of AI across diverse social, professional, and everyday contexts, thereby promoting digital literacy, independence, and privacy through clear, explainable AI communication tailored for non-experts.",
issn="2730-5961",
doi="10.1007/s43681-025-00946-8",
url="https://doi.org/10.1007/s43681-025-00946-8"
}


@inproceedings{10.1007/978-3-032-08649-5_17,
 abstract = {Quality assurance in data-intensive software systems is challenging due to complex code-data interactions and diverse data scenarios. Traditional testing methods often fail to address these issues, particularly for edge cases and domain-specific constraints. We propose Prompt-Driven Test Generation (PDTG), a novel framework that integrates large language models (LLMs) and knowledge graphs to automate test case generation. PDTG uses engineered prompts to guide LLMs, enriched by knowledge graphs that provide semantic context and domain constraints. Evaluated on three real-world applications (financial, healthcare, ecommerce), PDTG achieves a 27.3{\%} increase in data scenario coverage, 35.8{\%} better fault detection, and 41.2{\%} less manual effort compared to baselines like EvoSuite and Randoop. This approach enhances test relevance and coverage, offering a scalable solution for testing complex systems, with applications in collaborative intelligence and distributed workflows.},
 address = {Cham},
 author = {Kosna, Srinivas Reddy},
 booktitle = {Software and Data Engineering},
 editor = {Rahimi, Nick
and Margapuri, Venkat
and Golilarz, Noor Amiri},
 isbn = {978-3-032-08649-5},
 pages = {267--288},
 publisher = {Springer Nature Switzerland},
 title = {Prompt Driven Test Generation: Leveraging Large Language Models and Knowledge Graphs for Quality Assurance in Data Intensive Software System},
 url = {https://link.springer.com/chapter/10.1007/978-3-032-08649-5_17},
 year = {2026}
}

@inproceedings{10.1007/978-981-96-7438-1_3,
 abstract = {With the wide application of IoT devices, the proliferation of vulnerability information poses a serious challenge to cyber security. Vulnerability classification, as an important application of natural language processing technology in cybersecurity, aims to extract key features from vulnerability descriptions and classify them to help security managers quickly identify and fix vulnerabilities. Traditional dimensionality reduction methods such as TF-IDF, despite reducing algorithmic complexity, have limited understanding of semantics and context and are prone to ignore some important information. In recent years, statistical methods such as information gain and chi-square test have been introduced to improve feature extraction, but there are still problems such as high-dimensional sparse data dependency. The rise of deep learning techniques, especially the application of word embedding and pre-trained language models, has dramatically improved the accuracy and robustness of vulnerability classification and alleviated the limitations of traditional methods. Combined with artificial intelligence techniques, vulnerability classification is developing in a more efficient and intelligent direction, providing strong support for IoT cybersecurity.},
 address = {Singapore},
 author = {Wang, Yifang},
 booktitle = {Cognitive Computation and Systems},
 editor = {Xu, Bin
and Qiu, Jianlong},
 isbn = {978-981-96-7438-1},
 pages = {27--36},
 publisher = {Springer Nature Singapore},
 title = {A Synthesis of Techniques for Feature Downgrading Processing in IoT Security},
 url = {https://link.springer.com/chapter/10.1007/978-981-96-7438-1_3},
 year = {2025}
}

@Article{Yang2024,
author="Yang, Xueqi
and Jakubowski, Mariusz
and Kang, Li
and Yu, Haojie
and Menzies, Tim",
title="SparseCoder: Advancing source code analysis with sparse attention and learned token pruning",
journal="Empirical Software Engineering",
year="2024",
month="Dec",
day="10",
volume="30",
number="1",
pages="38",
abstract="As software projects rapidly evolve, software artifacts become more complex and defects behind them get harder to identify. The emerging Transformer-based approaches, though achieving remarkable performance, struggle with long code sequences due to their self-attention mechanism, which scales quadratically with the sequence length. This paper introduces SparseCoder, an innovative approach incorporating sparse attention and learned token pruning (LTP) method (adapted from natural language processing) to address this limitation. Compared to previous state-of-the-art models (CodeBERT, RoBERTa and CodeT5), our experiments demonstrate that SparseCoder can handle significantly longer input sequences -- at least twice as long, within the limits of our hardware resources and data statistics. Additionally, SparseCoder is four times faster than other methods measured in runtime, achieving a 50{\%} reduction in floating point operations per second (FLOPs) with a negligible performance drop of less than 1{\%} compared to Transformers using sparse attention (Sparse Atten). Plotting FLOPs of model inference against token lengths reveals that SparseCoder scales linearly, whereas other methods, including the current state-of-the-art model CodeT5, scale quadratically. Moreover, SparseCoder enhances interpretability by visualizing non-trivial tokens layer-wise.",
issn="1573-7616",
doi="10.1007/s10664-024-10558-1",
url="https://doi.org/10.1007/s10664-024-10558-1"
}


@Article{Liu2025,
author="Liu, Sijia
and Yao, Yuanshun
and Jia, Jinghan
and Casper, Stephen
and Baracaldo, Nathalie
and Hase, Peter
and Yao, Yuguang
and Liu, Chris Yuhao
and Xu, Xiaojun
and Li, Hang
and Varshney, Kush R.
and Bansal, Mohit
and Koyejo, Sanmi
and Liu, Yang",
title="Rethinking machine unlearning for large language models",
journal="Nature Machine Intelligence",
year="2025",
month="Feb",
day="01",
volume="7",
number="2",
pages="181--194",
abstract="We explore machine unlearning in the domain of large language models (LLMs), referred to as LLM unlearning. This initiative aims to eliminate undesirable data influence (for example, sensitive or illegal information) and the associated model capabilities, while maintaining the integrity of essential knowledge generation and not affecting causally unrelated information. We envision LLM unlearning becoming a pivotal element in the life-cycle management of LLMs, potentially standing as an essential foundation for developing generative artificial intelligence that is not only safe, secure and trustworthy but also resource-efficient without the need for full retraining. We navigate the unlearning landscape in LLMs from conceptual formulation, methodologies, metrics and applications. In particular, we highlight the often-overlooked aspects of existing LLM unlearning research, for example, unlearning scope, data--model interaction and multifaceted efficacy assessment. We also draw connections between LLM unlearning and related areas such as model editing, influence functions, model explanation, adversarial training and reinforcement learning. Furthermore, we outline an effective assessment framework for LLM unlearning and explore its applications in copyright and privacy safeguards and sociotechnical harm reduction.",
issn="2522-5839",
doi="10.1038/s42256-025-00985-0",
url="https://doi.org/10.1038/s42256-025-00985-0"
}


@Article{Poth2024,
author="Poth, Alexander
and Rrjolli, Olsi
and Arcuri, Andrea",
title="Technology adoption performance evaluation applied to testing industrial REST APIs",
journal="Automated Software Engineering",
year="2024",
month="Dec",
day="02",
volume="32",
number="1",
pages="5",
abstract="Testing is an important task within software development. To write test cases and integrate them into an automated test suite requires a significant amount of work. Given a set of requirements and specifications of a software, testing is needed to verify its correctness. When done manually, it is an expensive and error prone task. To facilitate such work, automated test-case generation via tools could be useful. Test-case generation can be facilitated by deterministic algorithm-driven approaches or non-deterministic approaches such as with AI (e.g., evolutionary and LLM). The different approaches come with their strengths and weaknesses, which must be considered when integrating these approaches into a product test procedure in industry. Several novel testing techniques and tools have been developed in academia and industry, but how effective they are and how to integrate them in real-world large industrial scenarios is still unclear. In this paper, a systematic approach is presented to evaluate test-case generation methodologies and integrate them into a scalable enterprise setup. The specific context is black-box testing of REST APIs, based on their OpenAPI schemas. The aim is to facilitate IT product development and service delivery. The proposed Technology Adoption Performance Evaluation (TAPE) approach is evaluated by a case study within the Group IT of Volkswagen AG. We evaluated existing tools such as OpenAPI Generator, EvoMaster and StarCoder which are built on different technologies. Our results show that these tools are of benefit for test engineers to facilitate test-case specification and design within the Group IT of Volkswagen AG.",
issn="1573-7535",
doi="10.1007/s10515-024-00477-2",
url="https://doi.org/10.1007/s10515-024-00477-2"
}


@Inbook{Fritchey2026,
author="Fritchey, Grant",
title="Query Performance Tuning",
bookTitle="SQL Server 2025 Query Performance Tuning: Troubleshoot and Optimize Query Performance",
year="2026",
publisher="Apress",
address="Berkeley, CA",
pages="1--20",
abstract="Query tuning is not easy.",
isbn="979-8-8688-1865-3",
doi="10.1007/979-8-8688-1865-3_1",
url="https://doi.org/10.1007/979-8-8688-1865-3_1"
}


@inproceedings{10.1007/978-3-031-65630-9_16,
 abstract = {Formal verification provides a rigorous and systematic approach to ensure the correctness and reliability of software systems. Yet, constructing specifications for the full proof relies on domain expertise and non-trivial manpower. In view of such needs, an automated approach for specification synthesis is desired. While existing automated approaches are limited in their versatility, i.e., they either focus only on synthesizing loop invariants for numerical programs, or are tailored for specific types of programs or invariants. Programs involving multiple complicated data types (e.g., arrays, pointers) and code structures (e.g., nested loops, function calls) are often beyond their capabilities. To help bridge this gap, we present AutoSpec, an automated approach to synthesize specifications for automated program verification. It overcomes the shortcomings of existing work in specification versatility, synthesizing satisfiable and adequate specifications for full proof. It is driven by static analysis and program verification, and is empowered by large language models (LLMs). AutoSpec addresses the practical challenges in three ways: (1) driving AutoSpec by static analysis and program verification, LLMs serve as generators to generate candidate specifications, (2) programs are decomposed to direct the attention of LLMs, and (3) candidate specifications are validated in each round to avoid error accumulation during the interaction with LLMs. In this way, AutoSpec can incrementally and iteratively generate satisfiable and adequate specifications. The evaluation shows its effectiveness and usefulness, as it outperforms existing works by successfully verifying 79{\%} of programs through automatic specification synthesis, a significant improvement of 1.592x. It can also be successfully applied to verify the programs in a real-world X509-parser project.},
 address = {Cham},
 author = {Wen, Cheng
and Cao, Jialun
and Su, Jie
and Xu, Zhiwu
and Qin, Shengchao
and He, Mengda
and Li, Haokun
and Cheung, Shing-Chi
and Tian, Cong},
 booktitle = {Computer Aided Verification},
 editor = {Gurfinkel, Arie
and Ganesh, Vijay},
 isbn = {978-3-031-65630-9},
 pages = {302--328},
 publisher = {Springer Nature Switzerland},
 title = {Enchanting Program Specification Synthesis by Large Language Models Using Static Analysis and Program Verification},
 url = {https://link.springer.com/chapter/10.1007/978-3-031-65630-9_16},
 year = {2024}
}

@inproceedings{10.1007/978-3-031-76934-4_7,
 abstract = {The rapid deployment of Large Language Models (LLMs) requires careful consideration of their effect on cybersecurity. Our work aims to improve the selection process of LLMs that are suitable for facilitating secure coding (SC). This raises challenging research questions, such as (RQ1) Which functionality can streamline the LLM evaluation? (RQ2) What should the evaluation measure? (RQ3) How to attest that the evaluation process is impartial? To address these questions, we introduce LLMSecCode, an open-source evaluation framework designed to assess LLM SC capabilities objectively. We validate the LLMSecCode implementation through experiments. We find a 10{\%} and 9{\%} difference in performance when varying parameters and prompts, respectively. We also compare some results to reliable external actors, where our results show a 5{\%} difference. We strive to ensure the ease of use of our open-source framework and encourage further development by external actors. With LLMSecCode, we hope to encourage the standardization and benchmarking of LLMs' capabilities in security-oriented code and tasks.},
 address = {Cham},
 author = {Ryd{\'e}n, Anton
and N{\"a}slund, Erik
and Schiller, Elad Michael
and Almgren, Magnus},
 booktitle = {Cyber Security, Cryptology, and Machine Learning},
 editor = {Dolev, Shlomi
and Elhadad, Michael
and Kuty{\l}owski, Miros{\l}aw
and Persiano, Giuseppe},
 isbn = {978-3-031-76934-4},
 pages = {100--118},
 publisher = {Springer Nature Switzerland},
 title = {LLMSecCode: Evaluating Large Language Models for Secure Coding},
 url = {https://link.springer.com/chapter/10.1007/978-3-031-76934-4_7},
 year = {2025}
}

@inproceedings{10.1007/978-3-031-40744-4_2,
 abstract = {In recent years, language models (LMs), such as GPT-4, have been widely used in multiple domains, including natural language processing, visualization, and so on. However, applying them for analyzing and optimizing high-performance computing (HPC) software is still challenging due to the lack of HPC-specific support. In this paper, we design the LM4HPC framework to facilitate the research and development of HPC software analyses and optimizations using LMs. Tailored for supporting HPC datasets, AI models, and pipelines, our framework is built on top of a range of components from different levels of the machine learning software stack, with Hugging Face-compatible APIs. Using three representative tasks, we evaluated the prototype of our framework. The results show that LM4HPC can help users quickly evaluate a set of state-of-the-art models and generate insightful leaderboards.},
 address = {Cham},
 author = {Chen, Le
and Lin, Pei-Hung
and Vanderbruggen, Tristan
and Liao, Chunhua
and Emani, Murali
and de Supinski, Bronis},
 booktitle = {OpenMP: Advanced Task-Based, Device and Compiler Programming},
 editor = {McIntosh-Smith, Simon
and Klemm, Michael
and de Supinski, Bronis R.
and Deakin, Tom
and Klinkenberg, Jannis},
 isbn = {978-3-031-40744-4},
 pages = {18--33},
 publisher = {Springer Nature Switzerland},
 title = {LM4HPC: Towards Effective Language Model Application in High-Performance Computing},
 url = {https://link.springer.com/chapter/10.1007/978-3-031-40744-4_2},
 year = {2023}
}

@Inbook{Huang2024,
author="Huang, Ken
and Li, Yale
and Thaine, Patricia",
editor="Huang, Ken
and Wang, Yang
and Goertzel, Ben
and Li, Yale
and Wright, Sean
and Ponnapalli, Jyoti",
title="Use GenAI Tools to Boost Your Security Posture",
bookTitle="Generative AI Security: Theories and Practices",
year="2024",
publisher="Springer Nature Switzerland",
address="Cham",
pages="305--338",
abstract="This chapter provides an analysis of emerging GenAI tools and techniques that are transforming cybersecurity and ethical AI capabilities. It explores tools with innovative solutions across application security, data privacy, threat detection, governance, observability, and bias detection. The chapter analyzes how natural language processing, neural networks, reinforcement learning, and other GenAI technologies are being applied in purpose-built platforms to boost security, optimize workflows, and uphold transparency. Focus areas include leveraging GenAI tools to strengthen resilience, improve security posture, and promote responsible AI development.",
isbn="978-3-031-54252-7",
doi="10.1007/978-3-031-54252-7_10",
url="https://doi.org/10.1007/978-3-031-54252-7_10"
}


@inproceedings{10.1007/978-3-031-96096-3_23,
 abstract = {With the advent of novel Artificial Intelligence technologies, in particular Large Language Models and generative techniques, many researchers try to understand their scopes and limitations. In particular, Software Engineering can benefit from these approaches, as long as the generated results are properly validated and checked. This represents both a big opportunity and a challenge, as harnessing the power of these new technologies is not a simple task.},
 address = {Cham},
 author = {Colucci Cante, Luigi
and Esposito, Antonio
and Graziano, Mariangela
and Di Martino, Beniamino},
 booktitle = {Complex, Intelligent and Software Intensive Systems},
 editor = {Barolli, Leonard
and Enokido, Tomoya
and Woungang, Isaac},
 isbn = {978-3-031-96096-3},
 pages = {240--249},
 publisher = {Springer Nature Switzerland},
 title = {Artificial Intelligent Technologies to Support Software Engineering: A Survey},
 url = {https://link.springer.com/chapter/10.1007/978-3-031-96096-3_23},
 year = {2025}
}

@Inbook{Camayd-Freixas2025,
author="Camayd-Freixas, Erik",
title="Translational Science",
bookTitle="Metagrammar: Critical Thinking and Problem Solving in the Age of Artificial Intelligence",
year="2025",
publisher="Springer Nature Switzerland",
address="Cham",
pages="297--335",
abstract="Translational science developed in the medical and public health fields with the aim of translating research data into concrete applications and interventions that directly benefit humans.",
isbn="978-3-031-92337-1",
doi="10.1007/978-3-031-92337-1_16",
url="https://doi.org/10.1007/978-3-031-92337-1_16"
}


@Article{Liu2024,
author="Liu, Xuanzhe
and Zhao, Yihao
and Liu, Shufan
and Li, Xiang
and Zhu, Yibo
and Liu, Xin
and Jin, Xin",
title="MuxFlow: efficient GPU sharing in production-level clusters with more than 10000 GPUs",
journal="Science China Information Sciences",
year="2024",
month="Dec",
day="13",
volume="67",
number="12",
pages="222101",
abstract="Large-scale GPU clusters are widely used to speed up both latency-critical (online) and best-effort (offline) deep learning (DL) workloads. However, similar to the common practice, the DL clusters at ByteDance dedicate each GPU to one workload or share workloads in time dimension, leading to very low GPU resource utilization. Existing techniques like NVIDIA MPS provide an opportunity to share multiple workloads in space on widely-deployed NVIDIA GPUs, but it cannot guarantee the performance of online workloads. We present MuxFlow, the first production system that can scale over massive GPUs to support highly efficient space-sharing for DL workloads. MuxFlow introduces a two-level protection mechanism for both memory and computation to guarantee the performance of online workloads. MuxFlow leverages dynamic streaming multiprocessor (SM) allocation to improve the efficiency of offline workloads. Based on our practical error analysis, we design a mixed error-handling mechanism to improve system reliability. MuxFlow has been deployed at ByteDance on more than 18000 GPUs. The deployment results indicate that MuxFlow substantially improves the GPU utilization from 26{\%} to 76{\%}, SM activity from 16{\%} to 33{\%}, and GPU memory usage from 42{\%} to 48{\%}.",
issn="1869-1919",
doi="10.1007/s11432-024-4227-2",
url="https://doi.org/10.1007/s11432-024-4227-2"
}


@Article{Jiang2024,
author="Jiang, Wenxin
and Banna, Vishnu
and Vivek, Naveen
and Goel, Abhinav
and Synovic, Nicholas
and Thiruvathukal, George K.
and Davis, James C.",
title="Challenges and practices of deep learning model reengineering: A case study on computer vision",
journal="Empirical Software Engineering",
year="2024",
month="Aug",
day="20",
volume="29",
number="6",
pages="142",
abstract="Many engineering organizations are reimplementing and extending deep neural networks from the research community. We describe this process as deep learning model reengineering. Deep learning model reengineering --- reusing, replicating, adapting, and enhancing state-of-the-art deep learning approaches --- is challenging for reasons including under-documented reference models, changing requirements, and the cost of implementation and testing.",
issn="1573-7616",
doi="10.1007/s10664-024-10521-0",
url="https://doi.org/10.1007/s10664-024-10521-0"
}


@Article{Xu2026,
author="Xu, Hui
and Gong, Fan'ao
and Huang, Lifang
and Tan, Yongtao",
title="Knowledge Graph Application for Emergency Plan--Accident Case Integration: a Framework of Intelligent Urban Rail Transit Emergency Management",
journal="International Journal of Intelligent Transportation Systems Research",
year="2026",
month="Jan",
day="21",
abstract="Urban rail transit systems are expanding rapidly, but the increasing frequency of operational accidents has posed significant challenges to public safety and emergency management. A key issue lies in the structural disconnect between predefined emergency response plans and the measures adopted in real accident scenarios, and the emergency plan text cannot support fast and accurate emergency decision-making. To address this challenge, this study proposes a novel knowledge graph construction framework that integrates multi-source information from emergency plans and accident cases, using the Shanghai urban rail transit system as a case study. 219 accident cases and multiple emergency plan documents were collected, and a Bidirectional Encoder Representations from Transformers-Bidirectional Long Short-Term Memory-Conditional Random Field model (BERT-BILSTM-CRF model) combined with large language model (LLM) was employed to automatically extract key entities and response measures. Furthermore, a semantic clustering and multi-dimensional mapping strategy was developed to establish fine-grained correspondences between plan-defined measures and accident-specific responses, enabling unified cross-source knowledge modeling. Experimental evaluations demonstrate that the proposed approach substantially improves the standardization and semantic consistency of response measures and achieves high-precision matching between planned strategies and real-world cases. The resulting knowledge graph significantly enhances information retrieval and reasoning capabilities in emergency response scenarios. This work provides a systematic, interpretable, and traceable framework for knowledge representation in urban rail transit safety management and establishes a solid methodological foundation for building intelligent emergency decision-support systems.",
issn="1868-8659",
doi="10.1007/s13177-026-00627-8",
url="https://doi.org/10.1007/s13177-026-00627-8"
}


@Article{Liu2023,
author="Liu, Peng
and Ye, Wenzhe
and Duan, Haiying
and Li, Xianxian
and Zhang, Shuyi
and Yao, Chuanjian
and Li, Yongnan",
title="Graph neural network based approach to automatically assigning common weakness enumeration identifiers for vulnerabilities",
journal="Cybersecurity",
year="2023",
month="Nov",
day="02",
volume="6",
number="1",
pages="29",
abstract="Vulnerability reports are essential for improving software security since they record key information on vulnerabilities. In a report, CWE denotes the weakness of the vulnerability and thus helps quickly understand the cause of the vulnerability. Therefore, CWE assignment is useful for categorizing newly discovered vulnerabilities. In this paper, we propose an automatic CWE assignment method with graph neural networks. First, we prepare a dataset that contains 3394 real world vulnerabilities from Linux, OpenSSL, Wireshark and many other software programs. Then, we extract statements with vulnerability syntax features from these vulnerabilities and use program slicing to slice them according to the categories of syntax features. On top of slices, we represent these slices with graphs that characterize the data dependency and control dependency between statements. Finally, we employ the graph neural networks to learn the hidden information from these graphs and leverage the Siamese network to compute the similarity between vulnerability functions, thereby assigning CWE IDs for these vulnerabilities. The experimental results show that the proposed method is effective compared to existing methods.",
issn="2523-3246",
doi="10.1186/s42400-023-00160-1",
url="https://doi.org/10.1186/s42400-023-00160-1"
}


@Inbook{Barroso2026,
author="Barroso, Luiz Andr{\'e}
and H{\"o}lzle, Urs
and Ranganathan, Parthasarathy",
title="Overview of WSC Workloads",
bookTitle="The Data Center as a Computer: Designing Warehouse-Scale Machines",
year="2026",
publisher="Springer Nature Switzerland",
address="Cham",
pages="19--53",
abstract="The applications that run on warehouse-scale computers (WSCs) define the system design tradeoffs that we will be discussing in the rest of this book. Hence, we first discuss these applications in this chapter. We start with an overview of broad workload classes including both hyperscale and cloud compute consumption at Google. We then discuss the design of some key workloads: Search, YouTube, Gmail, and key software platforms for machine learning and data processing, and Google Cloud Platform. From these examples, we highlight the key distinguishing characteristics of software that run in WSCs and identify common themes specific to large-scale distributed systems, and discuss key commonalities and differences between hyperscale and cloud workloads.",
isbn="978-3-031-99489-0",
doi="10.1007/978-3-031-99489-0_3",
url="https://doi.org/10.1007/978-3-031-99489-0_3"
}


@Article{Liu2026,
author="Liu, Yunyou
and Wang, Fei
and Wang, Peng
and Zhou, Zhen
and Wang, Hongqian
and Li, Jingyao
and Qiu, Yang
and Wang, Haidong
and Miao, Siwei",
title="AI-MDT: an automatic and intelligent multidisciplinary team consultations platform for lung cancer diagnosis",
journal="Journal of Cancer Research and Clinical Oncology",
year="2026",
month="Jan",
day="08",
volume="152",
number="1",
pages="32",
abstract="Multidisciplinary team (MDT) consultations are crucial for managing pulmonary nodules, yet face challenges in efficiency, evidence-based decision support, and data utilization within the MDT process. We present an integrated artificial intelligence (AI)-MDT platform that serves as an assistive tool for lung cancer MDT workflows by incorporating AI across various processes.The aim of this study is to evaluate the clinical utility and preliminary efficacy of the AI-MDT platform. ",
issn="1432-1335",
doi="10.1007/s00432-025-06413-5",
url="https://doi.org/10.1007/s00432-025-06413-5"
}


@inproceedings{10.1007/978-3-031-64171-8_18,
 abstract = {As the reliance on open-source software dependencies increases, managing the security vulnerabilities in these dependencies becomes complex. State-of-the-art industry tools use reachability analysis of code to alert developers when security vulnerabilities in dependencies are likely to impact their projects. These tools heavily rely on precisely identifying the location of the vulnerability within the dependency, specifically vulnerable functions. However, the process of identifying vulnerable functions is currently either manual or uses a naive automated approach that falsely assumes all changed functions in a security patch link are vulnerable. In this paper, we explore using open-source large language models (LLMs) to improve pairing security advisories with vulnerable functions. We explore various prompting strategies, learning paradigms (i.e., zero-shot vs. few-shot), and show our approach generalizes to other open-source LLMs. Compared to the naive automated approach, we show a 173{\%} increase in precision while only having an 18{\%} decrease in recall. The significant increase in precision to enhance vulnerable function identification lays the groundwork for downstream techniques that depend on this critical information for security analysis and threat mitigation.},
 address = {Cham},
 author = {Dunlap, Trevor
and Meyers, John Speed
and Reaves, Bradley
and Enck, William},
 booktitle = {Detection of Intrusions and Malware, and Vulnerability Assessment},
 editor = {Maggi, Federico
and Egele, Manuel
and Payer, Mathias
and Carminati, Michele},
 isbn = {978-3-031-64171-8},
 pages = {350--369},
 publisher = {Springer Nature Switzerland},
 title = {Pairing Security Advisories with Vulnerable Functions Using Open-Source LLMs},
 url = {https://link.springer.com/chapter/10.1007/978-3-031-64171-8_18},
 year = {2024}
}

@inproceedings{10.1007/978-3-031-98414-3_19,
 abstract = {Large language models (LLMs) are artificial intelligence (AI) tools that can generate answers to a variety of student questions. While these tools have the potential to provide personalized instruction, there are also concerns that students will over-rely on them to obtain answers without trying to learn the target concepts. Since LLMs are relatively new, research is needed on their impact on student learning and ways to scaffold learning. We conducted an experimental study (N = 101) in which students solved code-tracing problems. We manipulated (1) the presence of prompts encouraging active and constructive engagement, and (2) the type of instructional resource available, either ChatGPT or traditional materials. The prompts significantly increased learning across both instructional resource contexts; there was no significant difference between the ChatGPT and traditional-materials groups. To gain insight into how students used ChatGPT, we used qualitative methods to analyze their interactions with the tool.},
 address = {Cham},
 author = {Manakina, Olga
and Muldner, Kasia},
 booktitle = {Artificial Intelligence in Education},
 editor = {Cristea, Alexandra I.
and Walker, Erin
and Lu, Yu
and Santos, Olga C.
and Isotani, Seiji},
 isbn = {978-3-031-98414-3},
 pages = {267--280},
 publisher = {Springer Nature Switzerland},
 title = {Prompts Eliciting Active and Constructive Engagement Improve Learning Across ChatGPT and Traditional Resource Contexts},
 url = {https://link.springer.com/chapter/10.1007/978-3-031-98414-3_19},
 year = {2025}
}

@Article{Li2025,
author="Li, Yingling
and Wu, Yuhan
and Wang, Zi'ao
and Huang, Lei
and Wang, Junjie
and Li, Jianping
and Huang, Minying",
title="CodeDoctor: multi-category code review comment generation",
journal="Automated Software Engineering",
year="2025",
month="Feb",
day="27",
volume="32",
number="1",
pages="25",
abstract="Code review is an effective software quality assurance activity. However, this process is labor-intensive and time-consuming, requiring reviewers to carefully review under various categories (e.g., function, refactoring, documentation, etc) to generate review comments. Several approaches have been proposed for automatic review comment generation, although they can generate review comments, they hardly cover all manual review comments. Because most of these approaches simply utilize the information of submitted code and review comments, not fully modeling the features of code review (i.e., ignoring review category, the association of issue snippets and review comments). In this paper, we propose CodeDoctor, an automatic review comment generator with data augmentation and category-aware encoder-decoder to generate multi-category review comments. It consists of three main phases: (1) Data augmentation phase, which classifies review comments and builds review exemplars (i.e., the pairs of issue snippet and its comment) to augment review data by using a large language model (LLM) with prompt engineering and feedback loops; (2) Encoder phase, which encodes the inputs (i.e., review category, diff code and review exemplar) into semantic and token representations; (3) Decoder phase, which designs a category-focused decoder to capture the most relevant information of given category for multi-category review comment generation. Evaluations with five commonly-used and state-of-the-art baselines on two datasets show that CodeDoctor outperforms all baselines, with 1770{\%} higher average BLEU-4, 111{\%} higher average ROUGE-L and 49{\%} higher average F1 than the best baseline. Furthermore, a human evaluation also confirms the significant potential of applying CodeDoctor in practical usage. Our approach can relieve the burden of reviewers by automatically generating multi-category review comments, and helps developers better detect code issues as early as possible, thereby facilitating software development.",
issn="1573-7535",
doi="10.1007/s10515-025-00491-y",
url="https://doi.org/10.1007/s10515-025-00491-y"
}


@inproceedings{10.1007/978-3-031-70879-4_6,
 abstract = {Large Language Models (LLMs) are attracting significant research attention due to their instruction-following abilities, allowing users and developers to leverage LLMs for a variety of tasks. However, LLMs are vulnerable to prompt-injection attacks: a class of attacks that hijack the model's instruction-following abilities, changing responses to prompts to undesired, possibly malicious ones. In this work, we introduce Jatmo , a method for generating task-specific models resilient to prompt-injection attacks. Jatmo leverages the fact that LLMs can only follow instructions once they have undergone instruction tuning. It harnesses a teacher instruction-tuned model to generate a task-specific dataset, which is then used to fine-tune a base model (i.e., a non-instruction-tuned model). Jatmo only needs a task prompt and a dataset of inputs for the task: it uses the teacher model to generate outputs. For situations with no pre-existing datasets, Jatmo can use a single example, or in some cases none at all, to produce a fully synthetic dataset. Our experiments on seven tasks show that Jatmo models provide similar quality of outputs on their specific task as standard LLMs, while being resilient to prompt injections. The best attacks succeeded in less than 0.5{\%} of cases against our models, versus 87{\%} success rate against GPT-3.5-Turbo. We release Jatmo at https://github.com/wagner-group/prompt-injection-defense.},
 address = {Cham},
 author = {Piet, Julien
and Alrashed, Maha
and Sitawarin, Chawin
and Chen, Sizhe
and Wei, Zeming
and Sun, Elizabeth
and Alomair, Basel
and Wagner, David},
 booktitle = {Computer Security -- ESORICS 2024},
 editor = {Garcia-Alfaro, Joaquin
and Kozik, Rafa{\l}
and Chora{\'{s}}, Micha{\l}
and Katsikas, Sokratis},
 isbn = {978-3-031-70879-4},
 pages = {105--124},
 publisher = {Springer Nature Switzerland},
 title = {Jatmo: Prompt Injection Defense by Task-Specific Finetuning},
 url = {https://link.springer.com/chapter/10.1007/978-3-031-70879-4_6},
 year = {2024}
}

@inproceedings{10.1007/978-981-95-5719-6_5,
 abstract = {Programming skills are an essential competency in today's digital age. As online judgement systems have gained popularity, students are contributing vast amounts of behavioral data through programming exercises. Traditional knowledge tracing can be employed to predict students' performance in programming. Nevertheless, these methods frequently fall short by neglecting the role of code faults in modeling programming learning state. Furthermore, they fail to capture the dynamic and semantic characteristics of source code due to reliance on general-purpose code representation techniques. In this study, we propose a novel method called Fault-Aware Code Knowledge Tracing (FACKT), which integrates a dual-LSTM architecture with attention mechanisms and memory decay theory to track the evolution of both fault patterns and knowledge states of students. Moreover, by incorporating test case analysis with Large Language Models (LLMs), FACKT autonomously generates fault types, thereby enriching the code representation. Experimental results indicate that FACKT achieves an 8.3{\%} improvement in AUC and a 3.5{\%} increase in ACC compared to baseline methods. Ablation studies further demonstrate the effectiveness of modeling the fault evolution process. Additionally, we publicly release FACKT{\_}2024, a dataset comprising 74,269 code submissions from university students.},
 address = {Singapore},
 author = {Zhou, Mengshuang
and Zhou, Qing
and Xia, Wenyuan
and Jiang, Ying
and Lin, Cifa
and Zhao, Subo},
 booktitle = {Web and Big Data},
 editor = {Li, Jiajia
and Chbeir, Richard
and Li, Lei
and Zong, Chuanyu
and Zhang, Yanfeng
and Zhang, Mengxuan},
 isbn = {978-981-95-5719-6},
 pages = {67--80},
 publisher = {Springer Nature Singapore},
 title = {FACKT: A Fault-Aware Model for Code Knowledge Tracing},
 url = {https://link.springer.com/chapter/10.1007/978-981-95-5719-6_5},
 year = {2026}
}

@inproceedings{10.1007/978-3-032-08317-3_13,
 abstract = {To effectively deploy Large Language Models (LLMs) in application-specific settings, fine-tuning techniques are applied to enhance performance on specialized tasks. This process often involves fine-tuning on user data, which may contain sensitive information. Although not recommended, it is not uncommon for users to send passwords in messages, and fine-tuning models on this could result in passwords being leaked. In this study, a Large Language Model is fine-tuned with customer support data and passwords from the RockYou password wordlist using Low-Rank Adaptation (LoRA). RockYou is selected as it is one of the most well-known passwords and is including in most Kali Linux distributions. Out of the first 200 passwords from the list, 37 were successfully recovered. Further, causal tracing is used to identify that password information is largely located in a few layers. Lastly, Rank One Model Editing (ROME) is used to remove the password information from the model, resulting in the number of passwords recovered going from 37 to 0.},
 address = {Cham},
 author = {Marinelli, Ryan
and Eckhoff, Magnus},
 booktitle = {Explainable Artificial Intelligence},
 editor = {Guidotti, Riccardo
and Schmid, Ute
and Longo, Luca},
 isbn = {978-3-032-08317-3},
 pages = {281--294},
 publisher = {Springer Nature Switzerland},
 title = {Leaking LoRa: An Evaluation of Password Leaks and Knowledge Storage in Large Language Models},
 url = {https://link.springer.com/chapter/10.1007/978-3-032-08317-3_13},
 year = {2026}
}

@inproceedings{10.1007/978-3-031-84312-9_19,
 abstract = {Courses offered in a Computer Science program of a large university differ greatly in their structure. Such a variety is driven by the diversity of backgrounds of faculty members, as well as by the breadth and depth of topics covered. The pragmatic goal of our research is to help students to adopt an effective learning strategy while engaging in a graduate program of several dozen very different courses. As a first step, we established a database of frequently asked questions regarding class logistics. Building on this foundation we launched the chatbot to be readily available to all program registrants. This paper can serve as a step-by-step introduction to How-To-Build-A-Chatbot while dealing with challenges and successes encountered. A student is able to quickly browse through relevant questions and responses. Additionally, a student can actively participate in a conversation with the bot, select among five language models, adjust semantic matching, and switch on generative mode. We used Ragas framework to formally evaluate and measure the performance of the bot, while focusing on its ability to simulate the human-like interaction. The experimental nature of exchange with the bot served as a strong motivation for students to keep exploring and learning. The evolution from a list-bound bot design into a more fluid approach involving language technologies is not trivial. As a benefit, we were able to offer students the personalized and engaging learning experience.},
 address = {Cham},
 author = {Elentukh, Alex
and Motipally, Shravan
and Kustra, Jacob
and March, Toby},
 booktitle = {Computer Science and Education in Computer Science},
 editor = {Zlateva, Tanya
and Tuparov, Georgi},
 isbn = {978-3-031-84312-9},
 pages = {276--296},
 publisher = {Springer Nature Switzerland},
 title = {Building a Chatbot to Adopt an Effective Learning Strategy for Graduate Courses in Computer Science},
 url = {https://link.springer.com/chapter/10.1007/978-3-031-84312-9_19},
 year = {2025}
}

@inproceedings{10.1007/978-3-032-06725-8_16,
 abstract = {Availability of higher education is still the priority concern for Indian socio-economically underprivileged and rural students at 25{\%}, according to the National University of Educational Planning and Administration (NUEPA) 2021--2022 annual report. The major hurdles are financial constraints and unavailability of information regarding government-funded education programs. An AI-based learning advice chatbot leveraging the Large Language Model Meta AI (LLaMA) has been proposed to address the issue. The model is founded on the LLaMA 3.1 transformer model specifically optimized for low-resource environments. The model was fine-tuned on a 1,200 proprietary educational dataset of scholarship and government program-oriented queries with a test accuracy of 94.39{\%} with an 80{\%}−20{\%} train-test split. The 60--40{\%} was employed as another split to verify its performance with the test accuracy at 92.97{\%}. The chatbot provides user-specific recommendations for 24/7 availability, ease, and strong data security. Providing timely and accurate information will help reduce the financial and informational barrier, thereby increasing the equitable opportunity for people from low-income backgrounds and the community to access higher education.},
 address = {Cham},
 author = {Chowdary, Sandireddy Vyshnavi Prasad
and Charan, Atluri Sai
and Satapathy, Ashutosh
and Prasanna, Chinnam Sri Sai
and Thottempudi, Kokila},
 booktitle = {Technological Innovations for Sustainable Development},
 editor = {Boudriki Semlali, Badr-Eddine
and Ben Abdel Ouahab, Ikram
and Angeletti, Fabio},
 isbn = {978-3-032-06725-8},
 pages = {184--196},
 publisher = {Springer Nature Switzerland},
 title = {An AI-Powered Chatbot Using a Transformer-Based Language Model for Graduate Educational Schemes Recommendation},
 url = {https://link.springer.com/chapter/10.1007/978-3-032-06725-8_16},
 year = {2025}
}

@inproceedings{10.1007/978-3-031-73254-6_13,
 abstract = {Out-of-distribution (OOD) object detection is a challenging task due to the absence of open-set OOD data. Inspired by recent advancements in text-to-image generative models, such as Stable Diffusion, we study the potential of generative models trained on large-scale open-set data to synthesize OOD samples, thereby enhancing OOD object detection. We introduce SyncOOD, a simple data curation method that capitalizes on the capabilities of large foundation models to automatically extract meaningful OOD data from text-to-image generative models. This offers the model access to open-world knowledge encapsulated within off-the-shelf foundation models. The synthetic OOD samples are then employed to augment the training of a lightweight, plug-and-play OOD detector, thus effectively optimizing the in-distribution (ID)/OOD decision boundaries. Extensive experiments across multiple benchmarks demonstrate that SyncOOD significantly outperforms existing methods, establishing new state-of-the-art performance with minimal synthetic data usage. The project is available at https://github.com/CVMI-Lab/SyncOOD.},
 address = {Cham},
 author = {Liu, Jiahui
and Wen, Xin
and Zhao, Shizhen
and Chen, Yingxian
and Qi, Xiaojuan},
 booktitle = {Computer Vision -- ECCV 2024},
 editor = {Leonardis, Ale{\v{s}}
and Ricci, Elisa
and Roth, Stefan
and Russakovsky, Olga
and Sattler, Torsten
and Varol, G{\"u}l},
 isbn = {978-3-031-73254-6},
 pages = {213--231},
 publisher = {Springer Nature Switzerland},
 title = {Can OOD Object Detectors Learn from Foundation Models?},
 url = {https://link.springer.com/chapter/10.1007/978-3-031-73254-6_13},
 year = {2025}
}

@inproceedings{10.1007/978-981-96-6008-7_23,
 abstract = {This paper analyzes the application of artificial intelligence (AI), with a particular focus on large language models (LLMs), in the domain of offensive security. The research aims to assess the potential of AI-assisted approaches to improve the efficiency, scalability, and effectiveness of security testing processes. A comprehensive analysis of the current state of AI-assisted offensive security is presented, including a review of existing literature, tools, and methodologies. A novel framework for integrating AI techniques, specifically LLMs, into traditional offensive security workflows is developed. A series of practical experiments and case studies are conducted, covering various offensive security scenarios, including cloud misconfigurations, web application vulnerabilities, binary exploitation, and source code analysis. The capabilities and limitations of AI-assisted approaches are evaluated in comparison to manual security testing methods. The research findings demonstrate that AI-assisted techniques can significantly enhance the efficiency and coverage of security testing, enabling security professionals to identify a greater number of vulnerabilities in less time compared to manual approaches. At the same time, they highlight the continued importance of human expertise and oversight in the process.},
 address = {Singapore},
 author = {Kisielewicz, Maciej
and Kedziora, Michal
and Jozwiak, Ireneusz},
 booktitle = {Intelligent Information and Database Systems},
 editor = {Nguyen, Ngoc Thanh
and Matsuo, Tokuro
and Gaol, Ford Lumban
and Manolopoulos, Yannis
and Fujita, Hamido
and Hong, Tzung-Pei
and Wojtkiewicz, Krystian},
 isbn = {978-981-96-6008-7},
 pages = {317--330},
 publisher = {Springer Nature Singapore},
 title = {Analysis of Artificial Intelligence Solutions in Offensive Cybersecurity Domains},
 url = {https://link.springer.com/chapter/10.1007/978-981-96-6008-7_23},
 year = {2025}
}

@Inbook{Han2026,
author="Han, Bo
and Liu, Tongliang",
title="Advanced Topics in Trustworthy Machine Learning",
bookTitle="Trustworthy Machine Learning under Imperfect Data",
year="2026",
publisher="Springer Nature Singapore",
address="Singapore",
pages="165--267",
abstract="As machine learning systems gain significance in practical applications, guaranteeing their reliability across various deployment contexts has become a significant problem. This section examines three essential areas in trustworthy machine learning: federated learning, graph learning, and foundation models (FMs), each offering distinct problems and potential for developing trustworthy machine learning systems.",
isbn="978-981-96-9396-2",
doi="10.1007/978-981-96-9396-2_5",
url="https://doi.org/10.1007/978-981-96-9396-2_5"
}


@inproceedings{10.1007/978-981-96-0151-6_1,
 abstract = {In the domain of Large Vision-Language Models (LVLMs), securing these models has emerged as a critical issue for both researchers and practitioners. In this paper, we highlight and analyze the security-related issues of LVLMs, with a special emphasis on the reliability challenges in practical deployments. We begin by reviewing recent studies on threats like jailbreak and backdoor attacks, alongside discussing the potential countermeasures implemented to mitigate these risks. Additionally, we touch on real-world application problems, such as hallucinations and privacy leakages, as well as the ethical and legal related researches around them. We also outline the shortcomings observed in current studies and discuss directions for future research, with the aim of promoting LVLMs towards a safer direction. A curated list of LVLMs-security-related resources is also available at https://github.com/MingyuJ666/LVLM-Safety.},
 address = {Singapore},
 author = {Wang, Taowen
and Fang, Zheng
and Xue, Haochen
and Zhang, Chong
and Jin, Mingyu
and Xu, Wujiang
and Shu, Dong
and Yang, Shanchieh
and Wang, Zhenting
and Liu, Dongfang},
 booktitle = {Frontiers in Cyber Security},
 editor = {Chen, Biwen
and Fu, Xinwen
and Huang, Min},
 isbn = {978-981-96-0151-6},
 pages = {3--22},
 publisher = {Springer Nature Singapore},
 title = {Large Vision-Language Model Security: A Survey},
 url = {https://link.springer.com/chapter/10.1007/978-981-96-0151-6_1},
 year = {2024}
}

@Inbook{Imran2025,
author="Imran, Alishba
and Gopalakrishnan, Keerthana",
title="Foundation Models in Robotics",
bookTitle="AI for Robotics: Toward Embodied and General Intelligence in the Physical World",
year="2025",
publisher="Apress",
address="Berkeley, CA",
pages="139--210",
abstract="This chapter explores how large language models (LLMs) enable robotic planning, control, and mapping through techniques like supervised fine-tuning (SFT) and direct preference optimization (DPO). It covers transformer-based models such as SayCan and RT-1, which facilitate multimodal task execution, and diffusion-based policies for flexible action generation. The chapter also highlights security risks in AI-driven robots and emphasizes the need for robust safety measures, anomaly detection, and interpretability to ensure safe deployment.",
isbn="979-8-8688-0989-7",
doi="10.1007/979-8-8688-0989-7_4",
url="https://doi.org/10.1007/979-8-8688-0989-7_4"
}


@inproceedings{10.1007/978-3-031-78925-0_6,
 abstract = {Since its inception, ChatGPT has emerged as a potent tool in various Natural Language Processing (NLP) tasks in several domains including health care, education, Business, management, etc. In particular, the education sector is experiencing significant utilization of ChatGPT. The automated correction of code plays a crucial role in the educational process within eLearning platforms and is highly valued by programmers engaged in programming activities. In this study, we conducted an analysis of the performance of ChatGPT in the automated correction and evaluation of small Python code snippets that contain syntax errors. The performance of ChatGPT in correcting and evaluating single line code has been observed to be exceptional, with minimal errors. Nevertheless, as the number of lines of code and errors increases, the performance experiences a decline. However, it is noteworthy that the system still achieves a performance rate exceeding 90{\%} across all potential scenarios. It has been observed that ChatGPT demonstrates enhanced response quality through reinforcement learning when provided with multiple prompts, in contrast to its initial performance at the beginning of prompt execution. Furthermore, it has been observed that while ChatGPT is capable of providing corrections for specific code segments, it is not always reliable in accurately evaluating that corrected code to produce the appropriate result.},
 address = {Cham},
 author = {Ganguli, Isha
and Atreja, Satyam
and Bhasin, Sanya},
 booktitle = {Hybrid Intelligent Systems},
 editor = {Bajaj, Anu
and Madureira, Ana Maria
and Abraham, Ajith},
 isbn = {978-3-031-78925-0},
 pages = {52--61},
 publisher = {Springer Nature Switzerland},
 title = {Performance Analysis of ChatGPT in Erroneous Python Code Correction and Evaluation},
 url = {https://link.springer.com/chapter/10.1007/978-3-031-78925-0_6},
 year = {2025}
}

@Article{Picard2025,
author="Picard, Cyril
and Edwards, Kristen M.
and Doris, Anna C.
and Man, Brandon
and Giannone, Giorgio
and Alam, Md Ferdous
and Ahmed, Faez",
title="From concept to manufacturing: evaluating vision-language models for engineering design",
journal="Artificial Intelligence Review",
year="2025",
month="Jul",
day="01",
volume="58",
number="9",
pages="288",
abstract="Engineering design is undergoing a transformative shift with the advent of AI, marking a new era in how we approach product, system, and service planning. Large language models have demonstrated impressive capabilities in enabling this shift. Yet, with text as their only input modality, they cannot leverage the large body of visual artifacts that engineers have used for centuries and are accustomed to. This gap is addressed with the release of multimodal vision-language models (VLMs), such as GPT-4V, enabling AI to impact many more types of tasks. Our work presents a comprehensive evaluation of VLMs across a spectrum of engineering design tasks, categorized into four main areas: Conceptual Design, System-Level and Detailed Design, Manufacturing and Inspection, and Engineering Education Tasks. Specifically in this paper, we assess the capabilities of two VLMs, GPT-4V and LLaVA 1.6 34B, in design tasks such as sketch similarity analysis, CAD generation, topology optimization, manufacturability assessment, and engineering textbook problems. Through this structured evaluation, we not only explore VLMs' proficiency in handling complex design challenges but also identify their limitations in complex engineering design applications. Our research establishes a foundation for future assessments of vision language models. It also contributes a set of benchmark testing datasets, with more than 1000 queries, for ongoing advancements and applications in this field.",
issn="1573-7462",
doi="10.1007/s10462-025-11290-y",
url="https://doi.org/10.1007/s10462-025-11290-y"
}


@inproceedings{10.1007/978-3-031-95452-8_16,
 abstract = {Cyber attacks are increasingly emerging as problems. They are caused not only by technological aspects but also by human factors that are often overlooked during the design of interactive systems. Reports by cybersecurity giants such as IBM and Verizon have shown that up to 95{\%} of security incidents result from human error. This phenomenon is dramatically amplified in contexts such as public administrations, which often lack the financial and human resources to defend themselves against cyber attacks. To address this issue, this paper presents a web platform called DAMOCLES that aims to support the digital defense of Italian public administrations through human factor assessments related to cyber incidents and the mitigation of emerging vulnerabilities. In particular, this paper illustrates the EUD techniques used in DAMOCLES to facilitate the creation of ethical phishing campaigns, which serve as a tool within the platform to assess the vulnerabilities of organization's employees.},
 address = {Cham},
 author = {Breve, Bernardo
and Buono, Paolo
and Caruccio, Loredana
and Cau, Federico Maria
and Cimino, Gaetano
and Desolda, Giuseppe
and Deufemia, Vincenzo
and Lanzilotti, Rosa
and Spano, Lucio Davide
and Tucci, Cesare},
 booktitle = {End-User Development},
 editor = {Santoro, Carmen
and Schmidt, Albrecht
and Matera, Maristella
and Bellucci, Andrea},
 isbn = {978-3-031-95452-8},
 pages = {264--282},
 publisher = {Springer Nature Switzerland},
 title = {Leveraging EUD and Generative AI for Ethical Phishing Campaigns},
 url = {https://link.springer.com/chapter/10.1007/978-3-031-95452-8_16},
 year = {2025}
}

@Article{Huang2025,
author="Huang, Yiping
and Huang, Shixin
and Chen, Xiangjian",
title="Predictive model on employee stock ownership impacting corporate performance",
journal="Scientific Reports",
year="2025",
month="Jul",
day="01",
volume="15",
number="1",
pages="22133",
abstract="Enterprises in the context of smart manufacturing face great challenges in terms of human capital strategies as well as incentive mechanisms. Employee Stock Ownership Plans (ESOPs) is one of the key incentive mechanisms with long-term oriented function, but due to the lack of relevant explanations in the context of smart manufacturing, the mechanism of the dynamic impact of ESOPs on corporate performance has not yet been elucidated. In this study, with the idea of combining AI and accounting, we constructed a prediction model of the impact of ESOPs on enterprise performance that integrates language modeling and social sentiment mass data analysis, and introduced the prediction model to analyze the long-term, dynamic and nonlinear impact of ESOPs on enterprises; finally, we constructed an explainable AI (XAI) based on the LSTM model, and used the SHAP value method to explain the impact of ESOPs on enterprise performance. Finally, the Explainable AI (XAI) is built based on the LSTM model, and the SHAP value method is used to downsize the performance of the complex black box model LSTM, present the model ``black box'', and analyze the common roles played by the elements of ESOPs, the maturity level of smart manufacturing, and the social sentiment on ESOPs in the long term and nonlinear process. Aiming at the above research problems and shortcomings, the main contributions of this paper include: analyzing the dynamic evolution path of ESOP effectiveness from the perspective of intelligent transformation of manufacturing enterprises; predicting the ESOP effectiveness of enterprises through multi-source heterogeneous data (financial data, social sentiment data, operation data) and advanced AI models (LSTM, LLM), and proposing new prediction tools and prediction theories; using XAI technology to realize ESOP effectiveness; and using XAI technology to realize ESOP effectiveness in the long term and non-linear process. Theory; the use of XAI technology to achieve ESOP incentive effect attribution analysis, for management accounting decision support to provide a new dimension of interpretation, which can be used as a research on ESOP dynamic incentive evaluation, integration of non-financial information, predictive analysis of new perspectives for the field of accounting to develop a new research direction, and for the transformation of intelligent manufacturing design and optimization of ESOP to provide empirical data basis and decision support. The study also provides empirical data basis and decision support for the design and optimization of ESOPs during the transformation of smart manufacturing.",
issn="2045-2322",
doi="10.1038/s41598-025-06280-7",
url="https://doi.org/10.1038/s41598-025-06280-7"
}


@Article{Rahman2025,
author="Rahman, Md Rayhanur
and Wroblewski, Brandon
and Matthews, Quinn
and Morgan, Brantley
and Menzies, Timothy
and Williams, Laurie",
title="Mining temporal attack patterns from cyberthreat intelligence reports",
journal="Knowledge and Information Systems",
year="2025",
month="Oct",
day="01",
volume="67",
number="10",
pages="8941--8981",
abstract="Cyberthreat intelligence (CTI) reports on past cyberattacks describe the sequence of actions of attackers in terms of time. The sequence contains temporal relations among attack actions, such as a malware is first downloaded and then executed. Information related to temporal relations enables cybersecurity practitioners to investigate past cyberattack incidents and analyze attackers' behavior. However, cybersecurity practitioners must extract such information automatically, in a structured manner, through a common vocabulary to reduce human effort and enable sharing, and collaboration. The goal of this paper is to aid security practitioners in proactive defense against attacks by automatic information extraction of temporal relations among attack actions from cyberthreat intelligence reports. We propose ChronoCTI, an automated pipeline for extracting temporal relations among attack actions from CTI reports. The attack actions are represented as MITRE ATT{\&}CK techniques, and the relations are represented as a knowledge graph. To construct ChronoCTI, we build a ground truth dataset of temporal relations and apply large language models, natural language processing, and machine learning techniques. ChronoCTI demonstrates higher precision but lower recall performance on a real-world dataset of 94 CTI reports. We apply ChronoCTI on a set of 713 CTI reports, where we identify 9 categories of temporal attack patterns consisting of 124 temporal attack patterns. We identify that the most prevalent pattern category is to trick victim users into executing malicious code to initiate the attack, followed by bypassing the anti-malware system in the victim software systems. Based on the observed patterns, we advocate for training users about cybersecurity best practices, introducing appropriate warning messages for end-users, introducing immutable operating systems, and enforcing multi-user authentications. Moreover, we advocate that practitioners leverage the automated mining capability of ChronoCTI and design countermeasures against recurring attack patterns.",
issn="0219-3116",
doi="10.1007/s10115-025-02491-6",
url="https://doi.org/10.1007/s10115-025-02491-6"
}


@Article{Abdelzaher2025,
author="Abdelzaher, Esra
and Alshammari, Shahd",
title="Learning from Machinery Misspeaks: Cognitive Basis of Ableism and Bias in AI-Generated Content",
journal="Corpus Pragmatics",
year="2025",
month="Nov",
day="25",
volume="10",
number="1",
pages="8",
abstract="This study investigates whether AI's representation of People With Disability (PWD) in different social contexts are biased. We performed a corpus-based cognitive-linguistic analysis of Arabic and English AI-generated corpora. The cognitive linguistic analysis revealed a judgmental framing of disability and an inherent inequality in the conceptual metaphorization of ability and physical strength, while the corpus-based analysis of AI-generated content showed a systematic use of linguistic markers that constructed identities of PWD characterized by vulnerability, which were generally negative and evocative of bodily and medical frames. We detected clusters of biases in the representations of PWD, which inclined towards positivity in the English texts describing males from American or European societies than in the Arabic texts describing females. We argue that despite socio-cultural differences in the depictions of PWD, the same cognitive-linguistic strategies, mainly conceptual metaphors and semantic frames, can be used to construct counter-narratives to the current biased ones.",
issn="2509-9515",
doi="10.1007/s41701-025-00216-2",
url="https://doi.org/10.1007/s41701-025-00216-2"
}


@Article{Godoy2025,
author="Godoy, Javier
and Torres, Eden
and Galeotti, Juan P.
and Garbervetsky, Diego
and Uchitel, Sebastian",
title="Automated construction of predicate abstractions for smart contract validation",
journal="Software and Systems Modeling",
year="2025",
month="Oct",
day="27",
abstract="Verification and validation of smart contracts is crucial, as these programs administer valuable assets. Indeed, there is a proliferation of companies, specialized in smart contract audits that has become a key component of the blockchain ecosystem. Current verification and validation practice is heavily reliant on the insight and experience of auditors. However, there is an increasing rich toolbox that supports their work. In this paper, we show how predicate abstraction can be leveraged to construct models that support smart contract auditing. These models provide auditors with a systematic approach to analyzing and validating the behavior of smart contracts at the function call level. By proposing predicates, auditors can generate alternative perspectives on a contract's behavior, refine transitions to investigate specific scenarios, and incorporate transient states into the abstraction to uncover reentrancy vulnerabilities. We formally define predicate abstractions for smart contracts and introduce default predicates to support initial exploration. We also present PASCo, a tool that automatically builds an abstract finite state automaton from a set of predicates and a smart contract implementation. We evaluate PASCo on established benchmarks and with an experienced smart contract auditor. Furthermore, we compare its effectiveness in detecting reentrancy vulnerabilities (measured in terms of precision and recall) against 14 state-of-the-art Solidity vulnerability detectors.",
issn="1619-1374",
doi="10.1007/s10270-025-01333-x",
url="https://doi.org/10.1007/s10270-025-01333-x"
}


@inproceedings{10.1007/978-3-031-53022-7_5,
 abstract = {The emergence of Large Language Models and their deployment in systems such as ChatGPT are poised to have a major impact on STEM education, particularly Computer Science. These generative large language models can produce program code as well as human language output. This has potentially serious implications for computer science programs and pedagogy. This work provides a qualitative assessment sample code generated by ChatGPT, as an example of an LLM explores implications for computing pedagogy....},
 address = {Cham},
 author = {Wolfer, James},
 booktitle = {Towards a Hybrid, Flexible and Socially Engaged Higher Education},
 editor = {Auer, Michael E.
and Cukierman, Uriel R.
and Vendrell Vidal, Eduardo
and Tovar Caro, Edmundo},
 isbn = {978-3-031-53022-7},
 pages = {43--53},
 publisher = {Springer Nature Switzerland},
 title = {A Qualitative Assessment of ChatGPT Generated Code in the Computer Science Curriculum},
 url = {https://link.springer.com/chapter/10.1007/978-3-031-53022-7_5},
 year = {2024}
}

@Inbook{Burgess2024,
author="Burgess, Andrew",
title="AI in Action",
bookTitle="The Executive Guide to Artificial Intelligence: Cutting Through the Hype - How to get the most from AI in your Organization",
year="2024",
publisher="Springer International Publishing",
address="Cham",
pages="61--73",
abstract="This chapter discusses the use of Artificial Intelligence (AI) in business, focusing on three main areas: enhancing customer service, optimising processes, and generating insights. AI enhances customer service through chatbots, recommendation engines, and image recognition, leading to improved customer satisfaction and revenue generation. It optimises processes by transforming unstructured data into structured data, using capabilities like Image Recognition, Voice Recognition, and Natural Language Processing (NLP). This leads to cost reduction, cost avoidance, and improved compliance. AI generates insights by creating new sources of value from existing data, enabling better, more consistent, and faster decisions. This can help a company mitigate risks, reduce unnecessary losses, and minimise revenue leakage. The author provides examples from various industries, including retail, banking, insurance, legal, telecom, museums, and social housing, demonstrating the wide applicability of AI in different sectors.",
isbn="978-3-031-50722-9",
doi="10.1007/978-3-031-50722-9_5",
url="https://doi.org/10.1007/978-3-031-50722-9_5"
}


@inproceedings{10.1007/978-981-99-8024-6_17,
 abstract = {Security incidents and data breaches are increasing rapidly, and only a fraction of them is being reported. Public vulnerability databases, e.g., national vulnerability database (NVD) and common vulnerability and exposure (CVE), have been leading the effort in documenting vulnerabilities and sharing them to aid defenses. Both are known for many issues, including brief vulnerability descriptions. Those descriptions play an important role in communicating the vulnerability information to security analysts in order to develop the appropriate countermeasure. Many resources provide additional information about vulnerabilities, however, they are not utilized to boost public repositories. In this paper, we devise a pipeline to augment vulnerability description through third party reference (hyperlink) scrapping. To normalize the description, we build a natural language summarization pipeline utilizing a pretrained language model that is fine-tuned using labeled instances and evaluate its performance against both human evaluation (golden standard) and computational metrics, showing initial promising results in terms of summary fluency, completeness, correctness, and understanding.},
 address = {Singapore},
 author = {Althebeiti, Hattan
and Mohaisen, David},
 booktitle = {Information Security Applications},
 editor = {Kim, Howon
and Youn, Jonghee},
 isbn = {978-981-99-8024-6},
 pages = {213--227},
 publisher = {Springer Nature Singapore},
 title = {Enriching Vulnerability Reports Through Automated and Augmented Description Summarization},
 url = {https://link.springer.com/chapter/10.1007/978-981-99-8024-6_17},
 year = {2024}
}

@Article{Chen2024,
author="Chen, Lei
and Chen, Yiqi
and Chu, Zhufei
and Fang, Wenji
and Ho, Tsung-Yi
and Huang, Ru
and Huang, Yu
and Khan, Sadaf
and Li, Min
and Li, Xingquan
and Li, Yu
and Liang, Yun
and Liu, Jinwei
and Liu, Yi
and Lin, Yibo
and Luo, Guojie
and Pan, Hongyang
and Shi, Zhengyuan
and Sun, Guangyu
and Tsaras, Dimitrios
and Wang, Runsheng
and Wang, Ziyi
and Wei, Xinming
and Xie, Zhiyao
and Xu, Qiang
and Xue, Chenhao
and Yan, Junchi
and Yang, Jun
and Yu, Bei
and Yuan, Mingxuan
and Young, Evangeline F. Y.
and Zeng, Xuan
and Zhang, Haoyi
and Zhang, Zuodong
and Zhao, Yuxiang
and Zhen, Hui-Ling
and Zheng, Ziyang
and Zhu, Binwu
and Zhu, Keren
and Zou, Sunan",
title="Large circuit models: opportunities and challenges",
journal="Science China Information Sciences",
year="2024",
month="Sep",
day="25",
volume="67",
number="10",
pages="200402",
abstract="Within the electronic design automation (EDA) domain, artificial intelligence (AI)-driven solutions have emerged as formidable tools, yet they typically augment rather than redefine existing methodologies. These solutions often repurpose deep learning models from other domains, such as vision, text, and graph analytics, applying them to circuit design without tailoring to the unique complexities of electronic circuits. Such an ``AI4EDA'' approach falls short of achieving a holistic design synthesis and understanding, overlooking the intricate interplay of electrical, logical, and physical facets of circuit data. This study argues for a paradigm shift from AI4EDA towards AI-rooted EDA from the ground up, integrating AI at the core of the design process. Pivotal to this vision is the development of a multimodal circuit representation learning technique, poised to provide a comprehensive understanding by harmonizing and extracting insights from varied data sources, such as functional specifications, register-transfer level (RTL) designs, circuit netlists, and physical layouts. We champion the creation of large circuit models (LCMs) that are inherently multimodal, crafted to decode and express the rich semantics and structures of circuit data, thus fostering more resilient, efficient, and inventive design methodologies. Embracing this AI-rooted philosophy, we foresee a trajectory that transcends the current innovation plateau in EDA, igniting a profound ``shift-left'' in electronic design methodology. The envisioned advancements herald not just an evolution of existing EDA tools but a revolution, giving rise to novel instruments of design-tools that promise to radically enhance design productivity and inaugurate a new epoch where the optimization of circuit performance, power, and area (PPA) is achieved not incrementally, but through leaps that redefine the benchmarks of electronic systems' capabilities.",
issn="1869-1919",
doi="10.1007/s11432-024-4155-7",
url="https://doi.org/10.1007/s11432-024-4155-7"
}


@Article{Shrestha2023,
author="Shrestha, Yash Raj
and von Krogh, Georg
and Feuerriegel, Stefan",
title="Building open-source AI",
journal="Nature Computational Science",
year="2023",
month="Nov",
day="01",
volume="3",
number="11",
pages="908--911",
abstract="Artificial intelligence (AI) drives innovation across society, economies and science. We argue for the importance of building AI technology according to open-source principles to foster accessibility, collaboration, responsibility and interoperability.",
issn="2662-8457",
doi="10.1038/s43588-023-00540-0",
url="https://doi.org/10.1038/s43588-023-00540-0"
}


@Article{Sergeyuk2026,
author="Sergeyuk, Agnia
and Zakharov, Ilya
and Koshchenko, Ekaterina
and Izadi, Maliheh",
title="Human-AI experience in integrated development environments: a systematic literature review",
journal="Empirical Software Engineering",
year="2026",
month="Jan",
day="03",
volume="31",
number="3",
pages="55",
abstract="The integration of Artificial Intelligence (AI) into Integrated Development Environments (IDEs) is reshaping software development, fundamentally altering how developers interact with their tools. This shift marks the emergence of Human-AI Experience in Integrated Development Environment (in-IDE HAX), a field that explores the evolving dynamics of Human-Computer Interaction in AI-assisted coding environments. Despite rapid adoption, research on in-IDE HAX remains fragmented, which highlights the need for a unified overview of current practices, challenges, and opportunities. To provide a structured overview of existing research, we conduct a systematic literature review of 90 studies, summarizing current findings and outlining areas for further investigation. We organize key insights from reviewed studies into three aspects: Impact, Design, and Quality of AI-based systems inside IDEs. Impact findings show that AI-assisted coding enhances developer productivity but also introduces challenges, such as verification overhead and over-reliance. Design studies show that effective interfaces surface context, provide explanations and transparency of suggestion, and support user control. Quality studies document risks in correctness, maintainability, and security. For future research, priorities include productivity studies, design of assistance, and audit of AI-generated code. The agenda calls for larger and longer evaluations, stronger audit and verification assets, broader coverage across the software life cycle, and adaptive assistance under user control.",
issn="1573-7616",
doi="10.1007/s10664-025-10793-0",
url="https://doi.org/10.1007/s10664-025-10793-0"
}


@inproceedings{10.1007/978-981-96-7008-6_27,
 abstract = {Federated learning facilitates collaborative model training across decentralized data sources, enhancing personalized natural language understanding for real-world applications. However, its distributed nature increases the attack surfaces, posing new challenges in ensuring the security and integrity of language models against textual backdoor attacks. This paper introduces a federated fine-tuning approach for the Joint Intent Detection and Slot Prediction (JIDSP) task in heterogeneous environments, which is a crucial part of personalized task-oriented dialog systems. Additionally, we perform a detailed investigation of visible or invisible textual backdoor attacks on JIDSP, which has not yet been explored in the literature. Extensive experiments on the benchmark ATIS, SNIPS, and MASSIVE datasets reveal that a malicious participant can manipulate the global model by injecting visible or invisible backdoor patterns, compromising the performance of the global JIDSP task. We assess the success rates and stealthiness of these attacks against the ONION defense mechanism in terms of perplexity and semantic similarity metrics, finding invisible triggers as effective as visible ones. Our results highlight the stealthy nature of textual backdoors in federated settings and the critical need for robust defenses in to mitigate these threats.},
 address = {Singapore},
 author = {Verma, Mridula
and Babu, Shimil S.},
 booktitle = {Neural Information Processing},
 editor = {Mahmud, Mufti
and Doborjeh, Maryam
and Wong, Kevin
and Leung, Andrew Chi Sing
and Doborjeh, Zohreh
and Tanveer, M.},
 isbn = {978-981-96-7008-6},
 pages = {371--386},
 publisher = {Springer Nature Singapore},
 title = {Silent Intruders: Dissecting Textual Backdoor Attacks in Federated Dialog Systems},
 url = {https://link.springer.com/chapter/10.1007/978-981-96-7008-6_27},
 year = {2026}
}

@Article{Sarwatt2026,
author="Sarwatt, Doreen Sebastian
and Kulwa, Frank
and Philipo, Adamu Gaston
and Runyoro, Angela-Aida Karugila
and Ning, Huansheng
and Ding, Jianguo",
title="Aigc-driven human-machine intelligence in ITS: technologies, applications, evaluation framework, challenges, and future directions",
journal="Artificial Intelligence Review",
year="2026",
month="Jan",
day="05",
volume="59",
number="2",
pages="75",
abstract="This paper explores the integration of Artificial Intelligence Generated Content (AIGC), a rapidly evolving branch of generative AI, with Human-Machine intelligence (HMI) to enhance the functionality of Intelligent Transportation Systems (ITS). As transportation systems grow increasingly complex, adaptive decision-making becomes essential for interpreting vast streams of real-time data from vehicles, infrastructure, and users. AIGC plays a transformative role in optimizing traffic flow through dynamic routing and real-time traffic management, while human intelligence ensures these systems remain responsive to evolving real-world conditions. For safety, AIGC is used to simulate complex driving scenarios for autonomous vehicle training and detect traffic anomalies, with human oversight providing contextual decisions in ambiguous situations. For sustainability, AIGC supports data-driven strategies to reduce emissions and energy use, while human expertise ensures alignment with ethical and environmental goals. This synergy enhances real-time decision-making, improving both accuracy and adaptability across ITS scenarios. The paper presents a comprehensive review of core and supporting AIGC technologies and their applications across key ITS domains. Case studies and initiatives from industry leaders demonstrate practical implementations of AIGC-driven HMI collaboration. To guide future deployments, we propose a conceptual five-layer evaluation framework for assessing AIGC-HMI systems, encompassing functional performance, human interaction, explainability, ethical compliance, and robustness. We also address challenges such as legacy system integration, data privacy, model bias, and scalability. The paper concludes by outlining future research directions, emphasizing the need for scalable, interpretable, and ethically aligned AIGC models. This work contributes to the development of intelligent, adaptive, and trustworthy transportation systems.",
issn="1573-7462",
doi="10.1007/s10462-025-11467-5",
url="https://doi.org/10.1007/s10462-025-11467-5"
}


@inproceedings{10.1007/978-3-031-50385-6_7,
 abstract = {Edge-cloud system aims to reduce the processing time of Big data by bringing massive infrastructures closer to the source of data. Infrastructure as Code (IaC) supports the automatic deployment and management of these infrastructures through reusable code, and Ansible is the most popular IaC tool. As the quality of Ansible script directly influences the quality of Edge-cloud system, many researchers have studied improving the quality of Ansible scripts. However, there has yet to be an attempt to leverage the power of ChatGPT. Thus, we study to explore the feasibility of ChatGPT to improve the quality of Ansible scripts. Three raters evaluate ChatGPT's code recommendation ability on 48 code revision cases from 25 Ansible project GitHub repositories, and we analyze the rating results. As a result, we can confirm that ChatGPT can recognize and understand Ansible script. However, its ability largely depends on how to user formulates the questions. Thus, we can confirm the need for prompt engineering for ChatGPT to acquire stable code recommendation results.},
 address = {Cham},
 author = {Kwon, Sunjae
and Lee, Sungu
and Kim, Taehyoun
and Ryu, Duksan
and Baik, Jongmoon},
 booktitle = {Current Trends in Web Engineering},
 editor = {Casteleyn, Sven
and Mikkonen, Tommi
and Garc{\'i}a Sim{\'o}n, Alberto
and Ko, In-Young
and Loseto, Giuseppe},
 isbn = {978-3-031-50385-6},
 pages = {75--83},
 publisher = {Springer Nature Switzerland},
 title = {Exploring the Feasibility of ChatGPT for Improving the Quality of Ansible Scripts in Edge-Cloud Infrastructures Through Code Recommendation},
 url = {https://link.springer.com/chapter/10.1007/978-3-031-50385-6_7},
 year = {2024}
}

@Inbook{Rajpoot2024,
author="Rajpoot, Abha Kiran
and Agrawal, Gaurav",
editor="Raj, Pethuru
and Rocha, Alvaro
and Singh, Simar Preet
and Dutta, Pushan Kumar
and Sundaravadivazhagan, B.",
title="Synergistic Fusion: Vision-Language Models in Advancing Autonomous Driving and Intelligent Transportation Systems",
bookTitle="Building Embodied AI Systems: The Agents, the Architecture Principles, Challenges, and Application Domains",
year="2024",
publisher="Springer Nature Switzerland",
address="Cham",
pages="205--221",
abstract="In recent years, combining visualization and language models has opened many possibilities. Control and Intelligent Traffic Investigation (ITS) is a very useful application. This article examines the role and importance of integrating visual and linguistic models to increase the efficiency and safety of traffic management and improve traffic outcomes. Integrating vision and language models has been proven to increase the performance and safety of autonomous vehicles and improve traffic and transportation. Perception and decision processes in autonomous vehicles. Leveraging the power of both methods, these models help gain a deeper understanding of the environment by interpreting multimodal information such as images, videos, and text documents. Thanks to image recognition, object recognition and natural language understanding, these machines can understand traffic conditions, road signs, pedestrian behavior, technology, and contextual information, enabling safe and reliable autonomous navigation. In addition, the use of visual language models includes a wide range of intelligent traffic in addition to the intelligent car. By analyzing and interpreting various streams of data from various sources, including data collected from cameras, sensors and city information systems, social media and databases, these models help with real-time traffic management, congestion forecasting and monitoring and optimization methods. Their ability to transmit information from different components allows vehicles to respond to changing conditions, improving safety and efficiency. This article provides an overview of the current advances, approaches and challenges in the development and application of visual modeling in the automotive industry and intelligent transportation systems. Explores the integration of computer vision and powerful language processing techniques and discusses their impact on improving comprehension, decision making, and overall performance. The need to be strong, meaningful, and able to improve the nature of driving and traffic capital according to principles is also important in future guidance and research. In short, it can be stated that the fusion model of vision and language has a lot of promise in the development of driving and intelligent driving. These models combine the understanding and interpretation of multimodal information, resulting in safer, more efficient, and more flexible transportation, bringing us closer to a future of intelligent and more connected people. Finally, we discuss issues and research gaps in depth paper the goal is to provide researchers with current work and the future Trends in VLM in AD and ITS.",
isbn="978-3-031-68256-8",
doi="10.1007/978-3-031-68256-8_9",
url="https://doi.org/10.1007/978-3-031-68256-8_9"
}


@Article{Abid2024,
author="Abid, Shamsa
and Cai, Xuemeng
and Jiang, Lingxiao",
title="Measuring model alignment for code clone detection using causal interpretation",
journal="Empirical Software Engineering",
year="2024",
month="Dec",
day="19",
volume="30",
number="2",
pages="46",
abstract="Deep Neural Network-based models have demonstrated high accuracy for semantic code clone detection. However, the lack of generalization poses a threat to the trustworthiness and reliability of these models. Furthermore, the black-box nature of these models makes interpreting the model's decisions very challenging. Currently, there is only a limited understanding of the semantic code clone detection behavior of existing models. There is a lack of transparency in understanding how a model identifies semantic code clones and the exact code components influencing its prediction. In this paper, we introduce the use of a causal interpretation framework based on the Neyman-Rubin causal model to gain insight into the decision-making of four state-of-the-art clone detection models. Using the causal interpretation framework, we derive causal explanations of models' decisions by performing interventions guided by expert-labeled data. We measure the alignment of models' decision-making with expert intuition by evaluating the causal effects of code similarities and differences on the clone predictions of the models. Additionally, we evaluate the similarity intuition alignment, robustness to confounding influences, and prediction consistency of the models. Finally, we rank the models in order of most aligned and thus most reliable to least aligned and thus least reliable for semantic code clone detection. Our contributions lay a foundation for building and evaluating trustworthy semantic code clone detection systems.",
issn="1573-7616",
doi="10.1007/s10664-024-10583-0",
url="https://doi.org/10.1007/s10664-024-10583-0"
}


@inproceedings{10.1007/978-981-97-7707-5_29,
 abstract = {Since the Turing Test was proposed in the 1950s, humanity began exploring artificial intelligence, with an aim to bridge the interaction gap between machines and human language. This exploration enables machines to comprehend how humans acquire, produce, and understand language, as well as the relationship between linguistic expression and the world. The paper explores the basic principles of natural language representation, the formalization of natural language, and the modeling methods of language models. The paper analyzes, summarizes and compares the mainstream technologies and methods, including vector space-based, topic model-based, graph-based, and neural network-based approaches. And how to improve the development trend and direction of language model understanding ability is predicted and further discussed.},
 address = {Singapore},
 author = {Liu, Yuanrui
and Zhou, Jingping
and Sang, Guobiao
and Huang, Ruilong
and Zhao, Xinzhe
and Fang, Jintao
and Wang, Tiexin
and Li, Bohan},
 booktitle = {Web Information Systems and Applications},
 editor = {Jin, Cheqing
and Yang, Shiyu
and Shang, Xuequn
and Wang, Haofen
and Zhang, Yong},
 isbn = {978-981-97-7707-5},
 pages = {331--363},
 publisher = {Springer Nature Singapore},
 title = {The Journey of Language Models in Understanding Natural Language},
 url = {https://link.springer.com/chapter/10.1007/978-981-97-7707-5_29},
 year = {2024}
}

@Article{Zia2025,
author="Zia, Muhammad Umer
and Xiang, Wei
and Huang, Tao
and Ahmad, Jameel
and Chattha, Jawwad Nasar
and Naqvi, Ijaz Haider
and Butt, Faran Awais",
title="Unifying ground and air: a comprehensive review of deep learning-enabled CAVs and UAVs",
journal="Artificial Intelligence Review",
year="2025",
month="Nov",
day="20",
volume="59",
number="1",
pages="19",
abstract="The tremendous advancements in artificial intelligence (AI) techniques, particularly those pertinent to computer vision and image recognition, are revolutionizing the automotive industry towards the development of intelligent transportation systems for smart cities. Integrating AI techniques into connected autonomous vehicles (CAVs) and unmanned aerial vehicles (UAVs) and their data fusion, enables a new paradigm that allows for unparalleled real-time awareness of the surrounding environment. The potential of emerging wireless technologies can be fully exploited by establishing communication and cooperation among AI-augmented CAVs and UAVs. However, configuring appropriate deep learning (DL) models for connected vehicles is a complex task. Any errors can result in severe consequences, including loss of vehicles, infrastructure, and human lives. These systems are also susceptible to cyber attacks, necessitating a thorough and timely threat analysis and countermeasures to prevent catastrophic events. Our findings highlight the effectiveness of AI-driven data fusion in enhancing cooperative perception between CAVs and UAVs, identify security vulnerabilities in DL-based systems, and demonstrate how V2X-enabled UAVs can significantly improve situational awareness in corner cases.",
issn="1573-7462",
doi="10.1007/s10462-025-11425-1",
url="https://doi.org/10.1007/s10462-025-11425-1"
}


@Inbook{Reddy2023,
author="Reddy, Sreedhar",
title="Democratized Hyper-automated Software Development",
bookTitle="The AI-Enabled Enterprise",
year="2023",
publisher="Springer International Publishing",
address="Cham",
pages="85--100",
abstract="Modern enterprises are evolving into a complex system of systems that need to deliver stated goals while operating in a dynamic and uncertain environment. Given the ever-increasing pervasiveness of software, enterprises are relying heavily on software systems to address a variety of adaptive needs at strategy, process and system levels. The demand-supply situation for trained software developers is already skewed, and the skew is likely to increase further with time. Recent advances in AI techniques in general and Generative AI in particular may lead to a promising solution wherein Subject Matter Experts are empowered to play a greater and more direct role in software development. This chapter motivates the need, proposes a pragmatic line of attack and discusses technology enablers to support this line of attack.",
isbn="978-3-031-29053-4",
doi="10.1007/978-3-031-29053-4_5",
url="https://doi.org/10.1007/978-3-031-29053-4_5"
}


@Inbook{Panichella2024,
author="Panichella, Sebastiano",
editor="Kucharavy, Andrei
and Plancherel, Octave
and Mulder, Valentin
and Mermoud, Alain
and Lenders, Vincent",
title="Enhancing Security Awareness and Education for LLMs",
bookTitle="Large Language Models in Cybersecurity: Threats, Exposure and Mitigation",
year="2024",
publisher="Springer Nature Switzerland",
address="Cham",
pages="165--173",
abstract="Large Language Models (LLMs) have gained widespread use in multiple applications, making end-user education and training a vital security component. Education involves creating awareness of the security concerns related to LLMs, such as data privacy concerns, bias, and cyberattacks, to encourage ethical and responsible use. Training can teach users to detect and mitigate security threats, configure security settings, and perform regular system updates to prevent vulnerabilities.",
isbn="978-3-031-54827-7",
doi="10.1007/978-3-031-54827-7_18",
url="https://doi.org/10.1007/978-3-031-54827-7_18"
}


@Inbook{Sikos2025,
author="Sikos, Leslie F.",
title="Securing GenAI Deployments and Preventing Misuse",
bookTitle="Generative AI in Cybersecurity",
year="2025",
publisher="Springer Nature Switzerland",
address="Cham",
pages="55--66",
abstract="The misuse of generative AI, whether via unsecure or adversarial prompts, can lead to information leakage and inappropriate responses, and can trigger unintended behavior. Therefore, facilitating generative AI adoption requires managed, secured access mechanisms and protection against data loss and misuse. Considering the complexity of generative AI use, multilayered countermeasures are required, such as limiting the topics for questions, verifying inputs (including with a generative AI tool different from the one used), implementing malicious prompts to recognize, and monitoring suspicious behavior of the generative AI system and the infrastructure that communicates with it.",
isbn="978-3-032-05250-6",
doi="10.1007/978-3-032-05250-6_4",
url="https://doi.org/10.1007/978-3-032-05250-6_4"
}


@Inbook{Singh2025,
author="Singh, Akansha
and Singh, Krishna Kant",
title="Capstone Projects and Future Roadmap with R for Generative AI",
bookTitle="Generative AI in R: Transforming Data Science with Synthetic Data and Advanced Modeling Techniques",
year="2025",
publisher="Apress",
address="Berkeley, CA",
pages="475--530",
abstract="Generative AI has opened new frontiers in data simulation and creative content generation across domains. In R, packages like keras and torch now enable building and training generative models for a variety of data types. This chapter explores five case studies of Generative AI applications in R---spanning healthcare, finance, education, design, and agriculture---each with practical R code examples, model explanations, and sample results. We demonstrate how deep generative models (e.g., Variational Autoencoders, GANs, and diffusion models) can be implemented in R to synthesize realistic data or content. Short, focused sections provide background on each use case and why generative modeling is suitable, followed by detailed R code and discussion of model architecture, evaluation techniques, and example outputs. By the end of this chapter, a reader should grasp how to apply Generative AI methods in R for diverse projects, understand the underlying model structures, and evaluate the quality of generated outputs.",
isbn="979-8-8688-1763-2",
doi="10.1007/979-8-8688-1763-2_10",
url="https://doi.org/10.1007/979-8-8688-1763-2_10"
}


@Inbook{Panichella2024,
author="Panichella, Sebastiano",
editor="Kucharavy, Andrei
and Plancherel, Octave
and Mulder, Valentin
and Mermoud, Alain
and Lenders, Vincent",
title="Vulnerabilities Introduced by LLMs Through Code Suggestions",
bookTitle="Large Language Models in Cybersecurity: Threats, Exposure and Mitigation",
year="2024",
publisher="Springer Nature Switzerland",
address="Cham",
pages="87--97",
abstract="Code suggestions from generative language models like ChatGPT contain vulnerabilities as they often rely on older code and programming practices, over-represented in the older code libraries the LLMs rely on for their coding abilities. Advanced attackers can leverage this by injecting code with known but hard-to-detect vulnerabilities in the training datasets. Mitigation can include user education and engineered safeguards such as LLMs trained for vulnerability detection or rule-based checking of codebases. Analysis of LLMs' code generation capabilities, including formal verification and source training dataset (code-comment pairs) analysis, is necessary for effective vulnerability detection and mitigation.",
isbn="978-3-031-54827-7",
doi="10.1007/978-3-031-54827-7_9",
url="https://doi.org/10.1007/978-3-031-54827-7_9"
}


@Inbook{Walther2024,
author="Walther, Cornelia C.",
title="WHY: Perspective: POZE---A Multidisciplinary Framework of Life",
bookTitle="Human Leadership for Humane Technology: The New AI: Agency Ignited",
year="2024",
publisher="Springer Nature Switzerland",
address="Cham",
pages="1--101",
abstract="This foundational chapter introduces the multidisciplinary POZE framework to analyze the interplay of humans and technology. The m4-matrix is offered to map out the arenas where transformative changes are occurring at the intersection of individual (aspiration, emotions, thoughts, sensations) and societal (individual, community, country, planet) dimensions. The argument is made that a mindset of agency amid AI, across generations, cultures, and socioeconomic backgrounds can transform technology into an ally for humankind. The A-Frame is introduced to cultivate such agency intentionally, which involves awareness of interplays and appreciation of individual vulnerability, requiring acceptance of the unique opportunity that humans have to create the reality they want, and accountability for the responsibility that comes with that privilege. Combined these attitudes result in the type of leadership mindset that is needed to promote and achieve agency amid AI. As it will be explained, leadership should be approached as a way of life, independent of hierarchy; perceived as a path that is driven by values. If it is framed in such a manner it opens a path for anyone to rise beyond personal interest to the bigger picture perspective of the Common Good, which is a pragmatic perspective considering the interconnected nature of everything. We look at the Spiral of influence which allows humans to trigger change, and the potential of technology to support them. Everyone can win if everyone has a fair chance to play; this book shows possible ways to achieve this. The binomen of GIGO (Garbage in, Garbage out) versus VIVO (Values in, Values out) is introduced as the central tension of the relationship that humans entertain with technology; it reflects the argument that humans cannot expect the technology of tomorrow to reflect values that they do not manifest today.",
isbn="978-3-031-67823-3",
doi="10.1007/978-3-031-67823-3_1",
url="https://doi.org/10.1007/978-3-031-67823-3_1"
}


@Article{Hussain2024,
author="Hussain, Zak
and Binz, Marcel
and Mata, Rui
and Wulff, Dirk U.",
title="A tutorial on open-source large language models for behavioral science",
journal="Behavior Research Methods",
year="2024",
month="Dec",
day="01",
volume="56",
number="8",
pages="8214--8237",
abstract="Large language models (LLMs) have the potential to revolutionize behavioral science by accelerating and improving the research cycle, from conceptualization to data analysis. Unlike closed-source solutions, open-source frameworks for LLMs can enable transparency, reproducibility, and adherence to data protection standards, which gives them a crucial advantage for use in behavioral science. To help researchers harness the promise of LLMs, this tutorial offers a primer on the open-source Hugging Face ecosystem and demonstrates several applications that advance conceptual and empirical work in behavioral science, including feature extraction, fine-tuning of models for prediction, and generation of behavioral responses. Executable code is made available at github.com/Zak-Hussain/LLM4BeSci.git. Finally, the tutorial discusses challenges faced by research with (open-source) LLMs related to interpretability and safety and offers a perspective on future research at the intersection of language modeling and behavioral science.",
issn="1554-3528",
doi="10.3758/s13428-024-02455-8",
url="https://doi.org/10.3758/s13428-024-02455-8"
}


@Article{Aktas2024,
author="Aktas, Ethem Utku
and Cakmak, Ebru
and Inan, Mete Cihad
and Yilmaz, Cemal",
title="Improving the quality of software issue report descriptions in Turkish: An industrial case study at Softtech",
journal="Empirical Software Engineering",
year="2024",
month="Feb",
day="12",
volume="29",
number="2",
pages="43",
abstract="Issue reports are an important part of the software development process. They help developers identify and fix problems in their code. However, problems described in these reports often lack important information, such as the Observed Behavior (OB), Expected Behavior (EB), and Steps to Reproduce (S2R). This can lead to valuable developer time being wasted on gathering the relevant information. This study aims to address this issue by developing a tool that guides reporters in providing the necessary information in an industrial setting. The study is conducted at Softtech, a software subsidiary of the largest private bank in Turkey. The proposed approach is developed for issue reports written specifically in Turkish language. It is motivated by the need for issue report classification tools that can handle the unique characteristics of the Turkish language, such as the presence of many compound words. We first manually analyze and label 1, 041 issue reports for the existence of OB, S2R, and EB, and then present the specific patterns we found describing the related information. Next, we use morphological analysis to extract keywords and suffixes, and then use them for classification with a machine learning based approach. In addition, we conduct a feasibility study to assess the potential of using large language models for issue report classification tasks as a direction for future research. The results indicate that the tool using the machine learning-based approach can be used to guide in improving the quality of issue reports at Softtech, thereby saving valuable developer time.",
issn="1573-7616",
doi="10.1007/s10664-023-10434-4",
url="https://doi.org/10.1007/s10664-023-10434-4"
}


@Article{Prajapati2026,
author="Prajapati, Manish
and Baliarsingh, Santos Kumar
and Dev, Prabhu Prasad",
title="Detecting AI-generated essays using fine-tuned XLNet-CNN hybrid techniques: a study of the academic integrity challenge",
journal="International Journal of Machine Learning and Cybernetics",
year="2026",
month="Feb",
day="02",
volume="17",
number="2",
pages="65",
abstract="The rapid advancement of large language models (LLMs) such as GPT-3 and GPT-4 has raised serious concerns for academic integrity, as AI-generated essays often bypass conventional plagiarism detection systems and threaten the credibility of student assessments. To address this challenge, we propose XLNet-CNN, a hybrid detection framework that combines XLNet (Generalized Autoregressive Pretraining for Language Understanding) with a convolutional neural network (CNN) for local feature extraction. Using the EnglishQA Essays Corpus containing 161,640 essays from both human and AI sources, we benchmark the model against representative machine learning, deep learning, and transformer-based baselines, as well as widely used external detection tools. The framework achieves accuracy 0.98, recall 0.96, F2 score 0.97, precision 1.00, BLEU (Bilingual Evaluation Understudy) 30.76, and perplexity (PPL) 3.99. Notably, it eliminates false negatives, ensuring that no AI-generated essay is misclassified as human-written, a critical safeguard for high-stakes academic contexts. These results demonstrate the effectiveness of hybrid architectures in capturing both semantic coherence and stylistic artifacts, offering a reliable and scalable solution for maintaining fairness in academic evaluation while supporting responsible AI use in education.",
issn="1868-808X",
doi="10.1007/s13042-025-02880-x",
url="https://doi.org/10.1007/s13042-025-02880-x"
}


@Inbook{Gadani2026,
author="Gadani, Naimil Navnit",
editor="Pandey, Bishwajeet
and Patel, Advait",
title="Next-Generation Cloud Automation with Generative AI",
bookTitle="Revolutionizing the Cloud: Generative AI, Security, and Sustainability",
year="2026",
publisher="Springer Nature Switzerland",
address="Cham",
pages="275--299",
abstract="The integration of Generative Artificial Intelligence (AI) with cloud automation represents a paradigmatic shift in how organizations deploy, manage, and optimize their cloud infrastructure. This paper presents a comprehensive analysis of next-generation cloud automation powered by generative AI technologies, examining current trends, implementation strategies, and emerging challenges. Through systematic review of existing frameworks and case studies, we explore how generative AI enhances traditional cloud management approaches, particularly focusing on the CloudCAMP model-driven engineering approach and AI-Generated Everything (AIGX) concept. Our analysis reveals significant improvements in operational efficiency, cost reduction, and decision-making capabilities, while identifying critical challenges including data privacy concerns, algorithmic bias, and integration complexity. The research demonstrates that organizations implementing generative AI in cloud automation can achieve up to 40{\%} reduction in manual deployment tasks and substantial cost savings through predictive resource allocation. However, successful implementation requires careful consideration of regulatory compliance, security frameworks, and organizational readiness. This study provides practical recommendations for enterprises seeking to leverage generative AI for cloud automation while addressing associated risks and implementation challenges.",
isbn="978-3-032-07479-9",
doi="10.1007/978-3-032-07479-9_14",
url="https://doi.org/10.1007/978-3-032-07479-9_14"
}


@Inbook{Wendt2025,
author="Wendt, Donnie W.",
title="Securing AI",
bookTitle="AI Strategy and Security: A Roadmap for Secure, Responsible, and Resilient AI Adoption",
year="2025",
publisher="Apress",
address="Berkeley, CA",
pages="71--95",
abstract="Traditional cybersecurity best practices, such as access management, secure protocols, penetration testing, vulnerability management, and intrusion detection, help protect AI systems and infrastructure. AI development, deployment, and operations necessitate a comprehensive security architecture that safeguards the infrastructure against traditional cybersecurity threats, including social engineering, intrusions, data breaches, malware, credential compromises, and privilege escalations. Therefore, the security architecture should include the concepts of zero trust, least privilege, and layered defenses. However, the shift towards AI-enabled applications increases complexity and opens up new avenues for cyberattacks. Therefore, organizations must integrate AI-specific security measures, including adversarial testing, model robustness techniques, data lineage tracking, and LLM firewalls.",
isbn="979-8-8688-1733-5",
doi="10.1007/979-8-8688-1733-5_4",
url="https://doi.org/10.1007/979-8-8688-1733-5_4"
}


@Article{Mostafavi2026,
author="Mostafavi, Sepehr
and Yahyavi, Yeganeh
and Ravanmehr, Reza",
title="Systematic literature review on sentiment analysis using transformers",
journal="International Journal of Data Science and Analytics",
year="2026",
month="Jan",
day="31",
volume="22",
number="1",
pages="49",
abstract="With the rapid expansion and growing complexity of social networks, developing effective approaches for extracting meaningful knowledge from the vast volumes of user-generated data has become an essential challenge. Among these approaches, sentiment extraction and analysis- one of the core tasks in natural language processing (NLP)---plays a critical role in uncovering users' attitudes, opinions, and emotions. Insights gained from such analysis can inform strategic decision-making across a wide range of domains, including marketing, public policy, healthcare, and service management. Over the past decade, sentiment analysis has attracted increasing attention within interdisciplinary research, with many studies leveraging artificial intelligence-based techniques. In particular, transformer architectures have emerged as leading methods in NLP, primarily due to their capacity to model long-range textual dependencies and to generate richer, more precise semantic representations. This paper presents a systematic literature review of research on transformer-based sentiment analysis. Specifically, we categorize and analyze different models, widely adopted benchmark datasets, evaluation metrics, as well as the challenges and diverse applications associated with these methods. A qualitative assessment of the reviewed studies, based on established systematic review standards, highlights the progress and maturity of the field from technical, linguistic, and applied perspectives. Furthermore, we analyze current research trends and identify promising directions for future work. The findings of this review underscore both the substantial growth in research output and the increasing significance of transformer-based sentiment analysis, reflecting its dynamism, innovation, and impact on interdisciplinary studies.",
issn="2364-4168",
doi="10.1007/s41060-026-01021-z",
url="https://doi.org/10.1007/s41060-026-01021-z"
}


@inproceedings{10.1007/978-3-031-98235-4_4,
 abstract = {This paper aims to explore the development of agentic artificial intelligence workflows utilizing the open-source LangGraph framework. With the adoption of Large Language Models (LLMs) for various applications like text generation, summarization, image analysis, and code generation, many industries are looking for ways to utilize the power of these models to automate necessary tasks efficiently. LangGraph enables the creation of these multi-agentic workflows by providing an elaborative method for controlling agent interactions and execution flow. We present a methodology for software development lifecycle (SDLC) automation by leveraging LangGraph and discuss how agentic workflows are capable of enhancing efficiency and scalability through AI-driven automation.},
 address = {Cham},
 author = {Mandulapalli, Shriraj
and Hernandez, Emilio
and Hall, Wayne Jordan
and Chakeri, Alireza
and Jaimes, Luis},
 booktitle = {Industrial Engineering and Operations Management},
 editor = {Florez, Hector
and Rabelo, Luis
and Diaz, Cesar},
 isbn = {978-3-031-98235-4},
 pages = {45--54},
 publisher = {Springer Nature Switzerland},
 title = {Development of Agentic Workflows with LangGraph for Software Development Life Cycle Automation},
 url = {https://link.springer.com/chapter/10.1007/978-3-031-98235-4_4},
 year = {2026}
}

@inproceedings{10.1007/978-3-031-83796-8_15,
 abstract = {In an era of rapid information distribution, the presence of fake news presents enormous difficulties to society, influencing public opinion and decision-making on a global scale. To address this issue, a reliable and efficient system capable of detecting and classifying fake news in real time must be developed. This project proposes the design and implementation of a specialized Software Development Life Cycle (SDLC) model, called the Grapevine SDLC, specifically designed for developing a real-time fake news classifier using Large Language Models (LLMs) and Apache Kafka. The Grapevine SDLC takes a methodical, iterative approach, starting with a thorough requirements analysis that identifies both system capabilities and limitations. During the design and development phase, the system architecture is crafted with a focus on scalability and real-time processing, integrating LLMs for highly accurate content analysis and categorization. Kafka's distributed messaging platform ensures seamless and efficient data streaming, enabling the system to handle large volumes of data in real time. Further, the model includes continuous monitoring and feedback loops to improve detection accuracy and adapt to evolving fake news patterns. },
 address = {Cham},
 author = {Aishwarya, C.
and Shekokar, Tanay Praveen
and Naga Mukesh, Konatham
and Venkatesan, M.
and Prabhavathy, P.},
 booktitle = {Advanced Network Technologies and Intelligent Computing},
 editor = {Verma, Anshul
and Verma, Pradeepika
and Pattanaik, Kiran Kumar
and Buyya, Rajkumar
and Dasgupta, Dipankar},
 isbn = {978-3-031-83796-8},
 pages = {222--239},
 publisher = {Springer Nature Switzerland},
 title = {Grapevine SDLC Model for Real-Time Fake News Classification},
 url = {https://link.springer.com/chapter/10.1007/978-3-031-83796-8_15},
 year = {2025}
}

@Inbook{Codabux2024,
author="Codabux, Zadia
and Fard, Fatemeh
and Verdecchia, Roberto
and Palomba, Fabio
and Di Nucci, Dario
and Recupito, Gilberto",
editor="Mendez, Daniel
and Avgeriou, Paris
and Kalinowski, Marcos
and Ali, Nauman Bin",
title="Teaching Mining Software Repositories",
bookTitle="Handbook on Teaching Empirical Software Engineering",
year="2024",
publisher="Springer Nature Switzerland",
address="Cham",
pages="325--362",
abstract="Mining software repositories (MSR) has become a popular research area recently. MSR analyzes different sources of data, such as version control systems, code repositories, defect tracking systems, archived communication, deployment logs, and so on, to uncover interesting and actionable insights from the data for improved software development, maintenance, and evolution. This chapter provides an overview of MSR and how to conduct an MSR study, including setting up a study, formulating research goals and questions, identifying repositories, extracting and cleaning the data, performing data analysis and synthesis, and discussing MSR study limitations. Furthermore, the chapter discusses MSR as part of a mixed method study and how to mine data ethically and gives an overview of recent trends in MSR as well as reflects on the future. As a teaching aid, the chapter provides tips for educators, exercises for students at all levels, and a list of repositories that can be used as a starting point for an MSR study.",
isbn="978-3-031-71769-7",
doi="10.1007/978-3-031-71769-7_12",
url="https://doi.org/10.1007/978-3-031-71769-7_12"
}


@Article{Syraji2025,
author="Syraji, Yonas
and Jeyaramraja, P. R.
and Mada, Tanje
and Gobikanila, K.",
title="Comprehensive review of aflatoxin contamination, its occurrence, effects, management, and future perspectives",
journal="Discover Food",
year="2025",
month="Nov",
day="12",
volume="5",
number="1",
pages="377",
abstract="Mycotoxins are harmful substances produced by fungi, with aflatoxins - primarily produced by Aspergillus flavus and A. parasiticus -- posing the greatest threat to food safety. First identified in 1960, aflatoxin B1 (AFB1), aflatoxin B2 (AFB2), aflatoxin G1 (AFG1), and aflatoxin G2 (AFG2) are of particular concern, with AFB1 recognized as the most potent hepatotoxin. These crystalline compounds fluoresce under ultraviolet light and are moderately soluble in organic solvents. Aflatoxins contaminate an estimated 25{\%} of global crops, with contamination levels in several regions frequently exceeding international safety standards. Studies from Africa report widespread contamination in staple crops such as maize, groundnut, and sorghum, with both raw and processed products affected. In some cases, aflatoxin levels have reached as high as 2410 ppb -- well above the EU's maximum tolerable limit of 4 ppb. Recent findings in Ethiopia also show several groundnut varieties exceeding regulatory limits for AFB1. Climate change is expected to further aggravate contamination risks, particularly in Europe, where safe maize-growing regions are predicted to shift northward by 2050. Aflatoxins are associated with acute and chronic toxicity in humans and animals, leading to distinct and varied effects, including aflatoxicosis, hepato-carcinogenicity, retarded growth, suppressed immune system, and damage to the liver and kidneys. Aflatoxin production is influenced by both biotic (e.g., microbial interactions, host plant type) and abiotic (e.g., temperature, humidity, pH, water activity) factors. Given the severity of this issue, this review provides a comprehensive overview of aflatoxin occurrence, toxicological impacts, current mitigation strategies, and future approaches to strengthen prevention and control efforts globally.",
issn="2731-4286",
doi="10.1007/s44187-025-00680-4",
url="https://doi.org/10.1007/s44187-025-00680-4"
}


@inproceedings{10.1007/978-3-031-70239-6_24,
 abstract = {Cyber Threat Intelligence (CTI) provides a structured and interconnected model for threat information through Cybersecurity Knowledge Graphs. This allows researchers and practitioners to represent and organize complex relationships and entities in a more coherent form. Above all, the discovery of hidden relationships between different CTI entities, such as threat actors, malware, infrastructure, and attacks, is becoming a crucial task in this domain, facilitating proactive defense measures and helping to identify Tactics, Techniques, and Procedures (TTPs) employed by malicious parties. In this paper, we provide a Systematization of Knowledge (SoK) to analyze the existing literature and give insights into the important CTI task of Relation Extraction. In particular, we design a categorization of the relations used in CTI; we analyze the techniques employed for their extraction, the emerging trends and open issues in this context, and the main future directions. This work provides a novel and fresh perspective that can help the reader understand how relationships among entities can be schematized to provide a better view of the cyber threat landscape.},
 address = {Cham},
 author = {Arikkat, Dincy R.
and Vinod, P.
and K. A., Rafidha Rehiman
and Nicolazzo, Serena
and Nocera, Antonino
and Conti, Mauro},
 booktitle = {Natural Language Processing and Information Systems},
 editor = {Rapp, Amon
and Di Caro, Luigi
and Meziane, Farid
and Sugumaran, Vijayan},
 isbn = {978-3-031-70239-6},
 pages = {348--363},
 publisher = {Springer Nature Switzerland},
 title = {Relation Extraction Techniques in Cyber Threat Intelligence},
 url = {https://link.springer.com/chapter/10.1007/978-3-031-70239-6_24},
 year = {2024}
}

@inproceedings{10.1007/978-3-031-70242-6_13,
 abstract = {Issue Tracking Systems (ITSs) are used to manage software issues, such as `feature requests' and `bug' reports. Issues are often interconnected by labelled links to show the relationships between issues. Recent studies in duplicate bug report retrieval (DBRR) aim to retrieve relevant duplicate bug reports o for a query bug report s to form duplicate pairs (s, o). However, the DBRR task has its limitations: (1) only focuses on the issues with the `bug' type and overlooks other issue types, e.g., `task' and `improvement'; (2) only issues with respect to the `duplicate' link type are retrieved, whereas a query s may have other link types, e.g., `relates' and `contains', with candidate issues o. This paper goes beyond the DBRR task and proposes a new task of issue link retrieval (ILR) for any issue types and link types. For a given query s, the ILR task will locate all relevant tail issues o with respect to a given link type r to build issue links (s, r, o). This paper presents novel methods using pre-trained language models (e.g., GPT-2) to embed issue links for computing probability scores to retrieve issue links. Our methods are evaluated on four datasets, demonstrating high effectiveness in issue link retrieval. The code and datasets of this paper can be obtained from GitHub (https://github.com/MiaoHu-Pro/ILR).},
 address = {Cham},
 author = {Hu, Miao
and Lin, Zhiwei
and Marshall, Adele},
 booktitle = {Natural Language Processing and Information Systems},
 editor = {Rapp, Amon
and Di Caro, Luigi
and Meziane, Farid
and Sugumaran, Vijayan},
 isbn = {978-3-031-70242-6},
 pages = {126--138},
 publisher = {Springer Nature Switzerland},
 title = {Issue Links Retrieval for New Issues in Issue Tracking Systems},
 url = {https://link.springer.com/chapter/10.1007/978-3-031-70242-6_13},
 year = {2024}
}

@inproceedings{10.1007/978-3-031-10542-5_6,
 abstract = {The application of Artificial Intelligence (AI) in the Software Engineering (SE) field is always a bit delayed compared to state-of-the-art research results. While the Generative Pre-trained Transformer (GPT-2) model was published in 2018, only a few recent works used it for SE tasks. One of such tasks is Automated Program Repair (APR), where the applied technique should find a fix to software bugs without human intervention. One problem emerges here: the creation of proper training data is resource-intensive and requires several hours of additional work from researchers. The sole reason for it is that training a model to repair programs automatically requires both the buggy program and the fixed one on large scale and presumably in an already pre-processed form. There are currently few such databases, so teaching and fine-tuning models is not an easy task. In this work, we wanted to investigate how the GPT-2 model performs when it is not fine-tuned for the APR task, compared to when it is fine-tuned. From previous work, we already know that the GPT-2 model can automatically generate patches for buggy programs, although the literature lacks studies where no fine-tuning has taken place. For the sake of the experiment we evaluated the GPT-2 model out-of-the-box and also fine-tuned it before the evaluation on 1559 JavaSript code snippets. Based on our results we can conclude that although the fine-tuned model was able to learn how to write syntactically correct source code almost on every attempt, the non-fine-tuned model lacked some of these positive features.},
 address = {Cham},
 author = {Lajk{\'o}, M{\'a}rk
and Horv{\'a}th, D{\'a}niel
and Csuvik, Viktor
and Vid{\'a}cs, L{\'a}szl{\'o}},
 booktitle = {Computational Science and Its Applications -- ICCSA 2022 Workshops},
 editor = {Gervasi, Osvaldo
and Murgante, Beniamino
and Misra, Sanjay
and Rocha, Ana Maria A. C.
and Garau, Chiara},
 isbn = {978-3-031-10542-5},
 pages = {79--91},
 publisher = {Springer International Publishing},
 title = {Fine-Tuning GPT-2 to Patch Programs, Is It Worth It?},
 url = {https://link.springer.com/chapter/10.1007/978-3-031-10542-5_6},
 year = {2022}
}

@Inbook{Shao2025,
author="Shao, Yulin",
title="AI-Enhanced Network Control for IoT",
bookTitle="AI-Empowered IoT Communications",
year="2025",
publisher="Springer Nature Switzerland",
address="Cham",
pages="201--257",
abstract="The increasing complexity and scale of IoT networks demand intelligent, adaptive network control mechanisms to ensure efficiency, scalability, and robustness. Traditional rule-based approaches are insufficient to handle the dynamic nature of IoT systems, necessitating AI-driven solutions. This chapter explores how Deep Reinforcement Learning (DRL), Graph Neural Networks (GNNs), and Large Language Models (LLMs) revolutionize IoT network management by enabling real-time adaptation, decentralized decision-making, and human-like interaction with network systems. The chapter first introduces the fundamental challenges of IoT network control, including dynamic topologies, fluctuating traffic patterns, and interference management. It then presents DRL-based adaptive monitoring, which optimizes the significant sampling problem, ensuring efficient data collection while minimizing energy consumption and network overhead. Next, GNN-based decentralized control is explored, demonstrating how AI enhances multi-device collaboration, interference mitigation, and dynamic resource allocation. Finally, LLM-driven network orchestration is introduced, illustrating how natural language interfaces can bridge human input with AI-powered IoT control, making network management more accessible. Through extensive case studies and numerical analyses, this chapter showcases the impact of AI-enhanced control in improving network efficiency, reducing communication delays, and enhancing fault tolerance. The discussion extends to real-world IoT applications, including smart cities, industrial automation, and autonomous systems.",
isbn="978-3-031-98816-5",
doi="10.1007/978-3-031-98816-5_5",
url="https://doi.org/10.1007/978-3-031-98816-5_5"
}


@Article{Voutyrakou2025,
author="Voutyrakou, Dialekti Athina
and Skordoulis, Constantine",
title="Algorithmic Governance: Gender Bias in AI-Generated Policymaking?",
journal="Human-Centric Intelligent Systems",
year="2025",
month="Sep",
day="01",
volume="5",
number="3",
pages="385--417",
abstract="Artificial Intelligence (AI) tools are becoming deeply embedded in everyday life and increasingly influence or automate decision-making processes that could shape not only public opinion but also policies. As their potential impact grows, it is essential to assess the inclusivity of the policy recommendations they could generate and potential biases they may reinforce. This study examines whether AI systems inherently consider gender in policy proposals, both when gender is explicitly mentioned in prompts and when it is not. We conduct four experiments across diverse policy-making contexts to evaluate whether AI-generated recommendations include, overlook, or misrepresent gender considerations. We tested these experiments in two different AI tools, namely ChatGPT (GPT-4) and Microsoft Copilot. To ensure neutrality and reproducibility, we minimize user-specific context and repeat each prompt multiple times. Our findings offer insights into the limitations of current AI tools as policy advisors and contribute to ongoing discussions on algorithmic fairness, implicit gender bias, and the need for gender-aware AI governance. They also raise broader questions about how AI tools understand and represent gender, and how these representations influence the politics of policy-making.  ",
issn="2667-1336",
doi="10.1007/s44230-025-00109-2",
url="https://doi.org/10.1007/s44230-025-00109-2"
}


@Article{Wang2025,
author="Wang, Guangshuo
and He, Jiajun
and Li, Hao
and Zhang, Min
and Feng, Dengguo",
title="RAG-leaks: difficulty-calibrated membership inference attacks on retrieval-augmented generation",
journal="Science China Information Sciences",
year="2025",
month="May",
day="21",
volume="68",
number="6",
pages="160102",
abstract="Recently, retrieval-augmented generation (RAG) systems have attracted attention for addressing issues like hallucinations and reliance on outdated knowledge in large language models (LLMs). Privacy studies have revealed that RAG systems are vulnerable to membership leakage in determining whether a specific target sample is included in the RAG knowledge base. Existing membership inference attack (MIA) methods for RAGs primarily rely on similarity scores between the system's responses and the true answers. These methods assume that a higher similarity score indicates the sample is more likely to have been used by the RAG system to enhance its response, suggesting it is a member of the knowledge base. However, this study uncovers an important insight: the similarity metric does not directly represent the membership status, instead measures the response difficulty of the sample. To address this, we propose a novel membership inference attack for RAG systems, called difficulty-calibrated membership inference attack (DC-MIA). It first classifies high-similarity samples as members, and then calibrates the membership scores of samples with comparable raw similarity scores using a likelihood ratio test. Experimental results demonstrate that our approach significantly improves the performance of membership inference attacks on RAG systems.",
issn="1869-1919",
doi="10.1007/s11432-024-4441-4",
url="https://doi.org/10.1007/s11432-024-4441-4"
}


@Inbook{HopkinsJr.2025,
author="Hopkins Jr., Bruce
and Hopkins Sr., Bruce",
title="Using AI in the Enterprise! Creating a Text Summarizer for Slack Messages",
bookTitle="Creating ChatGPT Apps with JavaScript: A Hands-on Guide for AI Applications with OpenAI APIs",
year="2025",
publisher="Apress",
address="Berkeley, CA",
pages="91--120",
abstract="In today's corporate world, it's extremely common for companies to have an instance of Slack (or Microsoft Teams) to organize themselves and use it as a central place of communication to everyone in the company. Now, if you've ever used Slack before, I think you know how easily a channel can become flooded with a ton of messages because SOME important thing happened SOMEWHERE in the company or the world.",
isbn="979-8-8688-1221-7",
doi="10.1007/979-8-8688-1221-7_4",
url="https://doi.org/10.1007/979-8-8688-1221-7_4"
}


@Inbook{Fawareh2026,
author="Fawareh, Hamed
and Alhamli, Sulaiman Yahya",
editor="Sarea, Adel
and Echchabi, Abdelghani
and Salami, Monsurat Ayojimi
and Mahmood, Abdullah",
title="Measuring the Impact of Incorporation for the Artificial Intelligence (AI) Tools into Software Development Lifecycle for Improving Software System Projects",
bookTitle="Artificial Intelligence for Sustainable Innovation Management and Risk Management: A Systems (and Network) Perspective",
year="2026",
publisher="Springer Nature Switzerland",
address="Cham",
pages="2571--2590",
abstract="The incorporation for the Artificial Intelligence (AI) into software development has drastically revolutionized conventional approaches, improving efficiency, precision, and scalability. AI powered solutions for code generation, and debugging utilize sophisticated machine learning (ML), and natural language processing (NLP) methodologies to automate repetitive operations, enhancing efficiency, and elevating software quality. Utilizing pre-trained models like Codex, these technologies enable contextually pertinent code generation, and the early detection for the vulnerabilities during the Software Development Life Cycle (SDLC). This study examines the progress within AI-driven code creation, its impact upon software quality assurance, and its function across the software development life cycle (SDLC) phases. It also tackles the problems related to AI adoption, encompassing ethical issues, dependence upon data quality, and implementation expenses. The findings emphasize AI like an essential component within contemporary software engineering, fostering innovation, and revealing prospects for enhancement to optimize efficiency, and dependability within software development.",
isbn="978-3-031-95310-1",
doi="10.1007/978-3-031-95310-1_185",
url="https://doi.org/10.1007/978-3-031-95310-1_185"
}


@inproceedings{10.1007/978-3-032-07989-3_14,
 abstract = {As AI models scale to billions of parameters and operate with increasing autonomy, ensuring their safe, reliable operation demands engineering-grade security and assurance frameworks. This study presents an enterprise-level, risk-aware, security-by-design approach for large-scale autonomous AI systems, integrating standardized threat metrics, adversarial hardening techniques, and real-time anomaly detection into every phase of the development lifecycle. We detail a unified pipeline - from design-time risk assessments and secure training protocols to continuous monitoring and automated audit logging - that delivers provable guarantees of model behavior under adversarial and operational stress. Case studies in national security, open-source model governance, and industrial automation demonstrate measurable reductions in vulnerability and compliance overhead. Finally, we advocate cross-sector collaboration - uniting engineering teams, standards bodies, and regulatory agencies - to institutionalize these technical safeguards within a resilient, end-to-end assurance ecosystem for the next generation of AI.},
 address = {Cham},
 author = {Tallam, Krti},
 booktitle = {Proceedings of the Future Technologies Conference (FTC) 2025, Volume 2},
 editor = {Arai, Kohei},
 isbn = {978-3-032-07989-3},
 pages = {209--227},
 publisher = {Springer Nature Switzerland},
 title = {Engineering Risk-Aware, Security-By-Design Frameworks for Assurance of Large-Scale Autonomous AI Models},
 url = {https://link.springer.com/chapter/10.1007/978-3-032-07989-3_14},
 year = {2026}
}

@Article{Jiang2026,
author="Jiang, Shan
and Zhou, Xuecheng
and Zhang, Mingjin
and Xu, Changfu
and Liao, Guocheng
and Chen, Jianguo
and Cao, Jiannong",
title="Edge large language models: a comprehensive survey",
journal="CCF Transactions on Pervasive Computing and Interaction",
year="2026",
month="Jan",
day="29",
abstract="The paradigm of large language models (LLMs) is undergoing a fundamental shift from centralized cloud to the network edge, driven by urgent needs for data privacy, low-latency interaction, and offline reliability. However, deploying resource-intensive LLMs on constrained edge devices, such as smartphones and vehicles, presents a significant technical challenge. This survey provides a holistic, end-to-end engineering perspective on edge LLMs, systematically charting the path from on-device adaptation to efficient inference. We begin with emerging fine-tuning techniques, such as parameter-efficient fine-tuning and federated learning, which are crucial for personalization. The core of this work is a deep dive into the three pillars of efficient inference: model compression (quantization, pruning, and distillation), optimized architectural designs (e.g., attention mechanism variants), and critical runtime and system-level optimizations (e.g., key-value cache management, speculative decoding, and memory paging). We further establish a framework for holistic evaluation and survey the burgeoning applications across diverse domains. Unlike existing reviews that may focus on narrower topics, such as security, specific device categories, or high-level future concepts, this survey serves as a practical, unified roadmap for researchers and practitioners. It bridges the gap between algorithmic theory and system-level implementation, providing a comprehensive guide to building the next generation of private, responsive, and intelligent edge LLM applications.",
issn="2524-5228",
doi="10.1007/s42486-025-00227-7",
url="https://doi.org/10.1007/s42486-025-00227-7"
}


@Inbook{Wienholt2025,
author="Wienholt, Nick",
title="Security in the Time of Copilot",
bookTitle="GitHub Copilot and AI Coding Tools in Practice: Accelerate AI Adoption from Individual Developers to Enterprise",
year="2025",
publisher="Apress",
address="Berkeley, CA",
pages="121--137",
abstract="Before delving into security risks with Copilot and LLMs, it is worth revisiting the world's largest financial heist which was successfully completed by a North Korean hacker group in early 2025, resulting in the theft of {\$}1.4 billion of the crypto asset Ethereum (ETH) from the crypto exchange ByBit during a standard business process where ETH tokens were being transferred from a cold wallet during a routine transfer.",
isbn="979-8-8688-1784-7",
doi="10.1007/979-8-8688-1784-7_6",
url="https://doi.org/10.1007/979-8-8688-1784-7_6"
}


@inproceedings{10.1007/978-3-032-04190-6_22,
 abstract = {Large Language Models (LLMs) have facilitated the definition of autonomous intelligent agents. Such agents have already demonstrated their potential in solving complex tasks in different domains. And they can further increase their performance when collaborating with other agents in a multi-agent system. However, the orchestration and coordination of these agents is still challenging, especially when they need to interact with humans as part of human-agentic collaborative workflows. These kinds of workflows need to be precisely specified so that it is clear who is responsible for each task, what strategies agents can follow to complete individual tasks or how decisions will be taken when different alternatives are proposed, among others. Current business process modeling languages fall short when it comes to specifying these new mixed collaborative scenarios. In this paper, we extend a well-known process modeling language (i.e., BPMN) to enable the definition of this new type of workflow. Our extension covers both the formalization of the new modeling concepts required and the proposal of a BPMN-like graphical notation to facilitate the definition of these workflows. Our extension has been implemented and is available as an open-source human-agentic workflow modeling editor on GitHub.},
 address = {Cham},
 author = {Ait, Adem
and Izquierdo, Javier Luis C{\'a}novas
and Cabot, Jordi},
 booktitle = {Software Engineering and Advanced Applications},
 editor = {Taibi, Davide
and Smite, Darja},
 isbn = {978-3-032-04190-6},
 pages = {367--382},
 publisher = {Springer Nature Switzerland},
 title = {Towards Modeling Human-Agentic Collaborative Workflows: A BPMN Extension},
 url = {https://link.springer.com/chapter/10.1007/978-3-032-04190-6_22},
 year = {2026}
}

@Article{War2025,
author="War, Aicha
and Diallo, Alioune
and Habib, Andrew
and Klein, Jacques
and Bissyand{\'e}, Tegawend{\'e} F.",
title="Vulnerabilities in infrastructure as code: what, how many, and who?",
journal="Empirical Software Engineering",
year="2025",
month="Jun",
day="05",
volume="30",
number="5",
pages="120",
abstract="Infrastructure as Code (IaC) is a pivotal approach for deploying and managing IT systems and services using scripts, offering flexibility and numerous benefits. However, the presence of security flaws in IaC scripts can have severe consequences, as exemplified by the recurring exploits of Cloud Web Services. Recent studies in the literature have investigated IaC security issues, but they often focus on individual components (IaC tools or scripts), providing only preliminary insights. Our research extends the current knowledge by conducting a comprehensive investigation into various aspects of IaC security, encompassing its components. We explore vulnerabilities in terms of types, their predominant locations, contributor responsibilities for introducing vulnerabilities, and more. Our methodology relies on widely adopted static security testing tools, which analyze over 1600 repositories to identify IaC vulnerabilities. Our empirical study yields valuable observations, highlighting severe and recurrent vulnerabilities within IaC, while also categorizing their severity and types. We delve deeper into vulnerability patterns, examining source code, dependencies, and manifest files across IaC components, including tools, scripts, and add-ons (libraries or plugin tools). The study uncovers that IaC components are plagued by exploitable vulnerabilities that span all ten categories of security bugs outlined in the OWASP Top 10 2021. Furthermore, our investigation reveals that even when maintainers employ security tools to address vulnerabilities, they do not integrate them systematically into their automation routines. Consequently, we propose that IT teams need to foster stronger collaboration across DevOps profiles (developers and IT operators) and break down the boundaries with security operators to enhance Infrastructure as Code's security posture through the adoption of DevSecOps practices.",
issn="1573-7616",
doi="10.1007/s10664-025-10672-8",
url="https://doi.org/10.1007/s10664-025-10672-8"
}


@Inbook{O’Connor2024,
author="O'Connor, Arthur J.",
title="The History and Potential Future of Human--Machine Collaboration",
bookTitle="Organizing for Generative AI and the Productivity Revolution: Reshaping Organizational Roles in the Age of Artificial Intelligence",
year="2024",
publisher="Apress",
address="Berkeley, CA",
pages="3--21",
abstract="With the emergence of Gen AI, academic researchers have begun to explore the impact of AI technologies on organizational leadership, particularly in the areas of automation, engagement, decision-making, and innovation, which could change traditional corporate power hierarchies and organizational structures. As a professor at the University College London put it in his article in Columbia Business Law Review, recent developments ``{\ldots}clearly highlight AI's growing importance in management and hint at the enormous changes that corporate leadership may experience in the future.''",
isbn="979-8-8688-0959-0",
doi="10.1007/979-8-8688-0959-0_1",
url="https://doi.org/10.1007/979-8-8688-0959-0_1"
}


@Inbook{Pandiya2025,
author="Pandiya, Dileep Kumar
and Charankar, Nilesh",
title="AI in Microservices Development",
bookTitle="AI and Microservices: Integrating AI into API Design and Distributed Microservice Architecture ",
year="2025",
publisher="Apress",
address="Berkeley, CA",
pages="49--86",
abstract="Microservices are one of the approaches to modern software development that involves developing applications as a set of independent services. This architecture makes the systems more scalable, flexible, and manageable, which in turn means faster deployment.",
isbn="979-8-8688-1306-1",
doi="10.1007/979-8-8688-1306-1_3",
url="https://doi.org/10.1007/979-8-8688-1306-1_3"
}


@Article{He2026,
author="He, Guo-qian
and He, Si-jia
and Jing, Xiao-yu
and Dai, Yi-ling
and Guo, Xia
and Gao, Ju
and Zhang, Wei",
title="Dissecting neuroblastoma heterogeneity through single-cell multi-omics: insights into development, immunity, and therapeutic resistance",
journal="Oncogene",
year="2026",
month="Jan",
day="01",
volume="45",
number="2",
pages="123--139",
abstract="Neuroblastoma (NB), the most common extracranial solid tumor in children, is characterized by remarkable cellular heterogeneity and clinical variability ranging from spontaneous regression to aggressive progression and relapse. Despite advances in multimodal therapies, including surgery, chemotherapy, radiotherapy, differentiation therapy, and immunotherapy---treatment resistance remains the principal barrier to improving survival in high-risk patients. Recent single-cell and spatial multi-omics studies have revolutionized our understanding of NB by revealing its developmental origins, lineage hierarchy, and adaptive evolution under therapeutic pressure. These technologies have delineated distinct cellular states along an adrenergic--mesenchymal continuum and uncovered the dynamic interplay between tumor cells and their microenvironment. Genetic instability, epigenetic reprogramming, and metabolic plasticity cooperate with immune and stromal remodeling to drive tumor persistence and relapse. At the molecular level, mechanisms such as MYCN-driven chromatin remodeling, super-enhancer reorganization, bypass signaling activation, quiescent persister programs, immune checkpoint engagement, and metabolic rewiring collectively enable therapeutic escape. Importantly, these processes are reversible, highlighting tumor plasticity as both a hallmark and a potential vulnerability of NB. Integrating single-cell transcriptomics, epigenomics, and spatial profiling provides an unprecedented framework to map resistance evolution, identify lineage-specific vulnerabilities, and guide rational combination strategies. Targeting epigenetic regulators, metabolic checkpoints, and immune suppressive networks in a temporally coordinated manner holds promise for converting NB from an adaptive to a controllable disease.",
issn="1476-5594",
doi="10.1038/s41388-025-03635-2",
url="https://doi.org/10.1038/s41388-025-03635-2"
}


@Article{ReshefKera2025,
author="Reshef Kera, Denisa
and Natan, Odelya
and Turgeman, Merav
and Ofek, Hila",
title="Europe's AI future as a meta-sovereign imaginary: negotiating global norms, sovereign machines, and infrastructural futures through AI simulations",
journal="European Journal of Futures Research",
year="2025",
month="Nov",
day="24",
volume="14",
number="1",
pages="1",
abstract="The emerging AI Cold War exposes a paradox: the more states strive to territorialize AI infrastructures, the more they expose their dependence on globally entangled systems of chips, cloud, and compute that no actor fully commands. Sovereignty, once grounded in territorial control, has moved from the logic of possession to the symbolic and infrastructural processes through which authority is performed. It is enacted through symbolic gestures, strategic investments, and infrastructural imaginaries that project authority without securing it. We call this meta-sovereignty, a mode of rule sustained by infrastructures that do not yet exist, by fictions that persuade before they materialize, and by costly signals that substitute for control. Using the EU's 2025 ``AI Continent'' plan alongside U.S. and Chinese initiatives, we examine the transition from multilateral coordination toward signaling and the political labor of imagination. We argue that AI Cold War strategies function simultaneously as costly signals of intent and as political fictions that project agency amid structural interdependence. Drawing on Fearon's signaling theory and Ezrahi's account of political fiction, we conceptualize these as meta-sovereign imaginaries in which legitimacy is negotiated through narrative projection and partial infrastructural realization. Building on this framework, we introduce an AI Cold War governance simulation in which policy-grounded LLM agents stage sovereignty under asymmetric constraints, revealing how such imaginaries adapt, fracture, or consolidate.",
issn="2195-2248",
doi="10.1186/s40309-025-00261-9",
url="https://doi.org/10.1186/s40309-025-00261-9"
}


@inproceedings{10.1007/978-3-032-12089-2_38,
 abstract = {The use of large language models (LLMs) is being explored for a multitude of tasks in software engineering (SE), ranging from code generation to bug report assignment. Although LLMs provide impressive results, they require more time and energy than some other machine learning models. For some tasks, simpler models may be more sustainable than LLMs. In this paper, we construct natural language classifiers of different complexity for a use case in the SE domain: commit message classification. We compare the performance of each model with the state-of-the-art with regard to energy consumption for training and inference. We find that simpler models based on Na{\"i}ve Bayes and LSTM perform similarly to LLMs, while using a fraction of the energy, suggesting that choosing a small model can lead to significant reduction in power usage without compromising performance.},
 address = {Cham},
 author = {Bexell, Andreas
and Heander, Lo Gullstrand
and S{\"o}derberg, Emma
and Eldh, Sigrid
and Runeson, Per},
 booktitle = {Product-Focused Software Process Improvement},
 editor = {Scanniello, Giuseppe
and Lenarduzzi, Valentina
and Romano, Simone
and Vegas, Sira
and Francese, Rita},
 isbn = {978-3-032-12089-2},
 pages = {525--532},
 publisher = {Springer Nature Switzerland},
 title = {Exploring the Performance of ML Model Size for Classification in Relation to Energy Consumption},
 url = {https://link.springer.com/chapter/10.1007/978-3-032-12089-2_38},
 year = {2026}
}

@inproceedings{10.1007/978-981-99-8101-4_4,
 abstract = {Ethereum smart contracts are programs that execute transactions on a distributed ledger platform without intermediaries. However, they are prone to various types of vulnerabilities that can affect their security and functionality. In this paper, we present IntelliCon, a novel framework that leverages a pre-trained identifier-aware encoder-decoder CodeT5 model and confident learning to detect seven types of vulnerabilities in Ethereum smart contracts. Confident learning is a technique that improves dataset quality by identifying and correcting noisy labels, particularly in the presence of multiple annotators with varying levels of accuracy. We fine-tune CodeT5 on a dataset of 27,426 smart contracts annotated by multiple tools and pruned by confident learning to ensure that the model learns genuine vulnerability features rather than tool-specific features. Furthermore, we utilize abstract syntax tree (AST) analysis to extract code gadgets with sliding windows to locate the function that may contain code vulnerabilities. We evaluate the effectiveness of our framework in vulnerability detection with F1-score. Our results indicate that IntelliCon achieves high Micro-F1 (0.9591) and Macro-F1 (0.9293), outperforming existing methods. Moreover, our framework demonstrates its ability to handle imbalanced data, noisy labels, and complex code structures. IntelliCon contributes to improving the security and reliability of smart contracts, providing insights for future research on code generation tasks.},
 address = {Singapore},
 author = {Shen, Yiming
and Li, Kunhua
and Mao, Lin
and Li, Wenkai
and Li, Xiaoqi},
 booktitle = {Blockchain and Trustworthy Systems},
 editor = {Chen, Jiachi
and Wen, Bin
and Chen, Ting},
 isbn = {978-981-99-8101-4},
 pages = {45--59},
 publisher = {Springer Nature Singapore},
 title = {IntelliCon: Confidence-Based Approach for Fine-Grained Vulnerability Analysis in Smart Contracts},
 url = {https://link.springer.com/chapter/10.1007/978-981-99-8101-4_4},
 year = {2024}
}

@Article{Liang2026,
author="Liang, Chen
and Yang, Donghua
and Liang, Zheng
and Liang, Zhiyu
and Zhang, Tianle
and Xiao, Boyu
and Yang, Yuqing
and Wang, Wenqi
and Wang, Hongzhi",
title="Revisiting data analysis with Pre-trained foundation models",
journal="The VLDB Journal",
year="2026",
month="Jan",
day="12",
volume="35",
number="1",
pages="10",
abstract="Data analysis focuses on harnessing advanced statistics, programming, and machine learning techniques to extract valuable insights from vast datasets. An increasing volume and variety of research emerged, addressing datasets of diverse modalities, formats, scales, and resolutions across various industries. However, experienced data analysts often find themselves overwhelmed by intricate details in ad-hoc solutions or attempts to extract the semantics of real-world data properly. This makes the infrastructure difficult to maintain system performance and scale to more complex analytical scenarios. Pre-trained foundation models (PFMs), grounded (reality-anchored) with a large amount of real-world data that previous data analysis methods cannot fully understand, leverage complete statistics that combine reasoning of an admissible subset of results and statistical approximations by surprising engineering effects, to automate and enhance the analysis process. It pushes us to revisit data analysis to make better sense of data with PFMs. This paper provides a comprehensive review of systematic approaches to improving data analysis through the power of PFMs, while critically identifying the limitations of PFMs and also discusses possible directions for future research.",
issn="0949-877X",
doi="10.1007/s00778-025-00953-5",
url="https://doi.org/10.1007/s00778-025-00953-5"
}


@Article{Hofmann2025,
author="Hofmann, Andreas
and Rietsch, Jonas
and Haase, Ilka",
title="A perspective by the German NRZ-Authent on information and knowledge management in governmental authorities",
journal="npj Science of Food",
year="2025",
month="Jul",
day="05",
volume="9",
number="1",
pages="123",
abstract="Efforts in the filing and organisation of primary data, despite being a pillar of digital transformation, appear to be less pronounced when it comes to unstructured or text-based data. This situation is contrasted by the demand for knowledge management in organisational units operating at the interface between different stakeholders, including, for instance, governmental authorities. Supporting the specific objectives of our institution, we have developed a central platform that serves as an information management system for food. By providing a flexible infrastructure for the management of text-based information, as well as an interactive and intuitive user interface for application and integration into every-day business, we have implemented a system that facilitates building of a data pool of information relevant for food and feed fraud/authenticity. While transforming the institutional information management within the NRZ-Authent, the platform allows access to the data pool by external partners at national and, potentially, European or international level.",
issn="2396-8370",
doi="10.1038/s41538-025-00487-8",
url="https://doi.org/10.1038/s41538-025-00487-8"
}


@Inbook{Mastrogiacomo2025,
author="Mastrogiacomo, Rosario",
title="INTERPRET---Trust, Explainability, and AI Agent Reputation",
bookTitle="AI Identities: Governing the Next Generation of Autonomous Actors",
year="2025",
publisher="Apress",
address="Berkeley, CA",
pages="145--168",
abstract="Trust is the cornerstone of autonomy. Just as you might hesitate to trust a brand-new employee fresh out of college, whose every experience is unproven, organizations need explicit, verifiable trust in AI identities to grant autonomy confidently. Without trust---grounded in transparency, explainability, and established reputation---AI agents can't safely make independent decisions. This chapter examines how trust, explainability, and AI agent reputation intersect, demonstrating why building and continuously validating trust in AI identities is essential to safely managing their growing autonomy within your organization.",
isbn="979-8-8688-2034-2",
doi="10.1007/979-8-8688-2034-2_13",
url="https://doi.org/10.1007/979-8-8688-2034-2_13"
}


@Article{Alam2025,
author="Alam, Khairul
and Roy, Banani
and Roy, Chanchal K.
and Mittal, Kartik",
title="An Empirical Investigation on the Challenges in Scientific Workflow Systems Development",
journal="Empirical Software Engineering",
year="2025",
month="Aug",
day="01",
volume="30",
number="5",
pages="151",
abstract="Scientific Workflow Systems (SWSs) are advanced software frameworks that drive modern research by orchestrating complex computational tasks and managing extensive data pipelines. These systems offer a range of essential features, including modularity, abstraction, customization, interoperability, workflow composition tools, resource management, error handling, and comprehensive documentation. Utilizing these frameworks accelerates the development of scientific computing, resulting in more efficient and reproducible research outcomes. Despite their significance, developing a user-friendly, efficient, and adaptable SWS poses several challenges that are not always well-documented or understood. This study explores these challenges through an in-depth analysis of interactions on Stack Overflow (SO) and GitHub, key platforms where developers and researchers discuss and resolve issues. In particular, we leveraged topic modeling (BERTopic) to understand the topics SWSs developers discuss on these platforms. Then, we examined the popularity and difficulty of those topics. We identified 10 topics developers discuss on SO (e.g., Workflow Creation and Scheduling, Data Structures and Operations, Workflow Execution) and found that workflow execution is the most challenging among them. By analyzing GitHub issues, we identified 13 topics (e.g., Errors and Bug Fixing, Documentation, Dependencies) and discovered that errors and bug fixing is the most dominant topics in this context. We found system redesign and API migration to be the most challenging topics utilizing GitHub data. A cross-platform comparison revealed overlapping concerns such as task management and data operations. Additionally, we categorized each topic by type (How, Why, What, and Others) and observed that the How type consistently dominates across all topics, indicating a need for procedural guidance among developers. This dominance of the How type is also prevalent in other domains, such as Chatbots and Mobile development. We believe that our study will guide future research in proposing tools and techniques to help the community overcome the challenges developers face when developing SWSs.",
issn="1573-7616",
doi="10.1007/s10664-025-10705-2",
url="https://doi.org/10.1007/s10664-025-10705-2"
}


@Article{Li2025,
author="Li, Wenhao
and Manickam, Selvakumar
and Chong, Yung-Wey",
title="FedPhishLLM: A privacy-preserving and explainable phishing detection mechanism using federated learning and LLMs",
journal="Journal of King Saud University Computer and Information Sciences",
year="2025",
month="Oct",
day="14",
volume="37",
number="8",
pages="252",
abstract="Phishing attacks remain a significant cybersecurity threat, exploiting social engineering techniques to deceive users and steal sensitive information. Traditional phishing detection mechanisms based on machine learning and deep learning often rely on centralized data collection, raising privacy concerns and limiting adaptability to diverse user environments. Recent advancements in large language models (LLMs) have introduced new opportunities for phishing detection by leveraging multimodal data, yet existing approaches primarily depend on commercial LLMs, which increase operational costs and potentially compromise user privacy. To address these challenges, we propose FedPhishLLM, a privacy-preserving and explainable phishing detection framework that integrates federated learning (FL) with fine-tuned multimodal LLMs. Our approach enables decentralized model training, allowing users to collaboratively improve phishing detection without sharing raw data, thereby preserving privacy while maintaining high detection accuracy. FedPhishLLM enhances explainability by providing phishing explanations, including brand identification and intent recognition, fostering user trust and informed decision-making. Experimental results confirm that our framework outperforms all baseline models, achieving up to 95{\%} accuracy, precision, and F1-score with 96{\%} recall. Further analyses validate the rationality of chosen training parameters and prompting strategies, demonstrate the complementary value of multimodal phishing indicators, and highlight the framework's adaptability to linguistic heterogeneity as well as resilience against adversarial and evasive attacks. To the best of our knowledge, FedPhishLLM is one of the first FL-based multimodal LLM approaches for phishing detection, offering a scalable, privacy-conscious, and explainable security solution practical for real-world deployments.",
issn="2213-1248",
doi="10.1007/s44443-025-00267-0",
url="https://doi.org/10.1007/s44443-025-00267-0"
}


@Article{Altun2025,
author="Altun, U{\u{g}}ur Can
and G{\"o}{\c{c}}men, Ismail Sergen
and S{\"u}l{\"u}n, Emre
and Tuna, Erdem
and T{\"u}z{\"u}n, Eray",
title="Process smells in practice: an evaluative case study",
journal="Empirical Software Engineering",
year="2025",
month="May",
day="23",
volume="30",
number="5",
pages="115",
abstract="Software development comprises many processes, including Code Review (CR) and Bug Tracking (BT). Although no perfect practices exist, recognizing bad practices helps avoid detrimental habits in CR and BT.",
issn="1573-7616",
doi="10.1007/s10664-025-10664-8",
url="https://doi.org/10.1007/s10664-025-10664-8"
}


@Inbook{Rhodes2024,
author="Rhodes, Geoffrey Alan
and Huang, Songhao",
editor="Geroimenko, Vladimir",
title="Augmented and Virtual Reality in the World of GPT Text and Image Creations: AI, Metaverse, and Art",
bookTitle="Augmented and Virtual Reality in the Metaverse",
year="2024",
publisher="Springer Nature Switzerland",
address="Cham",
pages="227--246",
abstract="This chapter discusses popular aspirations and fears of Augmented Reality (AR) and Virtual Reality (VR). For both, the ideal content required has all the complexity, persistence, and interactivity of nature: a metaverse and AI. There are dystopias and utopias colorfully portrayed in science fiction which speak to our fears and desires for the future. But, has the metaverse already arrived? There are now convincing AI metaverses of text, virtual collaborations and presence, 'digital twin' ontologies, deep fakes, and fake news. It seems that the technological ground on which our reality and industries rest is again shifting with the sudden advances in artificial intelligence---especially with text and still-image generation. Does this shift give us a new perspective on the desires, horrors, and useful definitions of Augmented Reality, Virtual Reality, and the potential metaverse? In this chapter, the authors seek to interrogate the terms: `AR', `VR', and `metaverse' in light of the fast-moving AI phenomenon. A survey of popular science fiction draws out the hopes and anxieties of these media, and a journey through the progress of virtual and metaverse art in China over the last decade elucidates the creative challenges ahead.",
isbn="978-3-031-57746-8",
doi="10.1007/978-3-031-57746-8_12",
url="https://doi.org/10.1007/978-3-031-57746-8_12"
}


@inproceedings{10.1007/978-3-031-89063-5_56,
 abstract = {Modern software systems undergo frequent updates, continuously evolving with new versions and variants to offer new features, improve functionality, and expand usability. Given the rapid pace of software evolution, organizations require effective tools and methods to mitigate the challenges associated with these changes, also called deltas. To address these challenges, the international SmartDelta Project joined industry and academia to develop and test solutions for incremental development and quality assurance. This paper provides insights into the SmartDelta project achievements and highlights one main contribution: the SmartDelta Methodology, a domain-unspecific concept for delta management in incremental software engineering. This methodology enables companies to identify gaps in their continuous engineering environment across six stages and helps to discover new tools in various technical areas. Additionally, the paper presents seven selected tools at different stages of the methodology.},
 address = {Cham},
 author = {Dornauer, Benedikt
and Felderer, Michael
and Saadatmand, Mehrdad
and Abbas, Muhammad
and Bonnotte, Nicolas
and Dreschinski, Andreas
and Enoiu, Eduard Paul
and T{\"u}z{\"u}n, Eray
and U{\c{c}}ar, Baykal Mehmet
and Devran, {\"O}mercan
and Gr{\"o}pler, Robin},
 booktitle = {The 22nd International Conference on Information Technology-New Generations (ITNG 2025)},
 editor = {Latifi, Shahram},
 isbn = {978-3-031-89063-5},
 pages = {648--659},
 publisher = {Springer Nature Switzerland},
 title = {SmartDelta Methodology: Automated Quality Assurance and Optimization for Incremental System Engineering},
 url = {https://link.springer.com/chapter/10.1007/978-3-031-89063-5_56},
 year = {2025}
}

@Article{Roth2025,
author="Roth, Johannes
and Duan, Yunyan
and Mahner, Florian P.
and Kaniuth, Philipp
and Wallis, Thomas S. A.
and Hebart, Martin N.",
title="Ten principles for reliable, efficient, and adaptable coding in psychology and cognitive neuroscience",
journal="Communications Psychology",
year="2025",
month="Apr",
day="15",
volume="3",
number="1",
pages="62",
abstract="Writing code is becoming essential for psychology and neuroscience research, supporting increasingly advanced experimental designs, processing of ever-larger datasets and easy reproduction of scientific results. Despite its critical role, coding remains challenging for many researchers, as it is typically not part of formal academic training. We present a range of practices tailored to different levels of programming experience, from beginners to advanced users. Our ten principles help researchers streamline and automate their projects, reduce human error, and improve the quality and reusability of their code. For principal investigators, we highlight the benefits of fostering a collaborative environment that values code sharing. Maintaining basic standards for code quality, reusability, and shareability is critical for increasing the trustworthiness and reliability of research in experimental psychology and cognitive neuroscience.",
issn="2731-9121",
doi="10.1038/s44271-025-00236-3",
url="https://doi.org/10.1038/s44271-025-00236-3"
}


@Article{Adewale2025,
author="Adewale, Muyideen Dele
and Muhammad, Umaina Ibrahim",
title="From Virtual Companions to Forbidden Attractions: The Seductive Rise of Artificial Intelligence Love, Loneliness, and Intimacy---A Systematic Review",
journal="Journal of Technology in Behavioral Science",
year="2025",
month="Jul",
day="24",
abstract="The rapid proliferation of artificial intelligence (AI) companions and social chatbots has reshaped human emotional landscapes, raising urgent questions about intimacy, ethics, and mental health in human-AI relationships. This AI-assisted systematic review synthesises 37 studies (2021--2025) to examine how AI mediates intimacy, balancing its potential to alleviate loneliness with risks of dependency and privacy violations. Guided by Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) 2020 guidelines, we identify four core themes: (1) emotional and social mechanisms, such as anthropomorphism and identity exploration; (2) dual psychological impacts, including reduced isolation versus over-reliance; (3) ethical dilemmas like data exploitation and algorithmic manipulation; and (4) cultural variation in adoption, such as therapeutic acceptance in East Asia compared to authenticity debates in Western contexts. Key findings reveal that AI companions, such as Replika, simulate trust and reciprocity through personalised interactions, yet users often grapple with authenticity dilemmas and emotional dissonance. Anchored in attachment theory and socio-affective frameworks, the analysis highlights emerging risks, such as AI-induced role-reversal and pseudo-intimacy, while underscoring design and policy imperatives. We propose actionable safeguards: transparency in AI's emotional limitations, culturally nuanced regulations, and user-controlled `emotional break' features to mitigate harm. By bridging interdisciplinary perspectives, this review advances a global roadmap for ethical AI design that prioritises human dignity alongside technological innovation.",
issn="2366-5963",
doi="10.1007/s41347-025-00549-4",
url="https://doi.org/10.1007/s41347-025-00549-4"
}


@inproceedings{10.1007/978-3-032-02555-5_5,
 abstract = {Advances in generative AI (GenAI) facilitated the development of AI-based non-player characters (NPCs) capable of interacting with players more dynamically. These NPCs further allow for natural language communication with players via chat or speech. However, little is known about how players interact with GenAI-based NPCs in collaborative tasks, particularly when NPCs exhibit different levels of autonomy. We developed a video game where players collaborate with an NPC across three levels varying in NPC autonomy: NPC strictly follows player instructions, NPC has autonomy but prioritizes player guidance, and NPC operates fully autonomously. We conducted a user study ({\$}{\$}n{\textasciitilde}={\textasciitilde}16{\$}{\$}n=16) to examine how players perceive and interact with NPCs under these conditions. Results show that while greater autonomy enhances helpfulness and collaboration, players prefer a balance between proactive AI support and personal control. Our findings provide insights into player preferences, engagement, and the perceived agency of AI-driven NPCs in collaborative settings.},
 address = {Cham},
 author = {Dratzidis, Leon Tristan
and Zargham, Nima
and Malaka, Rainer},
 booktitle = {Entertainment Computing -- ICEC 2025},
 editor = {Sugimoto, Maki
and Di Iorio, Angelo
and Figueroa, Pablo
and Yamanishi, Ryosuke
and Matsumura, Kohei},
 isbn = {978-3-032-02555-5},
 pages = {60--77},
 publisher = {Springer Nature Switzerland},
 title = {``Follow My Lead'': Role of AI-Based NPC Autonomy in Player-NPC Collaboration},
 url = {https://link.springer.com/chapter/10.1007/978-3-032-02555-5_5},
 year = {2025}
}

@Article{Rouholamini2025,
author="Rouholamini, Mahdi
and Wang, Caisheng
and Magableh, Sharaf
and Wang, Xuesong",
title="Resiliency of electric power distribution networks: a review",
journal="Journal of Infrastructure Preservation and Resilience",
year="2025",
month="Nov",
day="20",
volume="6",
number="1",
pages="39",
abstract="Although relatively new in the context of power systems, the concept of resiliency is gaining increasing importance due to the growing vulnerability of electric infrastructure and the widespread outages triggered by extreme weather events. Unlike reliability studies, there still exists no well-agreed-upon method or criterion for quantifying the resiliency of power systems. This paper will lay out an overview of the concept of resiliency versus reliability and then review the foundational literature on power system resiliency with a focus on the distribution sector. The cwill be discussed as well..",
issn="2662-2521",
doi="10.1186/s43065-025-00154-y",
url="https://doi.org/10.1186/s43065-025-00154-y"
}


@Inbook{Norberg2024,
author="Norberg, Scott",
title="Advanced Web Security",
bookTitle="Advanced ASP.NET Core 8 Security: Move Beyond ASP.NET Documentation and Learn Real Security",
year="2024",
publisher="Apress",
address="Berkeley, CA",
pages="335--360",
abstract="Up until now, we've largely been discussing security topics that have been applicable to websites for decades now, even if the specific ways we discussed might be relatively new. For example, Indirect Object Reference (IDOR) vulnerabilities (like accessing a past order in our insecure version of the JuiceShop application) have been around for almost as long as the Web has been around, even if the idea of using IActionFilter to help you fix the issue might have been new to you.",
isbn="979-8-8688-0494-6",
doi="10.1007/979-8-8688-0494-6_10",
url="https://doi.org/10.1007/979-8-8688-0494-6_10"
}


@inproceedings{10.1007/978-981-96-9682-6_17,
 abstract = {Cutting-edge technology are necessary for cybersecurity to stay ahead of the ever changing landscape of cyberattacks. This industry has changed as a result of AI, which makes automated incident response, predictive analytics, and real-time threat identification possible. With an emphasis on LLMs and federated learning, this research examines recent developments in cybersecurity powered by AI. Large amounts of structured and unstructured data may be processed by LLMs like GPT-4, which improves threat intelligence and vulnerability identification. Federated learning allows for collaborative protection without disclosing sensitive data by offering privacy-preserving anomaly detection techniques across distributed platforms. The hybrid approach employs adversarial training to make the model resilient to complex attacks, federated learning for decentralized anomaly detection, and LLMs for threat intelligence. The method preserves data privacy while achieving 95{\%} detection accuracy on benchmark datasets such as CICIDS2017 and MITRE ATT{\&}CK. Furthermore, LLMs enhance attack pattern mitigation and prediction, resulting in a 30{\%} decrease in false positives. This study demonstrates that federated learning and LLMs can manage the privacy, scalability, and adaptability concerns of cybersecurity. The results demonstrate that AI can significantly increase resistance to APTs. Future directions include more reliable federated optimization techniques to boost efficiency and dependability in changing cyber environments and blockchain for immutable audit trails.},
 address = {Singapore},
 author = {Ramya, G. R.
and Vimal Harihar, S. K.
and Keshav, S.
and Niharika, V.},
 booktitle = {Proceedings of International Conference on Information Technology and Artificial Intelligence},
 editor = {Kumar, Sandeep
and Bye, Robin T.
and Prasad, Mukesh},
 isbn = {978-981-96-9682-6},
 pages = {223--236},
 publisher = {Springer Nature Singapore},
 title = {SecureAI-Cyber: An AI-Powered Cybersecurity Solution for Scalable Threat Management},
 url = {https://link.springer.com/chapter/10.1007/978-981-96-9682-6_17},
 year = {2026}
}

@Article{Mindom2024,
author="Mindom, Paulina Stevia Nouwou
and Nikanjam, Amin
and Khomh, Foutse",
title="Harnessing pre-trained generalist agents for software engineering tasks",
journal="Empirical Software Engineering",
year="2024",
month="Dec",
day="11",
volume="30",
number="1",
pages="39",
abstract="Nowadays, we are witnessing an increasing adoption of Artificial Intelligence (AI) to develop techniques aimed at improving the reliability, effectiveness, and overall quality of software systems. Deep reinforcement learning (DRL) has recently been successfully used for automation in complex tasks such as game testing and solving the job-shop scheduling problem, as well as learning efficient and cost-effective behaviors in various environments. However, these specialized DRL agents, trained from scratch on specific tasks, suffer from a lack of generalizability to other tasks and they need substantial time to be developed and re-trained effectively. Recently, DRL researchers have begun to develop generalist agents, able to learn a policy from various environments (often Atari game environments) and capable of achieving performance similar to or better than specialist agents in new tasks. In the Natural Language Processing or Computer Vision domain, these generalist agents are showing promising adaptation capabilities to never-before-seen tasks after a light fine-tuning phase and achieving high performance. To the best of our knowledge, no study has investigated the applicability of these generalist agents to SE tasks. This paper investigates the potential of generalist agents for solving SE tasks. Specifically, we conduct on three increasingly used SE tasks: playtesting in games (for two games), bug localization in software projects (i.e., six software projects) and the minimization of makespan in task scheduling in cloud computing (for two instances). Our results show that the generalist agents outperform the specialist agents with very little effort for fine-tuning, achieving a 20{\%} reduction of the makespan over specialized agent performance on task-based scheduling. In the context of game testing, some generalist agent configurations find bugs 3-8{\%} faster than the specialist agents. Finally, in the context of bug localization, generalist agents perform at least 9{\%} better than specialist agents in terms of Mean Reciprocal Rank. Building on our analysis, we provide recommendations for researchers and practitioners looking to select generalist agents for SE tasks, to ensure that they perform effectively.",
issn="1573-7616",
doi="10.1007/s10664-024-10597-8",
url="https://doi.org/10.1007/s10664-024-10597-8"
}


@Inbook{Ing2024,
author="Ing, David",
editor="Nousala, Susu
and Metcalf, Gary
and Ing, David",
title="Incremental Adaptation or Generational Shift?",
bookTitle="Industry 4.0 to Industry 5.0: Explorations in the Transition from a Techno-economic to a Socio-technical Future",
year="2024",
publisher="Springer Nature Singapore",
address="Singapore",
pages="151--184",
abstract="As Industry 4.0 matures, what's next? A generational shift to 5.0? Or an incremental adaptation to 4.x? Systems changes may involve both Socio-Technical Systems (STS) changes and Socio-Ecological Systems (SES) changes. Distinctions are explored historically circa 1492 with The Age of Discovery and Industry 0.0, evolving through centuries before a 1.0 Industrial Revolution. From the late twentieth century, The Age of Information was led by STS changes bringing a service economy and a knowledge society. Into 2024, polycrisis appears to be building with SES changes of natural disruptions due to climate change and the pandemic. Prospects for 2030 see eras of a maturing 4.x and emerging 5.0 alongside each other, with uncertainty as to which system characterizes the period.",
isbn="978-981-99-9730-5",
doi="10.1007/978-981-99-9730-5_7",
url="https://doi.org/10.1007/978-981-99-9730-5_7"
}


@Article{İçen2025,
author="{\.{I}}{\c{c}}en, Sarper",
title="Cognitive-Behavioral and Psychodynamic Lenses on Adolescent Gaming Disorder Through AI-Generated Case Formulations: A Qualitative Analysis ",
journal="Psychiatric Quarterly",
year="2025",
month="Oct",
day="28",
abstract="Case formulation (CF) is central to personalized mental health care, yet little is known about how artificial intelligence (AI) may simulate theory-informed processes. This exploratory study examined how ChatGPT-4.0 generated CFs for adolescent gaming disorder using cognitive-behavioral and psychodynamic frameworks. Eight standardized fictional vignettes describing demographics, gaming behaviors, psychiatric symptoms, and family context were submitted with prompts requesting framework-specific formulations. Outputs underwent thematic analysis with structured parallel frameworks and reflexive coding. Cognitive-behavioral formulations emphasized schemas, distortions, avoidance, gaming's psychological functions, and motivational themes, yielding three exploratory subtypes: Avoidant--Anxious, Defiant--Externalizing, and Depression-Driven. Psychodynamic formulations highlighted intrapsychic conflict, defense mechanisms, relational templates, and symbolic meanings, producing four subtypes: Shame-Regulating, Grief-Avoidant, Inhibited/Anxious-Avoidant, and Control-Oriented. Across frameworks, convergences emerged around low self-worth, avoidance, family dysfunction, and gaming as emotional regulation. These findings suggest that large language models can approximate framework-based case formulations and highlight clinically relevant themes, though they are not generalizable beyond simulated cases. With ethical oversight, such tools may support integrative clinical thinking, education, and reflective supervision. Future work should compare AI- and clinician-generated formulations with real patient data to evaluate validity and utility.",
issn="1573-6709",
doi="10.1007/s11126-025-10231-w",
url="https://doi.org/10.1007/s11126-025-10231-w"
}


@Inbook{Stoica2024,
author="Stoica, Ion",
editor="Balas, Valentina Emilia
and Dzemyda, Gintautas
and Belciug, Smaranda
and Kacprzyk, Janusz",
title="Sky Computing: Opportunities and Challenges",
bookTitle="Decision Making and Decision Support in the Information Era: Dedicated to Academician Florin Filip",
year="2024",
publisher="Springer Nature Switzerland",
address="Cham",
pages="15--27",
abstract="In this chapter, we explore the potential evolution of the cloud computing ecosystem, emphasizing the similarities and difference to the developmental trajectories of telephony, the Internet, and PCs. So far the cloud computing market has been dominated by proprietary interfaces from its early entrants, like Amazon Web Services, Windows Azure, and Google Compute Engine. However, in recent years has been an increasing drive from organizations towards enhanced compatibility so they can manage their workloads across various clouds. Unfortunately, unlike the emergence of universal technology standards in previous tech domains, we argue that a comprehensive compatibility standard for the cloud is neither imminent nor necessary for facilitating workload mobility across clouds and could potentially hinder innovation. Instead, we advocate for the adoption of ``intercloud brokers''--systems that optimize workload placement based on various customer criteria, such as price and performance, thereby eliminating the need for customers to navigate through cloud selection. This approach, which we term ``Sky Computing,'' is anticipated to lower entry barriers to cloud usage, spur technical innovation through specialized clouds, facilitate comprehensive integration of computational options, and enhance compliance, security, and resilience via cross-cloud deployments. We hope the opportunities and challenges we describe in this chapter will help trigger a broader collaboration effort between researchers and practitioners to develop Sky Computing, steering the future of cloud computing towards this vision.",
isbn="978-3-031-62158-1",
doi="10.1007/978-3-031-62158-1_2",
url="https://doi.org/10.1007/978-3-031-62158-1_2"
}


@Inbook{Mertoguno2024,
author="Mertoguno, J. Sukarno
and Briskin, Gregory
and Li, Jason H.
and Kwak, Kyung",
editor="Chen, Yingying
and Wu, Jie
and Yu, Paul
and Wang, Xiaogang",
title="Lessons Learned and Future Directions for Security, Resilience and Artificial Intelligence in Cyber Physical Systems",
bookTitle="Network Security Empowered by Artificial Intelligence",
year="2024",
publisher="Springer Nature Switzerland",
address="Cham",
pages="403--432",
abstract="Cyber-physical systems (CPS) are used in various safety-critical domains such as robotics, industrial manufacturing systems, and power systems. Faults and cyber attacks have been shown to cause safety violations, which can damage these systems and endanger human lives. The past decade has seen the proliferation of research efforts related to security and resilience in cyber physical systems, with an abundance of publications, workshops, and even media attention. More recently, artificial intelligence (AI) and machine learning (ML) have been reinvigorated and become the topic of paramount attention across the research community, mass media and society, highlighted by trademark successes in the fields of gaming, video, audio, and language translation. While it seems natural to apply AI/ML in CPS security and resilience, the authors would like to share some lessons learned and future directions as cautionary notes, which include: (1) the critical importance of physics and the physical world (P); (2) various means and effects that are introduced by cyber (C); (3) interactions and ramifications of P and C in a system (S); (4) system model and control in CPS; (5) enhanced robustness of control and autonomy in CPS by AI/ML; (6) pitfalls and appropriate positioning of AI/ML in CPS security and resilience; and (7) some challenges and opportunities for research and development.",
isbn="978-3-031-53510-9",
doi="10.1007/978-3-031-53510-9_15",
url="https://doi.org/10.1007/978-3-031-53510-9_15"
}


@Article{Palumickas2025,
author="Palumickas, M.
and Yamin, M. Mudassar
and Katt, B.
and Lal, Chhagan",
title="Evaluating ASCERT: generative AI for cyber-range scenario generation",
journal="International Journal of Information Security",
year="2025",
month="Dec",
day="15",
volume="25",
number="1",
pages="8",
abstract="In this paper, we worked in collaboration with the ASCERT (AI-based scenario management for cyber-range training) project and its generative AI prototype that generates dynamic and interactive cyber-range exercise scenarios. We evaluate the model by focusing on two objectives: (i) its ability to replicate real-world cyber attacks, and (ii) its consistency across multiple simulations that uses same inputs. To assess realism, we examine how well the model reproduces three well-documented cyber incidents namely Colonial Pipeline, Equifax, and SolarWinds, when it is provided with relevant source material for training. We then analyze repeatability by comparing outputs across fixed-input simulation runs. As the evaluation results indicate, overall the model generated varied and context-appropriate scenarios. Moreover, it introduced an interactivity feature that allows users to choose responses and observe consequences in real time. However, the consistency in repeated runs was limited: simulations are not reliably repeatable, although what was interesting is that the variability reflects the unpredictability of real attacks. These findings suggest that, while ASCERT already supports scenario variety and meaningful user interaction, it requires targeted refinements to improve stability and repeatability. With such improvements, the ASCERT model has strong potential to contribute to scalable and adaptive cybersecurity education and training.",
issn="1615-5270",
doi="10.1007/s10207-025-01179-w",
url="https://doi.org/10.1007/s10207-025-01179-w"
}


@Article{Mehrtabar2025,
author="Mehrtabar, Saba
and Marey, Ahmed
and Desai, Anushka
and Saad, Abdelrahman  M.
and Desai, Vishal 
and Go{\~{n}}i, Julian
and Pal, Basudha 
and Umair, Muhammad",
title="Ethical Considerations in Patient Privacy and Data Handling for AI in Cardiovascular Imaging and Radiology",
journal="Journal of Imaging Informatics in Medicine",
year="2025",
month="Sep",
day="24",
abstract="The integration of artificial intelligence (AI) into cardiovascular imaging and radiology offers the potential to enhance diagnostic accuracy, streamline workflows, and personalize patient care. However, the rapid adoption of AI has introduced complex ethical challenges, particularly concerning patient privacy, data handling, informed consent, and data ownership. This narrative review explores these issues by synthesizing literature from clinical, technical, and regulatory perspectives. We examine the tensions between data utility and data protection, the evolving role of transparency and explainable AI, and the disparities in ethical and legal frameworks across jurisdictions such as the European Union, the USA, and emerging players like China. We also highlight the vulnerabilities introduced by cloud computing, adversarial attacks, and the use of commercial datasets. Ethical frameworks and regulatory guidelines are compared, and proposed mitigation strategies such as federated learning, blockchain, and differential privacy are discussed. To ensure ethical implementation, we emphasize the need for shared accountability among clinicians, developers, healthcare institutions, and policymakers. Ultimately, the responsible development of AI in medical imaging must prioritize patient trust, fairness, and equity, underpinned by robust governance and transparent data stewardship.",
issn="2948-2933",
doi="10.1007/s10278-025-01656-7",
url="https://doi.org/10.1007/s10278-025-01656-7"
}


@inproceedings{10.1007/978-3-031-92823-9_10,
 abstract = {Widespread integration of Generative AI tools is transforming white-collar work, reshaping how workers define their roles, manage their tasks, and collaborate with peers. This has created a need to develop an overarching understanding of common worker-driven patterns around these transformations. To fill this gap, we conducted a systematic literature review of 23 studies from the ACM Digital Library that focused on workers' lived-experiences and practitioners with GenAI. Our findings reveal that while many professionals have delegated routine tasks to GenAI to focus on core responsibilities, they have also taken on new forms of AI managerial labor to monitor and refine GenAI outputs. Additionally, practitioners have restructured collaborations, sometimes bypassing traditional peer and subordinate interactions in favor of GenAI assistance. These shifts have fragmented cohesive tasks into piecework creating tensions around role boundaries and professional identity. Our analysis suggests that current frameworks, like job crafting, need to evolve to address the complexities of GenAI-driven transformations.},
 address = {Cham},
 author = {Law, Matthew
and Varanasi, Rama Adithya},
 booktitle = {HCI in Business, Government and Organizations},
 editor = {Siau, Keng Leng
and Nah, Fiona Fui-Hoon},
 isbn = {978-3-031-92823-9},
 pages = {131--152},
 publisher = {Springer Nature Switzerland},
 title = {Generative AI and Changing Work: Systematic Review of Practitioner-Led Work Transformations Through the Lens of Job Crafting},
 url = {https://link.springer.com/chapter/10.1007/978-3-031-92823-9_10},
 year = {2025}
}

@inproceedings{10.1007/978-3-032-09572-5_29,
 abstract = {In response to the increasing need to secure networked systems against unauthorized access and exploitation, it is paramount that some tools and techniques are present. Hence, even well-secured systems can be compromised due to common misconfigurations, often overlooked during setup and maintenance. This is also a significant concern in OWASP TOP 10, as in 2021 A05:2021 - Implies on Security Misconfigurations. This problem holds the fifth rank in OWASP TOP 10 and hence can be considered a relative problem for naive users. This project presents the development of a comprehensive Network and System Misconfiguration Analyzer, a tool designed to identify and rectify vulnerabilities stemming from misconfigurations in network devices, operating systems, and services. The analyzer leverages automated scanning techniques and best practice rule sets to detect common issues such as weak authentication protocols, improper file permissions, exposed services, and outdated software. The tool also includes remediations, enabling users to implement fixes efficiently. Implementing this analyzer is a significant step toward enhancing organizational security by systematically identifying and resolving configuration-related vulnerabilities. This paper presents a novel system misconfiguration detection framework that reduces vulnerability exposure by 95{\%}, improves incident response time by 90{\%}, and integrates seamlessly with 90{\%} of existing security tools.},
 address = {Cham},
 author = {Dewangan, Narendra Kumar
and Shankar, Gauri
and Srivastava, Naman
and Nigam, Akshat
and Bhatt, Divyansh
and Taufiq, Ayman},
 booktitle = {Computational Intelligence and Network Security},
 editor = {Chandrakar, Preeti
and Naik, K. Jairam
and Akhtar, Zahid
and Ming Chen, Chien
and Mohan, Karnati},
 isbn = {978-3-032-09572-5},
 pages = {367--379},
 publisher = {Springer Nature Switzerland},
 title = {Enhancing Linux Security: A Framework for Automated Misconfiguration Analysis and Remediation},
 url = {https://link.springer.com/chapter/10.1007/978-3-032-09572-5_29},
 year = {2026}
}

@inproceedings{10.1007/978-3-031-52183-6_8,
 abstract = {Component-based software engineering (CBSE) is a widely used software development paradigm. With software systems becoming increasingly sophisticated, CBSE provides an effective approach to construct reusable, extensible, and maintainable software systems. Formal verification provides a rigorous and systematic approach to validate the correctness of software systems by mathematically proving properties or checking them exhaustively against specified requirements. Using formal verification techniques in component-based development can further enhance the correctness of the development process. However, the adoption of component-based development supported by formal methods is hardly widespread in the industry. It serves to a limited extent in domains with stringent requirements for safety and reliability. In this paper, we aim to analyze the successful application scenarios of formal methods in component-based development, identify the challenges faced during their application, and explore methods to further broaden their adoption.},
 address = {Cham},
 author = {Li, Yi
and Sun, Meng},
 booktitle = {Formal Aspects of Component Software},
 editor = {C{\'a}mara, Javier
and Jongmans, Sung-Shik},
 isbn = {978-3-031-52183-6},
 pages = {153--167},
 publisher = {Springer Nature Switzerland},
 title = {Challenges Engaging Formal CBSE in Industrial Applications},
 url = {https://link.springer.com/chapter/10.1007/978-3-031-52183-6_8},
 year = {2024}
}

@Inbook{Jeyaraman2025,
author="Jeyaraman, Brindha Priyadarshini",
title="Deployment Strategies for LLMs",
bookTitle="Large Language Models Ops for Finance: A Practical Guide to Infrastructure, Implementation, and Innovation",
year="2025",
publisher="Apress",
address="Berkeley, CA",
pages="103--134",
abstract="This chapter explores key strategies for deploying large language models (LLMs) in production environments, particularly within the finance industry. It focuses on the essential components for building efficient, scalable, and reliable deployment systems for LLMs, ensuring that models can handle high-volume, real-time workloads while meeting strict regulatory and performance standards. The chapter also provides best practices for optimizing performance, monitoring system health, and managing resource usage to ensure smooth and cost-effective operations. By understanding how to efficiently manage LLM deployment, organizations can ensure their models deliver accurate and timely results without interruptions.",
isbn="979-8-8688-1700-7",
doi="10.1007/979-8-8688-1700-7_4",
url="https://doi.org/10.1007/979-8-8688-1700-7_4"
}


@Article{Wang2026,
author="Wang, Jin
and Gao, Yikun
and Chen, Qing
and Xiong, Xiaoxing
and Miao, Sen
and Chen, Xuemei
and Tang, Youjia
and Gu, Lijuan",
title="Microglia Mitochondrial Metabolism in Neurological Diseases",
journal="Molecular Neurobiology",
year="2026",
month="Jan",
day="03",
volume="63",
number="1",
pages="337",
abstract="Microglia, the resident immune cells of the central nervous system (CNS), play critical roles in maintaining brain homeostasis and responding to neurological insults. Recent advances have fundamentally reshaped our understanding of how microglial mitochondrial metabolism influences neuroinflammation and disease progression. Single-cell transcriptomics has revealed unexpected metabolic heterogeneity, identifying distinct phenotypes such as disease-associated microglia (DAM) and lipid-laden microglia (LLM) that represent not merely activated states but terminal endpoints of metabolic paralysis. These discoveries converge on a unified pathogenic mechanism: mitochondrial quality control failure leads to mitochondrial DNA release, which activates the cGAS--STING pathway to create an ``epigenetic lock'' that drives sustained neuroinflammation. Interestingly, we highlight that the loss of metabolic flexibility---rather than glycolysis per se---is the true driver of pathology, explaining why the same metabolic shift can be protective during acute injury but pathological when sustained chronically. We critically examine conflicting evidence across Alzheimer's disease, Parkinson's disease, multiple sclerosis, and ischemic stroke, including the puzzling dual roles of glycolysis, controversies surrounding the experimental autoimmune encephalomyelitis (EAE) model in multiple sclerosis research, and the paradoxical worsening of stroke outcomes following microglial depletion. By synthesizing these mechanistic insights with lessons from failed clinical trials, we identify critical translational gaps---including the lack of longitudinal human data and validated biomarkers---and propose a precision medicine framework focused on restoring mitochondrial dynamics and metabolic flexibility in neurological diseases.",
issn="1559-1182",
doi="10.1007/s12035-025-05640-8",
url="https://doi.org/10.1007/s12035-025-05640-8"
}


@Article{Ishida2025,
author="Ishida, Kentaro
and Murakami, Ryusuke
and Yamanoi, Koji
and Hamada, Kohei
and Hasebe, Koki
and Sakurai, Azusa
and Miyamoto, Taito
and Mizuno, Rin
and Taki, Mana
and Yamaguchi, Ken
and Hamanishi, Junzo
and Saito, Kenichi
and Kishimoto, Kazumasa
and Yamamoto, Goshiro
and Kuroda, Tomohiro
and Mandai, Masaki",
title="Real-world application of large language models for automated TNM staging using unstructured gynecologic oncology reports",
journal="npj Precision Oncology",
year="2025",
month="Nov",
day="19",
volume="9",
number="1",
pages="366",
abstract="Manual data entry in cancer registries is both time-consuming and prone to error. Although large language models (LLMs) offer promising solutions, prior studies have frequently relied on preprocessed datasets or required complex fine-tuning, limiting their applicability in clinical settings. Here, we assessed the performance of out-of-the-box LLMs on TNM classification tasks using only prompt engineering, without data anonymization or model fine-tuning. We identified manual registry error rates of 5.5--17.0{\%} in a real-world gynecologic cancer registry. Both a cloud-based LLM (Gemini 1.5; T- and N-stage accuracy: 0.994 and 0.993, respectively) and the top-performing local model (Qwen2.5 72B; T- and N-stage accuracy: 0.971 and 0.923, respectively) outperformed existing manual entries in extracting pathological T and N classifications. These models also achieved accuracies of 0.909 and 0.895 in clinical M classification, respectively. Our approach reflects real-world clinical workflows and offers a practical solution for enhancing data integrity in clinical registries using LLMs.",
issn="2397-768X",
doi="10.1038/s41698-025-01157-4",
url="https://doi.org/10.1038/s41698-025-01157-4"
}


@Inbook{Huang2025,
author="Huang, Ken
and Wu, Daniel
and Ponnapalli, Jyoti
and Huang, Grace",
editor="Huang, Ken",
title="AI Agents in Banking",
bookTitle="Agentic AI: Theories and Practices",
year="2025",
publisher="Springer Nature Switzerland",
address="Cham",
pages="237--277",
abstract="This chapter explores the transformative impact of AI agents on the banking industry, detailing how their sophisticated, multilayered architecture is revolutionizing operations, customer engagement, and value creation. It examines the key drivers of AI agent adoption, including the exponential growth of data, the need for real-time decision-making, evolving customer expectations, regulatory compliance, cost optimization, and innovation in financial products and services. The chapter provides an in-depth analysis of how AI agents are being applied in critical areas such as credit risk assessment, fraud detection and prevention, customer service, personalized banking, risk management, trading and securities, payments, and regulatory compliance. It also introduces the concept of ``digital workers'' as the next frontier in banking AI, highlighting their capabilities and providing real-world examples of their implementation. Finally, it addresses the challenges and considerations associated with AI adoption in banking, including data privacy, ethical concerns, regulatory compliance, human--AI collaboration, and the need for explainable AI, while also outlining steps banks should take to prepare for an AI-driven future.",
isbn="978-3-031-90026-6",
doi="10.1007/978-3-031-90026-6_8",
url="https://doi.org/10.1007/978-3-031-90026-6_8"
}


@Inbook{Nicoletti2025,
author="Nicoletti, Bernardo",
title="Project for AI-Driven Logistics Implementation and Utilization",
bookTitle="Artificial Intelligence for Logistics 5.0: From Foundation Models to Agentic AI",
year="2025",
publisher="Springer Nature Switzerland",
address="Cham",
pages="237--266",
abstract="This chapter examines implementing AIs in organizational logistics and operations management. The analysis emphasizes the importance of a structured approach to AI integration, distinguishing between standard AIs for general applications and vertical AIs tailored to specific industry needs. The implementation framework includes several critical components, including AI-powered digital twins, Industrial Internet of Things (IIoT) integration, natural language interfaces, and big data analytics (Srai et al., Supply chain digital twins: Opportunities and challenges beyond the hype. In 23rd Cambridge International Manufacturing Symposium, 26--27, 2019). The discussion highlights the importance of ethical governance and change management in AI implementation and advocates for dedicated ethics committees and systematic oversight processes. Cybersecurity is an important topic, particularly about AI agents in logistics, focusing on dealing with data breaches, hostile attacks, and logistics vulnerabilities through encryption, access controls, and regular audits. The technical implementation framework proposes a hybrid approach that combines proprietary and public data sources to optimize AI performance. The system architecture integrates Large Language Models (LLMs), AI kernels, and knowledge-based systems (KBS) with digital twins and IoT infrastructure (Nicoletti {\&} Appolloni, Framework of IoT, Blockchain, Digital twins, and Artificial Intelligence solutions in support of the digital business transformation of Logistics 5.0. In L. Ferreira, M. Cruz, E. Cruz, H. Quintela, {\&} M. Cunha (Eds.), Supporting technologies and the impact of blockchain on organizations and society (pp. 195--219). IGI Global. https://doi.org/10.4018/978-1-6684-5747-4.ch012, 2023b). This integration enables advanced monitoring, control, and optimization of logistics processes while maintaining user-friendly interfaces for operational accessibility. The chapter highlights the importance of comprehensive planning, ethical considerations, and ongoing maintenance for successful AI implementation in logistics operations.",
isbn="978-3-031-94046-0",
doi="10.1007/978-3-031-94046-0_9",
url="https://doi.org/10.1007/978-3-031-94046-0_9"
}


@Article{Kariv2025,
author="Kariv, Dafna
and Attar, Itay
and Haber, Yuval
and Elyoseph, Zohar",
title="AI-simulated entrepreneurship under uncertainty: forecasting university-driven capability evolution",
journal="The Journal of Technology Transfer",
year="2025",
month="Nov",
day="24",
abstract="Universities represent the crucial nexus between research and technology transfer, yet the high venture failure rates raise a fundamental question: Are academic institutions failing to equip entrepreneurs with the capabilities essential for navigating uncertainty? Despite expanding entrepreneurial programs, universities maintain outdated knowledge-delivery models focused primarily on traditional horizontal and vertical business knowledge, rather than crisis-relevant capabilities. This study examines whether universities develop essential entrepreneurial capabilities for navigating uncertainties, such as psychological and community-related, by investigating the manifestation of four capability domains: horizontal, vertical, psychological and community-related. By developing and training an AI-bot investor simulation, we evaluated how academically-supported versus independent entrepreneurs signal these capabilities and attract investor interest. Using LLM-based topic modeling and sentiment analysis, we discovered investor assessments strongly favor psychological and community capabilities alongside traditional business expertise, with substantial value placed on positive sentiment across all domains. This preference creates a critical mismatch with academically-supported entrepreneurs, who disproportionately emphasize vertical knowledge while neglecting psychological and community domains---resulting in lower overall positive sentiment that undermines their investment appeal. Surprisingly, while academic affiliation itself provides inherent credibility with investors, universities paradoxically fail to capitalize on this advantage. This missed opportunity becomes especially compelling as our data shows correlation between discussion frequency and positivity, especially in psychological and community domains. This finding represents fertile ground where university programs could refine entrepreneurial preparation. Our research advances effectuation theory and the CAVE model and challenges dynamic capabilities (DC) by demonstrating how complementary capabilities outside university's traditional focus impact entrepreneurial navigation in uncertainty.",
issn="1573-7047",
doi="10.1007/s10961-025-10306-7",
url="https://doi.org/10.1007/s10961-025-10306-7"
}


@Article{Liu2025,
author="Liu, Yunhao
and Liu, Li
and Zheng, Yawen
and Liu, Yunhuai
and Dang, Fan
and Li, Ningbo
and Ma, Ke",
title="Embodied navigation",
journal="Science China Information Sciences",
year="2025",
month="Mar",
day="13",
volume="68",
number="4",
pages="141101",
abstract="Navigation is a fundamental component of modern information application systems, ranging from military, transportations, and logistic, to explorations. Traditional navigations are based on an absolute coordination system that provides a precise map of the physical world, the locations of the moving objects, and the optimized navigation routes. In recent years, many new emerging applications have presented new demands for navigation, e.g., underwater/underground navigations where no GPS or other localizations are available, an un-explored area with no maps, and task-oriented navigations without specific routes. The advances in IoT and AI enable us to design new navigation paradigms, embodied navigation that allows the moving object to interact with the physical world to obtain the local map, localize the objects, and optimize the navigation routes accordingly. We make a systematic and comprehensive review of research in embodied navigation, encompassing key aspects on perceptions, navigation and efficiency optimization. Beyond advancements in these areas, we also examine the emerging tasks enabled by embodied navigation which require flexible mobility in diverse and evolving environments. Moreover, we identify the challenges associated with deploying embodied navigation systems in the real world and extend them to substantial areas. We aim for this article to provide valuable insights into this rapidly developing field, fostering future research to close existing gaps and advance the development of general-purpose autonomous systems grounded in embodied navigation.",
issn="1869-1919",
doi="10.1007/s11432-024-4303-8",
url="https://doi.org/10.1007/s11432-024-4303-8"
}


@Article{Pirouzkhah2025,
author="Pirouzkhah, Shirin
and Rani, Pooja
and Sovrano, Francesco
and Hellendoorn, Vincent
and Bacchelli, Alberto",
title="The price of precision: the cost of preprocessing for automated code revision in code review",
journal="Empirical Software Engineering",
year="2025",
month="Dec",
day="15",
volume="31",
number="2",
pages="47",
abstract="Code review is a widespread practice in software engineering during which developers examine each other's source code changes to identify potential issues and improve code quality. Among the automated techniques proposed by researchers to reduce the manual workload of code review, Automated Code Revision (ACR) aims to automatically address reviewers' feedback by producing a revised version of the code. Transformer-based language models have demonstrated state-of-the-art results in ACR. The performance of these models, however, is significantly influenced by the quality and preparation of the training and evaluation data. We present several systematic analyses of prevalent preprocessing steps, examined both cumulatively and in isolation, across three established preprocessing pipelines and two dataset splitting strategies (time-level vs. project-level). Our study spans across models of different scales: OpenNMT (small), T5 and CodeReviewer (mid-sized), LoRA-tuned CodeLLaMA-7B (large), and GPT-3.5-Turbo (large, black-box). Using datasets up to 496k training records, we evaluate and statistically compare models' performance using exact match ratio (EXM), CodeBLEU, and Levenshtein ratio. Our findings show that preprocessing may be a significant component in the success of the different techniques: OpenNMT relies on heavy preprocessing; T5 benefits from light filtering (selective removal of records); CodeReviewer performs best when trained on larger, less aggressively filtered data; CodeLLaMA-7B and ChatGPT-3.5 Turbo are largely indifferent to preprocessing. Overall, the effectiveness of ACR tools depends on aligning preprocessing with model scale and training setup. In general, small models need abstraction, mid-sized ones benefit from light filtering, and large-scale models perform best when trained on the original, unprocessed form of the code.",
issn="1573-7616",
doi="10.1007/s10664-025-10781-4",
url="https://doi.org/10.1007/s10664-025-10781-4"
}


@Inbook{Hariharan2025,
author="Hariharan, B.
and Wilfred Blessing, N. R.
and Palarimath, Suresh
and Neerukonda, Ratna Kumari
and Anupama, C. G.
and Sutherlin Subitha, G.",
editor="Al Qamashoui, Aziza
and Al Baimani, Nasser",
title="Speeding Up the Recruitment Process Using ChatGPT APIs",
bookTitle="AI Integration for Business Sustainability: For a Resilient Future",
year="2025",
publisher="Springer Nature Singapore",
address="Singapore",
pages="301--316",
abstract="Manual recruitment can be time- and resource-intensive. A company's success lies in identifying and hiring the right candidates. However, traditional recruitment can be time-consuming and tedious. It typically involves various processes, such as creating job descriptions, researching job resumes, scheduling interviews, and connecting with potential employees. This study examines ChatGPT APIs, an artificial intelligence (AI) technology used to speed up the recruitment process. This article explores the potential of ChatGPT APIs, using a large language sample, to accelerate the workflow by simplifying systems in multiple environments. We discuss the use of ChatGPT APIs for initial testing, highly targeted outreach messages, and the automation of services. We will explore the potential advantages and disadvantages of using ChatGPT APIs in the recruitment process, including the need for human oversight, reduced bias, and increased productivity.",
isbn="978-981-96-3464-4",
doi="10.1007/978-981-96-3464-4_18",
url="https://doi.org/10.1007/978-981-96-3464-4_18"
}


@Article{Chen2025,
author="Chen, Yi-Ge
and Fan, Yu-Jia
and Wang, Si-Nan
and Tao, Yi-Da
and Liu, Ye-Pang",
title="HmTest: Automated Testing of HarmonyOS Apps via Model-Driven Navigation and Reinforcement Learning",
journal="Journal of Computer Science and Technology",
year="2025",
month="Jul",
day="01",
volume="40",
number="4",
pages="1006--1021",
abstract="HarmonyOS is a new all-scenario operating system for smart devices. As its software ecosystem expands rapidly, how to conduct automated testing of HarmonyOS apps for quality assurance has become a crucial task. This paper presents HmTest, an automated testing framework for HarmonyOS apps, which consists of two complementary modules: targeted exploration and reinforcement learning (RL)-based exploration. Targeted exploration performs white-box testing, leveraging static analysis to construct a page transition graph (PTG). By systematically traversing PTGs to guide testing, HmTest can quickly achieve high page coverages. On the other hand, RL-based exploration performs black-box testing, utilizing reinforcement learning to achieve a comprehensive exploration of app states. Additionally, an automaton-based mechanism is employed to efficiently recover and restart the testing process when reinforcement learning encounters stagnation. We have evaluated HmTest on nine HarmonyOS NEXT apps and compared it with two official HarmonyOS app testing tools. The experimental results demonstrate that targeted exploration can generate highly-precise PTGs and help achieve high page coverages within a few minutes. RL-based exploration can significantly outperform other methods in terms of finer-grained statement coverage on the majority of the tested apps and benefits from the recovery mechanism. To facilitate future research, we have made HmTest open-source at https://github.com/sqlab-sustech/hmtestand provided a video demo at https://jcst.ict.ac.cn/news/361.",
issn="1860-4749",
doi="10.1007/s11390-025-5142-4",
url="https://doi.org/10.1007/s11390-025-5142-4"
}


@inproceedings{10.1007/978-3-031-93724-8_7,
 abstract = {What is complex systems' role in augmented cognition? Neurotechnology---especially electroencephalography (EEG), brain-computer interface (BCI), and neural networks---have seen extensive use in augmented cognition (AugCog). Functional near-infrared spectroscopy (fNIRS) was also used in [5], and functional magnetic resonance imaging (fMRI) in [6]. To what extent can such technologies address the total complexity of augmented cognition?},
 address = {Cham},
 author = {Sood, Suraj},
 booktitle = {Augmented Cognition},
 editor = {Schmorrow, Dylan D.
and Fidopiastis, Cali M.},
 isbn = {978-3-031-93724-8},
 pages = {79--92},
 publisher = {Springer Nature Switzerland},
 title = {Is Augmented Cognition a Complex System?},
 url = {https://link.springer.com/chapter/10.1007/978-3-031-93724-8_7},
 year = {2025}
}

@inproceedings{10.1007/978-981-96-0602-3_18,
 abstract = {In-context Learning (ICL) has achieved notable success in large language models (LLMs) applications. By adding only a few input-output pairs demonstrating a new task, LLMs can efficiently learn the task during inference without modifying their parameters. Such mysterious ability of LLMs has attracted great research interests in understanding, formatting, and improving the in-context demonstrations, while still suffering from drawbacks like black-box mechanisms and sensitivity against the selection of examples. In this work, inspired by the foundations of adopting testing techniques in machine learning (ML) systems, we propose a mutation testing framework designed to characterize the quality and effectiveness of test data for ICL systems. First, we propose several mutation operators specialized for ICL demonstrations, as well as the corresponding mutation scores for ICL test sets. With comprehensive experiments, we showcase the effectiveness of our framework in evaluating the reliability and quality of ICL test suites. Our code is available at https://github.com/weizeming/MILE.},
 address = {Singapore},
 author = {Wei, Zeming
and Zhang, Yihao
and Sun, Meng},
 booktitle = {Dependable Software Engineering. Theories, Tools, and Applications},
 editor = {Bourke, Timothy
and Chen, Liqian
and Goharshady, Amir},
 isbn = {978-981-96-0602-3},
 pages = {327--343},
 publisher = {Springer Nature Singapore},
 title = {MILE: A Mutation Testing Framework of In-Context Learning Systems},
 url = {https://link.springer.com/chapter/10.1007/978-981-96-0602-3_18},
 year = {2025}
}

@Article{Karmakar2023,
author="Karmakar, Anjan
and Allamanis, Miltiadis
and Robbes, Romain",
title="JEMMA: An extensible Java dataset for ML4Code applications",
journal="Empirical Software Engineering",
year="2023",
month="Mar",
day="10",
volume="28",
number="2",
pages="54",
abstract="Machine Learning for Source Code (ML4Code) is an active research field in which extensive experimentation is needed to discover how to best use source code's richly structured information. With this in mind, we introduce JEMMA: An Extensible Java Dataset for ML4Code Applications, which is a large-scale, diverse, and high-quality dataset targeted at ML4Code. Our goal with JEMMA is to lower the barrier to entry in ML4Code by providing the building blocks to experiment with source code models and tasks. JEMMA comes with a considerable amount of pre-processed information such as metadata, representations (e.g., code tokens, ASTs, graphs), and several properties (e.g., metrics, static analysis results) for 50,000 Java projects from the 50K-C dataset, with over 1.2 million classes and over 8 million methods. JEMMA is also extensible allowing users to add new properties and representations to the dataset, and evaluate tasks on them. Thus, JEMMA becomes a workbench that researchers can use to experiment with novel representations and tasks operating on source code. To demonstrate the utility of the dataset, we also report results from two empirical studies on our data, ultimately showing that significant work lies ahead in the design of context-aware source code models that can reason over a broader network of source code entities in a software project---the very task that JEMMA is designed to help with.",
issn="1573-7616",
doi="10.1007/s10664-022-10275-7",
url="https://doi.org/10.1007/s10664-022-10275-7"
}


@Article{Yao2025,
author="Yao, Yuan",
title="Seemingly understood, nobody answerable",
journal="AI {\&} SOCIETY",
year="2025",
month="Dec",
day="28",
issn="1435-5655",
doi="10.1007/s00146-025-02821-w",
url="https://doi.org/10.1007/s00146-025-02821-w"
}


@Inbook{Marchant2025,
author="Marchant, Neil G.
and Zhao, Ying
and Rubinstein, Benjamin I.  P.
and Ohrimenko, Olga",
editor="Sadiq, Shazia",
title="Data Privacy in Enterprise AI",
bookTitle="Enterprise AI",
year="2025",
publisher="Springer Nature Switzerland",
address="Cham",
pages="129--181",
abstract="Enterprise AI systems often process vast amounts of personal and sensitive data, making them vulnerable to breaches and misuse. Consider, for example, a financial institution that trains a predictive model on sensitive customer data that is to be shared with partner organizations. If the model is shared without proper precautions, the partner organization or a malicious employee may attempt to breach the privacy of the original training data by launching an attack to reconstruct this data from model parameters. Such vulnerabilities erode trust in enterprise and AI systems, may fail to comply with privacy legislation, and can lead to financial, intellectual property or reputational loss. Privacy is a cornerstone of ethical and responsible AI deployment. This chapter provides a guide to the tools necessary to navigate the landscape of AI privacy, ultimately facilitating broader and more secure AI adoption in the enterprise.",
isbn="978-3-032-01940-0",
doi="10.1007/978-3-032-01940-0_5",
url="https://doi.org/10.1007/978-3-032-01940-0_5"
}


@Article{Bozzolan2025,
author="Bozzolan, Simone
and Calzavara, Stefano
and Porcu, Davide",
title="Cookie Baker: gray-box login automation for web application security testing",
journal="International Journal of Information Security",
year="2025",
month="Dec",
day="11",
volume="25",
number="1",
pages="6",
abstract="We present Cookie Baker, the first gray-box login automation framework designed to enhance web application security testing. Cookie Baker is designed as a conservative extension of Cookie Hunter, a state-of-the-art black-box login automation tool. By combining static analysis and automated credential harvesting, Cookie Baker significantly increases the success rate of Cookie Hunter and improves the diversity of the available account types, thus making security testing more effective and realistic. Our experimental evaluation on public web applications shows that the additional capabilities of Cookie Baker make it able to automatically login on 4x more web applications than Cookie Hunter. This substantial improvement in login automation translates into greater code coverage during web crawling, ultimately leading to a higher rate of vulnerability detection. The integration of Cookie Baker with the Wapiti security scanner enables us to identify several new potential vulnerabilities in existing software, including two confirmed stored XSS. These findings highlight Cookie Baker's potential to enhance web application security testing, equipping security researchers and penetration testers with a powerful tool to uncover security flaws that would otherwise remain undetected.",
issn="1615-5270",
doi="10.1007/s10207-025-01168-z",
url="https://doi.org/10.1007/s10207-025-01168-z"
}


@Article{Coeckelbergh2024,
author="Coeckelbergh, Mark
and Gunkel, David J.",
title="ChatGPT: deconstructing the debate and moving it forward",
journal="AI {\&} SOCIETY",
year="2024",
month="Oct",
day="01",
volume="39",
number="5",
pages="2221--2231",
abstract="Large language models such as ChatGPT enable users to automatically produce text but also raise ethical concerns, for example about authorship and deception. This paper analyses and discusses some key philosophical assumptions in these debates, in particular assumptions about authorship and language and---our focus---the use of the appearance/reality distinction. We show that there are alternative views of what goes on with ChatGPT that do not rely on this distinction. For this purpose, we deploy the two phased approach of deconstruction and relate our finds to questions regarding authorship and language in the humanities. We also identify and respond to two common counter-objections in order to show the ethical appeal and practical use of our proposal.",
issn="1435-5655",
doi="10.1007/s00146-023-01710-4",
url="https://doi.org/10.1007/s00146-023-01710-4"
}


@Inbook{Pandey2026,
author="Pandey, Pravin",
editor="Pandey, Bishwajeet
and Patel, Advait",
title="The Future of Generative AI in Cloud Infrastructure",
bookTitle="Revolutionizing the Cloud: Generative AI, Security, and Sustainability",
year="2026",
publisher="Springer Nature Switzerland",
address="Cham",
pages="151--172",
abstract="Generative Artificial Intelligence (GenAI) is revolutionizing how we develop, manage, and operate cloud infrastructure. It goes beyond simple automation, offering intelligent, adaptable, and scalable capabilities that meet the swiftly changing demands of digital businesses. GenAI is establishing new benchmarks for cloud agility and efficiency with its increasing significance in Infrastructure as Code (IaC), architectural design, FinOps, DevSecOps, and Site Reliability Engineering (SRE). This chapter provides a comprehensive guide for understanding GenAI's applications, potential, and challenges in cloud platforms. We explore the strategies employed to leverage GenAI, the security and adoption challenges faced, and the critical steps necessary to ensure their infrastructure strategies are future ready. Featuring practical case studies and a proactive roadmap, this chapter is aimed at students, professionals, and business leaders committed to embracing AI-driven cloud solutions modernization.",
isbn="978-3-032-07479-9",
doi="10.1007/978-3-032-07479-9_8",
url="https://doi.org/10.1007/978-3-032-07479-9_8"
}


@Article{Anand2026,
author="Anand, Parul
and Coeckelbergh, Mark",
title="AI, climate change and justice: elements for a normative framework centring the Global South",
journal="AI and Ethics",
year="2026",
month="Feb",
day="05",
volume="6",
number="1",
pages="144",
abstract="AI's inextricable materiality is central to its environmental impact. From energy-intensive computation to the extractive demands of hardware and infrastructure, AI technologies are deeply embedded in global material and ecological systems. Examined through a socio-political lens, these environmental dimensions raise pressing justice concerns. This paper argues that discussions of AI's environmental impacts must move beyond efficiency metrics to incorporate a global climate justice perspective within AI ethics, with particular attention to unequal global impacts. The paper unfolds in three sections. First, it demonstrates how AI's environmental footprint across its full lifecycle, from development to deployment, constitutes a global climate justice concern. This includes analysis of energy use and water consumption in data centres, as well as the geopolitical implications of mineral extraction for hardware. Second, the paper examines what applying a climate justice perspective to AI ethics entails. Drawing on political philosophy, it focuses on two core dimensions of justice: distributional (who bears the costs and benefits of AI's environmental impacts) and procedural (who is included in, or excluded from, decision-making about AI development and deployment). Together, these dimensions inform a set of conceptual and normative tools for assessing the AI-climate nexus, with particular relevance to Global South contexts. Finally, the paper grounds this discussion in a case study of India, illustrating how AI development intersects with climate vulnerability and global environmental inequality. The paper concludes by reflecting on how a climate justice approach can clarify the ethical stakes of AI's environmental impacts and contribute to more inclusive and globally relevant AI governance.",
issn="2730-5961",
doi="10.1007/s43681-025-00949-5",
url="https://doi.org/10.1007/s43681-025-00949-5"
}


@Inbook{ReddyVootukuri2025,
author="Reddy Vootukuri, Naga Santhosh",
title="GitHub Copilot Chat in Developer Workflow",
bookTitle="Vibe Coding with GitHub Copilot: Enhancing Productivity by Leveraging GitHub Copilot Inside Visual Studio ",
year="2025",
publisher="Apress",
address="Berkeley, CA",
pages="39--77",
abstract="In Chapter 2, we explored the core features of GitHub Copilot including inline suggestions, autocompletions, and various prompting techniques. We learned how to get meaningful Copilot suggestions by providing effective prompts using comments, method signatures, or through partial code. In this chapter, we will dive deeper into how GitHub Copilot integrates seamlessly into developer workflow. We will discuss GitHub Copilot Chat and explore its features including Ask mode and Agent mode. We will examine how we can leverage GitHub Copilot Chat in various stages of Software Development Life Cycle (SDLC) ranging from rapid prototyping, feature development, refactoring, testing, and during code reviews. The learning objectives for this chapter include the following:",
isbn="979-8-8688-2196-7",
doi="10.1007/979-8-8688-2196-7_3",
url="https://doi.org/10.1007/979-8-8688-2196-7_3"
}


@Inbook{Evelyn2024,
author="Evelyn, Lydia
and Hopkins, Bruce",
title="Using AI in the Enterprise! Creating a Text Summarizer for Slack Messages",
bookTitle="Beginning ChatGPT for Python: Build Intelligent Applications with OpenAI APIs",
year="2024",
publisher="Apress",
address="Berkeley, CA",
pages="75--100",
abstract="In today's corporate world, it's extremely common for companies to have an instance of Slack (or Microsoft Teams) to organize themselves and use it as a central place of communication to everyone in the company. Now, if you've ever used Slack before, I think you know how easily a channel can become flooded with a ton of messages because some important thing happened somewhere in the company or the world.",
isbn="979-8-8688-0929-3",
doi="10.1007/979-8-8688-0929-3_4",
url="https://doi.org/10.1007/979-8-8688-0929-3_4"
}


@Inbook{Khan2026,
author="Khan, Obaid
and Tripathi, Nirnaya
and Xu, Yueqiang
and Duc, Anh Nguyen",
editor="Tripathi, Nirnaya
and Edison, Henry
and Wang, Xiaofeng",
title="AI-Powered Toolchain Framework for Agile Software Development in Startups",
bookTitle="Advances in Software Startups: Generative AI, Product Engineering and Business Development",
year="2026",
publisher="Springer Nature Switzerland",
address="Cham",
pages="193--210",
abstract="The integration of AI-powered tools into Agile methodologies within startups is explored in this chapter, emphasizing how AI-driven tools enhance efficiency, innovation, and team collaboration. Drawing from interviews with professionals from six software startups, it highlights the transformative impact of tools like GitHub Copilot, ChatGPT, and Fireflies.ai in streamlining Agile practices, such as task automation, code quality assurance, sprint planning, and backlog management. The study examines AI-powered tool's role in boosting productivity, reducing workloads, and accelerating delivery through automated documentation, predictive insights, and real-time analytics. An AI-enhanced Agile toolchain framework is introduced, showcasing strategies for backlog prioritization, continuous integration and delivery (CI/CD), and workflow optimization. It also addresses challenges like balancing automation with Agile's human-centric values, ethical concerns, and resource constraints.",
isbn="978-3-032-04294-1",
doi="10.1007/978-3-032-04294-1_11",
url="https://doi.org/10.1007/978-3-032-04294-1_11"
}


@Article{Lang2025,
author="Lang, Benjamin H.",
title="Dropping Anchor or Chasing the Horizon? Theoretical and Practical Challenges for Personalized AI Advisors",
journal="Philosophy {\&} Technology",
year="2025",
month="Oct",
day="23",
volume="38",
number="4",
pages="150",
abstract="Unlike generic AI advisors which aid in normative deliberation according to preloaded values and creeds (i.e., Singerian Utilitarianism, Calvinist Protestantism, or Stoicism), personalized AI advisors aim to aid in users' decision-making by their own lights. In this paper, I argue personalized AI advisors face a challenge called the Anchoring Problem: the difficulty of adjudicating between competing temporal and psychological reference points for normative guidance---whether to ``chase the horizon,'' defined as dynamically calibrating to whichever aspirational self or set of beliefs, dispositions, or values, a user presently holds, or to ``drop anchor,'' defined as stubbornly preserving whichever values the user set out with. This problem is compounded by several observations about aspirational selfhood, namely that a person's conception of their aspirational self may be underdetermined and the identity it purports to describe may be incoherent, romanticized, or contradictory. Even if this is not the case, the aspirational self is rarely diachronically stable such that who one wishes to be or how one wishes to improve remains static over time. In this paper, I argue personalized AI advisors promise a number of advantages over generic ones if the Anchoring Problem proves surmountable, and I argue it is within certain constraints. Namely, I argue for the necessity of a `co-reasoning,' dialectical model of personalization. Co-reasoning should safeguard the user against value hijacking by encouraging their deliberative and decisional participation while also pushing them to confront unaccounted discrepancies between stated values, aspiring values, and actual behavior (both past and present).",
issn="2210-5441",
doi="10.1007/s13347-025-00990-6",
url="https://doi.org/10.1007/s13347-025-00990-6"
}


@Inbook{Al-Ahmad2025,
author="Al-Ahmad, Mohammad Hussein
and Al-Khazraji, Istabraq Saad",
editor="Hannoon, Azzam
and Mahmood, Abdullah",
title="Criminal Liability for Artificial Intelligence Crimes",
bookTitle="Intelligence-Driven Circular Economy: Regeneration Towards Sustainability and Social Responsibility---Volume 2",
year="2025",
publisher="Springer Nature Switzerland",
address="Cham",
pages="575--587",
abstract="The widespread implementation of artificial intelligence (AI) technologies and their rapid advancement have raised concerns about the potential for errors in AI operations. Of particular concern is the sophisticated programming of certain AI technologies, which may eventually possess the ability to make independent decisions and engage in criminal activities without human intervention. Therefore, it is equally important to study the issue of criminal liability for committed crimes in order to identify the real perpetrator and the prescribed punishment. After careful analysis, researchers found that the current legal structure is insufficient to hold AI technologies accountable for their actions. Therefore, it is recommended that the legislator develops a comprehensive legal framework for the use of AI technologies. The framework should precisely assign liability for crimes committed by technology, as well as impose penalties. This way we can be sure that justice will prevail and the required penalties will be imposed if deemed necessary for the use of AI technology.",
isbn="978-3-031-74220-0",
doi="10.1007/978-3-031-74220-0_45",
url="https://doi.org/10.1007/978-3-031-74220-0_45"
}


@Article{Dessureault2025,
author="Dessureault, Jean-S{\'e}bastien
and Lamontagne, Robert
and Paris{\'e}, Pierre-Olivier",
title="The ethics of creating artificial superintelligence: a global risk perspective",
journal="AI and Ethics",
year="2025",
month="Dec",
day="01",
volume="5",
number="6",
pages="6241--6263",
abstract="As artificial intelligence (AI) continues its exponential growth and nears the threshold of artificial general intelligence (AGI), it is timely and urgent to initiate reflections on artificial superintelligence (ASI), which may emerge rapidly after AGI. While ASI remains hypothetical, its potential emergence could be abrupt and profoundly transformative, necessitating proactive ethical and strategic inquiry. This paper proposes a multidimensional reflection on ASI, not only in its technical form but also in relation to humanity and the planetary context. It seeks to answer the question: ``Should Homo sapiens develop an artificial superintelligence on their planet?'' The paper introduces key definitions, outlines major existential risks to humanity and the biosphere, and considers whether ASI could mitigate these threats. It ultimately proposes a conceptual equation to assess the potential net impact of ASI, and introduces an original Venn diagram that classifies problem domains across AI, AGI, and ASI. Together, these tools aim to advance theoretical understanding and guide future inquiry into the core research question.",
issn="2730-5961",
doi="10.1007/s43681-025-00793-7",
url="https://doi.org/10.1007/s43681-025-00793-7"
}


@Article{Kilian2026,
author="Kilian, Kyle A.",
title="Beyond accidents and misuse: decoding the structural risk dynamics of artificial intelligence",
journal="AI {\&} SOCIETY",
year="2026",
month="Jan",
day="01",
volume="41",
number="1",
pages="23--42",
abstract="As artificial intelligence (AI) becomes increasingly embedded in the core functions of social, political, and economic life, it catalyzes structural transformations with far-reaching societal implications. This paper advances the concept of structural risk by introducing a framework grounded in complex systems research to examine how rapid AI integration can generate emergent, system-level dynamics beyond conventional, proximate threats such as system failures or malicious misuse. It argues that such risks are both influenced by and constitutive of broader sociotechnical structures. We classify structural risks into three interrelated categories: antecedent structural causes, antecedent AI system causes, and deleterious feedback loops. By tracing these interactions, we show how unchecked AI development can destabilize trust, shift power asymmetries, and erode decision-making agency across scales. To anticipate and govern these dynamics, this paper proposes a methodological agenda incorporating scenario mapping, simulation, and exploratory foresight. We conclude with policy recommendations aimed at cultivating institutional resilience and adaptive governance strategies for navigating an increasingly volatile AI risk landscape.",
issn="1435-5655",
doi="10.1007/s00146-025-02419-2",
url="https://doi.org/10.1007/s00146-025-02419-2"
}


@Inbook{Nachuma2025,
author="Nachuma, Costain
and Rabbi, Md. Fazle
and Champa, Arifa I.
and Zibran, Minhaz F.",
editor="Lee, Roger",
title="Analyzing ChatGPT Assistance in Programming",
bookTitle="Software Engineering and Management: Theory and Applications: Volume 17",
year="2025",
publisher="Springer Nature Switzerland",
address="Cham",
pages="203--215",
abstract="In this study, we investigate how developers interact with ChatGPT when seeking programming assistance to identify trends in language preferences and access communication efficiency. We analyze a public dataset, DevGPT, comprising 17,622 ChatGPT prompts, and 12,031 code snippets. Our findings show Python, Bash, and JavaScript as primary languages used in developer-ChatGPT interactions. We find that data manipulation/exchange languages/formats, namely JSON and SQL, require fewer interactions, while general-purpose languages such as C++ and C{\#} need lengthier conversations. The findings provide valuable insights for improving training programs and enhancing ChatGPT's language understanding, ultimately optimizing collaboration between developers and AI tools in software development.",
isbn="978-3-031-82610-8",
doi="10.1007/978-3-031-82610-8_13",
url="https://doi.org/10.1007/978-3-031-82610-8_13"
}


@Inbook{Fredericks2025,
author="Fredericks, Erik M.
and Bobeldyk, Denton
and Moore, Jared M.",
editor="Winkler, Stephan M.
and Banzhaf, Wolfgang
and Hu, Ting
and Lalejini, Alexander",
title="Crafting Generative Art Through Genetic Improvement: Managing Creative Outputs in Diverse Fitness Landscapes",
bookTitle="Genetic Programming Theory and Practice XXI",
year="2025",
publisher="Springer Nature Singapore",
address="Singapore",
pages="321--335",
abstract="Generative artGenerative art is a rules-driven approach to creating artistic outputs in various mediums. For example, a fluid simulation can govern the flow of colored pixels across a digital display or a rectangle placement algorithm can yield a Mondrian-style painting. Previously, we investigated how genetic improvementGenetic improvement, a sub-field of genetic programming, can automatically create and optimize generative artGenerative art drawing programs. One challenge of applying genetic improvement to generative artGenerative art is defining fitness functions and their interaction in a many-objective evolutionary algorithm such as Lexicase selection. Here, we assess the impact of each fitness function in terms of the their individual effects on generated images, characteristics of generated programs, and impact of bloat on this specific domain. Furthermore, we have added an additional fitness function that uses a classifierClassifier for mimicking a human's assessment as to whether an output is considered as ``art.'' This classifierClassifier is trained on a dataset of input images resembling the glitch art aesthetic that we aim to create. Our experimental results show that with few fitness functions, individual generative techniques sweep across populations. Moreover, we found that compositions tended to be driven by one technique with our current fitness functions. Lastly, we show that our classifierClassifier is best suited for filtering out noisy images, ideally leading toward more outputs relevant to user preference.",
isbn="978-981-96-0077-9",
doi="10.1007/978-981-96-0077-9_16",
url="https://doi.org/10.1007/978-981-96-0077-9_16"
}


@Inbook{Rahman2024,
author="Rahman, Rezwanur",
title="Mastering the Core: Copilot's Engine Room",
bookTitle="Microsoft Copilot for Power Apps: Transforming App Development with AI Assistance",
year="2024",
publisher="Apress",
address="Berkeley, CA",
pages="81--117",
abstract="What new, magical technology from Microsoft makes Copilot so incredible? How does this magic operate behind the scenes to transform complex programming tasks into simple, intuitive experiences? This chapter looks closely at the sophisticated technology that makes Copilot such a powerful assistant---AI Builder.",
isbn="979-8-8688-0512-7",
doi="10.1007/979-8-8688-0512-7_3",
url="https://doi.org/10.1007/979-8-8688-0512-7_3"
}


@Inbook{Hopkins2024,
author="Hopkins, Bruce",
title="Using AI in the Enterprise! Creating a Text Summarizer for Slack Messages",
bookTitle="ChatGPT for Java: A Hands-on Developer's Guide to ChatGPT and Open AI APIs",
year="2024",
publisher="Apress",
address="Berkeley, CA",
pages="57--97",
abstract="In today's corporate world, it's extremely common for companies to have an instance of Slack (or Microsoft Teams) to organize themselves, and use it as a central place of communication to everyone in the company. Now, if you've ever used Slack before, I think you know how easily a channel can become flooded with a ton of messages because SOME important thing happened SOMEWHERE in the company or the world.",
isbn="979-8-8688-0116-7",
doi="10.1007/979-8-8688-0116-7_3",
url="https://doi.org/10.1007/979-8-8688-0116-7_3"
}


@inproceedings{10.1007/978-3-031-75923-9_4,
 abstract = {The pressing need for sustainable and efficient energy solutions has driven the exploration of hybrid systems combining biomass, photovoltaic, and other renewable sources. This study introduces the Cognitive Innovator Xcelerator (CIX), an advanced AI framework designed to optimize the design and integration of these complex systems. CIX employs a multi-faceted approach, integrating Intelligent Problem Mapping and ASIT Thinking Modules to analyze and address challenges in hybrid energy system design. By leveraging AI-driven idea generation and solution refinement, CIX facilitates a systematic exploration of potential design configurations. The application of CIX led to the identification of novel, efficient configurations for hybrid energy systems, surpassing conventional designs in energy output and sustainability metrics. Physical testing of the optimized design confirmed theoretical predictions, demonstrating significant improvements in energy efficiency and system integration. Through intelligent problem mapping and creative solution generation, CIX proves to be a powerful tool in advancing the development of sustainable hybrid energy systems.},
 address = {Cham},
 author = {Brad, Stelian
and B{\u{a}}lan, Daniel},
 booktitle = {World Conference of AI-Powered Innovation and Inventive Design},
 editor = {Cavallucci, Denis
and Brad, Stelian
and Livotov, Pavel},
 isbn = {978-3-031-75923-9},
 pages = {57--82},
 publisher = {Springer Nature Switzerland},
 title = {Optimizing Green Hybrid Energy Systems Through Cognitive Innovator Xcelerator (CIX)},
 url = {https://link.springer.com/chapter/10.1007/978-3-031-75923-9_4},
 year = {2025}
}

@Article{Côté2024,
author="C{\^o}t{\'e}, Pierre-Olivier
and Nikanjam, Amin
and Ahmed, Nafisa
and Humeniuk, Dmytro
and Khomh, Foutse",
title="Data cleaning and machine learning: a systematic literature review",
journal="Automated Software Engineering",
year="2024",
month="Jun",
day="11",
volume="31",
number="2",
pages="54",
abstract="Machine Learning (ML) is integrated into a growing number of systems for various applications. Because the performance of an ML model is highly dependent on the quality of the data it has been trained on, there is a growing interest in approaches to detect and repair data errors (i.e., data cleaning). Researchers are also exploring how ML can be used for data cleaning; hence creating a dual relationship between ML and data cleaning. To the best of our knowledge, there is no study that comprehensively reviews this relationship. This paper's objectives are twofold. First, it aims to summarize the latest approaches for data cleaning for ML and ML for data cleaning. Second, it provides future work recommendations. We conduct a systematic literature review of the papers published between 2016 and 2022 inclusively. We identify different types of data cleaning activities with and for ML: feature cleaning, label cleaning, entity matching, outlier detection, imputation, and holistic data cleaning. We summarize the content of 101 papers covering various data cleaning activities and provide 24 future work recommendations. Our review highlights many promising data cleaning techniques that can be further extended. We believe that our review of the literature will help the community develop better approaches to clean data.",
issn="1573-7535",
doi="10.1007/s10515-024-00453-w",
url="https://doi.org/10.1007/s10515-024-00453-w"
}


@Article{Patel2024,
author="Patel, Harsh
and Adams, Bram
and Hassan, Ahmed E.",
title="Post deployment recycling of machine learning models",
journal="Empirical Software Engineering",
year="2024",
month="Jun",
day="15",
volume="29",
number="4",
pages="100",
abstract="Once a Machine Learning (ML) model is deployed, the same model is typically retrained from scratch, either on a scheduled interval or as soon as model drift is detected, to make sure the model reflects current data distributions and performance experiments. As such, once a new model is available, the old model typically is discarded. This paper challenges the notion of older models being useless by showing that old models still have substantial value compared to newly trained models, and by proposing novel post-deployment model recycling techniques that help make informed decisions on which old models to reuse and when to reuse. In an empirical study on eight long-lived Apache projects comprising a total of 84,343 commits, we analyze the performance of five model recycling strategies on three different types of Just-In-Time defect prediction models (Random Forest (RF), Logistic Regression (LR) and Neural Network (NN)). Comparison against traditional model retraining from scratch (RFS) shows that our approach significantly outperforms RFS in terms of recall, g-mean, AUC and F1 by up to a median of {\$}{\$}30{\backslash}{\%}{\$}{\$}, {\$}{\$}20{\backslash}{\%}{\$}{\$}, {\$}{\$}11{\backslash}{\%}{\$}{\$}and {\$}{\$}10{\backslash}{\%}{\$}{\$}, respectively, with the best recycling strategy (Model Stacking) outperforming the baseline in over {\$}{\$}50{\backslash}{\%}{\$}{\$}of the projects. Our recycling strategies provide this performance improvement at the cost of a median of 2x to 6-17x slower time-to-inference compared to RFS, depending on the selected strategy and variant.",
issn="1573-7616",
doi="10.1007/s10664-024-10492-2",
url="https://doi.org/10.1007/s10664-024-10492-2"
}


@Article{Liu2025,
author="Liu, Lixue
and Gu, Qiqi
and Ke, Wei",
title="AGTS: Novel automated generation of smart contract test suites for Hyperledger Fabric",
journal="Scientific Reports",
year="2025",
month="Aug",
day="13",
volume="15",
number="1",
pages="29675",
abstract="The robustness and reliability of blockchain applications, critically depend on thorough testing. This study introduces AGTS, an automated framework for generating smart contract test suites on Hyperledger Fabric, significantly reducing manual effort while improving test coverage and security. The framework integrates static and dynamic analysis, leveraging symbolic execution and fuzz testing to detect vulnerabilities. The project is implemented in C{\#}, and uses three test cases: CoCoME, LibraryMS and LoanPS, with their requirements as input, and generates their closely related test cases. We generated 34 test cases for CoCoME, 32 for LibraryMS, and 28 for LoanPS. Combining the test cases, their executable scripts were generated. Finally, we tested all the targeted smart contracts rigorously against the predefined criteria. The generated scripts undergo the quality assurance using ShellCheck, deploying on the continuous integration system, that further enhances the reliability and maintainability of these tests. AGTS automates the entire process of test script creation and verification, drastically reducing the manual effort involved in preparing test cases. It facilitates rapid and thorough testing cycles by generating dedicated scripts that execute specific test scenarios on smart contracts, verifying their behavior and performance. By automating the tedious and error-prone task of writing test cases, AGTS accelerates the development process and fortifies the security framework of blockchain applications. Comprehensive evaluation of AGTS across diverse real world smart contracts demonstrated the effectiveness in defect detection, significantly outperforming existing methods. The contribution of AGTS extends beyond providing a practical testing tool, by offering developers a robust framework for ensuring the reliability and security of their smart contracts. AGTS not only expedites test preparation but also ensures comprehensive test coverage, thus improving the overall quality of blockchain applications.",
issn="2045-2322",
doi="10.1038/s41598-025-14218-2",
url="https://doi.org/10.1038/s41598-025-14218-2"
}


@inproceedings{10.1007/978-3-031-34671-2_23,
 abstract = {The development of Artificial Intelligence (AI) based systems to automatically generate hardware systems has gained an impulse that aims to accelerate the hardware design cycle with no human intervention. Recently, the striking AI-based system ChatGPT from OpenAI has achieved a momentous headline and has gone viral within a short span of time since its launch. This chatbot has the capability to interactively communicate with the designers through a prompt to generate software and hardware code, write logic designs, and synthesize designs for implementation on Field Programmable Gate Array (FPGA) or Application Specific Integrated Circuits (ASIC). However, an unvetted ChatGPT prompt by a designer with an aim to generate hardware code may lead to security vulnerabilities in the generated code. In this work, we systematically investigate the necessary strategies to be adopted by a designer to enable ChatGPT to recommend secure hardware code generation. To perform this analysis, we prompt ChatGPT to generate code scenarios listed in Common Vulnerability Enumerations (CWEs) under the hardware design (CWE-1194) view from MITRE. We first demonstrate how a ChatGPT generates insecure code given the diversity of prompts. Finally, we propose techniques to be adopted by a designer to generate secure hardware code. In total, we create secure hardware code for 10 noteworthy CWEs under hardware design view listed on MITRE site.},
 address = {Cham},
 author = {Nair, Madhav
and Sadhukhan, Rajat
and Mukhopadhyay, Debdeep},
 booktitle = {Cyber Security, Cryptology, and Machine Learning},
 editor = {Dolev, Shlomi
and Gudes, Ehud
and Paillier, Pascal},
 isbn = {978-3-031-34671-2},
 pages = {320--336},
 publisher = {Springer Nature Switzerland},
 title = {How Hardened is Your Hardware? Guiding ChatGPT to Generate Secure Hardware Resistant to CWEs},
 url = {https://link.springer.com/chapter/10.1007/978-3-031-34671-2_23},
 year = {2023}
}

@Inbook{McTear2024,
author="McTear, Michael
and Ashurkina, Marina",
title="Designing Conversational Systems",
bookTitle="Transforming Conversational AI: Exploring the Power of Large Language Models in Interactive Conversational Agents",
year="2024",
publisher="Apress",
address="Berkeley, CA",
pages="17--42",
abstract="Conversation design plays a pivotal role in the development of a successful conversational system. In light of the customer dissatisfaction issues that arose from earlier telephone-based interactive voice response (IVR) systems, companies now recognize the criticality of delivering exceptional user experiences as they embrace the new and rapidly evolving technology of Conversational AI. Consequently, over the past few years, the demand for conversation designers has skyrocketed, giving rise to an entirely new industry centered around the art of conversation design.",
isbn="979-8-8688-0110-5",
doi="10.1007/979-8-8688-0110-5_2",
url="https://doi.org/10.1007/979-8-8688-0110-5_2"
}


@Inbook{Sarferaz2024,
author="Sarferaz, Siar",
title="Embedding Generative AI",
bookTitle="Embedding Artificial Intelligence into ERP Software : A Conceptual View on Business AI with Examples from SAP S/4HANA",
year="2024",
publisher="Springer Nature Switzerland",
address="Cham",
pages="277--288",
abstract="In this chapter, we specify the business requirements and propose the solution concept for embedding Generative AI into ERP software. Generative AI has the potential to radically change the way we apply artificial intelligence in various industries and fields. By levering the strength of these sophisticated models, users without technical backgrounds can address their business challenges just by expressing them in everyday language. This creates a vast range of opportunities for both companies and individuals. In the context of ERP systems, the key questions are: What is the added value of Generative AI, what are the technical requirements, and how to embed this technology into business applications? Providing answers for those questions is the objective of this chapter.",
isbn="978-3-031-54249-7",
doi="10.1007/978-3-031-54249-7_19",
url="https://doi.org/10.1007/978-3-031-54249-7_19"
}


@Article{Sinha2025,
author="Sinha, Aarush
and Umakanthan, OmKumar Chandra
and Gajendran, Sudhakaran",
title="DR-CoT: dynamic recursive chain of thought with meta reasoning for parameter efficient models",
journal="Scientific Reports",
year="2025",
month="Oct",
day="06",
volume="15",
number="1",
pages="34773",
abstract="Chain-of-Thought (CoT) prompting has revolutionized reasoning in Large Language Models (LLMs), enabling them to tackle complex tasks by mimicking step-by-step human thought processes. However, traditional CoT methods often suffer from high computational costs and context dilution, limiting their effectiveness, particularly in resource-constrained or real-time applications. To address these challenges, we introduce Dynamic Recursive Chain-of-Thought (DR-CoT), a novel reasoning framework for parameter-efficient models. DR-CoT synergistically integrates recursive reasoning, dynamic context truncation, and a voting mechanism. By selectively retaining the most salient context within a fixed token budget and aggregating inferences from multiple independent reasoning chains, DR-CoT significantly enhances reasoning accuracy. Extensive evaluations on challenging reasoning benchmarks, including GPQA Diamond and AIME2024, demonstrate the efficacy of DR-CoT. On GPQA Diamond, DR-CoT sees Pass@1 accuracy gains of 1.5{\%} for Gemini 2.0 Flash Thinking Experimental, 2.7{\%} for Grok 3 Beta, and 4.4{\%} for o3 Mini. Similarly, AIME2024 results reveal consistent improvements of 3-4 percentage points across evaluated models. Furthermore, DR-CoT enhances zero-shot classification performance on GPQA Diamond, enabling compact BERT-sized models to surpass larger language models such as GPT-4 and LLaMA 2. In code generation tasks using HumanEval, DRCoT empowers models to exceed the performance of established frontier LLMs, including LLaMA 70B, Phi-3, and Claude Sonnet. These comprehensive results underscore DR-CoT's effectiveness in bridging the performance gap between parameter-efficient models and state-of-the-art LLMs across multiple domains.",
issn="2045-2322",
doi="10.1038/s41598-025-18622-6",
url="https://doi.org/10.1038/s41598-025-18622-6"
}


@Inbook{Sehgal2024,
author="Sehgal, Attul",
title="Navigating the Technology Deluge",
bookTitle="Demystifying Digital Transformation: Non-Technical Toolsets for Business Professionals Thriving in the Digital Age",
year="2024",
publisher="Apress",
address="Berkeley, CA",
pages="259--290",
abstract="Up to this point, through classic case examples, we have established the concept of DX, delved into the fundamentals of big data and analytics, explored various productivity tools, overviewed the basics of data mining and algorithms, and examined strategies for making more business-oriented decisions in our DX endeavors.",
isbn="978-1-4842-9499-4",
doi="10.1007/978-1-4842-9499-4_7",
url="https://doi.org/10.1007/978-1-4842-9499-4_7"
}


@inproceedings{10.1007/978-3-031-60606-9_3,
 abstract = {This paper evaluates the impact of generative artificial intelligence (AI) on software design and engineering through a user-centered approach. The integration of generative AI tools in software development processes is scrutinized across various phases, from ideation to deployment. By conducting a literature review and a preliminary evaluation with 18 students, this study identifies critical tasks within the software development life cycle where generative AI tools can enhance productivity and creativity. The paper outlines the potential of generative AI to expedite tasks like code completion, prototype design, requirements validation, and documentation, thereby potentially transforming software engineering practices. It emphasizes a user-centered perspective, assessing tools based on criteria such as usability, effectiveness, and integration within existing workflows. Furthermore, the study highlights the importance of human-AI collaboration, suggesting that while generative AI can significantly support software development tasks, human oversight and critical evaluation of AI-generated outputs remain essential. This research contributes to understanding how generative AI tools can be effectively integrated into software development processes, offering insights into the benefits and challenges of these emerging technologies from a user-centric viewpoint.},
 address = {Cham},
 author = {Fischer, Mahsa
and Lanquillon, Carsten},
 booktitle = {Artificial Intelligence in HCI},
 editor = {Degen, Helmut
and Ntoa, Stavroula},
 isbn = {978-3-031-60606-9},
 pages = {31--47},
 publisher = {Springer Nature Switzerland},
 title = {Evaluation of Generative AI-Assisted Software Design and Engineering: A User-Centered Approach},
 url = {https://link.springer.com/chapter/10.1007/978-3-031-60606-9_3},
 year = {2024}
}

@Article{Resnik2025,
author="Resnik, David B.
and Hosseini, Mohammad",
title="The ethics of using artificial intelligence in scientific research: new guidance needed for a new tool",
journal="AI and Ethics",
year="2025",
month="Apr",
day="01",
volume="5",
number="2",
pages="1499--1521",
abstract="Using artificial intelligence (AI) in research offers many important benefits for science and society but also creates novel and complex ethical issues. While these ethical issues do not necessitate changing established ethical norms of science, they require the scientific community to develop new guidance for the appropriate use of AI. In this article, we briefly introduce AI and explain how it can be used in research, examine some of the ethical issues raised when using it, and offer nine recommendations for responsible use, including: (1) Researchers are responsible for identifying, describing, reducing, and controlling AI-related biases and random errors; (2) Researchers should disclose, describe, and explain their use of AI in research, including its limitations, in language that can be understood by non-experts; (3) Researchers should engage with impacted communities, populations, and other stakeholders concerning the use of AI in research to obtain their advice and assistance and address their interests and concerns, such as issues related to bias; (4) Researchers who use synthetic data should (a) indicate which parts of the data are synthetic; (b) clearly label the synthetic data; (c) describe how the data were generated; and (d) explain how and why the data were used; (5) AI systems should not be named as authors, inventors, or copyright holders but their contributions to research should be disclosed and described; (6) Education and mentoring in responsible conduct of research should include discussion of ethical use of AI.",
issn="2730-5961",
doi="10.1007/s43681-024-00493-8",
url="https://doi.org/10.1007/s43681-024-00493-8"
}


@Article{Tbaishat2025,
author="Tbaishat, Dina Mansour
and Elfadel, Maha Waleed",
title="Artificial intelligence (AI) for social innovation in health education: promoting health literacy through personalized ai-driven learning tools -- a systematic review",
journal="BMC Medical Education",
year="2025",
month="Dec",
day="18",
volume="26",
number="1",
pages="123",
abstract="Artificial Intelligence (AI) is transforming health education by enabling personalized, adaptive, and scalable approaches that may enhance aspects of health literacy. Despite rapid adoption, comprehensive synthesis of AI tools' impact on health literacy as social innovation is limited. Understanding these effects guides educators, developers, and policymakers in designing potentially effective, inclusive, and ethical AI interventions. This review examines generative AI models, chatbots, and adaptive learning systems in supporting health literacy globally.",
issn="1472-6920",
doi="10.1186/s12909-025-08462-3",
url="https://doi.org/10.1186/s12909-025-08462-3"
}


@inproceedings{10.1007/978-3-032-15987-8_11,
 abstract = {A significant portion of the textual corpus lacks the annotated labels needed for training supervised models. Manual labeling of such data is costly and impractical. In response to this challenge, semi-supervised learning (SSL) has emerged as a pivotal approach to harnessing the potential of both labeled and unlabeled data. This paper explores the integration of adversarial perturbation with SSL, particularly focusing on text classification tasks. We propose an innovative approach called APVAT, which incorporates adversarial perturbation into the Attention Virtual Adversarial Training (VAT) model. We performed experiments in five benchmark datasets, exploring the impact of different embeddings such as fastText, GloVe BERT, and GPT-2. Our contributions are twofold: First, our approach improves classification accuracy in datasets with little training data compared to previous methods, even when labeled data is scarce (e.g., 10{\%}). Perturbation augments the training data effectively, leading to efficient model learning and resource savings. Second, our method reduces the time of processing and required training epochs. The findings demonstrate that the fusion of adversarial perturbation in the attention mechanism with SSL, particularly when applied to text classification, offers a promising avenue for advancing the field.},
 address = {Cham},
 author = {Duarte, Jos{\'e} Marcio
and Milios, Evangelos
and Berton, Lilian},
 booktitle = {Intelligent Systems},
 editor = {de Freitas, Rosiane
and Furtado, Diego},
 isbn = {978-3-032-15987-8},
 pages = {162--176},
 publisher = {Springer Nature Switzerland},
 title = {APVAT: Attention Perturbation in Virtual Adversarial Training for Semi-supervised Learning},
 url = {https://link.springer.com/chapter/10.1007/978-3-032-15987-8_11},
 year = {2026}
}

@Article{Zhao2026,
author="Zhao, Shiyu",
title="General collective intelligence for multi-robot systems",
journal="Nature Electronics",
year="2026",
month="Jan",
day="01",
volume="9",
number="1",
pages="11--13",
abstract="Enhancing the generality of multi-robot systems is critical for their deployment in open-world applications. Achieving this will require the development of general collective intelligence.",
issn="2520-1131",
doi="10.1038/s41928-025-01558-0",
url="https://doi.org/10.1038/s41928-025-01558-0"
}


@Inbook{Majeed2025,
author="Majeed, Muhammad
and Umar, Aftab
and Fatima, Kinza
and Yasmin, Rukhsana
and Arfan, Sumreen
and Naz, Jamila
and khan, Jehanzeb
and Ramzan, MMuhammad",
editor="Rane, Nitin Liladhar
and Mallick, Suraj Kumar
and Rane, Jayesh
and Pande, Chaitanya Baliram",
title="Transforming Urban Green Spaces: The Impact of Large Language Models on Smart and Sustainable Urban Plantations",
bookTitle="Large Language Models for Sustainable Urban Development",
year="2025",
publisher="Springer Nature Switzerland",
address="Cham",
pages="157--183",
abstract="The Green Revolution (GR) period (1960--2000) was an exceptional time of enhanced global food security. World food production and distribution increased incredibly for grains such as wheat, rice, and maize through rural agriculture intensification. It has been achieved due to synergetic combination of immense investment on research into crops, increased agricultural cultivation, mechanization, as well as mass use of synthetic fertilizers and pesticides, and genetically engineered varieties of high-yielding crops, the HYV. Even as the population had doubled, cereal production had trebled when the cultivated area had grown by only 30{\%}. While the GR was followed by food prices that decreased sharply, mainly favoring consumers, many developing agrarian countries experienced ill effects that arise from an improvement in the quality of their ecosystems, associated with environmental degradation and loss of biodiversity. Integrating LLMs into the urbanization of green spaces is also critical for smart and sustainable urban agriculture. These models improve the processes involved with decision-making, offering insights and recommendations that are based on data about plant selection, urban planning, and resource management. The following sections describe the profound effects of LLMs on urban green spaces. LLMs support low-carbon plant options, which in turn further contribute to landscape architecture with sustainable orientation. They employ advanced databases and complex algorithms for suggesting ecologically suitable plant selections, thus solving the environmental problems. The reconciliation of LLMs in savvy metropolitan boards develops green space exercises through information investigation and IoT combination. These innovations enable continuous monitoring of environmental factors, enhancing resource allocation and maintenance strategies. LLMs make metropolitan maintainability appraisals much simpler to do by mechanizing the grouping of drives against set supportability rules. Such a methodology supports an all-encompassing comprehension of metropolitan undertakings and breaks storehouses in arranging, empowering cooperative endeavors. LMPs actually offer one fundamental benefit over the conventional organization of metropolitan green spaces as far as information precision and the translation of those requires human information.",
isbn="978-3-031-86039-3",
doi="10.1007/978-3-031-86039-3_7",
url="https://doi.org/10.1007/978-3-031-86039-3_7"
}


@inproceedings{10.1007/978-3-031-73741-1_16,
 abstract = {The field of artificial intelligence (AI) has experienced remarkable progress in recent years, driven by the widespread adoption of open-source machine learning models in both research and industry. Considering the resource-intensive nature of training on vast datasets, many applications opt for models that have already been trained. Hence, a small number of key players undertake the responsibility of training and publicly releasing large pre-trained models, providing a crucial foundation for a wide range of applications. However, the adoption of these open-source models carries inherent privacy and security risks that are often overlooked. To provide a concrete example, an inconspicuous model may conceal hidden functionalities that, when triggered by specific input patterns, can manipulate the behavior of the system, such as instructing self-driving cars to ignore the presence of other vehicles. The implications of successful privacy and security attacks encompass a broad spectrum, ranging from relatively minor damage like service interruptions to highly alarming scenarios, including physical harm or the exposure of sensitive user data. In this work, we present a comprehensive overview of common privacy and security threats associated with the use of open-source models. By raising awareness of these dangers, we strive to promote the responsible and secure use of AI systems.},
 address = {Cham},
 author = {Hintersdorf, Dominik
and Struppek, Lukas
and Kersting, Kristian},
 booktitle = {Bridging the Gap Between AI and Reality},
 editor = {Steffen, Bernhard},
 isbn = {978-3-031-73741-1},
 pages = {269--283},
 publisher = {Springer Nature Switzerland},
 title = {Balancing Transparency and Risk: An Overview of the Security and Privacy Risks of Open-Source Machine Learning Models},
 url = {https://link.springer.com/chapter/10.1007/978-3-031-73741-1_16},
 year = {2025}
}

@Article{Martvel2025,
author="Martvel, George
and Zamansky, Anna
and Shimshoni, Ilan
and Bremhorst, Annika",
title="Investigating the capabilities of large vision language models in dog emotion recognition",
journal="Scientific Reports",
year="2025",
month="Nov",
day="21",
volume="15",
number="1",
pages="41250",
abstract="Identifying emotional states in animals is a key challenge in behavioural science and a prerequisite for developing reliable welfare assessments, ethical frameworks, and robust human--animal communication models. Recently, large vision-language models (LVLMs) such as GPT-4o, Gemini, and LLaVA have shown promise in general image understanding tasks, and are beginning to be applied for emotion recognition in animals. In this study, we critically evaluated the ability of state-of-the-art LVLMs to classify emotional states in dogs using a zero-shot approach. We assessed model performance on two datasets: (1) the Dog Emotions (DE) dataset, consisting of web-sourced images with layperson-generated emotion labels, and (2) the Labrador Retriever cropped-face (LRc) dataset, which stems from a rigorously controlled experimental study where emotional states were systematically elicited in dogs and defined based on the experimental context in canine emotion research. Our results revealed that while LVLMs showed moderate classification accuracy on DE, performance is likely driven by superficial correlations, such as background context and breed morphology. When evaluated on LRc, where emotional states are experimentally induced and backgrounds are minimal, performance dropped to near-chance levels, indicating limited ability to generalise based on biologically relevant cues. Background manipulation experiments further confirmed that models relied heavily on contextual features. Prompt variation and system-level instructions slightly improved response rates but did not enhance classification accuracy. These findings highlight significant limitations in the current application of LVLMs to non-human species and raise ethical and epistemological concerns regarding potential anthropocentric biases embedded in their training data. We advocate for species-sensitive AI approaches grounded in validated behavioural science, emphasising the need for high-quality, preferably experimentally-based multimodal datasets and more transparent validation. Our study underscores both the potential and the risks of using general-purpose AI to infer internal states in animals and calls for rigorous, interdisciplinary development of animal-centred computational approaches.",
issn="2045-2322",
doi="10.1038/s41598-025-25199-7",
url="https://doi.org/10.1038/s41598-025-25199-7"
}


@Article{Alharbi2025,
author="Alharbi, Maha
and Alshayeb, Mohammad",
title="Automatic Code Generation Techniques: A Systematic Literature Review",
journal="Automated Software Engineering",
year="2025",
month="Sep",
day="12",
volume="33",
number="1",
pages="4",
abstract="As modern software systems become complex and the demand for rapid development cycles increases, automatic code generation techniques have attained a prominent focus in academic research and industrial practice. These techniques can significantly reduce human error, increase productivity, and ensure consistency across large codebases. However, the task of generating code automatically presents significant challenges. In this study, we investigate, identify, and analyze the existing automatic techniques for generating code from various input formats, highlighting their efficiencies and areas for potential improvement. A Systematic Literature Review (SLR) is conducted to systematically summarize and review 76 primary studies related to automatic code generation in the software engineering domain. The selected studies are investigated from several dimensions: paradigms, techniques, input types, intermediate representations, tool support, targeted programming languages, and validation methods, including performance metrics, datasets, and benchmarking status. Our investigation identified 12 main techniques, categorized into five paradigms, where the Model-to-Code paradigm and model-driven techniques are the most prevalent. Notably, 57{\%} of the studies utilized Java, and a limited number of studies showed multilingual support. Furthermore, 72{\%} of the selected studies did not compare their results with existing techniques, and 17{\%} lacked validation of the proposed techniques. We also noticed a lack of detailed information about the datasets used in the validation process, where 52{\%} of the studies omitted these details. This SLR provides several recommendations to enhance methodological rigor in future research, and it highlights opportunities for leveraging emerging technologies to improve the efficiency of the identified automatic code generation techniques.",
issn="1573-7535",
doi="10.1007/s10515-025-00551-3",
url="https://doi.org/10.1007/s10515-025-00551-3"
}


@inproceedings{10.1007/978-3-032-00239-6_12,
 abstract = {This article examines the application of intellectual analytics based on artificial intelligence (AI) to improve the quality of software (SW) releases. The main AI methods that significantly enhance the processes of testing, error detection, and prevention are analyzed. It is emphasized that their use contributes to more accurate defect identification, reduced testing time, and increased team productivity. Additionally, an experiment is conducted, comparing the results of SW development using traditional methods and AI tools. The research findings show that AI tools, such as GitHub Copilot, Facebook Infer, and SonarQube with predictive analytics, positively impact code quality by reducing the number of errors, improving structure, and accelerating the development process. Experimental data confirm that AI tools can significantly increase the efficiency of testing and quality management. The article also highlights the need to account for initial training and integration costs for incorporating AI into workflows. In conclusion, it is noted that, in the long term, the use of AI in SW development becomes an essential factor for improving quality and enhancing the competitiveness of products. {\#}CSOC1120.},
 address = {Cham},
 author = {Sheinman, Vered
and Ermilov, Denis
and Bolgov, Sergei
and Topalidi, Anna},
 booktitle = {Software Engineering: Emerging Trends and Practices in System Development},
 editor = {Silhavy, Radek
and Silhavy, Petr},
 isbn = {978-3-032-00239-6},
 pages = {176--187},
 publisher = {Springer Nature Switzerland},
 title = {Intellectual Analytics in Software Development: Improving Release Quality Through Error Detection and Prevention with Artificial Intelligence},
 url = {https://link.springer.com/chapter/10.1007/978-3-032-00239-6_12},
 year = {2025}
}

@Article{Manheim2025,
author="Manheim, David
and Martin, Sammy
and Bailey, Mark
and Samin, Mikhail
and Greutzmacher, Ross",
title="The necessity of AI audit standards boards",
journal="AI {\&} SOCIETY",
year="2025",
month="Dec",
day="01",
volume="40",
number="8",
pages="6609--6624",
abstract="Auditing of AI systems is a promising way to understand and manage ethical problems and societal risks associated with contemporary AI systems, as well as some anticipated future risks. Efforts to develop standards for auditing artificial intelligence (AI) systems have therefore understandably gained momentum. However, current approaches are not just insufficient, but can be actively harmful. Transparency alone does not address concerns about risk. Internal auditing is insufficient, and easily becomes safety-washing. External audit is better, but requires credible standards. Industry-led approaches to building standards or to perform audits lack credibility and undermine other efforts. Regulation often is ill adapted and becomes a static barrier. Lastly, all of these limited technical, governance, and even ethical assessments fail to ensure continued stakeholder input and engagement. Instead, the paper proposes the establishment of an AI Audit Standards Board, in line with best practices in other fields, including safety-critical industries like aviation and nuclear energy, as well as more prosaic ones such as financial accounting and pharmaceuticals. This would address the evolving nature of AI technologies, help maintain public trust in AI, and promote a culture of safety and ethical responsibility within the AI industry. By ensuring audits remain relevant, robust, and responsive to the rapid advancements in AI, auditing AI will not devolve into safety washing and addresses risks and ethical concerns that will continue to arise as AI becomes increasingly important in society, and as human interaction with these systems changes over time.",
issn="1435-5655",
doi="10.1007/s00146-025-02320-y",
url="https://doi.org/10.1007/s00146-025-02320-y"
}


@Article{Taromirad2025,
author="Taromirad, Masoumeh
and Runeson, Per",
title="Assertions in software testing: survey, landscape, and trends",
journal="International Journal on Software Tools for Technology Transfer",
year="2025",
month="Feb",
day="01",
volume="27",
number="1",
pages="117--135",
abstract="Assertions are one of the most useful automated techniques for checking program's behaviour and hence have been used for different verification and validation tasks. We provide an overview of the last two decades of research involving ``assertions'' in software testing. Based on a term-based search, we filtered the inclusion of relevant papers and synthesised them with respect to the problem addressed, the solution designed, and the evaluation conducted. The survey rendered 145 papers on assertions in software testing. After test oracle, the dominant problem is test generation, followed by engineering aspects of assertions. Solutions are typically embedded in tool prototypes and evaluated throughout a limited number of cases, whereas using large-scale industrial settings is still a noticeable method. We conclude that assertions would be worth more attention in future research, particularly regarding the new and emerging demands (e.g., verification of programs with uncertainty), for effective, applicable, and domain-specific solutions.",
issn="1433-2787",
doi="10.1007/s10009-025-00794-1",
url="https://doi.org/10.1007/s10009-025-00794-1"
}


@Inbook{Riva2024,
author="Riva, Gianluigi M.
and Accoto, Cosimo",
editor="Marseglia, G. Roberto
and Previtali, Pietro
and Reali, Alessandro",
title="Inside (Out) the Black Box. Legal and Philosophical Speculations on the Implications of ``Sentient'' Artificial Intelligence(s)",
bookTitle="Socio-economic Impact of Artificial Intelligence: A European Management Perspective",
year="2024",
publisher="Springer Nature Switzerland",
address="Cham",
pages="23--58",
abstract="The implications of ``sentient'' artificial intelligence (AI) are profound and multifaceted. This chapter delves into the legal and philosophical dimensions of AI, speculatively examining its potential to exhibit consciousness, self-awareness, and emotions. Despite advancements, no AI has yet demonstrated true sentience, but the discussion remains urgent due to rapid developments in generative AI, like ChatGPT and LaMDA. Legally, this chapter explores how sentient AI might challenge current frameworks that attribute rights and responsibilities on the basis of human-centric criteria. Philosophically, it scrutinizes the assumptions and paradigms that shape our understanding of AI, emphasizing the need to reconsider traditional notions of intelligence and consciousness. This chapter concludes that addressing the speculative emergence of sentient AI requires taking a comprehensive, interdisciplinary approach that combines legal foresight with philosophical inquiry, ensuring that societal impacts are managed responsibly and ethically.",
isbn="978-3-031-73514-1",
doi="10.1007/978-3-031-73514-1_4",
url="https://doi.org/10.1007/978-3-031-73514-1_4"
}


@Article{Lan2023,
author="Lan, Jinpeng
and Gong, Lina
and Zhang, Jingxuan
and Zhang, Haoxiang",
title="BTLink : automatic link recovery between issues and commits based on pre-trained BERT model",
journal="Empirical Software Engineering",
year="2023",
month="Jul",
day="12",
volume="28",
number="4",
pages="103",
abstract="Data traceability in software development can connect different software artifacts to enhance the observability of developer practices. In particular, traceability links between issues and commits (i.e., issue-commit links) play a key role in software maintenance tasks (e.g., bug localization and bug prediction). In practice, developers typically manually make the issue-commit links by adding the issue identifier into the message of the corresponding commits, which results in missing issue commit links being prevalent in software projects. To recover the missing issue commit links, previous studies have proposed some automatic approaches. However, due to the difference between heuristic rules and real-world behavior, as well as insufficient semantic understanding, these approaches cannot achieve the expected performance. Since the text contained in issues and commits contains highly related information, thorough text understanding can improve traceability links. Meanwhile, pre-trained models (i.e., PTMs) have been successfully used to explore the semantic information of text in various software engineering tasks (e.g., software code generation). Therefore, our study proposes a novel BERT -based method (i.e., BTLink) that employs the pre-trained models to automatically recover the issue-commits links. Our proposed BTlink method includes a BERT embedding layer, a fusion layer, and a classifier layer. First, we build two pre-trained BERT encoders to respectively explore the feature representation of the issue text in combination with commit code and commit text. Then we build the fusion layer to examine the joint feature vector. Finally, we build the classifier layer to identify the links between issue and commit. In addition, to further our investigation and verify the effectiveness of BTLink, we conduct an extensive case study on 12 issue-commit links datasets from open source software projects, and observe that: (i) compared to state-of-the-art approaches, our proposed BTLink improves the performance of automatic issue-commit links recovery on all studied measures; (ii) both text and code information in the issues and commits are effective to recover more accurate issue-commit links; (iii) our proposed BTLink is more applicable to the cross-project context compared to state-of-the-art approaches.",
issn="1573-7616",
doi="10.1007/s10664-023-10342-7",
url="https://doi.org/10.1007/s10664-023-10342-7"
}


@inproceedings{10.1007/978-3-031-84391-4_8,
 abstract = {JavaScript is one of the most preferred programming languages for web development and many novice programmers use it for learning programming through web development. JavaScript programmers often rely on browser devTools to debug their programs. The error messages in web browsers get logged into the web console, which is a part of the devTools. However, novice programmers often find it difficult to understand and interpret the error messages. Therefore, we propose JestViz, a tool aimed at helping novice web developers through the modification of the error reporting mechanism and visualization of the stack trace from error messages as a function call graph. We conjecture that these improvements provide novice programmers with a better interpretation of program errors, leading to faster debugging. Our evaluation of JestViz with 20 novice programmers involving the task of comprehending and debugging an erroneous JavaScript program showed that those who used JestViz took 46{\%} lesser time to debug the program and gained an improved understanding of the inputs and nested function calls that produced the errors.},
 address = {Cham},
 author = {Shanbhag, Shriram
and Chimalakonda, Sridhar},
 booktitle = {Computing Education Research},
 editor = {Karkare, Amey
and Prasad, Prajish
and Raman, Arun},
 isbn = {978-3-031-84391-4},
 pages = {99--112},
 publisher = {Springer Nature Switzerland},
 title = {JestViz: Towards Supporting Novice Web Developers Debug JavaScript},
 url = {https://link.springer.com/chapter/10.1007/978-3-031-84391-4_8},
 year = {2025}
}

@inproceedings{10.1007/978-3-031-76516-2_3,
 abstract = {While many in the domain of AI claim that their works are ``biologically inspired'', most strongly avoid the forms of dynamic complexity that are inherent in all of evolutionary history's more capable surviving organisms. This work seeks to illustrate examples of what introducing human-like forms of complexity into software systems looks like, why it is important, and why humans so frequently seek to avoid such complexity. The complex dynamics of these factors are discussed and illustrated in the context of Chaos Theory, the Three-Body Problem, category concepts, the tension between interacting forces and entities, and cognitive biases influencing how complexity is handled and reduced.},
 address = {Cham},
 author = {Atreides, Kyrtin},
 booktitle = {Biologically Inspired Cognitive Architectures 2024},
 editor = {Samsonovich, Alexei V.
and Liu, Tingting},
 isbn = {978-3-031-76516-2},
 pages = {19--58},
 publisher = {Springer Nature Switzerland},
 title = {The Complex Chaos of Cognitive Biases and Emotional Observers},
 url = {https://link.springer.com/chapter/10.1007/978-3-031-76516-2_3},
 year = {2024}
}

@Inbook{Gastaldi2024,
author="Gastaldi, Juan Luis",
editor="Sriraman, Bharath",
title="How to Do Maths with Words: Neural Machine Learning Applications to Mathematics and Their Philosophical Significance",
bookTitle="Handbook of the History and Philosophy of Mathematical Practice",
year="2024",
publisher="Springer International Publishing",
address="Cham",
pages="3191--3226",
abstract="Recent years have seen a remarkable development of deep neural network techniques for data analysis, along with their increasing application in scientific research across different disciplines. The field of mathematics has not been exempted from this general trend. The present chapter provides a survey of recent applications of neural models to mathematics and assesses their philosophical significance, related to the role of language in mathematics.",
isbn="978-3-031-40846-5",
doi="10.1007/978-3-031-40846-5_142",
url="https://doi.org/10.1007/978-3-031-40846-5_142"
}


@Inbook{Huang2024,
author="Huang, Ken
and Goertzel, Ben
and Wu, Daniel
and Xie, Anita",
editor="Huang, Ken
and Wang, Yang
and Goertzel, Ben
and Li, Yale
and Wright, Sean
and Ponnapalli, Jyoti",
title="GenAI Model Security",
bookTitle="Generative AI Security: Theories and Practices",
year="2024",
publisher="Springer Nature Switzerland",
address="Cham",
pages="163--198",
abstract="Safeguarding GenAI models against threats and aligning them with security requirements is imperative yet challenging. This chapter provides an overview of the security landscape for generative models. It begins by elucidating common vulnerabilities and attack vectors, including adversarial attacks, model inversion, backdoors, data extraction, and algorithmic bias. The practical implications of these threats are discussed, spanning domains like finance, healthcare, and content creation. The narrative then shifts to exploring mitigation strategies and innovative security paradigms. Differential privacy, blockchain-based provenance, quantum-resistant algorithms, and human-guided reinforcement learning are analyzed as potential techniques to harden generative models. Broader ethical concerns surrounding transparency, accountability, deepfakes, and model interpretability are also addressed. The chapter aims to establish a conceptual foundation encompassing both the technical and ethical dimensions of security for generative AI. It highlights open challenges and lays the groundwork for developing robust, trustworthy, and human-centric solutions. The multifaceted perspective spanning vulnerabilities, implications, and solutions is intended to further discourse on securing society's growing reliance on generative models. Frontier model security is discussed using Anthropic proposed approach.",
isbn="978-3-031-54252-7",
doi="10.1007/978-3-031-54252-7_6",
url="https://doi.org/10.1007/978-3-031-54252-7_6"
}


@inproceedings{10.1007/978-3-032-00627-1_17,
 abstract = {Several recent works have argued that Large Language Models (LLMs) can be used to tame the data deluge in the cybersecurity field, by improving the automation of Cyber Threat Intelligence (CTI) tasks. This work presents an evaluation methodology that other than allowing to test LLMs on CTI tasks when using zero-shot learning, few-shot learning, and fine-tuning, also allows to quantify their consistency and their confidence level. We run experiments with three state-of-the-art LLMs and a dataset of 350 threat intelligence reports and present new evidence of potential security risks in relying on LLMs for CTI. We show how LLMs cannot guarantee sufficient performance on real-size reports while also being inconsistent and overconfident. Few-shot learning and fine-tuning only partially improve the results, thus posing doubts about the possibility of using LLMs for CTI scenarios, where labelled datasets are lacking and where confidence is a fundamental factor.},
 address = {Cham},
 author = {Mezzi, Emanuele
and Massacci, Fabio
and Tuma, Katja},
 booktitle = {Availability, Reliability and Security},
 editor = {Dalla Preda, Mila
and Schrittwieser, Sebastian
and Naessens, Vincent
and De Sutter, Bjorn},
 isbn = {978-3-032-00627-1},
 pages = {343--364},
 publisher = {Springer Nature Switzerland},
 title = {Large Language Models Are Unreliable for Cyber Threat Intelligence},
 url = {https://link.springer.com/chapter/10.1007/978-3-032-00627-1_17},
 year = {2025}
}

@Article{Devereaux2025,
author="Devereaux, Abigail",
title="Hot nudges on hazy landscapes",
journal="Mind {\&} Society",
year="2025",
month="Dec",
day="01",
volume="24",
number="2",
pages="795--826",
abstract="Algorithmic decision-making systems trained on massive data sets, or ``assistive AI,'' has the potential to remove biases and errors in traditional decision-making, according to some behavioral paternalists. Assistive AI's democratization of expert advice represents as important and socially beneficial a technological advancement as the internet's democratization of consensus knowledge. Like any other tool, assistive AI has limitations. I model individuals as theory-based decision-makers whose social systems are open-ended and evolve through time---like ``hazy landscapes'' with unclear horizons. I consider both traditional nudges and ``hot nudges,'' automated nudges programmed to learn how to effectively influence their targets by collecting personalized data. I demonstrate that in open-ended social systems ``hot nudges'' on ``hazy landscapes'' can exacerbate the knowledge deficits of traditional nudges and may have uniquely pernicious effects on advancing non-beneficial policies and suppressing the emergence of beneficial social institutions.",
issn="1860-1839",
doi="10.1007/s11299-025-00358-5",
url="https://doi.org/10.1007/s11299-025-00358-5"
}


@Inbook{Kolade2025,
author="Kolade, Oluwaseun
and Egbetokun, Abiodun
and Owoseni, Adebowale",
title="Gen AI-Enabled Data Generation and Simulation in Social Sciences",
bookTitle="Generative AI in Research: Applications in Research Design, Data Analysis and Feedback",
year="2025",
publisher="Springer Nature Switzerland",
address="Cham",
pages="55--95",
abstract="This chapter explores the transformative role of Generative Artificial Intelligence (GenAI) in advancing synthetic dataSynthetic data generation and simulation, with a particular focus on applications in social science research. Researchers working in critical and sensitive areas, such as studies involving ethnic-minority populations in the Global SouthGlobal South, often face significant challenges related to limited, biased, or inaccessible datasetsdatasets. GenAI offers a novel approach by generating synthetic datadata that can serve as a valuable starting point for initial experimentation and methodological exploration. The chapter presents case studiesCase studies demonstrating how GenAI can support research samplingresearch sampling and simulate participant responses by adopting the personas of target populations, effectively generating context-relevant data for specific studies. It also examines the use of GenAI in the co-creation of data collection instruments, such as surveyssurveys and questionnaires, tailored to specific cultural, linguistic, or sectoral needs. This approach not only reduces development time but also enhances the usability and contextual relevance of research toolsResearch tools. While the primary emphasis is on applications within the social sciencessciences, the methods and insights presented have broader implications across a range of research domains. Finally, the chapter critically addresses the ethical dimensions of using GenAI for synthetic data simulationData simulation, including issues related to regulatory frameworks, biasbias and fairnessFairness, and the validation and reliability of AI-generated data.",
isbn="978-3-032-02440-4",
doi="10.1007/978-3-032-02440-4_3",
url="https://doi.org/10.1007/978-3-032-02440-4_3"
}


@Article{Mishra2025,
author="Mishra, Punya
and Henriksen, Danah
and Dunnigan, Jim",
title="From Symbols to Statistics: The Parallel Histories of Machine and Human Learning",
journal="TechTrends",
year="2025",
month="May",
day="01",
volume="69",
number="3",
pages="459--466",
issn="1559-7075",
doi="10.1007/s11528-025-01083-z",
url="https://doi.org/10.1007/s11528-025-01083-z"
}


@Inbook{Huang2025,
author="Huang, Ken
and Hughes, Chris",
title="The Commercial Landscape of Agentic AI Security",
bookTitle="Securing AI Agents: Foundations, Frameworks, and Real-World Deployment",
year="2025",
publisher="Springer Nature Switzerland",
address="Cham",
pages="347--373",
abstract="This chapter provides a detailed analysis of the commercial landscape for agentic AI security, a market rapidly bifurcating to address the dual challenges of this transformative technology. We examine this ecosystem through two critical lenses: first, the structural duality of established technology giants versus agile startups, and second, the philosophical duality of ``AI Agents for Security'' (using autonomous agents to defend enterprises) versus ``Security for AI Agents'' (protecting the agents themselves). The chapter offers a detailed, tiered analysis of the key commercial players, from hyperscalers like Microsoft, Google, and Amazon to security incumbents and a vibrant ecosystem of specialized startups including Zenity, Reco, Astrix, Aembit, and Splx. It explores the novel technologies being deployed to counter emergent threats like memory poisoning and sophisticated prompt injection, and it dissects the business models, market trends, and investment landscape shaping this critical sector. By synthesizing real-world examples and forward-looking analysis, this chapter serves as an essential guide for security professionals, AI developers, and strategic leaders navigating the opportunities and risks of the burgeoning agentic AI economy.",
isbn="978-3-032-02130-4",
doi="10.1007/978-3-032-02130-4_12",
url="https://doi.org/10.1007/978-3-032-02130-4_12"
}


@Article{ref1,
title="ACNP 64th Annual Meeting: Poster Abstracts P1-P291",
journal="Neuropsychopharmacology",
year="2026",
month="Jan",
day="01",
volume="51",
number="1",
pages="74--242",
issn="1740-634X",
doi="10.1038/s41386-025-02279-w",
url="https://doi.org/10.1038/s41386-025-02279-w"
}


@Inbook{Jalote2025,
author="Jalote, Pankaj",
title="Testing",
bookTitle="A Concise Introduction to Software Engineering: With Open Source and GenAI",
year="2025",
publisher="Springer Nature Switzerland",
address="Cham",
pages="173--201",
abstract="Testing is the most widely used approach for verifying if the code of an application performs as intended. There are other approaches for verification also, e.g. formal verification, model checking, and code reviews.",
isbn="978-3-031-74318-4",
doi="10.1007/978-3-031-74318-4_7",
url="https://doi.org/10.1007/978-3-031-74318-4_7"
}


@inproceedings{10.1007/978-3-032-07275-7_3,
 abstract = {Integrating blockchain technology with generative artificial intelligence (GAI) provides a transformative method for ensuring data provenance, model integrity, and trust in AI-generated content. Blockchain's decentralized and immutable qualities improve transparency, security, and accountability in AI workflows, addressing issues related to data authenticity, model tampering, and ethical standards. This paper examines the intersection of blockchain and GAI, discussing major challenges such as computational overhead, scalability limits, regulatory restrictions, and energy efficiency concerns. It also offers potential solutions, including hybrid blockchain models, Layer 2 scaling techniques, privacy-preserving AI frameworks, and energy-efficient consensus mechanisms. By proposing a structured framework for blockchain-enabled GAI, this research emphasizes its potential to develop resilient and trustworthy AI ecosystems while tackling important technical and ethical issues.},
 address = {Cham},
 author = {Alam, Mahfooz
and Deepak
and Mishra, Divya
and Shahid, Mohammad
and Lapina, Maria},
 booktitle = {AISMA-2025: International Workshop on Advanced Information Security Management and Applications},
 editor = {Lapina, Maria
and Raza, Zahid
and Babenko, Mikhail
and Sajid, Mohammad
and Zolotarev, Vyacheslav},
 isbn = {978-3-032-07275-7},
 pages = {19--30},
 publisher = {Springer Nature Switzerland},
 title = {Blockchain Driven Generative AI: Ensuring Data Provenance and Model Integrity},
 url = {https://link.springer.com/chapter/10.1007/978-3-032-07275-7_3},
 year = {2026}
}

@Article{Moradisani2025,
author="Moradisani, Hadiseh
and Zarrinkalam, Fattane
and Noorian, Zeinab
and Ensan, Faezeh",
title="Exploring unanswerability in machine reading comprehension: approaches, benchmarks, and open challenges",
journal="Artificial Intelligence Review",
year="2025",
month="Nov",
day="25",
volume="59",
number="1",
pages="23",
abstract="The challenge of unanswerable questions in Machine Reading Comprehension (MRC) has drawn considerable attention, as current MRC systems are typically designed under the assumption that every question has a valid answer within the provided context. However, these systems often encounter real-world situations where no valid answer is available. This paper provides a comprehensive review of existing methods for addressing unanswerable questions in MRC systems, categorizing them into model-agnostic and model-specific approaches. It explores key strategies, examines relevant datasets, and evaluates commonly used metrics. This work aims to provide a comprehensive understanding of current techniques and identify critical gaps in the field, offering insights and key challenges to direct future research toward developing more robust MRC systems capable of handling unanswerable questions.",
issn="1573-7462",
doi="10.1007/s10462-025-11421-5",
url="https://doi.org/10.1007/s10462-025-11421-5"
}


@Inbook{Gregory2025,
author="Gregory, Sunil
and Sircar, Anindya",
title="Ethical Guardrails for ``Dependable Enterprise AI''",
bookTitle="AI Governance Handbook: A Practical Guide for Enterprise AI Adoption",
year="2025",
publisher="Springer Nature Switzerland",
address="Cham",
pages="115--149",
abstract="AI systems in sensitive areas such as health care, law, and finance increasingly intersect with ethical concerns. Focusing on bias, trust, responsibility, reliability, and sustainability, this chapter uncovers a set of 15 principles enterprises must adopt for embedding ethics into AI systems. It outlines the need to curb algorithmic bias, enhance transparency and explainability, protect privacy and security, enforce accountable systems, and mitigate environmental impact. This chapter illustrates effective AI governance with policies from various countries while also developing frameworks for monitoring ethical compliance. AI trust intensely depends on design and constant stakeholder collaboration; thus, trust is dynamic rather than static. Adopting ethical frameworks isn't simply complying with policies; it fosters societal trust, drives innovation, and builds organizational resilience.",
isbn="978-3-031-89266-0",
doi="10.1007/978-3-031-89266-0_5",
url="https://doi.org/10.1007/978-3-031-89266-0_5"
}


@Inbook{Vorel2025,
author="Vorel, Roman",
title="Generative AI for System and Integration Testing",
bookTitle="NoOps: How AI Agents Are Reinventing DevOps and Software",
year="2025",
publisher="Apress",
address="Berkeley, CA",
pages="119--132",
abstract="In Chapter 6, we saw how generative AI can boost developer productivity by automatically suggesting code snippets and even generating unit tests. But testing doesn't stop at the function level. Modern software increasingly depends on multiservice architectures, dynamic user flows, and complex integrations---all of which require functional and integration testing to ensure a reliable end-to-end experience. This is where AI-driven testing tools---like Functionaize---come into play, offering advanced capabilities that automate or augment the creation, maintenance, and execution of more complex tests.",
isbn="979-8-8688-1694-9",
doi="10.1007/979-8-8688-1694-9_7",
url="https://doi.org/10.1007/979-8-8688-1694-9_7"
}


@Inbook{Fazzini2026,
author="Fazzini, Mattia
and Gambi, Alessio
and Riccio, Vincenzo
and Panichella, Annibale
and Klikovits, Stefan",
editor="Panichella, Sebastiano
and Arcaini, Paolo
and Cohen, Myra B.
and Arrieta, Aitor",
title="DevOps Testing for Cyber-Physical Systems",
bookTitle="Roadmap for DevOps in Cyber-Physical Systems: Challenges and Future Directions (Communications of Shonan Meetings)",
year="2026",
publisher="Springer Nature Singapore",
address="Singapore",
pages="45--81",
abstract="DevOps is driven by feedback signals that help developers identify issues and potential improvements. One of these signals comes from automated testing activities that stress the CPS under test at various levels during development and, potentially, at runtime, to verify its functional correctness and non-functional properties (e.g., timing and safety). After introducing the phases and levels of CPS testing, this chapter presents the unique challenges of applying automated testing to CPS within the DevOps lifecycle. Next, the chapter presents current automated testing solutions to address the most pressing issues of CPS Testing for DevOps, including addressing the reality gap via domain augmentation and simulation ensembles, improving automated test generation by generating valid and adequate test input and correct test oracles, and increasing testing efficiency by prioritizing and selecting test cases. The chapter concludes by presenting the path to achieve the DevOps vision of continuously evaluating CPS systems grounded in operational reliability and evolving environments. The proposed path forward includes defining effective methods for improving existing test suites using production data, as well as further enhancing testing efficiency through the optimized setup of the testing environment and reducing system tests.",
isbn="978-981-95-1786-2",
doi="10.1007/978-981-95-1786-2_4",
url="https://doi.org/10.1007/978-981-95-1786-2_4"
}


@Article{Motger2025,
author="Motger, Quim
and Miaschi, Alessio
and Dell'Orletta, Felice
and Franch, Xavier
and Marco, Jordi",
title="Leveraging encoder-only large language models for mobile app review feature extraction",
journal="Empirical Software Engineering",
year="2025",
month="Apr",
day="21",
volume="30",
number="4",
pages="104",
abstract="Mobile app review analysis presents unique challenges due to the low quality, subjective bias, and noisy content of user-generated documents. Extracting features from these reviews is essential for tasks such as feature prioritization and sentiment analysis, but it remains a challenging task. Meanwhile, encoder-only models based on the Transformer architecture have shown promising results for classification and information extraction tasks for multiple software engineering processes. This study explores the hypothesis that encoder-only large language models can enhance feature extraction from mobile app reviews. By leveraging crowdsourced annotations from an industrial context, we redefine feature extraction as a supervised token classification task. Our approach includes extending the pre-training of these models with a large corpus of user reviews to improve contextual understanding and employing instance selection techniques to optimize model fine-tuning. Empirical evaluations demonstrate that these methods improve the precision and recall of extracted features and enhance performance efficiency. Key contributions include a novel approach to feature extraction, annotated datasets, extended pre-trained models, and an instance selection mechanism for cost-effective fine-tuning. This research provides practical methods and empirical evidence in applying large language models to natural language processing tasks within mobile app reviews, offering improved performance in feature extraction.",
issn="1573-7616",
doi="10.1007/s10664-025-10660-y",
url="https://doi.org/10.1007/s10664-025-10660-y"
}


@Inbook{Wienholt2025,
author="Wienholt, Nick",
title="Management Challenges Introducing AI",
bookTitle="GitHub Copilot and AI Coding Tools in Practice: Accelerate AI Adoption from Individual Developers to Enterprise",
year="2025",
publisher="Apress",
address="Berkeley, CA",
pages="297--306",
abstract="No area of the software development landscape is exposed as brutally by the AI revolution as is the current software management practices. Software management is in a poor state, and ``agile,'' while well intentioned, has allowed shoddy, lazy management practices to proliferate under the all-encompassing excuse of ``we do agile now.'' At its core, the Agile Manifesto and the principles and practices contained within are excellent and represent the least-worst way to manage and structure a software project. The implementation of agile practices over the last two decades has had two fatal flaws -- organizations ``adapt'' their own agile implementation by ignoring and dropping-out the hard bits, and Agile Manifesto practices rely on the existence of an engaged, wise, and available product manager.",
isbn="979-8-8688-1784-7",
doi="10.1007/979-8-8688-1784-7_13",
url="https://doi.org/10.1007/979-8-8688-1784-7_13"
}


@Inbook{Gregory2025,
author="Gregory, Sunil
and Sircar, Anindya",
title="Legal {\&} Regulatory Guardrails for ``Lawful Enterprise AI''",
bookTitle="AI Governance Handbook: A Practical Guide for Enterprise AI Adoption",
year="2025",
publisher="Springer Nature Switzerland",
address="Cham",
pages="151--229",
abstract="As GenAI becomes more prominent in business and society, global policymakers are racing to legislate around its possible negative impacts. This chapter looks at legal and regulatory issues about corporate compliance in four fundamental pillars: AI taxonomy, intellectual property, data management, and liability of products and services. It describes the landmark EU AI Act as the world's first comprehensive, risk-based regulatory framework, contrasting it with the sectoral, pond-hopping approach of the USA. It covers the explosion of US state-level AI bills and legislative activity in other jurisdictions and aspires to international responsiveness. A deep dive into IP issues reveals a lack of consensus surrounding authorship and fair use of training data, while data governance deals with privacy, security, localization, and cross-border transfer obligations. It also looks at two principal AI product liability paradigms: those based on fault and those based on strict liability. With the maturing of legal frameworks, enterprises must remain proactive, ensuring compliance is built into the AI development lifecycle so that innovation remains within global legal frameworks.",
isbn="978-3-031-89266-0",
doi="10.1007/978-3-031-89266-0_6",
url="https://doi.org/10.1007/978-3-031-89266-0_6"
}


@inproceedings{10.1007/978-981-96-8126-6_25,
 abstract = {The detection and prevention of zero-day attacks represent a critical challenge in modern cybersecurity, particularly given the limitations of traditional signature-based methods that struggle to identify previously unseen threats. This research explores applying deep learning techniques to advance anomaly detection for zero-day malware detection. Specifically, the study uses Recurrent Neural Networks (RNNs) and autoencoders to model normal system behavior and detect subtle deviations that may indicate malicious activity. These deep learning models are designed to learn from vast datasets containing benign and anomalous system behaviors, ensuring their robustness and adaptability in diverse environments. A significant contribution of this work is developing a scalable framework capable of processing large volumes of data in real time, providing continuous monitoring for zero-day attacks. By integrating advanced feature engineering techniques, the proposed system enhances detection accuracy while minimizing false positives, a common challenge in malware detection systems. Moreover, the study uses a hybrid approach, combining supervised learning for malware classification with unsupervised learning to identify unknown threats based on behavioral anomalies. Experimental results demonstrate the effectiveness of the proposed approach in detecting novel malware strains that bypass traditional detection methods. The system achieves high detection rates with reduced latency, proving its potential for real-world application in dynamic cybersecurity environments. This research highlights the transformative potential of deep learning data-driven anomaly detection as a critical safeguard against emerging and sophisticated cyber threats.},
 address = {Singapore},
 author = {Sharma, Priynka
and Sharma, Sushita},
 booktitle = {Proceedings of the Third Congress on Control, Robotics, and Mechatronics},
 editor = {Sharma, Harish
and Jha, Pradeep Kumar
and Jamwal, Prashant
and Tripathi, Brajesh
and Kumar, Pankaj},
 isbn = {978-981-96-8126-6},
 pages = {347--360},
 publisher = {Springer Nature Singapore},
 title = {Deep Learning Data-Driven Anomaly Detection for Zero-Day Malware Prevention in Cybersecurity},
 url = {https://link.springer.com/chapter/10.1007/978-981-96-8126-6_25},
 year = {2026}
}

@Article{Yum2024,
author="Yum, Inchul",
title="Language Agents and Malevolent Design",
journal="Philosophy {\&} Technology",
year="2024",
month="Aug",
day="17",
volume="37",
number="3",
pages="104",
abstract="Language agents are AI systems capable of understanding and responding to natural language, potentially facilitating the process of encoding human goals into AI systems. However, this paper argues that if language agents can achieve easy alignment, they also increase the risk of malevolent agents building harmful AI systems aligned with destructive intentions. The paper contends that if training AI becomes sufficiently easy or is perceived as such, it enables malicious actors, including rogue states, terrorists, and criminal organizations, to create powerful AI systems devoted to their nefarious aims. Given the strong incentives for such groups and the rapid progress in AI capabilities, this risk demands serious attention. In addition, the paper highlights considerations suggesting that the negative impacts of language agents may outweigh the positive ones, including the potential irreversibility of certain negative AI impacts. The overarching lesson is that various AI-related issues are intimately connected with each other, and we must recognize this interconnected nature when addressing those issues.",
issn="2210-5441",
doi="10.1007/s13347-024-00794-0",
url="https://doi.org/10.1007/s13347-024-00794-0"
}


@Article{Al-Shukaili2025,
author="Al-Shukaili, Naeem Ali
and Kiah, Miss Laiha M.
and Ahmedy, Ismail",
title="Optimizing feature selection and deep learning techniques for precise detection of low-rate distributed denial of service (LDDoS) attack",
journal="Discover Internet of Things",
year="2025",
month="Jul",
day="24",
volume="5",
number="1",
pages="80",
abstract="The solution for cybersecurity faces significant challenges due to the growing complexity of denial of service (DoS) attacks, especially Low-rate Distributed Denial of Service (LDDoS) attacks. Low-rate DDoS refers to the small number of requests to overcome the sudden spikes that disrupt the server.This work aims to improve the detection of two common LDDoS attack types, slowloris and slowhttptest simulated attacks, by optimizing feature selection and utilizing deep learning techniques. Slowloris is a DoS attack program to overwhelm the attackers by creating several HTTPS connections between server and attackers. Slowhttptest is an application tool that simulates the data at the application layer and prolongs the HTTPS connection with different mechanisms. The misbalancing class features were handled by SMOTE, and k-best features were selected to train the network via recursive elimination of imbalanced features. Feature encoding to train the model with k-best feature is done by label encoder. Further, this study compares two alternative feature selection strategies filter-based and wrapper-based---to see which works best for detecting these sneaky but persistent dangers. The anticipated detection model executes perfectly with a modest hardware setup, which makes it appropriate for the Internet of Things (IoT) and edge device deployment. In addition, the model was verified on the publicly cic-ids2017 dataset. The results confirm that the wrapper-based method performs better than the filter-based method consistently, mainly when fifty features are used. It achieves a superior accuracy of 99.77{\%}, precision of 95.27{\%}, recall of 95.63{\%}, f1-score of 95.45{\%}, and area under curve (AUC) of 97.76{\%}.",
issn="2730-7239",
doi="10.1007/s43926-025-00182-w",
url="https://doi.org/10.1007/s43926-025-00182-w"
}


@Inbook{Vorel2025,
author="Vorel, Roman",
title="Generative AI for Coding and Unit Testing",
bookTitle="NoOps: How AI Agents Are Reinventing DevOps and Software",
year="2025",
publisher="Apress",
address="Berkeley, CA",
pages="105--118",
abstract="In the previous chapters, we established how a standardized, cloud-native DevOps architecture provides the ideal foundation for rapid, reliable software delivery. Now, we enter the domain of generative AI---a family of tools and approaches that can produce, refine, or transform content (including code). This chapter explores how AI-driven coding assistants and AI-based test generation can significantly boost developer productivity and code quality. While we'll dive deeper into AI for broader testing, security, and infrastructure in later chapters, the focus here is on coding and unit testing, where generative AI is already reshaping the developer experience.",
isbn="979-8-8688-1694-9",
doi="10.1007/979-8-8688-1694-9_6",
url="https://doi.org/10.1007/979-8-8688-1694-9_6"
}


@inproceedings{10.1007/978-981-95-0568-5_9,
 abstract = {Diabetic foot ulcers (DFUs) are a significant cause of lower limb amputations and hospitalizations, placing a substantial burden on patients and healthcare systems. Early assessments are critical for preventing complications, yet the shortage of specialists cannot meet the widespread prevalence of this condition. This paper explores augmenting the DFU clinical workflow by applying vision-language models for generating clinically relevant assessments of DFUs from images. Using the Wound-Ischemia-Foot Infection (WIfI) classification system as a structured framework, we assess the performance of LLaVA-Mistral models fine-tuned on annotated DFU datasets compared to LLaVA-Mistral and GPT-4o baselines. Our findings demonstrate that the initial fine-tuned LLaVA-Mistral model achieved on average 16{\%} higher average accuracy in predicting WIfI elements from a DFU image. In terms of clinical narrative generation quality, the LLaVA-Mistral model demonstrated a 22{\%} improvement in DFU-specific text coherence compared to the baseline model as measured by the dependency parse tree depth method. This research lays the groundwork for AI-assisted DFU assessment by creating publicly available annotations for DFU vision-language model developments and demonstrating the potential of fine-tuning to enhance clinical communication through improved classification accuracy.},
 address = {Singapore},
 author = {Basiri, Reza
and Ghaffar, Asad
and Ghiasi, Donya
and Mekonnen, Meaka Tesfaye
and Popovic, Milos R.
and Khan, Shehroz S.},
 booktitle = {ArtifiAI for Aging Rehabilitation and Intelligent Assisted Living},
 editor = {Khan, Shehroz S.
and Romeo, Luca
and Abedi, Ali},
 isbn = {978-981-95-0568-5},
 pages = {117--131},
 publisher = {Springer Nature Singapore},
 title = {Enhancing Diabetic Foot Ulcer Assessment Through Fine-Tuned Vision-Language Models},
 url = {https://link.springer.com/chapter/10.1007/978-981-95-0568-5_9},
 year = {2025}
}

@inproceedings{10.1007/978-981-96-8104-4_5,
 abstract = {Generative AI models are highly susceptible to adversarial attacks and are primarily employed for various use cases in cloud environments. These are subtle manipulations but can successfully leverage model vulnerabilities, hence posing critical security challenges when these models are exposed in shared multi-tenant cloud infrastructures. These could result in compromised outputs, model performance degradation, and eventual sensitive data breaches. This work studies vulnerabilities that generative AI models, especially those in the cloud, have due to unique infrastructure characteristics that amplify adversaries in their threat exposure. It first identifies core generative AI architecture weaknesses that make them vulnerable to those attacks. Then, it describes model testing and behavior-tracking frameworks for simulation, analysis, and understanding of adversarial attacks. It is followed by an in-depth review of state-of-the-art defense mechanisms, namely adversarial training, input sanitization, and robust design techniques, evaluated against their pros and cons concerning generative model protection. This work insists on a multi-layered defense strategy that combines cloud-specific security measures with model-based countermeasures, which will mitigate the risks effectively. It discusses future challenges by calling for adaptive security solutions and furthering standardized protocols for improved resilience from specified, constantly evolving adversarial threats in cloud-hosted generative AI. The knowledge captured will go a long way in designing more secure AI systems that can withstand adversarial manipulation while retaining performance integrity.},
 address = {Singapore},
 author = {Vadisetty, Rahul
and Polamarasetti, Anand
and Goyal, Mahesh Kumar
and Kakarala, Manikanta Rajendra Kumar},
 booktitle = {Proceedings of Sixth Doctoral Symposium on Computational Intelligence},
 editor = {Swaroop, Abhishek
and Kansal, Vineet
and Hassanien, Aboul Ella},
 isbn = {978-981-96-8104-4},
 pages = {71--87},
 publisher = {Springer Nature Singapore},
 title = {Analyzing and Defending Against Adversarial Attacks on Generative AI in the Cloud (Vulnerabilities)},
 url = {https://link.springer.com/chapter/10.1007/978-981-96-8104-4_5},
 year = {2026}
}

@inproceedings{10.1007/978-3-031-65014-7_7,
 abstract = {One of the most critical infrastructures of any customer-serving organization is its incident management and customer tracking system. Through this system, customers can report their issues by creating a ticket in the application which then gets assigned to the relevant support group, based on the nature of the customer's concern. The current system relies on manual review by human agents to dispatch tickets to appropriate SME's that can then take the appropriate steps to resolve the problem. Such a process relies on the availability of human dispatchers and suffers from a high error rate. The proposed approach leverages Large Language Models (LLMs) and traditional machine learning algorithms to handle both structured and unstructured data in each ticket. First, generative models are used to pre-process the ticket data and classify the tickets into different categories. Then, these categories are vectorized and used as features for a classification model that assigns the tickets to the appropriate support entities. Moreover, the proposed approach uses advanced LLMs to build a vector database of similar tickets and their resolutions, which can be used to suggest solutions for new tickets. The proposed approach aims to improve the efficiency and accuracy of the ticket dispatching process and enhance customer satisfaction.},
 address = {Cham},
 author = {Tharayil, Sarafudheen M.
and Alotaibi, Najd M.
and Idris, Muhammad Azmi
and Aldhalaan, Badr H.},
 booktitle = {Artificial Intelligence, Big Data, IOT and Block Chain in Healthcare: From Concepts to Applications},
 editor = {Farhaoui, Yousef},
 isbn = {978-3-031-65014-7},
 pages = {65--85},
 publisher = {Springer Nature Switzerland},
 title = {Combining NLP and Generative Models for Predicting Incident Category and Incident Routing in Incidents Management Systems},
 url = {https://link.springer.com/chapter/10.1007/978-3-031-65014-7_7},
 year = {2024}
}

@Inbook{Iñiguez2025,
author="I{\~{n}}iguez, Santiago",
title="Purgatory as a Learning Platform for Leaders",
bookTitle="Dante in the Workplace: How Leaders Can Avoid the Seven Deadly Sins",
year="2025",
publisher="Springer Nature Switzerland",
address="Cham",
pages="1--22",
abstract="Educators frequently ask what the best age is for young people to begin reading the classics. Secondary school is considered to be the period when we develop our personality, and so curricula include a broad range of books, covering diverse geographies, topics, and eras so as to provide a diverse, cultivated vision of humanity's thought. At this age, students do not always grasp the depth or meaning of some works, although they help to stimulate the imaginationImagination, broaden the intellectual horizon, and develop key skills for interpersonal communication.",
isbn="978-3-031-92202-2",
doi="10.1007/978-3-031-92202-2_1",
url="https://doi.org/10.1007/978-3-031-92202-2_1"
}


@inproceedings{10.1007/978-3-032-08704-1_20,
 abstract = {Mammography reports are crucial for the early detection of breast cancer; however, their unstructured format and high volume pose challenges for automated evaluation. This study aims to classify mammography reports by BI-RADS scores using both transformer-based models and classical machine learning approaches. We used a dataset of 4, 357 annotated reports to train and evaluate the models. Three classification tasks were explored: (1) binary classification of reports into high- and low-priority groups, (2) multiclass classification based on BI-RADS categories, and (3) a cascaded approach that combines both. Model performance was assessed using F1-score, precision, and recall, with the F1-score serving as the primary comparison metric. Results show that classical machine learning and transformer-based models achieved comparable performance across all tasks. For Task 1, we found that models struggled to correctly classify underrepresented classes. In contrast, Task 2 showed improved results across all models and the highest scores in the F1-score metric. Task 3 demonstrated that the cascaded approach improves multiclass classification performance, but it depends a lot in the first binary classification.},
 address = {Cham},
 author = {Semenov-Flores, Dimitri
and Torres-Hurtado, Jes{\'u}s-Alejandro
and Gomez-Adorno, Helena
and V{\'a}zquez Noguera, Jos{\'e} Luis
and Mello-Rom{\'a}n, Julio C{\'e}sar},
 booktitle = {Advances in Soft Computing. MICAI 2025 Posters Track},
 editor = {Mart{\'i}nez-Villase{\~{n}}or, Lourdes
and V{\'a}zquez, Roberto A.
and Ochoa-Ruiz, Gilberto},
 isbn = {978-3-032-08704-1},
 pages = {257--273},
 publisher = {Springer Nature Switzerland},
 title = {Automatic Classification of BI-RADS in Spanish Radiology Reports Using Transformers and Traditional Machine Learning Approaches},
 url = {https://link.springer.com/chapter/10.1007/978-3-032-08704-1_20},
 year = {2025}
}

@Inbook{Wienholt2025,
author="Wienholt, Nick",
title="Designing Applications with Copilot",
bookTitle="GitHub Copilot and AI Coding Tools in Practice: Accelerate AI Adoption from Individual Developers to Enterprise",
year="2025",
publisher="Apress",
address="Berkeley, CA",
pages="139--164",
abstract="There is a large amount of published material on software design, and this means that designing applications with Copilot will be one of the areas where we would expect to get high-quality results. This chapter will look at the use of GitHub Copilot across the initial design and build of an application -- an application of medium complexity will be designed and refined as the chapter progresses that follows the creation of a cash management offering for a hypothetical online stock broker.",
isbn="979-8-8688-1784-7",
doi="10.1007/979-8-8688-1784-7_7",
url="https://doi.org/10.1007/979-8-8688-1784-7_7"
}


@inproceedings{10.1007/978-3-032-04200-2_13,
 abstract = {Test smells degrade the quality of code and often occur together, indicating the need for combined refactorings to eliminate them. Although existing literature provides evidence of co-occurrences of test smells, little is known about the impact of refactoring such co-occurrences. In this context, it is crucial to understand the potential impact of combining test code refactoring strategies to eliminate multiple instances of test smells. This study reports on the analysis of test code refactoring of an open-source software project. The results indicate some consequences of test code removing concerning test smell co-occurrence: (i) no fixing of test smells; (ii) fixing multiple test smells instances together; and (iii) fixing single test smell instances.},
 address = {Cham},
 author = {Santana, Railana
and Martins, Luana
and Rocha, Larissa
and Bezerra, Carla Ilane
and Costa, Heitor
and Machado, Ivan},
 booktitle = {Software Engineering and Advanced Applications},
 editor = {Taibi, Davide
and Smite, Darja},
 isbn = {978-3-032-04200-2},
 pages = {188--198},
 publisher = {Springer Nature Switzerland},
 title = {Discovering Patterns in Test Code Refactorings: A Preliminary Study},
 url = {https://link.springer.com/chapter/10.1007/978-3-032-04200-2_13},
 year = {2026}
}

@Article{Wang2025,
author="Wang, Zhile
and Wang, Mengyao
and Dong, Bo
and Wang, Yuqing
and Ding, Zheyu
and Shen, Shensi",
title="Drug-tolerant persister cells in cancer: bridging the gaps between bench and bedside",
journal="Nature Communications",
year="2025",
month="Nov",
day="17",
volume="16",
number="1",
pages="10048",
abstract="Drug-tolerant persister (DTP) cells represent a major obstacle to achieving durable cancer remission, yet their biology and clinical relevance remain poorly understood. This perspective highlights key gaps hindering the translation of DTP research into clinical progress, emphasizing the need to move beyond reductionist models toward integrative, patient-aligned approaches that reflect clinical complexity. Bridging these divides will be crucial to reveal actionable biomarkers and develop therapies capable of eradicating these resilient cell populations.",
issn="2041-1723",
doi="10.1038/s41467-025-66376-6",
url="https://doi.org/10.1038/s41467-025-66376-6"
}


@inproceedings{10.1007/978-3-031-91481-2_12,
 abstract = {This paper explores the growing threat of GenAI in social engineering and phishing attacks. We analyze how GenAI can be misused, and the potential consequences of this. The study describes the impact of threat modeling and cybersecurity awareness training as safeguards. By identifying vulnerabilities and empowering users, we aim to develop a roadmap for mitigating these emerging GenAI-driven attacks.},
 address = {Cham},
 author = {Prada, Miguel},
 booktitle = {Extended Reality and Serious Games for Education, Competitiveness, and Wellbeing},
 editor = {Suni-Lopez, Franci
and Rinc{\'o}n Flores, Elvira G.
and Quintana Cruz, Hernan Alejandro
and Santos Nunes, Eunice Pereira dos},
 isbn = {978-3-031-91481-2},
 pages = {181--189},
 publisher = {Springer Nature Switzerland},
 title = {Countering Generative AI-Driven Social Engineering and Phishing: A Threat Modeling and Cybersecurity Awareness Framework},
 url = {https://link.springer.com/chapter/10.1007/978-3-031-91481-2_12},
 year = {2025}
}

@Article{Yeung2025,
author="Yeung, Lorraine Ka Chung
and Chow, Daisy Pui Lun
and Wong, Pak Hang
and Lau, Sam Shun Shun",
title="In ChatGPT they trust: a study of students' perceptions and misuse of ChatGPT in higher education",
journal="AI and Ethics",
year="2025",
month="Dec",
day="01",
volume="6",
number="1",
pages="11",
abstract="Generative artificial intelligence like ChatGPT paints a promising picture of enhancing students' learning in higher education. However, little is known regarding whether the university students actually adopt ChatGPT for enhancement or misuse it for disburdenment. This study employed a qualitative descriptive research approach to investigate if there is the problem of misuse through an examination of the adoption journey of 20 university students in Hong Kong. Our findings show only a few interviewees did not use ChatGPT for disburdenment while the majority allow the cognitive artifact to liberate them from burdens of learning. Moreover, reports from misusers also show more pronounced automation bias and perception of AI agency than non-misusers. Among the misusers who are aware of the problem of AI hallucination, some exhibit a paradoxical approach of adoption behavior: they continue to rely on ChatGPT despite noticing that the new burdens of cross-checking its output may be induced. Such a decision calls into question the reasons for continuous reliance on ChatGPT. Our finding suggests that this may be associated with the students' perception of ChatGPT as agentic, a factor known to positively shape users' affect-based trust in AI. This study illuminates how actual cases of misuse and problematic adoption behavior unfold, offering useful directions for education of sensible, responsible and effective use of AI in the higher education context.",
issn="2730-5961",
doi="10.1007/s43681-025-00855-w",
url="https://doi.org/10.1007/s43681-025-00855-w"
}


@inproceedings{10.1007/978-3-032-02853-2_50,
 abstract = {Intelligent Identity Orchestration with AI-driven policy reconciliation emerges as a comprehensive solution for enterprises navigating the complex security challenges of multi-cloud environments. This article addresses the fundamental limitations of traditional identity and access management systems through a decentralized identity control plane that harmonizes authentication and authorization across disparate cloud platforms while preserving their native capabilities. By leveraging advanced transformer-based models like BERT (Bidirectional Encoder Representations from Transformers) and RoBERTa, the system translates provider-specific IAM configurations into normalized vector representations that capture semantic intent regardless of syntactical differences. Natural language processing facilitates this reconciliation through specialized pipelines that perform entity recognition, dependency parsing, and semantic role labeling to extract core policy components such as principals, actions, resources, and conditions across varying provider terminologies. These capabilities enable organizations to automatically detect and resolve policy conflicts, implement just-in-time (JIT) identity provisioning, and remediate policy misconfigurations across AWS, Azure, GCP, and on-premises infrastructure. The architecture integrates with open standards such as Identity Query Language (IDQL), Open Policy Agent (OPA), and zero trust principles to ensure consistent governance without duplicating infrastructure. This paradigm shift delivers substantial benefits including enhanced security posture through the elimination of policy gaps, operational efficiency via automated management, simplified regulatory compliance across jurisdictions, scalability to accommodate emerging technologies, and comprehensive risk reduction that encompasses privilege escalation, unauthorized access, and compliance violations. While implementation challenges exist regarding AI explainability and organizational change management, future advancements in decentralized identity integration and adaptive risk-based authorization promise to further transform multi-cloud security approaches.},
 address = {Cham},
 author = {Mallesh, Aditi},
 booktitle = {ICT for Global Innovations and Solutions},
 editor = {Bhattacharya, Saurav},
 isbn = {978-3-032-02853-2},
 pages = {723--741},
 publisher = {Springer Nature Switzerland},
 title = {Intelligent Identity Orchestration with AI-Driven Policy Reconciliation for Multi-Cloud Security},
 url = {https://link.springer.com/chapter/10.1007/978-3-032-02853-2_50},
 year = {2026}
}

@Inbook{Huang2025,
author="Huang, Jerry
and Huang, Ken
and Hughes, Chris",
editor="Huang, Ken",
title="AI Agents in Defensive Security",
bookTitle="Agentic AI: Theories and Practices",
year="2025",
publisher="Springer Nature Switzerland",
address="Cham",
pages="207--236",
abstract="This chapter explores the role of AI agents in defensive cybersecurity. The discussion covers core functions of AI agents, architectural considerations, capabilities and benefits, implementation challenges, and real-world case studies. We also discussed the integration of AI agents with existing security frameworks, agent's autonomous decision-making capabilities, and the crucial balance between automation and human oversight. The chapter also explores training environments, emerging trends, and future developments in the field of defensive AI security.",
isbn="978-3-031-90026-6",
doi="10.1007/978-3-031-90026-6_7",
url="https://doi.org/10.1007/978-3-031-90026-6_7"
}


@Inbook{Trautsch2024,
author="Trautsch, Alexander",
editor="Bodden, Eric
and Felderer, Michael
and Hasselbring, Wilhelm
and Herber, Paula
and Koziolek, Heiko
and Lilienthal, Carola
and Matthes, Florian
and Prechelt, Lutz
and Rumpe, Bernhard
and Schaefer, Ina",
title="Usefulness of Automatic Static Analysis Tools: Evidence from Four Case Studies",
bookTitle="Ernst Denert Award for Software Engineering 2022: Practice Meets Foundations",
year="2024",
publisher="Springer Nature Switzerland",
address="Cham",
pages="149--176",
abstract="Automated Static Analysis Tools (ASATs) are an additional tool available to developers in their pursuit of high-quality software. ASATs match source code against configured rules and produce a warning when a rule is violated. However, the evaluation of the warnings by developers as well as the resolution of warnings requires time. This raises the question of whether we are able to evaluate the usefulness of ASATs empirically. Within this chapter, we present the results of four case studies, which investigate different aspects regarding the impact of ASATs on software quality and the perception of the developers thereof. We present results regarding the evolution of ASAT warnings from a longitudinal study of 54 open-source projects. To evaluate the impact on defects, we present results from two studies. The first study is evaluating predictive models in the context of defect prediction with ASAT-based features. The second study provides a statistical investigation of the differences between changes that induce a defect and all other changes. In order to observe the developer's perspective regarding ASAT warnings and other software quality metrics, we include the results of a study of developer intent, which compares changes where the developers intend to improve the quality of the code base with all other changes to see which quality metrics and ASAT warnings change in which way. We employ methods of empirical software engineering research to investigate these relationships and provide evidence-based information for researchers and practitioners alike. Within our studies, we can show empirically that we are able to measure an impact on quality. However, the effect is surprisingly small. Moreover, our investigation of developer intents yield information about the magnitude of bug fixing as a driver for complexity in software. Our results can help practitioners estimate the possible impact of introducing an ASAT on defects, as well as provide guidelines for managing the complexity of software.",
isbn="978-3-031-44412-8",
doi="10.1007/978-3-031-44412-8_6",
url="https://doi.org/10.1007/978-3-031-44412-8_6"
}


@inproceedings{10.1007/978-3-032-00633-2_14,
 abstract = {With the continuous increase in automated Internet traffic, bot detection systems become vital to safeguard users and website integrity, necessitating methods to differentiate between human and bot-generated traffic. CAPTCHA systems, such as Google's reCAPTCHA, serve as a primary defence by presenting tasks that only humans can solve. Nonetheless, these systems have generated privacy concerns due to their extensive data collection practices, especially considering Google's business model, heavily oriented towards advertising. This paper evaluates Cloudflare's Turnstile, a privacy-focused CAPTCHA alternative, and their claims of compliance with the European GDPR and ePrivacy Directive. A comparative analysis of reCAPTCHA and Turnstile was conducted by deploying both systems on test websites, analysing network traffic, HTTP requests, and script behaviour to assess data transfer and privacy implications. The study also reviewed relevant privacy regulations and de-obfuscated Turnstile's client-side code to examine its security mechanisms and data collection practices. Findings reveal that both systems operate similarly by issuing tokens post-verification for back-end validation, with Turnstile aligning with GDPR through its avoidance of tracking cookies. However, a vulnerability previously identified in reCAPTCHA was replicated in Turnstile, demonstrating its susceptibility to token exploitation. While reCAPTCHA, in its current form, faces challenges in conforming to GDPR requirements, Turnstile demonstrates potential as a privacy-conscious alternative offering comparable security features. Nonetheless, future regulations may challenge its compliance, highlighting the need for continuous adaptation to evolving privacy standards.},
 address = {Cham},
 author = {Sateur, Maxime
and Mart{\'i}nez Llamas, Javier
and Preuveneers, Davy
and Joosen, Wouter},
 booktitle = {Availability, Reliability and Security},
 editor = {Coppens, Bart
and Volckaert, Bruno
and Naessens, Vincent
and De Sutter, Bjorn},
 isbn = {978-3-032-00633-2},
 pages = {235--252},
 publisher = {Springer Nature Switzerland},
 title = {Evaluating Turnstile as a Privacy-Conscious Alternative to reCAPTCHA},
 url = {https://link.springer.com/chapter/10.1007/978-3-032-00633-2_14},
 year = {2025}
}

@inproceedings{10.1007/978-3-031-77489-8_40,
 abstract = {Smart contracts, due to their decentralized nature and immutability, have demonstrated significant potential in various sectors such as finance and supply chains. However, as their applications have expanded, their security vulnerabilities have increasingly come to light. In response to the frequent occurrences of smart contract attacks, researchers have undertaken a series of studies, including the development of automated vulnerability detection tools, dynamic monitoring techniques, and vulnerability remediation strategies. However, these detection tools often rely on static analysis and fail to capture dynamic vulnerabilities that occur during runtime. Additionally, dynamic monitoring is limited by the ability to acquire and process real-time data, and it lacks the capability to globally analyze security risks during phased attacks. Similarly, mechanisms for updating vulnerabilities might inadvertently introduce new risks by leaving backdoors in smart contracts. To address these issues, this paper proposes a new method for assessing the reputation and evaluating the risks of smart contracts, aimed at enhancing the security of the blockchain ecosystem through the analysis of smart contract invocation trajectories. The effectiveness and practicality of this method have been validated by evaluating real attack cases that have occurred on-chain. Finally, the paper summarizes the research findings and explores potential future research directions, aiming to provide new insights and solutions for the field of smart contract security.},
 address = {Cham},
 author = {Li, Zexin
and Wang, Chao
and Zhang, Xucan
and Yu, Xiang
and Cui, Ting
and Yu, Yifan},
 booktitle = {Computational and Experimental Simulations in Engineering},
 editor = {Zhou, Kun},
 isbn = {978-3-031-77489-8},
 pages = {526--534},
 publisher = {Springer Nature Switzerland},
 title = {Smart Contract Risk Assessment How Secure is the Contract You Are Calling},
 url = {https://link.springer.com/chapter/10.1007/978-3-031-77489-8_40},
 year = {2025}
}

@Article{Yang2025,
author="Yang, Wencheng
and Wang, Song
and Wu, Di
and Cai, Taotao
and Zhu, Yanming
and Wei, Shicheng
and Zhang, Yiying
and Yang, Xu
and Tang, Zhaohui
and Li, Yan",
title="Deep learning model inversion attacks and defenses: a comprehensive survey",
journal="Artificial Intelligence Review",
year="2025",
month="May",
day="13",
volume="58",
number="8",
pages="242",
abstract="The rapid adoption of deep learning in sensitive domains has brought tremendous benefits. However, this widespread adoption has also given rise to serious vulnerabilities, particularly model inversion (MI) attacks, posing a significant threat to the privacy and integrity of personal data. The increasing prevalence of these attacks in applications such as biometrics, healthcare, and finance has created an urgent need to understand their mechanisms, impacts, and defense methods. This survey aims to fill the gap in the literature by providing a structured and in-depth review of MI attacks and defense strategies. Our contributions include a systematic taxonomy of MI attacks, extensive research on attack techniques and defense mechanisms, and a discussion about the challenges and future research directions in this evolving field. By exploring the technical and ethical implications of MI attacks, this survey aims to offer insights into the impact of AI-powered systems on privacy, security, and trust. In conjunction with this survey, we have developed a comprehensive repository to support research on MI attacks and defenses. The repository includes state-of-the-art research papers, datasets, evaluation metrics, and other resources to meet the needs of both novice and experienced researchers interested in MI attacks and defenses, as well as the broader field of AI security and privacy. The repository will be continuously maintained to ensure its relevance and utility. It is accessible at https://github.com/overgter/Deep-Learning-Model-Inversion-Attacks-and-Defenses.",
issn="1573-7462",
doi="10.1007/s10462-025-11248-0",
url="https://doi.org/10.1007/s10462-025-11248-0"
}


@inproceedings{10.1007/978-3-031-82481-4_8,
 abstract = {The global community continues to experience an increase in the scale, sophistication, and successful perpetration of cyber-attacks. As the quantity and value of electronic information has increased, so too have the efforts of criminals and other malicious actors who have embraced the Internet as a more anonymous, convenient, and profitable way of conducting their activities. This paper examines the structure of cyberspace and artificial intelligence from the perspective of both an attacker and a defender in this space.},
 address = {Cham},
 author = {Lehto, Martti},
 booktitle = {Machine Learning, Optimization, and Data Science},
 editor = {Nicosia, Giuseppe
and Ojha, Varun
and Giesselbach, Sven
and Pardalos, M. Panos
and Umeton, Renato},
 isbn = {978-3-031-82481-4},
 pages = {106--117},
 publisher = {Springer Nature Switzerland},
 title = {Artificial Intelligence and Cyber Security},
 url = {https://link.springer.com/chapter/10.1007/978-3-031-82481-4_8},
 year = {2025}
}

@Article{Hansen2025,
author="Hansen, Paul",
title="On Automotive Electronics",
journal="ATZelectronics worldwide",
year="2025",
month="Feb",
day="01",
volume="20",
number="1",
pages="24--27",
issn="2524-8804",
doi="10.1007/s38314-025-1976-3",
url="https://doi.org/10.1007/s38314-025-1976-3"
}


@Article{Gauliya2025,
author="Gauliya, Kuldeep
and Pathak, Abhishek
and Mandal, Neetesh
and Manjhi, Manish Kumar
and Upadhyaya, Devanshi Chandel
and Raj, Aman
and Upadhyaya, Chandrama Prakash",
title="Fostering Nutritional Equity: Biofortification Strategies, Socioeconomic Implications, and Regulatory Policies for Developing Biofortified Staple Crop",
journal="Journal of Soil Science and Plant Nutrition",
year="2025",
month="Jun",
day="01",
volume="25",
number="2",
pages="3443--3465",
abstract="``Hidden Hunger,'' a prevalent global issue, underscores the imperative of addressing subtle micronutrient deficiencies worldwide. With staple crops often lacking essential nutrients, ``Biofortification'' emerges as a promising solution. This review explores the realm of biofortified crops, showcasing their potential beyond nutrient enhancement alone. From Golden Rice fortified with pro-vitamin A in Southeast Asia to robust orange-fleshed sweet potatoes thriving in Sub-Saharan Africa, biofortification offers a beacon of hope. Employing a blend of strategies including traditional breeding methods, advanced genetic engineering tools like CRISPR-Cas9, and microbial-assisted nutrient enhancement, biofortification precisely targets global micronutrient deficiencies. Despite promising strides, challenges such as reliability, societal concerns, regulatory frameworks, and accessibility barriers persist which have been discussed in detail. Thus, the present comprehensive review serves as a vital resource for researchers in agricultural and nutritional sciences, offering insights into future directions and challenges associated with revolutionizing global nutrition through biofortification.",
issn="0718-9516",
doi="10.1007/s42729-025-02345-8",
url="https://doi.org/10.1007/s42729-025-02345-8"
}


@Article{Zhou2024,
author="Zhou, Yushy
and Moon, Charles
and Szatkowski, Jan
and Moore, Derek
and Stevens, Jarrad",
title="Evaluating ChatGPT responses in the context of a 53-year-old male with a femoral neck fracture: a qualitative analysis",
journal="European Journal of Orthopaedic Surgery {\&} Traumatology",
year="2024",
month="Feb",
day="01",
volume="34",
number="2",
pages="927--955",
abstract="The integration of artificial intelligence (AI) tools, such as ChatGPT, in clinical medicine and medical education has gained significant attention due to their potential to support decision-making and improve patient care. However, there is a need to evaluate the benefits and limitations of these tools in specific clinical scenarios.",
issn="1432-1068",
doi="10.1007/s00590-023-03742-4",
url="https://doi.org/10.1007/s00590-023-03742-4"
}


@inproceedings{10.1007/978-3-032-01823-6_1,
 abstract = {Relation extraction (RE) plays a critical role in uncovering hidden connections and assisting security analysts in identifying complex patterns within cyber threat intelligence (CTI) data. Despite its importance, RE faces significant challenges, such as overlapping relations, complex sentence structures with long-range dependencies, and ambiguous relation types. Existing solutions primarily rely on model-centric approaches based on entity-marked and entity-tagging representations. However, these methods require modifying the original text and model architecture, increasing complexity. Furthermore, they fail to provide adequate contextual and semantic information, leading to suboptimal performance, particularly in joint extraction settings. This research introduces TIRE, a data-centric framework that addresses these challenges through an innovative multi-sequence representation (MSR) for the RE. By incorporating key features such as Entity Mask and Entity Type, TIRE enhances contextual and semantic understanding, enabling precise classification of relationships between entities. Unlike complex model-centric approaches, TIRE achieves state-of-the-art performance with simplified architectures. Extensive evaluations on the curated DNRTI-AUG-STIX2-JE dataset demonstrate TIRE's superior performance in both pipeline and joint extraction settings, consistently achieving an F1 score of {\$}{\$}99{\backslash}{\%}{\$}{\$}99{\%}in RE tasks while maintaining computational efficiency. TIRE's innovative design bridges the gap between NER and RE tasks for constructing high-quality cybersecurity knowledge graphs (CKGs) and shows adaptability to domains like finance, healthcare, and biomedical fields where structured information extraction is critical. This work underscores the potential of data-centric designs to advance relation extraction and support real-world applications.},
 address = {Cham},
 author = {Mouiche, Inoussa
and Saad, Sherif},
 booktitle = {Applied Cryptography and Network Security Workshops},
 editor = {Manulis, Mark},
 isbn = {978-3-032-01823-6},
 pages = {3--22},
 publisher = {Springer Nature Switzerland},
 title = {TIRE: Advancing Threat Intelligence Relation Extraction with a Novel Data-Centric Framework},
 url = {https://link.springer.com/chapter/10.1007/978-3-032-01823-6_1},
 year = {2026}
}

@inproceedings{10.1007/978-3-032-13056-3_3,
 abstract = {Undoubtedly, the future workforce will be impacted by the increasing adoption of generative AI across industries. To prepare students for this evolving landscape, AI must be integrated into undergraduate computer science education. To address this, we explore the redesign of a core computer science course to include experiential learning modules on AI literacy, generative AI tools, and responsible AI usage. In addition, we report on the benefits, challenges, and ethical considerations of using small language models (SLMs) for automated grading. Finally, through an analysis of student engagement, learning outcomes, and perceptions of AI-driven instruction, we provide insights into best practices for responsible AI adoption in academia.},
 address = {Cham},
 author = {Cenek, Martin
and Santos, Ronnie Delos},
 booktitle = {AI Revolution: Research, Ethics and Society},
 editor = {Arabnia, Hamid R.
and Deligiannidis, Leonidas
and Amirian, Soheyla
and Ghareh Mohammadi, Farid
and Shenavarmasouleh, Farzan},
 isbn = {978-3-032-13056-3},
 pages = {26--38},
 publisher = {Springer Nature Switzerland},
 title = {Teaching and Using AI in the Classroom: Integrating AI Modules in Undergraduate Computer Science Education},
 url = {https://link.springer.com/chapter/10.1007/978-3-032-13056-3_3},
 year = {2026}
}

@inproceedings{10.1007/978-3-032-10731-2_18,
 abstract = {Artificial intelligence, through tools such as ChatGPT, offers new opportunities to improve access to information, support continuous learning and facilitate everyday tasks, thus transforming the way people interact with technology in their daily lives. The main objective of this study is to evaluate the impact of the use of artificial intelligence models, specifically ChatGPT, on the learning of the elderly. The study focuses on identifying the advantages and challenges associated with the implementation of artificial intelligence in key areas such as access to information, assistance in daily life and continuous learning. The experiences of 73 older people, with an average age of 65.33 years, are analyzed to provide recommendations for inclusive and accessible pedagogical strategies. The results show that 69.7{\%} of the participants consider AI useful for their learning, highlighting the value of these technologies in education. After the training, 87.2{\%} of participants improved their interaction with ChatGPT, reflecting the positive impact of appropriate training. However, challenges related to initial frustration and the need for constant accompaniment to ensure the digital autonomy of older users were also identified. This study highlights the importance of specific AI training and underlines how these tools can transform both the education and daily lives of older people, emphasizing the need to develop continuous training strategies to improve their digital competence.},
 address = {Cham},
 author = {Mart{\'i}nez-Alcal{\'a}, Claudia I.
and Julio, Cabero- Almenara
and Ver{\'o}nica, Mart{\'i}nez Lazcano
and Lizbeth, Aguilar-Lira},
 booktitle = {Proceedings of 20th Iberian Conference on Information Systems and Technologies (CISTI 2025)},
 editor = {Rocha, Alvaro
and Garc{\'i}a Pe{\~{n}}alvo, Francisco
and Costa, Carlos J.
and Gon{\c{c}}alves, Ramiro},
 isbn = {978-3-032-10731-2},
 pages = {200--210},
 publisher = {Springer Nature Switzerland},
 title = {The Role of Artificial Intelligence Tools in Elderly Education},
 url = {https://link.springer.com/chapter/10.1007/978-3-032-10731-2_18},
 year = {2026}
}

@inproceedings{10.1007/978-3-031-73503-5_3,
 abstract = {The recent advancements in Machine Learning have led to significant developments in generative AI and large language models. These innovations present an opportunity for creating new features within development tools. However, as assisted programming becomes more prominent across various platforms, it is crucial to assess the implementation of such tools. The question remains: can these advancements genuinely improve the quality and speed of the development process, or are simply an intriguing buzzword?},
 address = {Cham},
 author = {Gon{\c{c}}alves, Carlos Adriano
and Gon{\c{c}}alves, C{\'e}lia Talma},
 booktitle = {Progress in Artificial Intelligence},
 editor = {Santos, Manuel Filipe
and Machado, Jos{\'e}
and Novais, Paulo
and Cortez, Paulo
and Moreira, Pedro Miguel},
 isbn = {978-3-031-73503-5},
 pages = {27--38},
 publisher = {Springer Nature Switzerland},
 title = {Assessment on the Effectiveness of GitHub Copilot as a Code Assistance Tool: An Empirical Study},
 url = {https://link.springer.com/chapter/10.1007/978-3-031-73503-5_3},
 year = {2025}
}

@Article{Hong2026,
author="Hong, Ji Hoon
and Lee, Chi-Hoon
and Kim, Hyeon Woo
and Jeon, Dong Won
and Kang, Jun Hyuk
and Kim, Yoon-Seo
and Bae, Ji-Su
and Heo, Seonggil
and Lee, Sangyeon
and Na, Jooyeong
and You, Jaegook
and Hong, Soyeon
and Cho, Hyunsouk
and Park, Jin-Seong
and Cho, Sung Beom",
title="AI-driven quantitative review of mobility--stability trade-off in oxide semiconductors",
journal="Nano Convergence",
year="2026",
month="Jan",
day="31",
volume="13",
number="1",
pages="4",
abstract="Oxide semiconductors have emerged as critical channel materials for advanced display and next-generation memory technologies, offering superior electron mobility, excellent uniformity, and low-temperature processability. Despite successful commercialization in display backplanes since 2012, their broader adoption remains limited by an inherent trade-off between carrier mobility and device stability---a fundamental challenge arising from the complex interplay between conduction band dispersion and defect chemistry. The development of strategies to overcome this trade-off has therefore become essential for realizing high-performance oxide semiconductor devices in emerging applications. In this review, a comprehensive data-driven analysis of the mobility-stability trade-off in oxide semiconductor thin-film transistors is presented through large language model-powered extraction of over 1,000 experimental datasets from literature. First, the methodology for systematic data extraction and the quantitative visualization of the mobility-stability relationship are introduced. Then, the critical roles of channel composition, gate insulator selection, and post-deposition annealing temperature in determining device performance are statistically analyzed through kernel density estimation and correlation studies. Next, the temporal evolution of process technologies from 2003 to 2025 is examined, revealing progressive improvements in both mobility and stability through advanced strategies. Afterward, detailed case studies of outlier devices---those successfully transcending the conventional trade-off---are presented, identifying key breakthrough approaches including multi-channel architectures, crystallinity engineering, hybrid gate dielectrics, and interface optimization. The superior performance of atomic layer deposition compared to physical vapor deposition methods is demonstrated through comparative analysis. Finally, the implications of this data-driven framework for accelerating materials development are discussed, and future perspectives on leveraging AI-assisted analysis for semiconductor research are provided.",
issn="2196-5404",
doi="10.1186/s40580-026-00535-3",
url="https://doi.org/10.1186/s40580-026-00535-3"
}


@Inbook{Przystalski2025,
author="Przystalski, Karol
and Argasi{\'{n}}ski, Jan K.
and Lipp, Natalia
and Pacholczyk, Dawid",
title="Psychology Hidden in the (Language) Model",
bookTitle="Building Personality-Driven Language Models: How Neurotic is ChatGPT",
year="2025",
publisher="Springer Nature Switzerland",
address="Cham",
pages="41--50",
abstract="The hidden power of Large Language Models (LLM) does not lie in the technology itself, but in the introduction of the human factor into the world of technology as we know it. We are not talking about introducing a new form of interaction, but elements of psychology, philosophy, and even social engineering. We have gained capabilities that far exceed just reproducing fragments of the reality we know. Thanks to generative AI, it is possible to create countless versions of reality for our own use. Enriched with this knowledge, we can now look at the use of technology from a completely different perspective. The current generation of students, college students, or even adults entering the workforce will be armed with tools that, just few years ago, seemed like the overly optimistic ideas of a visionary.",
isbn="978-3-031-80087-0",
doi="10.1007/978-3-031-80087-0_4",
url="https://doi.org/10.1007/978-3-031-80087-0_4"
}


@Article{Ali2025,
author="Ali, Usama
and Naseer, Mehwish",
title="Artificial intelligence and machine learning in enhancing software project management processes: A systematic literature review",
journal="Automated Software Engineering",
year="2025",
month="Nov",
day="11",
volume="33",
number="1",
pages="31",
abstract="The growing complexity of software systems and the increasing demand for global software development necessitate innovative approaches to enhance project management, quality assurance, and risk mitigation. In this study, Artificial Intelligence (AI) and Machine Learning (ML) techniques are examined for overcoming problems in software project management, notably effort estimation, scheduling, resource allocation, risk management, and defect prediction. By systematically reviewing the literature, we show that AI/ML models like Support Vector Machines, neural networks, and ensemble learning can enhance estimation accuracy, maximize resource utilization, and reduce risks. Furthermore, the practical benefits and challenges of implementing an AI/ML system into a real-world system are discussed using real-world case studies, which include data quality and integration issues, and the interpretability of the model. In addition, advanced models, such as graph convolutional networks and deep neural networks, hold great promise as a defect prediction and bug severity classifier. The focus of this research is to leverage the transformative capabilities of AI/ML toward defect-free, efficient, and customer-centric software development. Finally, it suggests future research interests, including integrating explanation model AI, managing data in a better way, and implementing the scalable hybrid approach to meet the newer needs of the industry.",
issn="1573-7535",
doi="10.1007/s10515-025-00578-6",
url="https://doi.org/10.1007/s10515-025-00578-6"
}


@inproceedings{10.1007/978-3-031-70245-7_23,
 abstract = {In today's technology-driven world, there is a growing interest in leveraging Artificial Intelligence (AI) to streamline software testing processes. Our research delves into GUI-based testing, a prominent technique for verifying software functionality. Preliminary findings from our industrial survey of 45 respondents provide insights into the use of AI in GUI-based software testing. The survey aims to understand how AI supports GUI-based testing, the AI techniques and tools used, and the perceived advantages and limitations.},
 address = {Cham},
 author = {Amalfitano, Domenico
and Coppola, Riccardo
and Distante, Damiano
and Ricca, Filippo},
 booktitle = {Quality of Information and Communications Technology},
 editor = {Bertolino, Antonia
and Pascoal Faria, Jo{\~a}o
and Lago, Patricia
and Semini, Laura},
 isbn = {978-3-031-70245-7},
 pages = {328--343},
 publisher = {Springer Nature Switzerland},
 title = {AI in GUI-Based Software Testing: Insights from a Survey with Industrial Practitioners},
 url = {https://link.springer.com/chapter/10.1007/978-3-031-70245-7_23},
 year = {2024}
}

@inproceedings{10.1007/978-981-97-9132-3_1,
 abstract = {In recent years, growth in AI technologies has influenced Agile software development lifecycles with AI-powered code generators. The following concept is essential because code generators based on AI have the potential to improve the efficiency, accuracy, and speed of Agile software testing procedures. Although there is little research on the utilization of AI-powered code generators for Agile software testing, little information is known on this topic. Greater amounts of professional literature should cover the use of code generators for Agile software testing, with more focus on the disadvantages and benefits of their usage. This chapter will try to fill the information gap on this topic by thoroughly investigating the impact of AI-powered code generators on the Agile software testing life cycle. Specifically, the current chapter will try to identify the threats associated with their use and determine whether this technology supports or complicates the Agile testing life cycle.},
 address = {Singapore},
 author = {Nidagundi, Padmaraj
and Jurenoks, Aleksejs
and Bevinamarad, Prabhu
and Bulla, Chetan},
 booktitle = {Proceedings of International Conference on Generative AI, Cryptography and  Predictive Analytics},
 editor = {Virmani, Deepali
and Castillo, Oscar
and Balas, Valentina Emilia
and Elngar, Ahmed A.},
 isbn = {978-981-97-9132-3},
 pages = {3--16},
 publisher = {Springer Nature Singapore},
 title = {Introducing AI Code Generators in Agile Software Testing},
 url = {https://link.springer.com/chapter/10.1007/978-981-97-9132-3_1},
 year = {2025}
}

@Inbook{Ximenes2024,
author="Ximenes, Filipe",
title="Technical Discipline",
bookTitle="Strategic Software Engineering: Software Engineering Beyond the Code",
year="2024",
publisher="Apress",
address="Berkeley, CA",
pages="51--124",
abstract="Discipline is neither a technique nor a process. Technique describes how to execute an activity given a certain set of constraints (such as the available tools). For example, how to cut tomatoes using a chef's knife. Process defines a plan on how and when things are going to get executed, it might or might not give away the techniques that are expected to be employed. For example, a recipe on how to make pizza might tell you the ingredients and the order in which they should be mixed and cooked but it might not specify the technique on how to knead the dough. While process and technique can often be enforced, discipline needs to be nurtured from within us. It's possible to assess that a meal tastes delicious but it's [the cook's] discipline that ensures vegetables were properly cleaned before they were cooked. The only way to really internalize discipline is by doing it. Practice it over and over until it's naturally part of your routine. ",
isbn="979-8-8688-0995-8",
doi="10.1007/979-8-8688-0995-8_2",
url="https://doi.org/10.1007/979-8-8688-0995-8_2"
}


@Article{Constantinescu2025,
author="Constantinescu, Mihaela
and Kaptein, Muel",
title="Responsibility Gaps, LLMs {\&} Organisations: Many Agents, Many Levels, and Many Interactions",
journal="Science and Engineering Ethics",
year="2025",
month="Nov",
day="13",
volume="31",
number="6",
pages="36",
abstract="In this article, we propose a business ethics-inspired approach to address the distribution dimension of responsibility gaps introduced by general-purpose AI models, particularly large language models (LLMs). We argue that the pervasive deployment of LLMs exacerbates the long-standing problem of ``many hands'' in business ethics, which concerns the challenge of allocating moral responsibility for collective outcomes. In response to this issue, we introduce the ``many-agents-many-levels-many-interactions'' approach, labelled M3, which addresses responsibility gaps in LLM deployment by considering the complex web of interactions among diverse types of agents operating across multiple levels of action. The M3 approach demonstrates that responsibility distribution is not merely a function of agents' roles or causal proximity, but primarily of the range and depth of their interactions. Contrary to reductionist views that suggest such complexity inevitably diffuses responsibility to the point of its disappearance, we argue that these interactions provide normative grounds for safeguarding the attribution of responsibility to agents. Central to the M3 approach is identifying agents who serve as nodes of interaction and therefore emerge as key loci of responsibility due to their capacity to influence others across different levels. We position LLM-developing organisations as an example of such agents. As nodes of interactions, LLM-developing organisations exert substantial influence over other agents and should be attributed broader responsibility for harmful outcomes of LLMs. The M3 approach thus offers a normative and practical tool for bridging potential gaps in the distribution of responsibility for LLM deployment.",
issn="1471-5546",
doi="10.1007/s11948-025-00560-1",
url="https://doi.org/10.1007/s11948-025-00560-1"
}


@Article{Wang2024,
author="Wang, Ziqing
and Khondowe, Paul
and Brannick, Erin
and Abasht, Behnam",
title="Spatial transcriptomics reveals alterations in perivascular macrophage lipid metabolism in the onset of Wooden Breast myopathy in broiler chickens",
journal="Scientific Reports",
year="2024",
month="Feb",
day="11",
volume="14",
number="1",
pages="3450",
abstract="This study aims to use spatial transcriptomics to characterize the cell-type-specific expression profile associated with the microscopic features observed in Wooden Breast myopathy. 1 cm3 muscle sample was dissected from the cranial part of the right pectoralis major muscle from three randomly sampled broiler chickens at 23 days post-hatch and processed with Visium Spatial Gene Expression kits (10X Genomics), followed by high-resolution imaging and sequencing on the Illumina Nextseq 2000 system. WB classification was based on histopathologic features identified. Sequence reads were aligned to the chicken reference genome (Galgal6) and mapped to histological images. Unsupervised K-means clustering and Seurat integrative analysis differentiated histologic features and their specific gene expression pattern, including lipid laden macrophages (LLM), unaffected myofibers, myositis and vasculature. In particular, LLM exhibited reprogramming of lipid metabolism with up-regulated lipid transporters and genes in peroxisome proliferator-activated receptors pathway, possibly through P. Moreover, overexpression of fatty acid binding protein 5 could enhance fatty acid uptake in adjacent veins. In myositis regions, increased expression of cathepsins may play a role in muscle homeostasis and repair by mediating lysosomal activity and apoptosis. A better knowledge of different cell-type interactions at early stages of WB is essential in developing a comprehensive understanding.",
issn="2045-2322",
doi="10.1038/s41598-024-53904-5",
url="https://doi.org/10.1038/s41598-024-53904-5"
}


@inproceedings{10.1007/978-981-95-4724-1_26,
 abstract = {The growing complexity and variety of Internet of Things (IoT) environments create significant challenges for keeping high-quality documentation throughout the Software Development Life Cycle (SDLC). This paper introduces a Multi-Agent System (MAS) architecture designed to automate documentation tasks across all phases of the SDLC, reducing manual effort and improving traceability. Outdated documentation methods, especially in distributed IoT systems, face hurdles due to constant changes, dynamically evolving requirements, and diverse stakeholder demands, resulting in an incessant need for updates, which makes traditional manual documentation unfeasible. The topic of this paper is to propose a novel architecture for Multi-Agent Systems (MAS) to formalize the automation of documentation development and updates throughout the SDLC stages in IoT system environments. The system proposed in this paper leverages the autonomous, cooperative, and specialized nature of MAS to assign the responsibility of generating structured documentation throughout SDLC stages to individual agents operating within coordinated processes. Every agent operates on domain-specific knowledge that includes task-related data and logic in accordance with its role, but they share a common memory system as a whole for coordination and data sharing. This allows for the production of verifiable, consistent, and traceable documents that are uniform but differ in granular detail across development phases. A detailed scenario of smart home development illustrates system effectiveness and practicality within real-world IoT environments while showcasing the initial simulations performed in controlled environments that demonstrate significant promise.},
 address = {Singapore},
 author = {Luu, Thuy Anh
and Cao, Ngoc Quynh Mai
and Quach, My Tan
and Do, Thieu Vy
and Le, Tu Anh
and Nguyen, Ngoc Tien
and Le, Hung Tien},
 booktitle = {Future Data and Security Engineering},
 editor = {Dang, Tran Khanh
and K{\"u}ng, Josef
and Chung, Tai M.},
 isbn = {978-981-95-4724-1},
 pages = {379--393},
 publisher = {Springer Nature Singapore},
 title = {A Multi-agent System for Automating SDLC Documentation in IoT Environments},
 url = {https://link.springer.com/chapter/10.1007/978-981-95-4724-1_26},
 year = {2026}
}

@Article{Deng2026,
author="Deng, Xin
and Li, Pengdeng
and Wang, Chenguang
and Wang, Rui
and Liu, Yuan
and Han, Weihong
and Tian, Zhihong",
title="A Stackelberg game based deception defense strategy against APT under resource constraints",
journal="Science China Information Sciences",
year="2026",
month="Jan",
day="28",
volume="69",
number="3",
pages="132109",
abstract="The advanced persistent threat (APT) has become a major challenge in cybersecurity due to its concealment, persistence, and complexity. Traditional passive defense methods, owing to their static and reactive characteristics, struggle to provide sufficient protection against APT attacks. By contrast, active defense solutions have gained increasing attention due to their ability to shift the defender from passivity to taking the initiative. Deception defense is a widely used active defense method to prevent threats in advance by deploying deception resources. Despite its development, existing deception methods often assume that the defender and the APT attacker take actions simultaneously. In practice, due to his advanced nature, the APT attacker can first observe the defender's strategy through reconnaissance and then make his best responses to the defender's strategy. In this paper, we develop a game model to accurately characterize this worst-case scenario from the defender's perspective. Specifically, we establish a Stackelberg game, called cyber deception Stackelberg game (CDSG), where the defender first announces the allocation strategy of limited deception resources to a set of services with the anticipation that the APT attacker will best respond to her strategy, and the attacker determines his action after observing the defender's strategy. In the game model, we also consider that different types of deception resources have varied probabilities of successfully capturing the APT attacker. Given the game model, we then devise a gradient descent based algorithm to solve CDSG for the equilibrium, which offers the defender a robust deception resource allocation strategy. Finally, experiments are conducted to verify the effectiveness of the deception defense strategy in defending against APT attackers and its superiority over several baselines.",
issn="1869-1919",
doi="10.1007/s11432-025-4530-7",
url="https://doi.org/10.1007/s11432-025-4530-7"
}


@inproceedings{10.1007/978-3-032-14495-9_32,
 abstract = {Multimodal Large Language Models (MLLMs) represent},
 address = {Cham},
 author = {Islam, Chashi Mahiul
and Chacko, Samuel Jacob
and Horne, Preston
and Liu, Xiuwen},
 booktitle = {Advances in Visual Computing},
 editor = {Bebis, George
and Ye, Jinwei
and Wang, Yuxiong
and Konakovi{\'{c}} Lukovi{\'{c}}, Mina
and Kalantari, Nima Khademi
and Cho, Isaac
and Yang, Yalong
and Dimara, Evanthia
and Brehmer, Matthew},
 isbn = {978-3-032-14495-9},
 pages = {416--428},
 publisher = {Springer Nature Switzerland},
 title = {Vision Embeddings and Their Role in Hallucination Vulnerabilities of Multimodal Large Language Models},
 url = {https://link.springer.com/chapter/10.1007/978-3-032-14495-9_32},
 year = {2026}
}

@Article{BenRhaiem2025,
author="Ben Rhaiem, Mohamed Amine
and Selmi, Mouna
and Farah, Imed Riadh
and Bouzeghoub, Amel",
title="Leveraging volunteered geographical information and spatio temporal big data in disaster management: opportunity and challenges",
journal="International Journal of Data Science and Analytics",
year="2025",
month="Nov",
day="28",
volume="21",
number="1",
pages="25",
abstract="The surge in the frequency of disasters worldwide has heightened the concern for efficient disaster risk management (DRM). All phases of DRM require substantial amounts of precise, pertinent, and timely geospatial data, also referred to as Big Spatiotemporal Data. The main sources of these Spatiotemporal data can be categorized into two broad categories: traditional sources, such as satellite imagery, aerial imagery; and emerging sources, such Internet of Things (IoT), and Volunteer Geographic Information (VGI) data. In fact, the emergence of VGI presents the potential for near real-time data gathering as well as extensive data collection during and after disasters. This paper reviews the current research landscape regarding the utilization of VGI and big spatiotemporal data in disaster risk management. The objective is to highlight key application areas, current trends, available data sources, and persistent challenges within these fields.",
issn="2364-4168",
doi="10.1007/s41060-025-00896-8",
url="https://doi.org/10.1007/s41060-025-00896-8"
}


@Article{Kozlovski2025,
author="Kozlovski, Atay",
title="Reasons underdetermination in meaningful human control",
journal="Ethics and Information Technology",
year="2025",
month="Oct",
day="15",
volume="27",
number="4",
pages="59",
abstract="The rapid proliferation of AI systems has raised many concerns about safety and responsibility in their design and use. The philosophical framework of Meaningful Human Control (MHC) was developed in response to these concerns, and tries to provide a standard for designing and evaluating such systems. While promising, the framework still requires further theoretical and practical refinement. This paper contributes to that effort by drawing on research in axiology and rational decision theory to identify a critical gap in the framework. Specifically, it argues that while `reasons' play a central role in MHC, there has been little discussion of the possibility that, when weighed against each other, reasons may not always point to a single, rationally preferable course of action. I refer to these cases as instances of reasons underdetermination, and this paper discusses the need to address this issue within the MHC framework. The paper begins by providing an overview of the key concepts of the MHC framework and then examines the role of `reasons' in the framework's two main conditions - Tracking and Tracing. It then discusses the phenomenon of reasons underdetermination and shows how it poses a challenge for the achievement of both Tracking and Tracing.",
issn="1572-8439",
doi="10.1007/s10676-025-09858-x",
url="https://doi.org/10.1007/s10676-025-09858-x"
}


@Inbook{Rajagopal2024,
author="Rajagopal
and Rajagopal, Ananya",
title="The Game Changer Models",
bookTitle="Unmasking Invisible Challenges in Entrepreneurship: Five Game Changer Models",
year="2024",
publisher="Springer Nature Switzerland",
address="Cham",
pages="99--129",
abstract="This chapter exhibits `the game map' plotting five game changer models comprising local area technology and innovation for transformation and ubiquitous development of entrepreneurship (LATITUDE), growth and acceleration in restructuring accountable gender entrepreneurship (GARAGE), and revamping innovation for social entrepreneurship (RISE). Other two game changer models identified as social ubiquity of innovation and technology in entrepreneurship (SUIT-E) and global accessibility to markets and entrepreneurship in a digital interphase (GAME-DI) are also discussed as critical game changers contributing to the transformation of conventional entrepreneurship to technology-driven and market-oriented entrepreneurial business model. These game changer models have been evaluated from the perspective of value propositions, collective intelligence, extent of business transformation, environmental mining, and latent opportunities and threat. The above game changer models are significant in entrepreneurial development as a critical component of social and economic evolution in the current innovation and technology ecosystems. The importance of the proposed models has potential to spark interest of government in developing economies to design compatible public programs to improve capability and competency of local enterprises (e.g., P{\'e}rez-P{\'e}rez, C et al., 2021).",
isbn="978-3-031-63653-0",
doi="10.1007/978-3-031-63653-0_4",
url="https://doi.org/10.1007/978-3-031-63653-0_4"
}


@Inbook{Xu2026,
author="Xu, Zhou",
title="Conclusion and Future Work",
bookTitle="Machine-Learning-Assisted Software Defect Prediction",
year="2026",
publisher="Springer Nature Switzerland",
address="Cham",
pages="417--418",
abstract="This conclusion summarizes the book and outlines future research directions.",
isbn="978-3-032-01336-1",
doi="10.1007/978-3-032-01336-1_8",
url="https://doi.org/10.1007/978-3-032-01336-1_8"
}


@Article{Vasileiou2025,
author="Vasileiou, Zoe
and Kumara, Indika
and Meditskos, Georgios
and Tokmakov, Kamil
and Radolovi{\'{c}}, Dragan
and Cruz, Jes{\'u}s Gorro{\~{n}}ogoitia
and Di Nitto, Elisabetta
and Tamburri, Damian Andrew
and Van Den Heuvel, Willem-Jan
and Vrochidis, Stefanos",
title="A knowledge-based approach for guided development of Infrastructure as Code",
journal="Software and Systems Modeling",
year="2025",
month="Jun",
day="23",
abstract="Infrastructure as Code (IaC) uses versionable software code to define, deploy, and configure physical computational resources, software execution platforms, and applications. As a result, IaC enables the scalable management of complex computing environments while preventing environment drift. IaC frameworks typically offer specific languages such as the industrial Terraform, Ansible, Chef, or TOSCA---standing for Topology and Orchestration Specification for Cloud Applications---the OASIS (Organization for the Advancement of Structured Information Standards) open standard approach to IaC. Developing high-quality IaC for deploying and managing applications demands expertise and knowledge in specific IaC languages, infrastructure resources, resource providers, quality issues in IaC scripts, and so on. While several model-driven engineering (MDE) approaches have been proposed to simplify IaC development, they cannot capture and use expert knowledge to assist with modeling tasks and MDE processes by providing interactive recommendations. This paper presents a knowledge-based framework for guiding the model-driven development of IaC. We use TOSCA as the target IaC language as it is an open standard. We enable IaC and resource experts to share their IaC and resource-related knowledge with application operational experts to help simplify the development of application deployment models. We use an ontology to record the relevant deployment knowledge and ontology reasoning to implement modeling guidance capabilities such as TOSCA model auto-completion, code smell and error detection, and model element matchmaking. We show the flexibility of our methodology by applying it to three industrial applications, covering cloud, edge, and HPC (High-Performance Computing) domains. Moreover, we also assess the use acceptance of our approach and framework by conducting controlled experiments with expert and non-expert IaC users. The results indicate that our method can simplify IaC development by providing appropriate recommendations.",
issn="1619-1374",
doi="10.1007/s10270-025-01294-1",
url="https://doi.org/10.1007/s10270-025-01294-1"
}


@Inbook{Lind2024,
author="Lind, Gregory
and Mishchenko, Maryna",
title="Cloud-Native Development within Radical Therapy Philosophy",
bookTitle="Radical Therapy for Software Development Teams: Lessons in Remote Team Management and Positive Motivation",
year="2024",
publisher="Apress",
address="Berkeley, CA",
pages="83--97",
abstract="Part of the broader Radical Therapy Development philosophy is the importance of cloud-native software development. This approach emphasizes the importance of building software that can be easily deployed and maintained in modern cloud environments. At the core of cloud-native software development is a focus on microservices, and while not every ``cloud'' application needs to be microservice driven, as we talked about before in most scenarios, breaking up the functionality of an app into truly independent functions, managed in their own repositories and with the ability to scale up or down independently from the rest of the application, can not only save you time but money as well.",
isbn="979-8-8688-0187-7",
doi="10.1007/979-8-8688-0187-7_5",
url="https://doi.org/10.1007/979-8-8688-0187-7_5"
}


@inproceedings{10.1007/978-3-031-81221-7_7,
 abstract = {We investigate the adaption of agents using plans on the Web despite its large and dynamic nature, as well as agents' constrained perception. Based on Semantic Web technologies and affordances, we compare how agents choose appropriate actions to adapt to their environment by condition-action rules or suggested actions of large language models. We conduct experiments on execution cost and plan stability distance to see whether agents choose appropriate actions to adapt their plans. We find that cost and stability of rule-based and LLMs for adaptation with affordances are close together, while performance differs greatly.},
 address = {Cham},
 author = {Schmid, Sebastian
and Freund, Michael
and Harth, Andreas},
 booktitle = {Knowledge Graphs and Semantic Web},
 editor = {Tiwari, Sanju
and Villaz{\'o}n-Terrazas, Boris
and Ortiz-Rodr{\'i}guez, Fernando
and Sahri, Soror},
 isbn = {978-3-031-81221-7},
 pages = {93--108},
 publisher = {Springer Nature Switzerland},
 title = {Adaptive Planning on the Web: Using LLMs and Affordances for Web Agents},
 url = {https://link.springer.com/chapter/10.1007/978-3-031-81221-7_7},
 year = {2025}
}

@Inbook{Wienholt2025,
author="Wienholt, Nick",
title="Current State of Play: The High-Level View",
bookTitle="GitHub Copilot and AI Coding Tools in Practice: Accelerate AI Adoption from Individual Developers to Enterprise",
year="2025",
publisher="Apress",
address="Berkeley, CA",
pages="1--6",
abstract="The threat of jobs being replaced by new tools and technologies is nothing new, and the fears of humans being replaced or subjugated have a very rich literary tradition. In Frankenstein (published in the 1810s), Victor Frankenstein (who was the creator of The Monster, which was never named) ponders The Monster's request for a female companion but concludes, ``A race of devils would be propagated upon Earth who might make the very existence of the species of man a condition precarious and full of terror.'' The 1920s play Rossumovi Univerz{\'a}ln{\'i} Roboti (RUR), which coined the phrase robot, features hybrid human-machine robots taking human factory jobs.",
isbn="979-8-8688-1784-7",
doi="10.1007/979-8-8688-1784-7_1",
url="https://doi.org/10.1007/979-8-8688-1784-7_1"
}


@inproceedings{10.1007/978-3-031-78093-6_10,
 abstract = {Biology today is heavily data-driven and knowledge-centric that are stored across the linked open web in numerous heterogeneous deep web databases. To improve searching, finding, accessing, and inter-operating among these diverse information sources to increase usability, the FAIR data principle has been proposed. Unfortunately, FAIR compliance is extremely low and linked open data does not guarantee FAIRness, leaving biologists on a solo hunt for information on the open network. In this paper, we propose SoDa, for intelligent data foraging on the internet. SoDa helps biologists discover resources based on analysis requirements, generate resource access plans, and store cleaned data and knowledge for community use. A secondary search index is also supported for community members to find archived information conveniently.},
 address = {Cham},
 author = {Jamil, Hasan M.},
 booktitle = {Information Integration and Web Intelligence},
 editor = {Delir Haghighi, Pari
and Gregu{\v{s}}, Michal
and Kotsis, Gabriele
and Khalil, Ismail},
 isbn = {978-3-031-78093-6},
 pages = {118--123},
 publisher = {Springer Nature Switzerland},
 title = {Supporting Data Foragers in Scientific Computing Community Ecosystems for Life Sciences},
 url = {https://link.springer.com/chapter/10.1007/978-3-031-78093-6_10},
 year = {2025}
}

@inproceedings{10.1007/978-3-031-34671-2_36,
 abstract = {This paper presents a semi-automated approach to distinguish between patches and exploits published on GitHub, by using the National Vulnerability Database (NVD) as a reference. For this purpose, we leverage two interpretable algorithms, FP-Growth rule mining and decision trees, to extract patterns from the data and provide insights into the relationships between variables. To mitigate the risk of overfitting, we use more than 30,000 GitHub pages labeled by NVD as ground truth and focus on simple models. Among our findings, we discover that it is feasible to semi-automatically identify GitHub webpages containing patches and exploits. In particular, after pre-filtering webpages of interest, we discovered that most commits refer to patches, whereas URLs containing screenshots correspond to exploits showcasing how to exploit a given target system. Our results suggest that NVD is valuable to bootstrap machine learning algorithms for assisting in the analysis of increasingly larger amounts of cybersecurity data shared over the Internet.},
 address = {Cham},
 author = {Miranda, Lucas
and Figueiredo, Cain{\~a}
and Menasch{\'e}, Daniel Sadoc
and Kocheturov, Anton},
 booktitle = {Cyber Security, Cryptology, and Machine Learning},
 editor = {Dolev, Shlomi
and Gudes, Ehud
and Paillier, Pascal},
 isbn = {978-3-031-34671-2},
 pages = {511--522},
 publisher = {Springer Nature Switzerland},
 title = {Patch or Exploit? NVD Assisted Classification of Vulnerability-Related GitHub Pages},
 url = {https://link.springer.com/chapter/10.1007/978-3-031-34671-2_36},
 year = {2023}
}

@Article{Oprişa2026,
author="Opri{\c{s}}a, Ciprian
and Grigoru{\c{T}}, Dominic Octavian
and Mouratidis, Haralambos
and Lakka, Eftychia
and Manta, Ourania
and Mavrias, Angelos
and Tsantekidis, Marinos
and Filippatos, Nikolas
and Daniil, George
and Gal, Ionel-Alexandru
and Gavrilu{\c{T}}, Drago{\c{s}}
and Kargatzis, Christos
and Ioannidis, Sotiris",
title="A large scale analysis of code security in public repositories",
journal="International Journal of Information Security",
year="2026",
month="Jan",
day="06",
volume="25",
number="1",
pages="24",
abstract="Cybersecurity is a growing concern for the software development sector, as insecure code and insecure software development practices lead to dangerous vulnerabilities in the released products. We propose CONSOLE, designed to boost cybersecurity for software development with an automated, unified platform of tools and services. The proposed framework integrates static and dynamic code analysis tools, real-time protection mechanisms, and adaptive training services. Multiple open-source tools for Static Application Security Testing offer a wide coverage of supported programming languages and types of vulnerabilities that can be detected. The code analysis features of the proposed platform were used in a large scale analysis on 10,855 public repositories. More than 60{\%} of the analyzed repositories contained verified security issues, which highlights the need for automatic code evaluation as a standard practice in the software development pipeline. While popular software projects are usually more secure, having less defects per Megabyte of code, they are not shielded from security issues. The vulnerabilities present in popular projects are likely to have a greater impact in production environments. The large scale analysis results can be used to improve CONSOLE. The paper proposes a strategy for mitigating the False Positives, focusing on prevalent types of alerts and prioritizing them so the developers will be able to examine the most serious issues first. This paper brings awareness on the security defects found in common software projects and proposes the CONSOLE platform as a solution to automatically detect and correct these defects during the software development lifecycle.",
issn="1615-5270",
doi="10.1007/s10207-025-01187-w",
url="https://doi.org/10.1007/s10207-025-01187-w"
}


@Inbook{Tang2025,
author="Tang, Chung Man
and Ng, Vanessa S. C.
and Leung, Henry M. F.
and Yuen, Joe C. H.",
editor="Papadakis, Stamatios",
title="Addressing Challenges and Seizing Opportunities with ChatGPT in Computer Programming Education",
bookTitle="AI Roles and Responsibilities in Education",
year="2025",
publisher="Springer Nature Switzerland",
address="Cham",
pages="249--267",
abstract="In recent years, Artificial Intelligence (AI) and large language models, such as ChatGPT, have gained attention for their ability to address diverse problems. However, their integration into educational settings raises concerns about academic integrity. Students increasingly rely on AI not only for essay-type questions but also for programming assignments, which is the focus of this study. This reliance undermines the evaluation of students' genuine understanding and skill development. Existing countermeasures often fail to effectively identify AI-generated programs. To explore this issue, we carried out a two-stage case study: first, an experiment to assess ChatGPT's accuracy in generating program solutions, followed by a focus group of experienced programming teachers. The group identified practices, strategies, and policies to address AI's impact on academic integrity and assessment accuracy, while exploring opportunities to integrate AI for improving teaching and learning. These insights provide a framework to uphold academic integrity while harnessing the potential benefits of AI in education.",
isbn="978-3-031-96855-6",
doi="10.1007/978-3-031-96855-6_11",
url="https://doi.org/10.1007/978-3-031-96855-6_11"
}


@inproceedings{10.1007/978-981-95-4674-9_24,
 abstract = {Document summarization has long been a key task in natural language processing, with numerous methods proposed over the years. The advent of Transformer models has significantly improved performance for general-purpose summarization. However, specialized domains often require fine-tuning, which can be costly due to the need for curated training data. In this paper, we propose a document summarization framework that leverages named entity recognition (NER), focusing on cyber threat intelligence. We show that effective summarization is achievable using only publicly available data. Furthermore, we demonstrate that our proposed framework enables summarization incorporating generative AI such as GPT, and that it can generate effective summaries compared to simple zero-shot prompting summarization.},
 address = {Singapore},
 author = {Mimoto, Tomoaki
and Kita, Kentaro
and Gempei, Yuta
and Isohara, Takamasa
and Kiyomoto, Shinsaku
and Tanaka, Toshiaki},
 booktitle = {Advances in Information and Computer Security},
 editor = {Cid, Carlos
and Yanai, Naoto},
 isbn = {978-981-95-4674-9},
 pages = {468--485},
 publisher = {Springer Nature Singapore},
 title = {Cyber Threat Intelligence Report Summarization with Named Entity Recognition},
 url = {https://link.springer.com/chapter/10.1007/978-981-95-4674-9_24},
 year = {2026}
}

@Article{Hahne2026,
author="Hahne, Pia-Zoe
and Schmoelz, Alexander",
title="Trusting the machine: a digital humanist perspective on misplaced trust in artificial intelligence",
journal="AI and Ethics",
year="2026",
month="Jan",
day="22",
volume="6",
number="1",
pages="115",
abstract="The widespread invocation of ``trust'' in discussions surrounding generative AI (GenAI) conceals a deeper conceptual and normative tension. This paper critically examines the increasing tendency to treat AI systems as recipients or objects of trust, arguing that such a move risks both conceptual distortion and ethical erosion. Drawing on a digital humanist perspective, the paper contends that trust is a foundationally human, moral concept---anchored in vulnerability, autonomy, and reciprocal recognition---that cannot be meaningfully transferred to machines. The paper differentiates trust from mere reliance and explores the consequences of anthropomorphising AI systems. Using cases such as AI chatbots in therapeutic contexts, it shows how the application of trust to AI not only undermines human agency and emotional depth but also encourages a mechanisation of human social practices and an erosion of moral responsibility. In contrast to functionalist or system-oriented views of trust, a digital humanist approach insists on maintaining the conceptual boundary between humans and machines, advocating for transparency, controllability, and accountability in AI systems---without misappropriating the language of trust. The paper scrutinizes how framing AI as trustworthy reconfigures social norms, blurs lines of responsibility, and endangers the cultivation of human morality. A digital humanist reassertion of trust as an intersubjective and ethical relation is vital to resist these trends and to uphold the dignity and agency of human actors in the age of artificial intelligence.",
issn="2730-5961",
doi="10.1007/s43681-025-00923-1",
url="https://doi.org/10.1007/s43681-025-00923-1"
}


@Inbook{Jeong2025,
author="Jeong, Hyen Seuk",
title="RCA Essentials",
bookTitle="Observability For Legacy Systems: Methods and Solutions with OpenTelemetry and AIOps",
year="2025",
publisher="Apress",
address="Berkeley, CA",
pages="1--93",
abstract="This book explains how to analyze root causes and uses observability and AIOps. There are differences in terminology:",
isbn="979-8-8688-1688-8",
doi="10.1007/979-8-8688-1688-8_1",
url="https://doi.org/10.1007/979-8-8688-1688-8_1"
}


@Inbook{Tiribelli2025,
author="Tiribelli, Simona",
editor="C. Altman, Matthew
and Schwan, David",
title="Toward Healthcare (Social) Justice: On the Value of Biases in Healthcare AI",
bookTitle="Ethics and Medical Technology: Essays on Artificial Intelligence, Enhancement, Privacy, and Justice",
year="2025",
publisher="Springer Nature Switzerland",
address="Cham",
pages="59--76",
abstract="Prominent debates in AI ethics contend that biases are one of the main problems to be eradicated to design fair AI systems, leading the community to develop many technical and non-technical methods mainly focused on eliminating them to create fairer AI tools in healthcare. However, such methods are shown to be often inadequate for the design of fairer healthcare AI. As I argue in this chapter, the problem is a passive understanding of biases as mere errors and technical bugs in AI systems, and of the related solutions mainly aimed at their simple removal. Instead, we should value them as proactive signals of those socio-relational determinants that intangibly and often unfairly shape healthcare access and outcomes in societies over time. This chapter focuses on gender and ethnic biases in healthcare AI and argues for their value in developing AI-based systems and services as drivers of fairer and more just healthcare ecosystems. Also, drawing on ethical theories of affirmative action, the chapter shows how this understanding of biases enables us to generate novel compensatory design actions for the development of truly fair healthcare AI systems, capable of addressing longstanding unfair inequalities in healthcare, therefore promoting better and more just healthcare ecosystems.",
isbn="978-3-031-94690-5",
doi="10.1007/978-3-031-94690-5_3",
url="https://doi.org/10.1007/978-3-031-94690-5_3"
}


@Inbook{Biancofiore2025,
author="Biancofiore, Giovanni Maria
and Di Palma, Dario
and Pomo, Claudio
and Narducci, Fedelucio
and Di Noia, Tommaso",
editor="Germanakos, Panagiotis
and Juhasz, Monika
and Kongot, Aparna
and Marathe, Devashree
and Sacharidis, Dimitris",
title="Conversational User Interfaces and Agents",
bookTitle="Human-Centered AI: An Illustrated Scientific Quest",
year="2025",
publisher="Springer Nature Switzerland",
address="Cham",
pages="399--438",
abstract="Conversational agents (CAs), such as chatbots or virtual assistants, represent Artificial Intelligence (AI) systems designed to facilitate human--machine interaction through natural language. These systems are revolutionizing communication by improving efficiency, effectiveness, and user-friendliness. CAs find applications across various domains, including customer service, health care, and education. This chapter thoroughly explores CAs, focusing on their underlying structure, the problems they aim to address, and the current challenges documented in the existing literature. Furthermore, we delve into the architectures of CAs, shedding light on the key distinctions between modular and end-to-end implementations. To achieve this objective, we introduce a comprehensive taxonomy categorizing the tasks CAs are designed to tackle, encompassing information retrieval, question answering, and chitchat. We scrutinize the advantages and disadvantages of potential models for each of these tasks, besides comprehensively investigating the recent developed methodologies and incorporating a detailed analysis of rule-based and data-driven strategies. Throughout this examination, we emphasize the strengths, limitations, and potential future directions, including the imperative need to develop ethical and reliable conversational agents.",
isbn="978-3-031-61375-3",
doi="10.1007/978-3-031-61375-3_4",
url="https://doi.org/10.1007/978-3-031-61375-3_4"
}


@inproceedings{10.1007/978-3-032-09694-4_22,
 abstract = {DevOps has become a standard practice in the industry to enhance the efficiency of software development and delivery. To address potential security risks throughout the software development lifecycle, DevSecOps integrates security measures into the DevOps process from the outset. Traditionally, security specialists manually analyze scanning results and provide feedback to development teams, which is time-intensive and inefficient. With the rapid advancement of Artificial Intelligence (AI), researchers have explored the potential of using AI to automate security analysis in the DevSecOps process. However, most existing approaches focus on theoretical models, leaving practical applications underdeveloped. Additionally, concerns about data privacy pose significant barriers to the industry's adoption of AI-based solutions. In this paper, we present a practical AI-driven security analysis solution for DevSecOps, designed to enhance the application of AI in enterprise software development. Our approach automates manual security analysis by integrating deployment and security analysis tools with ChatGPT, enabling the generation of detailed security reports without compromising sensitive information. Experiments demonstrate that our platform reduces analysis time by 92{\%} compared to manual efforts while maintaining high accuracy in the results. This solution is well-suited for large enterprises, showcasing the transformative potential of combining AI with DevSecOps practices.},
 address = {Cham},
 author = {Al Noman, Abdullah
and Idowu, Samson Olugbenga
and Kakanou, Rolly Davany Mougoue
and Ciancarini, Paolo
and Ren, Mengfei},
 booktitle = {Proceedings of the International Symposium on Intelligent Computing and Networking 2025},
 editor = {Rodriguez Martinez, Manuel
and Lu, Kejie
and Ye, Feng
and Qian, Yi},
 isbn = {978-3-032-09694-4},
 pages = {274--290},
 publisher = {Springer Nature Switzerland},
 title = {An AI-Based Security Analysis Solution for DevSecOps},
 url = {https://link.springer.com/chapter/10.1007/978-3-032-09694-4_22},
 year = {2026}
}

@inproceedings{10.1007/978-981-95-5294-8_36,
 abstract = {The widespread application of artificial intelligence (AI) technology faces critical challenges from inherent security threats, hindering the development of trusted AI ecosystems. This paper presents a systematic review of AI's intrinsic security, proposing a three-dimensional analytical framework covering data, model, and system layers to analyse core risks such as data contamination, adversarial sample attacks, and API abuse, alongside their cross-layer coupling effects. We then design a comprehensive dynamic defense system that integrates prevention, detection, and response and empirically validates the system across medical, financial, and autonomous driving domains, demonstrating significant risk reduction. Furthermore, the study outlines a phased technical roadmap for integrating quantum computing and edge AI to address the `impossible triangle' of privacy, efficiency, and real-time performance. Finally, it advocates for interdisciplinary collaborative governance---combining legal, ethical, and technological approaches---to build a resilient and trustworthy AI ecosystem, identifying future research priorities for adaptive defense and full lifecycle verification.},
 address = {Singapore},
 author = {Wang, Xinnian
and Hu, Ao
and Liu, Xincheng
and Wang, Botao},
 booktitle = {Computer Applications},
 editor = {Huang, Lan
and Xu, Bin
and Chen, Xuebin
and Wang, Shangguang
and Xia, Bing
and Pan, Jianguo
and Song, Xianhua
and Lu, Zeguang},
 isbn = {978-981-95-5294-8},
 pages = {529--546},
 publisher = {Springer Nature Singapore},
 title = {Intrinsic Security of Artificial Intelligence: System Vulnerabilities, Defense Mechanisms, and Theoretical Deepening},
 url = {https://link.springer.com/chapter/10.1007/978-981-95-5294-8_36},
 year = {2026}
}

@Inbook{Lee2026,
author="Lee, Heisook
and Yang, Sejung
and Chu, Yusung",
editor="Lee, Heisook
and Kim, Nayoung
and Oertelt-Prigione, Sabine",
title="Equitable AI in Healthcare: Navigating Sex, Gender, and Intersectional Biases in Diagnostics",
bookTitle="Sex, Gender, and Emerging Technology in Healthcare: Mitigating Bias and Fostering Equity: From Biology to Care: Sex and Gender Impacts on Health and Medicine",
year="2026",
publisher="Springer Nature Singapore",
address="Singapore",
pages="197--229",
abstract="The integration of Artificial Intelligence (AI) in healthcare offers transformative potential for disease prediction and diagnostics, yet this progress is frequently undermined by the replication and amplification of existing health disparities. This chapter critically examines how AI systems, often trained on historically biased and unrepresentative datasets, perpetuate diagnostic errors---specifically false positives and false negatives---that disproportionately affect women, racial minorities, and individuals with intersecting marginalized identities. We explore foundational concepts such as sex, gender, and intersectionality, alongside the crucial role of social determinants of health, in shaping health outcomes and AI performance. Empirical evidence from cardiovascular disease, kidney disease, and psychiatric diagnostics illustrates the tangible harm caused by the ``male-default'' in medical understanding and the ``black box'' nature of many AI models. We propose a multi-layered framework for equitable AI development, encompassing data-centric interventions, fairness-aware algorithmic design, robust human oversight, and comprehensive policy and regulatory governance. By advocating an ``equity-by-design'' approach that prioritizes sex- and gender-sensitive methodologies and intersectional analysis, this chapter argues that AI can be transformed from a bias amplifier into a powerful instrument for achieving genuinely equitable, transparent, and patient-centered healthcare.",
isbn="978-981-95-2070-1",
doi="10.1007/978-981-95-2070-1_10",
url="https://doi.org/10.1007/978-981-95-2070-1_10"
}


@Article{Johnson2025,
author="Johnson, Marka{\`e}",
title="AI and the urgency for proactive regulation",
journal="AI and Ethics",
year="2025",
month="Dec",
day="01",
volume="5",
number="6",
pages="6265--6278",
abstract="Artificial Intelligence (AI) is a technological advancement that has garnered attention for its benefits to society. Yet, there are concerns about the potential harmful impacts of its influence on decision-making processes within societal institutions. With it impacting almost every facet of our everyday lives, there is a valid concern about the lack of regulation regarding its implementation. This paper argues that to prevent people from reacting adversely to AI implementation, the ethical impacts of AI on society must be taken seriously and addressed proactively, without waiting for something to go wrong. I support this claim by conducting a review of the ethical concerns raised around the implementation of AI from the general U.S public, of larger societal institutions, i.e. the education system and healthcare, and of other historicalAmerican GPTs. This is followed by a review ofthe ethical issues that result from the same rapid, unregulated implementation that we are now seeing with AI. Additionally, a review of commentary from field leaders on the same topic and lastly, a review of the efforts made by the EU and the global call to action from the United Nations. I will ultimately propose an independent, multidisciplinary regulatory body that develops and enforces an ethical, preventative approach to AI implementation as a solution. This paper aims to act as a call to action for the United States, as this course of action is humanity's best chance at harnessing AI's transformative potential for society's benefit, while actively seeking to minimize the ethical dilemmas that have and will arise with its implementation.",
issn="2730-5961",
doi="10.1007/s43681-025-00799-1",
url="https://doi.org/10.1007/s43681-025-00799-1"
}


@Article{SantonideSio2024,
author="Santoni de Sio, Filippo",
title="Artificial Intelligence and the Future of Work: Mapping the Ethical Issues",
journal="The Journal of Ethics",
year="2024",
month="Sep",
day="01",
volume="28",
number="3",
pages="407--427",
abstract="This article introduces seven ethical issues raised by the introduction of artificial intelligence (AI) at work. Each ethical issue is presented in connection to broader and older philosophical topics as well as topics in the more specialised literature on applied ethics of technology. The seven issues are: (1) How to govern the impact of AI on job losses and other social issues raised by the reshaping of the job market? (2) AI may contribute to create new forms of oppression and violation of rights of the workforce; (3) AI may negatively affect workers' (moral) agency, autonomy or responsibility; (4) AI may create hidden labour, that is, economically valuable tasks are performed by human agents without their work being sufficiently recognised, rewarded or protected, with (technological) companies acquiring an unfair gain and an increasing socio-economic power over people; (5) To what extent can AI affect the opportunity for people to perform good or meaningful work, and how should meaningful work be defined in a pluralistic society? (6) The introduction of AI at work may have a broader impact on social values and norms; (7) Who is responsible for making AI have a positive rather than a negative impact on ethical and societal values? In addition to providing a critical introduction to the ethical debate on AI and the future of work, the article also positions on this broader ethical and philosophical map the five articles of The Journal of Ethics special issue `Artificial Intelligence and the Future of Work'.",
issn="1572-8609",
doi="10.1007/s10892-024-09493-6",
url="https://doi.org/10.1007/s10892-024-09493-6"
}


@Article{Gross2024,
author="Gross, Jason
and Erbsen, Andres
and Philipoom, Jade
and Agrawal, Rajashree
and Chlipala, Adam",
title="Towards a Scalable Proof Engine: A Performant Prototype Rewriting Primitive for Coq",
journal="Journal of Automated Reasoning",
year="2024",
month="Aug",
day="14",
volume="68",
number="3",
pages="19",
abstract="We address the challenges of scaling verification efforts to match the increasing complexity and size of systems. We propose a research agenda aimed at building a performant proof engine by studying the asymptotic performance of proof engines and redesigning their building blocks. As a case study, we explore equational rewriting and introduce a novel prototype proof engine building block for rewriting in Coq, utilizing proof by reflection for enhanced performance. Our prototype implementation can significantly improve the development of verified compilers, as demonstrated in a case study with the Fiat Cryptography toolchain. The resulting extracted command-line compiler is about 1000{\$}{\$}{\backslash}times {\$}{\$}faster while featuring simpler compiler-specific proofs. This work lays some foundation for scaling verification efforts and contributes to the broader goal of developing a proof engine with good asymptotic performance, ultimately aimed at enabling the verification of larger and more complex systems.",
issn="1573-0670",
doi="10.1007/s10817-024-09705-6",
url="https://doi.org/10.1007/s10817-024-09705-6"
}


@Inbook{Sewall2024,
author="Sewall, Adam",
editor="McClellan, Stan",
title="Dumb Devices/Smart Adversaries: Real Threats in Critical Infrastructure",
bookTitle="Data, Security, and Trust in Smart Cities",
year="2024",
publisher="Springer Nature Switzerland",
address="Cham",
pages="85--111",
abstract="The rapid advancement of technology has brought about numerous benefits and conveniences to our lives. However, it has also opened up new avenues for malicious actors to target critical infrastructure systems. In recent years, the number and complexity of cybersecurity threats targeting critical infrastructure have risen at an alarming pace. Hacktivists, insider threats, and foreign adversaries pose significant risks to our critical systems. The consequences of a successful cyberattack on critical infrastructure could be catastrophic, leading to widespread disruption and potentially endangering human lives. In fact, there are actual cyber kinetic events where a cyberattack has caused a kinetic or physical event to occur. Therefore, it is crucial for cybersecurity professionals to stay ahead of these threats and develop effective strategies to mitigate them.",
isbn="978-3-031-61117-9",
doi="10.1007/978-3-031-61117-9_5",
url="https://doi.org/10.1007/978-3-031-61117-9_5"
}


@Inbook{Degen2025,
author="Degen, Johanna L.",
title="One-Sided Parasociality with AI",
bookTitle="The Shaping of the Parasocial Self: On the Psychology of Relationships and Intimacy in the Digital Era",
year="2025",
publisher="Springer Nature Switzerland",
address="Cham",
pages="81--123",
abstract="A recently emerging form of parasocialityparasociality emerges between subjectssubjects and AIAI chatbotschatbots and AI companionscompanions. The chapter explores how AI chatbots, such as ChatGPTChatGPT, acquire epistemic authorityepistemic authority on the one hand and target parasocial relatednessrelatedness in their users on the other. Furthermore, challenges and potentials are discussed alongside the latest empirical insights, illustrating how subjects intellectually subordinatesubordinate themselves to AI, often ascribing superiority to it instead of using it as a tool for critical reflectionreflection.",
isbn="978-3-032-07438-6",
doi="10.1007/978-3-032-07438-6_5",
url="https://doi.org/10.1007/978-3-032-07438-6_5"
}


@inproceedings{10.1007/978-981-96-5566-3_22,
 abstract = {Software systems are becoming more and more complex in order to cope with the various complex needs of users. In this context, alerts are often used for analysis to detect faults in complex systems. However, due to the strong correlation between different components of a software system, a system failure typically triggers a large number of alerts. Therefore, reducing the number of alerts becomes a challenging task. This paper leverages pre-trained language models to address this challenge. Semantically related alerts within a short time window are likely to be caused by the same incident. Pre-trained language models can extract deep semantic information from alerts and thus aggregate them belonging to the same incident. This paper conducts a large number of experiments and analyzes on real public alerts datasets, proves the effectiveness of pre-trained language models in the field of alert aggregation, and obtains some interesting findings. Finally, this paper discusses the security of language models and reveals some security issues.},
 address = {Singapore},
 author = {Xu, Wenwu
and Wu, Pengyi
and Wang, Peng
and Zhou, GuoQiao
and Zhai, Lidong},
 booktitle = {Information Security and Cryptology -- ICISC 2024 },
 editor = {Kim, Jongsung
and Park, Jungsoo
and Lee, Wai-Kong},
 isbn = {978-981-96-5566-3},
 pages = {443--464},
 publisher = {Springer Nature Singapore},
 title = {Pre-trained Language Models for Alert Aggregation: Limitations and Opportunities},
 url = {https://link.springer.com/chapter/10.1007/978-981-96-5566-3_22},
 year = {2025}
}

@Article{Shin2025,
author="Shin, Hyojin
and Woo, JiYoung",
title="Semantic and structural fusion for malware detection: Leveraging CodeBERT, GraphCodeBERT, and AST-GCN",
journal="International Journal of Information Security",
year="2025",
month="Nov",
day="12",
volume="24",
number="6",
pages="239",
abstract="In recent years, advances in the Internet and information and communication technologies have made malware activity in cyberspace more sophisticated and diversified. In particular, JavaScript is the most commonly used web scripting language, which can be executed directly in web browsers, making it a potential security threat. However, traditional malware detection tools are limited by their inability to respond quickly to code changes and variants. In this study, we aim to develop a robust model against malware variants and newly emerging malicious code by leveraging a language model trained on diverse programming code, incorporating contextual information, data flow, and syntactic structures. For that, we adopt CodeBERT and GraphCodeBERT. In addition, we propose a model that can understand the grammatical and structural aspects of code by taking abstract syntax tree based edge graph information as input and combining it with graph convolutional network (GCN). Moreover, we propose a multi-modal model that combines GCN, CodeBERT, and GraphCodeBERT and reflect the importance of each model using the attention. This maximizes the strengths and compensates for the weaknesses of each model. The experimental results show that the proposed model has high accuracy and efficiency even on new malicious datasets, especially more recent and more obfuscated dataset. This study is expected to contribute to the construction of a safe Internet environment in the digital age.",
issn="1615-5270",
doi="10.1007/s10207-025-01142-9",
url="https://doi.org/10.1007/s10207-025-01142-9"
}


@Article{Jakku2025,
author="Jakku, Praveen Chaitanya",
title="AI-Powered Static {\&} Dynamic Analysis for Continuous Security in DevSecOps",
journal="SN Computer Science",
year="2025",
month="Sep",
day="25",
volume="6",
number="7",
pages="852",
abstract="This article discusses the combination of AI-based static and dynamic analysis in DevSecOps to ensure continuous security across the software development life cycle. Static analysis identifies security vulnerabilities in source code prior to execution, whereas dynamic analysis detects runtime threats. AI augments these processes by enhancing precision, minimizing false positives, and speeding up remediation. AI-based methods also facilitate automated vulnerability prioritization, secure coding enforcement, and self-healing capabilities. This paper further discusses AI's role in CI/CD pipeline security, compliance monitoring, and advanced threat simulation. Through AI integration, organizations can achieve real-time threat detection, automated mitigation, and enhanced resilience in modern DevSecOps environments.",
issn="2661-8907",
doi="10.1007/s42979-025-04382-7",
url="https://doi.org/10.1007/s42979-025-04382-7"
}


@Inbook{Earp2025,
author="Earp, Brian D.
and van Veenendaal, Tessa
and Porsdam Mann, Sebastian
and Savulescu, Julian",
editor="Chandra, Yanto
and Fan, Ruiping",
title="Digital Psychological Twins in Medicine: Addressing Risks to Human Relationships",
bookTitle="Artificial Intelligence and the Future of Human Relations: Eastern and Western Perspectives",
year="2025",
publisher="Springer Nature Singapore",
address="Singapore",
pages="239--257",
abstract="Medical decision-making on behalf of individuals who have lost the capacity to make their own treatment choices poses significant challenges. The ``substituted judgment'' standard, which prioritizes individual autonomy, requires surrogate decision-makers to choose the course of action that the patient would have chosen or endorsed if they were able to do so. However, research suggests that surrogates often face difficulties in accurately predicting patient preferences, even when making a good-faith effort to adhere to the substituted judgment standard. To address these challenges, the Patient Preference Predictor (P3), a computer-based algorithm that correlates demographic data with expressed medical preferences, has been proposed to supplement or replace aspects of the surrogate decision-making process. However, critics argue that the P3 fails to respect patient autonomy due to relying on population-level data rather than accounting for individuals' unique values, beliefs, and preferences. In response, we have proposed with our colleagues the Personalized Patient Preference Predictor (P4), an artificially intelligent (AI) system that would use machine learning to create a ``digital psychological twin'' or AI simulation of a patient, inclusive of their values, beliefs, and preferences, from individual-level data. While the P4 aims to address concerns raised by the P3, it has been argued that it may introduce new problems, particularly regarding its potential negative effects on the doctor--patient relationship and family relationships. In this chapter, we describe the P4, summarize objections related to its impact on human relationships, respond to these objections, and conclude with some thoughts on future directions.",
isbn="978-981-96-7185-4",
doi="10.1007/978-981-96-7185-4_13",
url="https://doi.org/10.1007/978-981-96-7185-4_13"
}


@Inbook{Shah2026,
author="Shah, Jyoti Kunal
and Patel, Nixalkumar",
editor="Pandey, Bishwajeet
and Patel, Advait",
title="AI-Driven Cloud Transformation",
bookTitle="Revolutionizing the Cloud: Generative AI, Security, and Sustainability",
year="2026",
publisher="Springer Nature Switzerland",
address="Cham",
pages="1--29",
abstract="Although the cloud has defaulted into the IT infrastructure of today, conventional manual techniques cannot adequately control its scope and complexity. This chapter offers a thorough framework for artificial intelligence-led cloud transformation spanning operational, structural, and executive levels. Methodologically, this study compiles architectural models, current industry surveys, and actual case studies to track AI's growing influence in cloud migration, AIOps, and multi-cloud optimization. AI is the driving force behind quantifiable efficiency, resiliency, and sustainability gains; it is no more a sidekick in the cloud.",
isbn="978-3-032-07479-9",
doi="10.1007/978-3-032-07479-9_1",
url="https://doi.org/10.1007/978-3-032-07479-9_1"
}


@Article{Gottschling2025,
author="Gottschling, Markus
and Kramer, Olaf",
title="Persuasive Surfaces and Calculating Machines. A Rhetorical Perspective on Artificial Intelligence",
journal="Global Philosophy",
year="2025",
month="May",
day="09",
volume="35",
number="3",
pages="15",
abstract="This essay examines the communicative implications of generative AI through a rhetorical lens. Rather than asking whether machines can truly `think,' this approach considers how generative AI's probabilistic outputs construct persuasive textual `surfaces' that interact rhetorically with human communication. Authorship and meaning are no longer localized to a single individual creator but emerge through the interplay of human and machine co-construction of `possible worlds.' By viewing generative AI as a `techn{\'e}' in the classical sense, we examine how models `imitate' prior texts through algorithmic recombination. As AI accelerates information flows, it becomes increasingly important for individuals to develop rhetorical literacy, to deconstruct the interests and values encoded in AI's persuasive surfaces. In contrast to the influential but one-directional information-theoretical communication model proposed by Shannon (1948), the essay advocates a rhetorical model of communication that emphasizes the active role of readers in reconstructing possible worlds from textual offers. Rather than neutral vessels of truth, generative AI outputs are inherently contingent yet interest-driven, requiring enhanced communicative competencies to navigate their impacts on changing concepts of reality, authorship, and common ground.",
issn="2948-1538",
doi="10.1007/s10516-025-09748-3",
url="https://doi.org/10.1007/s10516-025-09748-3"
}


@Inbook{Dwivedi2024,
author="Dwivedi, Arpit",
title="ChatGPT Charms: Wielding Words with AI",
bookTitle="CodeMosaic: Learn AI-Driven Development and Modern Best Practices for Enterprise",
year="2024",
publisher="Apress",
address="Berkeley, CA",
pages="271--291",
abstract="Welcome to the enthralling world of ChatGPT in ``ChatGPT Charms: Wielding Words with AI.'' This chapter is a deep dive into the art of prompt engineering using ChatGPT, particularly focusing on its version 3.5. Here, you'll discover not just how to communicate with an AI but how to make it an invaluable ally in your development projects.",
isbn="979-8-8688-0276-8",
doi="10.1007/979-8-8688-0276-8_10",
url="https://doi.org/10.1007/979-8-8688-0276-8_10"
}


@Article{Josifović2025,
author="Josifovi{\'{c}}, Sa{\v{s}}a",
title="Legal and administrative frameworks as foundations for AI alignment with human volition",
journal="AI and Ethics",
year="2025",
month="Jun",
day="01",
volume="5",
number="3",
pages="3057--3067",
abstract="This paper examines the need for regulatory frameworks in future AI development, questioning the common belief that AI, particularly Artificial General Intelligence (AGI), should be morally aligned with human welfare. It advocates for a legal-centric approach over a moral one, suggesting that in the event of AGI emergence, AI systems should be capable of self-regulation, informed by human law and jurisprudence. This aligns with Eliezer Yudkowsky's Coherent Extrapolated Volition (CEV), focusing on legal frameworks as historically grown manifestations of coherent human volition. This paper aims to contribute to the development of a comprehensive AI regulatory framework, integrating ethical and legal aspects and using historical legal records as a foundation for AI training, ensuring alignment with human values and societal norms.",
issn="2730-5961",
doi="10.1007/s43681-024-00640-1",
url="https://doi.org/10.1007/s43681-024-00640-1"
}


@Inbook{Katagiri2024,
author="Katagiri, Nori",
title="Conclusion",
bookTitle="How Liberal Democracies Defend Their Cyber Networks from Hackers: Strategies of Deterrence",
year="2024",
publisher="Springer Nature Switzerland",
address="Cham",
pages="115--135",
abstract="In this chapter, I investigate two potential game-changers in cyberspace dynamics. First, I consider how technological development affects cyberspace deterrence by using Artificial Intelligence (AI) as an example to illustrate some of the major challenges that emerge, for both authoritarian and democratic states. Second, I examine one of the financial technologies that are most relevant to hacking---virtual currencies---to show how the blockchain technology has complicated the effort of democratic states to deal with groups that use them often, including ransomware groups. Finally, I offer a set of policy implications for democratic states.",
isbn="978-3-031-54561-0",
doi="10.1007/978-3-031-54561-0_7",
url="https://doi.org/10.1007/978-3-031-54561-0_7"
}


@inproceedings{10.1007/978-3-031-91134-7_2,
 abstract = {Non-interactive zero-knowledge (NIZK) proofs tend to be randomized and there are many possible proofs for any fixed NP statement. Can we have NIZKs with only a single unique valid proof per statement? Such NIZKs are known under strong cryptographic assumptions (indistinguishability obfuscation), and are conversely known to require strong cryptographic assumptions (witness encryption). In this work, following Lepinski, Micali, and shelat (TCC '05), we consider the following relaxed notion of unique NIZKs (UNIZKs): 1.We only require (computationally) unique proofs for NP statements with a (computationally) unique witness; an adversary that can produce two distinct proofs must also know two distinct witnesses.2.We consider NIZKs with prover setup, where a potentially malicious prover initially publishes a public key {\$}{\$}{\backslash}textsf{\{}pk{\}}{\$}{\$}pkand keeps a corresponding secret key {\$}{\$}{\backslash}textsf{\{}sk{\}}{\$}{\$}sk, which it uses to produce arbitrarily many NIZK proofs {\$}{\$}{\backslash}pi {\$}{\$}$\pi$in the future. While the public key {\$}{\$}{\backslash}textsf{\{}pk{\}}{\$}{\$}pkis not required to be unique, once it is fixed, all the subsequent proofs {\$}{\$}{\backslash}pi {\$}{\$}$\pi$that the prover can produce should be unique.},
 address = {Cham},
 author = {Quach, Willy
and Tyner, LaKyah
and Wichs, Daniel},
 booktitle = {Advances in Cryptology -- EUROCRYPT 2025},
 editor = {Fehr, Serge
and Fouque, Pierre-Alain},
 isbn = {978-3-031-91134-7},
 pages = {34--63},
 publisher = {Springer Nature Switzerland},
 title = {Unique NIZKs and Steganography Detection},
 url = {https://link.springer.com/chapter/10.1007/978-3-031-91134-7_2},
 year = {2025}
}

@Inbook{Reiter2025,
author="Reiter, Ehud",
title="Evaluation",
bookTitle="Natural Language Generation",
year="2025",
publisher="Springer Nature Switzerland",
address="Cham",
pages="93--141",
abstract="From both a scientific and practical perspective, it is essential to evaluate how well NLG algorithms, models, and techniques work, using experiments that are rigorous and replicable. This chapter discusses basic evaluation concepts and then describes techniques for human and automatic (metric) evaluation. It also looks at evaluating real-world impact of NLG systems and commercial evaluation.",
isbn="978-3-031-68582-8",
doi="10.1007/978-3-031-68582-8_5",
url="https://doi.org/10.1007/978-3-031-68582-8_5"
}


@inproceedings{10.1007/978-981-96-9101-2_26,
 abstract = {Code duplication is a common phenomenon in large scale enterprise software systems, especially in regulated domains such as banking. However, most existing tools rely on tokenizers like javalang, which only support Java 8 and fail to handle modern language features introduced in Java 11 and Java 17. This limits their applicability in real world industrial environments. In this work, we propose a practical and scalable clone detection pipeline that integrates semantic similarity via TF-IDF with structural validation using Tree Edit Distance. Our method is lightweight and fully compatible with newer Java versions. Experimental results on the BigCloneBench dataset show that our approach achieves precision and recall comparable to those of two representative tools, Toma and Amain, while overcoming their limitation of Java version compatibility. Further evaluations on synthetically generated Java 11/17 code confirm the robustness of our method in detecting Type-1 and Type-2 clones in modern codebases.},
 address = {Singapore},
 author = {Zhang, Xiaowei
and Liu, Shigang
and Zhang, Jun
and Xiang, Yang},
 booktitle = {Information Security and Privacy},
 editor = {Susilo, Willy
and Pieprzyk, Josef},
 isbn = {978-981-96-9101-2},
 pages = {461--479},
 publisher = {Springer Nature Singapore},
 title = {Bridging Clone Detection and Industrial Compliance: A Practical Pipeline for Enterprise Codebases},
 url = {https://link.springer.com/chapter/10.1007/978-981-96-9101-2_26},
 year = {2025}
}

@Article{Mathew2024,
author="Mathew, Rita
and Stefaniak, Jill E.",
title="A Needs Assessment to Support Faculty Members' Awareness of Generative AI Technologies to Support Instruction",
journal="TechTrends",
year="2024",
month="Jul",
day="01",
volume="68",
number="4",
pages="773--789",
abstract="This paper provides an overview of a needs assessment study conducted in higher education to address the impact of generative AI, specifically ChatGPT, on faculty development and instructional strategies. The study acknowledges the disruptive effect of AI on academia and the varying responses from faculty members. It emphasizes the importance of equipping educators with the necessary knowledge and skills to effectively integrate generative AI into their teaching practices. The results of the needs assessment reveal various factors contributing to performance gaps, including policy and budget, knowledge, skills, competencies, information, and technological tools. The study suggests that faculty members need support in understanding the scope and implications of AI and AI-driven content creation, assessment, and personalized learning.",
issn="1559-7075",
doi="10.1007/s11528-024-00964-z",
url="https://doi.org/10.1007/s11528-024-00964-z"
}


@inproceedings{10.1007/978-3-031-49252-5_2,
 abstract = {This short paper associated to the invited lectures introduces two key concepts essential to artificial intelligence (AI), the area of trustworthy AI and the concept of responsible AI systems, fundamental to understand the technological, ethical and legal context of the current framework of debate and regulation of AI. The aim is to understand their dimension and their interrelation with the rest of the elements involved in the regulation and auditability of AI algorithms in order to achieve safe and trusted AI. We highlight concepts in bold in order to fix the moment when they are described in context.},
 address = {Cham},
 author = {Herrera, Francisco},
 booktitle = {Engineering of Computer-Based Systems},
 editor = {Kofro{\v{n}}, Jan
and Margaria, Tiziana
and Seceleanu, Cristina},
 isbn = {978-3-031-49252-5},
 pages = {7--11},
 publisher = {Springer Nature Switzerland},
 title = {Toward Responsible Artificial Intelligence Systems: Safety and Trustworthiness},
 url = {https://link.springer.com/chapter/10.1007/978-3-031-49252-5_2},
 year = {2024}
}

@Article{Jiang2025,
author="Jiang, Wenxin
and Kim, Mingyu
and Cheung, Chingwo
and Kim, Heesoo
and Thiruvathukal, George K.
and Davis, James C.",
title="``I see models being a whole other thing'': an empirical study of pre-trained model naming conventions and a tool for enhancing naming consistency",
journal="Empirical Software Engineering",
year="2025",
month="Aug",
day="16",
volume="30",
number="6",
pages="155",
abstract="As innovation in deep learning continues, many engineers are incorporating Pre-Trained Models (PTMs) as components in computer systems. Some PTMs are foundation models, and others are fine-tuned variations adapted to different needs. When these PTMs are named well, it facilitates model discovery and reuse. However, prior research has shown that model names are not always well chosen and can sometimes be inaccurate and misleading. The naming practices for PTM packages have not been systematically studied, which hampers engineers' ability to efficiently search for and reliably reuse these models. In this paper, we conduct the first empirical investigation of PTM naming practices in the Hugging Face PTM registry. We begin by reporting on a survey of 108 Hugging Face users, highlighting differences from traditional software package naming and presenting findings on PTM naming practices. The survey results indicate a mismatch between engineers' preferences and current practices in PTM naming. We then introduce DARA, the first automated DNN ARchitecture Assessment technique designed to detect PTM naming inconsistencies. Our results demonstrate that architectural information alone is sufficient to detect these inconsistencies, achieving an accuracy of 94{\%} in identifying model types and promising performance (over 70{\%}) in other architectural metadata as well. We also highlight potential use cases for automated naming tools, such as model validation, PTM metadata generation and verification, and plagiarism detection. Our study provides a foundation for automating naming inconsistency detection. Finally, we envision future work focusing on automated tools for standardizing package naming, improving model selection and reuse, and strengthening the security of the PTM supply chain. ``The main idea is to treat a program as a piece of literature, addressed to human beings rather than to a computer'' ---D. Knuth",
issn="1573-7616",
doi="10.1007/s10664-025-10711-4",
url="https://doi.org/10.1007/s10664-025-10711-4"
}


@Article{Arthur2025,
author="Arthur, John Kingsley
and Zhou, Conghua
and Shen, Xiang-Jun
and Amber-Doh, Ronky Wrancis
and Mantey, Eric Appiah
and Osei-Kwakye, Jeremiah",
title="DIMCAR: dynamic intent modeling and context-aware recommendations in sparse data environment towards next basket prediction",
journal="Applied Intelligence",
year="2025",
month="Oct",
day="03",
volume="55",
number="15",
pages="996",
abstract="In the fast-changing world of e-commerce, the success of recommender systems is crucial for boosting user engagement and increasing sales. Conventional models often struggle with evolving user preferences and data sparsity, hindering accurate predictions. Existing Graph-based regularization mechanisms and deep learning approaches address these challenges but remain sensitive to noise and computational complexity, limiting their effectiveness in large-scale, real-time settings. We propose a novel multi-layered Next Basket Recommender System called dynamic intent modelling and context-aware recommendation (DIMCAR) model to overcome these limitations. First, we resolve the data sparsity problem by constructing a novel optimized Graph Sparse Regularization framework for Non-negative Matrix Factorization (OGSR-NMF) framework integrating a time-varying graph structure, a novel hybrid sparsity norm, a modified Proximal Alternating Linearized Minimization (mPALM). Additionally, we dynamically model user intents and context using attention mechanisms and Gated Recurrent Units (GRUs). Finally, we integrate a novel Adaptive Reptile Basket Optimization Algorithm into a Deep Convolutional Neural Network, enhancing the model's adaptability to changing user behaviours in real time. Theoretical analysis and experiments on four benchmark datasets demonstrate that DIMCAR outperforms existing models in recommendation accuracy and user satisfaction.",
issn="1573-7497",
doi="10.1007/s10489-025-06796-5",
url="https://doi.org/10.1007/s10489-025-06796-5"
}


@Inbook{Vinora2025,
author="Vinora, A.
and Bojiah, Janaki
and Alfiras, M.",
editor="AlDhaen, Esra
and Braganza, Ashley
and Hamdan, Allam
and Chen, Weifeng",
title="Sentiment Analysis of Reviews on AI Interface ChatGPT: An Interpretative Study",
bookTitle="Business Sustainability with Artificial Intelligence (AI): Challenges and Opportunities: Volume 2",
year="2025",
publisher="Springer Nature Switzerland",
address="Cham",
pages="311--326",
abstract="ChatGPT is an artificial intelligence (AI) interface where in when you feed a question, it attempts to provide human-like responses in terms of text. This is particularly useful for producing content, responding to the queries, and explaining the ideas. Henceforth, without much of user resistance, this technology has been embraced by the users. They use ChatGPT predominantly for academic purposes and acquiring information. Therefore, it becomes essential to evaluate the effectiveness of performance of ChatGPT which becomes the intent of this study. To assess the efficiency of ChatGPT, sentiment analysis is done on the ratings of its reviews. Various algorithms like BERT, Multinomial Na{\"i}ve Bayes, XGBOOST classifier, and Random Forest classifier were deployed on the dataset that was obtained from Kaggle to perform sentiment analysis. These algorithms helped to cluster the ratings as a range where 4--5 was considered positive, 3 as neutral and 1--2 as negative. The model resulted in realizing the comprehending capabilities of ChatGPT with respect to the query raised by the user. The examining of user input, desired output, and the actual output helps us evaluate the quality of the understanding of the ChatGPT. The results of the study endorse that the responses or reviews that are rated and used for enhancing the performance of ChatGPT significantly and gradually shown progress on the subject. The scope of the study is to contribute towards the finetuning of the results regenerated by AI.",
isbn="978-3-031-71318-7",
doi="10.1007/978-3-031-71318-7_30",
url="https://doi.org/10.1007/978-3-031-71318-7_30"
}


@Inbook{Forgács2024,
author="Forg{\'a}cs, Istv{\'a}n
and Kov{\'a}cs, Attila",
title="Domain Testing",
bookTitle="Modern Software Testing Techniques: A Practical Guide for Developers and Testers",
year="2024",
publisher="Apress",
address="Berkeley, CA",
pages="119--206",
abstract="This chapter will delve into the extension and optimization of equivalence partitioning (EP) and boundary value analysis (BVA) techniques, offering you insights into their enhanced and efficient application. We show that the existing BVA methods are neither reliable nor cost-effective. By applying our proposed technique, which can be automated in part, the number of test cases can significantly be reduced and will detect all (or most of) the EP/BVA faults. This methodology is termed optimized domain testing. Notably, it was presented under the name ``general predicate testing (GPT)'' at various international conferences before the emergence of chatGPT.",
isbn="978-1-4842-9893-0",
doi="10.1007/978-1-4842-9893-0_3",
url="https://doi.org/10.1007/978-1-4842-9893-0_3"
}


@Inbook{Raizonville2026,
author="Raizonville, Adrien
and Lacroix, Daphn{\'e}",
editor="Pisarkiewicz, Anna Renata
and Brennan, Timothy J.
and Mazzoni, Leonardo
and Glass, Victor",
title="Digital Commons and Postal Operators: An Exploratory Approach",
bookTitle="Postal Strategies in a Digital and Green Transition: Universal Service, Challenges, and Innovations ",
year="2026",
publisher="Springer Nature Switzerland",
address="Cham",
pages="245--259",
abstract="Digital Commons aim to achieve collective production and management of non-rivalrous and possibly non-exclusive digital resources (e.g., data, software, technology) by a voluntarily formed group. Their governance rules generally favor the use and reuse of these digital resources, making them widely available to others. This is particularly relevant in the digital era because of low replication costs, which enhances the resource collective value. However, Digital Commons may also face a free-rider problem as individuals can benefit from the resource without any obligation to contribute to its production.",
isbn="978-3-032-00502-1",
doi="10.1007/978-3-032-00502-1_17",
url="https://doi.org/10.1007/978-3-032-00502-1_17"
}


@Inbook{Yang2025,
author="Yang, Lin
and Lu, Jiqiao
and Lu, Shuya",
editor="Fong, Ben Yuk Fai",
title="AI in Primary Healthcare",
bookTitle="The Handbook of Public Health in the Asia-Pacific",
year="2025",
publisher="Springer Nature Singapore",
address="Singapore",
pages="1--20",
abstract="Primary healthcare serves as the foundation of health systems, providing accessible, preventive, and continuous care for everyday needs, especially in managing chronic conditions and promoting community well-being. However, the COVID-19 pandemic exposed critical weaknesses such as information gaps, supply shortages, and fragmented health IT systems. Artificial intelligence (AI) offers transformative potential to address these gaps---enhancing disease forecasting, supporting remote care, and optimizing resource allocation---though ethical concerns around privacy, fairness, and inclusivity must be addressed to ensure safe and equitable implementation.",
isbn="978-981-97-1788-0",
doi="10.1007/978-981-97-1788-0_54-1",
url="https://doi.org/10.1007/978-981-97-1788-0_54-1"
}


@Article{Yadav2024,
author="Yadav, Venkteshwar
and Pal, Dharm
and Poonia, Anil Kumar",
title="A Study on Genetically Engineered Foods: Need, Benefits, Risk, and Current Knowledge",
journal="Cell Biochemistry and Biophysics",
year="2024",
month="Sep",
day="01",
volume="82",
number="3",
pages="1931--1946",
abstract="Food requirements have always been a top priority, and with the exponential growth of the human population, there is an increasing need for large quantities of food. Traditional cultivation methods are not able to meet the current demand for food products. One significant challenge is the shortened shelf-life of naturally occurring food items, which directly contributes to food scarcity. Contaminating substances such as weeds and pests play a crucial role in this issue. In response, researchers have introduced genetically engineered (GE) food as a potential solution. These food products are typically created by adding or replacing genes in the DNA of naturally occurring foods. GE foods offer various advantages, including increased quality and quantity of food production, adaptability to various climatic conditions, modification of vitamin and mineral levels, and prolonged shelf life. They address the major concerns of global food scarcity and food security. However, the techniques used in the production of GE foods may not be universally acceptable due to the genetic alteration of animal genes into plants or vice versa. Additionally, their unique nature necessitates further long-term studies. This study delves into the procedures and growth stages of DNA sequencing, covering the benefits, risks, industrial relevance, current knowledge, and future challenges of GE foods. GE foods have the potential to extend the shelf life of food items, alleviate food shortages, and fulfill the current nutritional food demand.",
issn="1559-0283",
doi="10.1007/s12013-024-01390-x",
url="https://doi.org/10.1007/s12013-024-01390-x"
}


@inproceedings{10.1007/978-3-031-70245-7_12,
 abstract = {Context. Large Language Models (LLMs) are now crucial for developers to increase productivity and reduce software development time and cost. Code Llama, an LLM from Meta, is one of the most recent LLM tools. However, currently there is no objective assessment of the energy efficiency of the source code generated by Code Llama.},
 address = {Cham},
 author = {Cursaru, Vlad-Andrei
and Duits, Laura
and Milligan, Joel
and Ural, Damla
and Sanchez, Berta Rodriguez
and Stoico, Vincenzo
and Malavolta, Ivano},
 booktitle = {Quality of Information and Communications Technology},
 editor = {Bertolino, Antonia
and Pascoal Faria, Jo{\~a}o
and Lago, Patricia
and Semini, Laura},
 isbn = {978-3-031-70245-7},
 pages = {161--176},
 publisher = {Springer Nature Switzerland},
 title = {A Controlled Experiment on the Energy Efficiency of the Source Code Generated by Code Llama},
 url = {https://link.springer.com/chapter/10.1007/978-3-031-70245-7_12},
 year = {2024}
}

@Inbook{Cagle2024,
author="Cagle, Anton
and Ahmed, Ahmed Ceifelnasr",
title="Testing Your Enterprise AI Application",
bookTitle="Architecting Enterprise AI Applications: A Guide to Designing Reliable, Scalable, and Secure Enterprise-Grade AI Solutions",
year="2024",
publisher="Apress",
address="Berkeley, CA",
pages="145--170",
abstract="Machine learning models and intelligent LLM systems are rapidly transforming industries, and the importance of robust testing cannot be overstated. As enterprises across various sectors use AI technologies, ensuring the reliability, accuracy, and ethical integrity of these systems becomes increasingly important. Testing, which has long been a cornerstone of traditional software development, takes on a new level of significance when applied to the world of Enterprise AI.",
isbn="979-8-8688-0902-6",
doi="10.1007/979-8-8688-0902-6_8",
url="https://doi.org/10.1007/979-8-8688-0902-6_8"
}


@Inbook{Šekrst2025,
author="{\v{S}}ekrst, Kristina",
title="Hallucinations",
bookTitle="The Illusion Engine: The Quest for Machine Consciousness",
year="2025",
publisher="Springer Nature Switzerland",
address="Cham",
pages="211--226",
abstract="Hallucinations---perceptual experiences without corresponding external stimuli---span both biological and artificial systems and seem to provoke fundamental questions about the nature of perception and reality construction. In humans, these phenomena manifest as everything from vivid sensory phantoms to philosophical puzzles like the brain-in-a-vat scenario, which Hilary Putnam argued is self-refuting since linguistic reference requires causal contact with real objects.",
isbn="978-3-032-05562-0",
doi="10.1007/978-3-032-05562-0_12",
url="https://doi.org/10.1007/978-3-032-05562-0_12"
}


@Inbook{Ralph2024,
author="Ralph, Paul
and Kuutila, Miikka
and Arif, Hera
and Ayoola, Bimpe",
editor="Mendez, Daniel
and Avgeriou, Paris
and Kalinowski, Marcos
and Ali, Nauman Bin",
title="Teaching Software Metrology: The Science of Measurement for Software Engineering",
bookTitle="Handbook on Teaching Empirical Software Engineering",
year="2024",
publisher="Springer Nature Switzerland",
address="Cham",
pages="101--154",
abstract="While the methodological rigor of computing research has improved considerably in the past two decades, quantitative software engineering research is hampered by immature measures and inattention to theory. Measurement---the principled assignment of numbers to phenomena---is intrinsically difficult because observation is predicated upon not only theoretical concepts but also the values and perspective of the research. Despite several previous attempts to raise awareness of more sophisticated approaches to measurement and the importance of quantitatively assessing reliability and validity, measurement issues continue to be widely ignored. The reasons are unknown, but differences in typical engineering and computer science graduate training programs (e.g., compared to psychology and management) are likely involved. This chapter therefore reviews key concepts in the science of measurement and applies them to software engineering research. A series of exercises for applying important measurement concepts to the reader's research are included, and a sample dataset for the reader to try some of the statistical procedures mentioned is provided.",
isbn="978-3-031-71769-7",
doi="10.1007/978-3-031-71769-7_5",
url="https://doi.org/10.1007/978-3-031-71769-7_5"
}


@Article{Huang2025,
author="Huang, Pu
and Bao, Zhenyu
and Guo, Jinqin
and Xie, Yuedong",
title="A Thickness Measurement Method Immune to Lift-Off Fluctuation Using Sweep-Frequency Eddy Current Testing",
journal="Journal of Nondestructive Evaluation",
year="2025",
month="Jun",
day="09",
volume="44",
number="3",
pages="69",
abstract="Eddy current testing (ECT) is a highly effective technique for measuring the thickness of metal samples. However, the fluctuation of lift-off distance easily affects the accuracy of measurement. In this paper, a thickness measurement immune to lift-off strategy based on the sweep frequency eddy current testing is investigated. First of all, we conducted an analysis on the relationship between the peak frequency of mutual inductance variation and the thickness of metal plates in line with Dodd-Deeds analytical solution. Moreover, we have demonstrated that the real part of mutual inductance variation at high frequencies ({\textasciitilde}{\thinspace}MHz) can be directly employed to invert and estimate lift-off, which is immune to the thickness and electromagnetic properties of metal samples. According to the estimated lift-off, the instrument factor for thickness measurement can be compensated to improve accuracy of thickness measurement. Both experiment and numerical solution have been applied to verify the proposed method, and the results indicate the relative error is only within 2.4{\%}, which provides an approach to actual online measurement in the future.",
issn="1573-4862",
doi="10.1007/s10921-025-01201-0",
url="https://doi.org/10.1007/s10921-025-01201-0"
}


@Article{Reuben-Owoh2025,
author="Reuben-Owoh, Blessing
and Haig, Ella",
title="A Systematic Review of Voluntary Cybersecurity Standards and Frameworks",
journal="International Journal of Information Security",
year="2025",
month="Sep",
day="12",
volume="24",
number="5",
pages="206",
abstract="As the world becomes increasingly digitalised, cyber insecurity has become the monster under the bed, an unseen threat that many either remain unaware of or ignore, hoping it will simply vanish. In today's digital landscape, nearly every individual with a digital footprint has, at some point, encountered spam calls or emails containing offers that appear too good to be true or attempt to induce fear and coerce them into parting with their money. Organisations of all sizes are on the front lines of this cyber war. A 2024 cybersecurity breaches survey by the UK Home Office found several organisations reported receiving countless phishing emails and other cyberattacks daily. This study evaluates and ranks the leading voluntary cybersecurity frameworks and standards based on their relevance and adoption in the academic and professional communities. A systematic review identified 12 voluntary cybersecurity frameworks and 17 cybersecurity standards. The top five frameworks and standards from each category were analysed, with the key components of each outlined in detail. The ranking system developed for this study was based on academic research interest, highlighting the cybersecurity measures most frequently studied and discussed in scholarly literature. The results provide a comprehensive overview of the most influential voluntary cybersecurity frameworks and standards, providing valuable insights for organisations seeking to improve their cybersecurity posture through voluntary measures. This study aimed to contribute to the ongoing discussion of how voluntary cybersecurity initiatives can complement regulatory efforts in safeguarding digital infrastructure.",
issn="1615-5270",
doi="10.1007/s10207-025-01121-0",
url="https://doi.org/10.1007/s10207-025-01121-0"
}


@Inbook{García-Marzá2024,
author="Garc{\'i}a-Marz{\'a}, Domingo
and Calvo, Patrici",
title="Hyperethics: The Automation of Morality",
bookTitle="Algorithmic Democracy: A Critical Perspective Based on Deliberative Democracy",
year="2024",
publisher="Springer International Publishing",
address="Cham",
pages="147--166",
abstract="The aim of this chapter is to examine the defining characteristics of and challenges posed by the advent of a datafied, hyperconnected and algorithmic approach to the clarification, substantiation and application of morality: hyperethics. For this purpose, ethics will be addressed as practical knowledge concerned with the rationalization of free modes of behaviour, and which locates its criterion of morality within the dialogue between those concerned by an ethical issue. Attention will then be turned to an in-depth analysis of ethification, the current trend towards establishing processes for transforming social and moral reality into computable online data. It will then show how artificially intelligent mathematical models are progressively and relentlessly colonizing meaning-based rationalization processes, giving rise to meaninglessness, anomie and psychopathologies in mature democracies. Finally, we offer a critical reflection on the ethical and democratic challenges underlying the use of Artificial intelligence as a tool to establish what is fair and pleasing for a digitally hyperconnected society.",
isbn="978-3-031-53015-9",
doi="10.1007/978-3-031-53015-9_8",
url="https://doi.org/10.1007/978-3-031-53015-9_8"
}


@Inbook{Crampton2024,
author="Crampton, Jeremy",
editor="Gl{\"u}ckler, Johannes
and Panitz, Robert",
title="How Digital Geographies Render Value: Geofences, the Blockchain, and the Possibilities of Slow Alternatives",
bookTitle="Knowledge and Digital Technology",
year="2024",
publisher="Springer Nature Switzerland",
address="Cham",
pages="257--279",
abstract="This chapter proceeds in two parts. First, it situates the way that digital geographies render value by creating new markets for the creation, extraction, and capture of that value. Two digital geographies are highlighted: geofences/geoframing and cryptocurrencies on the blockchain. Drawing on the renewed interest in theorizing the digital urban growth machine, I analyze these new markets as forms of toxic innovation dominated by a focus on growth and extractive value. Second, I examine the possibilities for alternative responsible innovation that creates social value, which I call the Slow Data Economy. Inspired by the ethics of slowness, slow data centers accountability, the human-in-the-loop, local co-design, and vision making for the future city. By working with the possibilities of responsible innovation, I explore how the Slow Data Economy can be a form of ``capital switching'' in which investment is switched from a focus on growth and extraction to care and repair.",
isbn="978-3-031-39101-9",
doi="10.1007/978-3-031-39101-9_14",
url="https://doi.org/10.1007/978-3-031-39101-9_14"
}


@Article{Xu2025,
author="Xu, Yunhao
and Deng, Yiping
and Wu, Peiyu
and Tian, Mengke
and Liu, Chen
and Liu, Xuyang
and Liu, Danhua
and Guo, Yinan
and Wang, Pengfei
and Xu, Yuming
and Wang, Yonggang
and Li, Yusheng",
title="Selective loss and transcriptional reprogramming of Nox4+ GABAergic neurons in the trigeminal nucleus caudalis of NTG-induced chronic migraine model",
journal="The Journal of Headache and Pain",
year="2025",
month="Nov",
day="18",
volume="26",
number="1",
pages="263",
abstract="Chronic migraine is a disabling neurological disorder with complex mechanisms. The trigeminal nucleus caudalis (TNC) is a critical relay in migraine pathogenesis, yet its cellular and molecular underpinnings remain unclear.",
issn="1129-2377",
doi="10.1186/s10194-025-02203-z",
url="https://doi.org/10.1186/s10194-025-02203-z"
}


@Article{Hind2025,
author="Hind, Sam",
title="From United Steel to Waymo: industrializing simulation",
journal="AI {\&} SOCIETY",
year="2025",
month="Apr",
day="01",
volume="40",
number="4",
pages="2015--2028",
abstract="The use of computers for simulation work can be traced back to the 1950s, and the pioneering work of Stafford Beer, KD Tocher and others at Cybor House in Sheffield, UK, the research and development (R{\&}D) department of British steelmakers, United Steel. This innovative simulation work sought to offer an abstracted, `total' environment of the steelmaking process in which different operational activities could be modeled. Critical to this work was the ability of computer simulations to perform such modelling at a fraction of the cost, wasting fewer material resources, and in a considerably shorter timeframe. Such work can be understood as the earliest example of the application of industrial-scale `automated computation' to a real-world industrial process. Similarly indebted to the early principles of computer simulation, Waymo engineers are also engaged in the building of so-called `conflict typologies' designed to encode material properties of everyday driving interactions between road users, rather than simply road users themselves. Through `motion planning', coupled with the categorization of driving interactions, Waymo engineers build instrumental understanding of their own system's purported intelligence in navigating everyday driving situations. Functioning as `generative mechanisms' rather than simply evaluative devices, engineers seek to industrialize---instrumentalize, scale up, rationalize---everyday driving knowledge. Through conflict typologies, instrumental knowledge of the actual capacities of autonomous vehicles is industrialized, materialized, and realized.",
issn="1435-5655",
doi="10.1007/s00146-024-02051-6",
url="https://doi.org/10.1007/s00146-024-02051-6"
}


@inproceedings{10.1007/978-3-031-44900-0_4,
 abstract = {Teaching text-based programming poses significant challenges in both school and university contexts. This study explores the potential of ChatGPT as a sustainable didactic tool to support students, freshmen, and teachers. By focusing on a beginner's course with examples also relevant to vocational schools, we investigated three research questions. First, the extent to which ChatGPT assists students in solving and understanding initial examples; secondly, the feasibility of teachers utilizing the chatbot for grading student solutions; and finally, the additional support ChatGPT provides in terms of teaching. Our findings demonstrate that ChatGPT offers valuable guidance for teachers in terms of assessment and grading and aids students in understanding and optimizing their solutions.},
 address = {Cham},
 author = {Wieser, Markus
and Sch{\"o}ffmann, Klaus
and Stefanics, Daniela
and Bollin, Andreas
and Pasterk, Stefan},
 booktitle = {Informatics in Schools. Beyond Bits and Bytes: Nurturing Informatics Intelligence in Education},
 editor = {Pellet, Jean-Philippe
and Parriaux, Gabriel},
 isbn = {978-3-031-44900-0},
 pages = {40--53},
 publisher = {Springer Nature Switzerland},
 title = {Investigating the Role of ChatGPT in Supporting Text-Based Programming Education for Students and Teachers},
 url = {https://link.springer.com/chapter/10.1007/978-3-031-44900-0_4},
 year = {2023}
}

@Article{Terziyan2025,
author="Terziyan, Vagan
and Tiihonen, Timo
and Shukla, Amit K.
and Gryshko, Svitlana
and Golovianko, Mariia
and Terziyan, Oleksandr
and Vitko, Oleksandra",
title="Towards ethical evolution: responsible autonomy of artificial intelligence across generations",
journal="AI and Ethics",
year="2025",
month="Oct",
day="01",
volume="5",
number="5",
pages="5165--5190",
abstract="The emergence of autonomous systems capable of designing subsequent generations of Artificial Intelligence (AI) introduces profound challenges in ensuring ethical integrity and accountability. This article presents a novel framework combining meta-responsibility, genetic algorithms, and time-travel-inspired abstractions to address these challenges. Central to this study is an immutable ethical principle: AI must not harm humanity or violate fundamental values, must monitor and mitigate misuse of its outcomes, and must ensure all derivative AI products inherit this principle as an immutable safeguard. The framework ensures that AI systems, acting as designers of subsequent AI generations, propagate these ethical principles reliably across generations, enabling ethical inheritance in AI-as-a-designer-of-AI scenarios. The meta-responsibility framework addresses the critical question of maintaining responsibility and ethical principles not only for AI systems designed by humans but also for those designed by other AI systems. At its core, the genetic responsibility model balances immutable and mutable principles, ensuring adaptability while preserving ethical standards during self-cloning, contextual adaptation, and intergenerational design. Tailored for wide range of potential applications of autonomous systems, this framework offers a scalable foundation for trustworthy AI design, ensuring consistent ethical behavior and reliable responsibility propagation across generations of autonomous agents.",
issn="2730-5961",
doi="10.1007/s43681-025-00759-9",
url="https://doi.org/10.1007/s43681-025-00759-9"
}


@Article{Paul2025,
author="Paul, Agyemang
and Cao, Jianbin
and Ge, Yuntian
and Gong, Shufeng
and Ye, Ting",
title="Robust visual recommendation via graph pairwise training",
journal="International Journal of Multimedia Information Retrieval",
year="2025",
month="Oct",
day="28",
volume="14",
number="4",
pages="38",
abstract="Visual recommender systems leverage images from sensor devices to enhance recommendations by integrating user feedback with visual features. Learning to rank algorithms are commonly used, with Visual Bayesian Personalized Ranking (VBPR) being popular. VBPR utilizes pairwise optimization to maximize the margin between positive and negative predictions. Adversarial Multimedia Ranking (AMR) enhances VBPR by robustifying it through adversarial pairwise ranking (APR). However, a challenge with APR is that not all unobserved feedback is negative; some unknowns could be positive, mislabeled as negative. This paper presents the Adversarial Graph Pairwise Training (AdvGPT) model, employing negative sampling for robust visual recommendation. Our contributions are threefold: (1) Integrating non-visual features in a factorization machine to capture users' aesthetic preferences. (2) Introducing a graph APR algorithm that constructs item neighbor sets based on similarity in both visual space and graphs. This strategy assumes that items in the neighbor set of a purchased item (positive sample) are more likely to match the user's aesthetics. We consider these potential positives as a third label type, between positive and negative. (3) Perturbations in connected items can cumulatively affect a target item in visual and graph spaces, making user-item graph structures vulnerable. To improve AdvGPT's robustness and generalization, we learn user and item embeddings, exploring various interaction types. Employing the adversarial pairwise ranking method, we optimize AdvGPT to generate more robust embeddings for recommendations. Extensive experiments on real-world datasets validate the effectiveness of our approach.",
issn="2192-662X",
doi="10.1007/s13735-025-00388-2",
url="https://doi.org/10.1007/s13735-025-00388-2"
}


@Article{Upadhyay2025,
author="Upadhyay, Ramakant
and Rohil, Mukesh Kumar",
title="Blockchain-enabled wireless body area networks for healthcare under digital india mission: a survey",
journal="Proceedings of the Indian National Science Academy",
year="2025",
month="Oct",
day="15",
abstract="During the last two decades, numerous technological platforms growing in Indian market to facilitate and serve the medical services. In this context, the amalgamation of Wireless Body Area Network (WBANS) and Blockchain technology (BT) has been in high demand and plays a prominent role due to its trends towards efficient use of Electronic Health records in health care industry. This article recommended complete, extensive and comprehensive survey of modern research efforts in conjunction of BT with WBANs in health care domain to secure medical related data and to ensure trustworthy communication over the network in the Indian context. In addition, the paper analyses several controlling techniques' themes-wise for privacy, security, energy efficiency, authentication, transparency and information sharing among numerous parties (e.g. patient, doctor, nurse and hospital etc.). The article also addresses the capability of blockchain in terms of confidentiality, integrity, availability and scalability over WBAN spaces. Furthermore, the survey recommended India-based real use cases, real adaption barriers under the Indian Digital mission and challenges under the streamline literatures, and futuristic research direction to design efficient and secure wireless system model that could be integrated with modernized healthcare environment using blockchain technique.",
issn="2454-9983",
doi="10.1007/s43538-025-00564-2",
url="https://doi.org/10.1007/s43538-025-00564-2"
}


@Article{Zhang2024,
author="Zhang, Xuejun
and Hou, Xia
and Qiao, Xiuming
and Song, Wenfeng",
title="A review of automatic source code summarization",
journal="Empirical Software Engineering",
year="2024",
month="Oct",
day="07",
volume="29",
number="6",
pages="162",
abstract="Code summarization plays a pivotal role in the field of software engineering by offering developers a concise natural language comprehension of source code semantics. As software complexity continues to escalate, code summarization confronts various challenges, including discrepancies between source code and summarization, the absence of crucial or up-to-date information, and the inefficiency and resource demands of manual summarization. To address these challenges, Automatic Source Code Summarization (ASCS) has garnered widespread attention. This paper presents a comprehensive review and synthesis of ASCS research. It aims to provide an in-depth understanding of the core issues and challenges inherent in each phase of ASCS, illustrated with specific examples and application scenarios. Around of the core phases of ASCS including data collection, source code modeling, the generation of code summaries, and the assessment of their quality, the paper thoroughly compiles and assesses existing datasets, categorizes and examines prevalent source code modeling techniques, and delves into the methods for generating and evaluating the quality of code summaries. Concluding with an exploration of future research avenues and emerging trends, this paper serves as a guide for readers to grasp the cutting-edge developments in this field, enriched by the analysis of pivotal research contributions.",
issn="1573-7616",
doi="10.1007/s10664-024-10553-6",
url="https://doi.org/10.1007/s10664-024-10553-6"
}


@Inbook{Kaiser2024,
author="Kaiser, Abhinav Krishna
and Meda, Vamshi",
title="Software Development and AI Augmentation",
bookTitle="AI Integration in Software Development and Operations: Transformation Through AI Infusion in DevOps, Testing, and SRE",
year="2024",
publisher="Apress",
address="Berkeley, CA",
pages="43--58",
abstract="The ability to develop software at record speed, with useful features and enhanced stability, separates the leaders from the flock. That's why we see the likes of Netflix and Amazon running ahead, with substantial market saturation. To this capability, add artificial intelligence, and imagine the force of power that can dominate the market. If it's done right, the combination is spellbinding and hard to beat.",
isbn="979-8-8688-1044-2",
doi="10.1007/979-8-8688-1044-2_3",
url="https://doi.org/10.1007/979-8-8688-1044-2_3"
}


@Article{Lukits2022,
author="Lukits, Stefan",
title="The Renunciation Paradox: an Analysis of Vulnerability and Intimacy in Nietzsche's Anti-Humanism",
journal="Philosophia",
year="2022",
month="Jul",
day="01",
volume="50",
number="3",
pages="1311--1325",
abstract="Nietzsche's texts contain a puzzle about the role of vulnerability in the creation of intimacy and its function on behalf of human flourishing. I describe the interpretive puzzle and its prima facie paradoxical aspects. On the one hand, there are texts in which Nietzsche expresses a longing for intimacy and other texts where he furnishes details about the possibility of intimacy between equals. On the other hand, Nietzsche is severely critical of certain types of intimacy (especially ones requiring vulnerability and authenticity) and advocates for a pathos of distance in human relationships. I claim that Nietzschean intimacy is not an inherently paradoxical concept. A proper understanding of Nietzsche's anti-humanism provides the resources to resolve the paradox and to use the solution of the puzzle to illuminate Nietzsche's insights about human psychology.",
issn="1574-9274",
doi="10.1007/s11406-021-00431-6",
url="https://doi.org/10.1007/s11406-021-00431-6"
}


@Article{Ojokoh2025,
author="Ojokoh, Bolanle Adefowoke
and Isinkaye, Folasade Olubusola
and Zhang, Ming
and Tom, Joshua Joshua
and Gabriel, Arome Junior
and Afolabi, Olaitan
and Afolabi, Bamidele",
title="Privacy and security in recommenders: an analytical review",
journal="Artificial Intelligence Review",
year="2025",
month="Aug",
day="22",
volume="58",
number="11",
pages="351",
abstract="Recommender systems (RSs) effectively curb information overload by providing personalized suggestions of items to users across different online domains. Their widespread use in e-commerce enhances user engagement, personalizes shopping experiences, and drives sales growth. However, despite the effectiveness of these systems at generating recommendations for users, they still raise major privacy and security concerns as their data could be exploited for malicious purposes, which can lead to data breaches and misuse. Therefore, this paper presents a comprehensive and systematic review of the underlying causes of privacy and security challenges in RS. It also provides a detailed taxonomy categorizing these concerns based on their targets and the risks they create. It further presents potential solutions that have been used in the literature while identifying challenges and possible research directions to pursue in a bid to address privacy and security concerns in RSs. This paper will be a useful resource for current and upcoming researchers in the domain of RSs. It will support knowledge advancement and steer appropriate research directions.",
issn="1573-7462",
doi="10.1007/s10462-025-11333-4",
url="https://doi.org/10.1007/s10462-025-11333-4"
}


@inproceedings{10.1007/978-981-97-3442-9_4,
 abstract = {The article delves into the integration of Artificial Intelligence (AI), Generative Artificial Intelligence (GAI), and Robotic Process Automation (RPA) within Cognitive Information System (CIS), exploring their profound impact on corporate decision-making (DM) processes. By emphasizing the innovative capabilities of AI in broad intelligence emulation, GAI's unparalleled prowess in creative content generation, and RPA's efficiency in streamlining business processes, the paper highlights how these technologies collectively transform the CIS framework. It explores the synergistic relationship between these technologies and their roles in reshaping business and technology landscapes. Our paper delves into the evolution and applications of AI, GAI, and RPA, with a particular focus on GAI's capabilities in content creation and the synergistic relationship between RPA and Intelligent Process Automation (IPA) in automating business processes. The integration of these technologies presents unique opportunities and challenges, particularly in harmonizing the structured world of CIS with the dynamic capabilities of AI technologies. We discuss key topics including the security and ethical implications of this integration, the importance of cognitive resonance in aligning CIS with user cognition, and the potential for significantly enhanced DM efficiency. Our goal is to provide a comprehensive overview of these technologies, their interplay, and their transformative impact on modern business and technology landscapes, serving as a guide to understanding AI's evolving role in shaping the future of business decision-making.},
 address = {Singapore},
 author = {Putnoki, Attila M{\'a}rton
and Orosz, Tam{\'a}s},
 booktitle = {Proceedings of International Conference on Recent Innovations in Computing},
 editor = {Ill{\'e}s, Zolt{\'a}n
and Verma, Chaman
and Gon{\c{c}}alves, Paulo J. Sequeira
and Singh, Pradeep Kumar},
 isbn = {978-981-97-3442-9},
 pages = {39--70},
 publisher = {Springer Nature Singapore},
 title = {Artificial Intelligence and Cognitive Information Systems: Revolutionizing Business with Generative Artificial Intelligence and Robotic Process Automation},
 url = {https://link.springer.com/chapter/10.1007/978-981-97-3442-9_4},
 year = {2024}
}

@Article{Kanauzia2026,
author="Kanauzia, Rohit
and Singh, Mridula
and Gulati, Sagar
and Singh, Brij Mohan
and Arora, Neha
and Gola, Kamal Kumar",
title="A comprehensive survey on federated learning for privacy preservation in digital healthcare applications",
journal="Knowledge and Information Systems",
year="2026",
month="Jan",
day="11",
volume="68",
number="1",
pages="55",
abstract="Internet of Medical Things (IoMT) is a relatively new service that has the possible to revolutionize healthcare by connecting previously analog technologies digitally. Consequently, numerous healthcare applications based on IoMT are utilized in the course of daily life. Despite the abundance of machine learning (ML) techniques aimed at improving healthcare data management, none of them have been able to guarantee the data's complete privacy and security. The precise nature of the clinical data makes ML application difficult and yields unsatisfactory results. A new paradigm in ML called federated learning (FL) has arisen as a means to discover untapped potential in digital healthcare uses that protect patients' and clients' privacy without compromising their data. This survey comprehensively reviews over 105 peer-reviewed publications (2018--2025) sourced from IEEE, Elsevier, Springer, ACM, and MDPI digital libraries. It classifies existing FL approaches for digital healthcare based on architecture, communication efficiency, privacy preservation, and application domain. Survey highlights key findings, comparative analyses with conventional FL-based systems, and lessons learned from prior studies. Finally, open challenges such as scalability, energy efficiency, model heterogeneity, and secure aggregation are deliberated, along with future research directions to enable trustworthy FL-based IoMT healthcare ecosystems.",
issn="0219-3116",
doi="10.1007/s10115-025-02673-2",
url="https://doi.org/10.1007/s10115-025-02673-2"
}


@Inbook{Huang2023,
author="Huang, Ken
and Chen, Xi
and Yang, Youwei
and Ponnapalli, Jyoti
and Huang, Grace",
editor="Huang, Ken
and Wang, Yang
and Zhu, Feng
and Chen, Xi
and Xing, Chunxiao",
title="ChatGPT in Finance and Banking",
bookTitle="Beyond AI: ChatGPT, Web3, and the Business Landscape of Tomorrow",
year="2023",
publisher="Springer Nature Switzerland",
address="Cham",
pages="187--218",
abstract="This chapter highlights the impactful role of ChatGPT in the finance and banking sector. We examine how ChatGPT can streamline operational processes, enhance fraud detection, and personalize financial services. We further discuss its transformative potential in customer service, providing 24/7 support, tailored financial advice, and multilingual assistance. The chapter also explores how ChatGPT can aid in risk assessment, portfolio optimization, and predictive analytics. We further discuss the intersection of ChatGPT and decentralized finance (DeFi), covering automation, security, and financial inclusion. The chapter also addresses security and privacy considerations, outlining strategies to ensure data protection, mitigate adversarial attacks, and enable continuous monitoring. Lastly, we gaze into the future, exploring emerging technologies, ethical considerations, workforce adaptation, and a maturity framework for AI adoption within financial institutions. This comprehensive exploration sets the stage for a new era of AI-driven innovation in the financial sector.",
isbn="978-3-031-45282-6",
doi="10.1007/978-3-031-45282-6_7",
url="https://doi.org/10.1007/978-3-031-45282-6_7"
}


@Article{Vairagade2025,
author="Vairagade, Vikrant S.",
title="Diffusion-based generative AI framework for long-term degradation forecasting and risk assessment of steel fibre reinforced fly ash-based concrete",
journal="Discover Computing",
year="2025",
month="Dec",
day="17",
volume="28",
number="1",
pages="309",
abstract="Reliable lifetime estimations are a prerequisite for durable transport and energy networks such as steel fibre reinforced fly ash (SFRC-FA) concrete, when subjected to combined ambient conditions such as chloride, carbonation, sulfate and thermal cycles. Conventional empirical equations and recurrent networks treat each stressor in isolation while ignoring space geometry and holding on to sparse long-term records, giving high safety margins and expensive over-designs. To close these gaps, the present study introduced a diffusion-based generative AI framework that unified long-horizon forecasting, synthetic data generation and multi-criteria risk aggregations. The Time-Entropy Residual Fusion Transformer (TERFT) couples' entropy-weighted residual blocks with temporal transformers, cutting 60-month forecasting RMSE by 18{\%} compared with long short-term memory / gated recurrent unit (LSTM/GRU) while preserving 85{\%} confidence bounds. Over 10 000 stressor-conditioned degradation trajectories were synthesised using physics informed multi-diffusion generative modelling simulator (MDGS), with{\thinspace}<{\thinspace}7{\%} deviation from 10--15 years field records, enriching scarce datasets. A Graph Integrated Structural Fusion Encoder (GISFE) incorporates topology of beams and slabs, sensor layout, and exposure mapping, improving prediction accuracy by 20{\%} location-wise. Outputs of forecast then find applications in the Multi-Criteria Fracture Risk Aggregator (MCFRA), lumping volume, velocity of degradation with severity of stressors and uncertainty into adaptive risk tensor format with 92{\%} accuracy in terms of failure classification and lead delays of 18--24 months. Latent Feature Attribution via Diffusion Influence Scoring (LFADIS) reverse-traces all predictions ideally latent to environmental and design vectors, lowering loss in attribution by 21{\%} while enhancing transparency. The pipeline would then give accurate lifetime predictions, create rare tail-scenario data, and provide understandable risk scores to optimise cost in design and proactive maintenance scheduling for sustainable concrete infrastructure across climatic zones and service conditions within global portfolios.",
issn="2948-2992",
doi="10.1007/s10791-025-09868-9",
url="https://doi.org/10.1007/s10791-025-09868-9"
}


@Article{Huang2024,
author="Huang, Zixin
and Dutta, Saikat
and Misailovic, Sasa",
title="Debugging convergence problems in probabilistic programs via program representation learning with SixthSense",
journal="International Journal on Software Tools for Technology Transfer",
year="2024",
month="Jun",
day="01",
volume="26",
number="3",
pages="249--268",
abstract="Probabilistic programming aims to open the power of Bayesian reasoning to software developers and scientists, but identification of problems during inference and debugging are left entirely to the developers and typically require significant statistical expertise. A common class of problems when writing probabilistic programs is the lack of convergence of the probabilistic programs to their posterior distributions.",
issn="1433-2787",
doi="10.1007/s10009-024-00737-2",
url="https://doi.org/10.1007/s10009-024-00737-2"
}


@Article{Mazzocco2025,
author="Mazzocco, Yanina Luciana
and Bergero, Gast{\'o}n
and Del Rosso, Sebastian
and Cejas Gallardo, Zo{\'e} Magal{\'i}
and Canalis, Alejandra Mariel
and Baigorri, Ruth Eliana
and Mezzano, Luciana
and Mladin, Juan Javier
and D{\'i}az-Gerevini, Gustavo Tom{\'a}s
and Mart{\'i}nez Benavidez, Claudia
and Cano, Roxana Carolina
and Aoki, Maria Pilar",
title="A novel mouse model for studying complications related to type 2 diabetes using a medium-fat diet, fructose, and streptozotocin",
journal="Scientific Reports",
year="2025",
month="Jul",
day="01",
volume="15",
number="1",
pages="20861",
abstract="The study of type 2 diabetes mellitus (T2DM) pathophysiology relies mainly on the use of animal models, the most common of which involves the consumption of high-fat diets comprising 60{\%} calories from fat. Although these models reproduce the onset and most complications associated with T2DM, they do not accurately mimic human dietary patterns, as they lack the addition of carbohydrates such as fructose in drinking water. The aim of this study was to develop a mouse model for studying complications related to T2DM. To this end, male C57BL/6 mice were fed a medium-fat diet (34.5{\%} kcal from fat), given 20{\%} fructose in drinking water, and injected with a single low dose of streptozotocin (STZ; 100 mg/kg) (D{\thinspace}+{\thinspace}T). At week 20, D{\thinspace}+{\thinspace}T mice exhibited significant weight gain, elevated fasting blood glucose levels, and the development of insulin resistance compared with control mice. Furthermore, the circulating levels of liver enzymes (GPT, GOT, and alkaline phosphatase), total cholesterol, and LDL increased. Multi-organ damage, including reduced pancreatic islet size and number, severe hepatic steatosis, inflammatory infiltration in visceral adipose tissue, and cardiac and renal dysfunction, was also detected. The proposed model replicates T2DM-associated complications in young mice by combining a medium-fat diet with fructose and STZ.",
issn="2045-2322",
doi="10.1038/s41598-025-04335-3",
url="https://doi.org/10.1038/s41598-025-04335-3"
}


@Inbook{Omand2024,
author="Omand, Sir David",
editor="Uusikyl{\"a}, Petri
and Jalonen, Harri
and Jokipii, Annukka",
title="Anticipating Crisis and Enhancing Overall Resilience",
bookTitle="Information Resilience and Comprehensive Security: Challenges and Complexities in Wicked Environments",
year="2024",
publisher="Springer Nature Switzerland",
address="Cham",
pages="21--38",
abstract="The historical record shows the extent to which our societies have suffered over the centuries from catastrophic natural disasters (Ferguson 2021). Examples include periodic plagues, earthquakes and bad harvests. Man-made catastrophes such as the wars and revolutions of the last three centuries have had long-lasting effects. Comparable events must be expected to occur at times in the future too. But we should prepare ourselves for the coming era of more and deeper crises, driven by a combination of the enhanced risks that derive both from climate change and accelerating technological innovation and from the greater vulnerability of globally connected society with its dependence upon digitally controlled infrastructure and corresponding data.",
isbn="978-3-031-66196-9",
doi="10.1007/978-3-031-66196-9_2",
url="https://doi.org/10.1007/978-3-031-66196-9_2"
}


@Article{ref1,
title="Meeting Abstracts from the 5th National Big Data Health Science Conference",
journal="BMC Proceedings",
year="2024",
month="May",
day="16",
volume="18",
number="8",
pages="9",
issn="1753-6561",
doi="10.1186/s12919-024-00292-3",
url="https://doi.org/10.1186/s12919-024-00292-3"
}


@Article{Kasirzadeh2025,
author="Kasirzadeh, Atoosa",
title="Two types of AI existential risk: decisive and accumulative",
journal="Philosophical Studies",
year="2025",
month="Jul",
day="01",
volume="182",
number="7",
pages="1975--2003",
abstract="The conventional discourse on existential risks (x-risks) from AI typically focuses on abrupt, dire events caused by advanced AI systems, particularly those that might achieve or surpass human-level intelligence. These events have severe consequences that either lead to human extinction or irreversibly cripple human civilization to a point beyond recovery. This decisive view, however, often neglects the serious possibility of AI x-risk manifesting gradually through an incremental series of smaller yet interconnected disruptions, crossing critical thresholds over time. This paper contrasts the conventional decisive AI x-risk hypothesis with what I call an accumulative AI x-risk hypothesis. While the former envisions an overt AI takeover pathway, characterized by scenarios like uncontrollable superintelligence, the latter suggests a different pathway to existential catastrophes. This involves a gradual accumulation of AI-induced threats such as severe vulnerabilities and systemic erosion of critical economic and political structures. The accumulative hypothesis suggests a boiling frog scenario where incremental AI risks slowly undermine systemic and societal resilience until a triggering event results in irreversible collapse. Through complex systems analysis, this paper examines the distinct assumptions differentiating these two hypotheses. It is then argued that the accumulative view can reconcile seemingly incompatible perspectives on AI risks. The implications of differentiating between the two types of pathway---the decisive and the accumulative---for the governance of AI as well as long-term AI safety are discussed.",
issn="1573-0883",
doi="10.1007/s11098-025-02301-3",
url="https://doi.org/10.1007/s11098-025-02301-3"
}


@Article{Nassif2025,
author="Nassif, Mathieu
and Robillard, Martin P.",
title="Evaluating interactive documentation for programmers",
journal="Empirical Software Engineering",
year="2025",
month="Feb",
day="26",
volume="30",
number="3",
pages="73",
abstract="Documentation for software programmers is often delivered online. Yet, its format seldom leverages interactive features supported by web technologies. Researchers have proposed techniques to include interactive elements in documents. We investigated one approach, Casdoc, to understand how programmers adapt their navigation behavior to interactive documents. During the study, participants completed programming tasks that involved an unfamiliar library. They used documents that presented information in two formats: one interactive and one traditional. We analyzed recordings of the participants' usage of the documents and their answers to a post-study questionnaire to identify strengths and limitations of the two formats. We found that the interactive format helped find answers rapidly for short factual queries, whereas the traditional format was better suited to open-ended queries that required synthesizing more information. Overall, our results show the potential of leveraging alternative formats, even within the same document, to address a broader spectrum of information needs.",
issn="1573-7616",
doi="10.1007/s10664-025-10618-0",
url="https://doi.org/10.1007/s10664-025-10618-0"
}


@Article{Chen2023,
author="Chen, Hong
and Yuan, Kang
and Huang, Yanjun
and Guo, Lulu
and Wang, Yulei
and Chen, Jie",
title="Feedback is all you need: from ChatGPT to autonomous driving",
journal="Science China Information Sciences",
year="2023",
month="Apr",
day="23",
volume="66",
number="6",
pages="166201",
issn="1869-1919",
doi="10.1007/s11432-023-3740-x",
url="https://doi.org/10.1007/s11432-023-3740-x"
}


@Inbook{Gordon2024,
author="Gordon, Ian
and Thompson, Neil",
title="Radical Technologies",
bookTitle="Data and the Built Environment: A Practical Guide to Building a Better World Using Data",
year="2024",
publisher="Springer Nature Switzerland",
address="Cham",
pages="239--337",
abstract="Over the course of this chapter, we provide an overview of a range of technologies that deliver the application of data to the built environment. We first use complexity theory to frame the technologies described in this chapter in terms of whether they are best suited to tackling complex or merely complicated problem sets. We then discuss key types of technology that one can use to deliver digital change in built environment organisations including Building Information Modelling (BIM), Business Intelligence (BI), Data Science and Artificial Intelligence (AI), Smart Buildings and Internet of Things (IoT), Digital Rehearsals and automated parametric design, Digital Twins, and Generative AI.",
isbn="978-3-031-51008-3",
doi="10.1007/978-3-031-51008-3_6",
url="https://doi.org/10.1007/978-3-031-51008-3_6"
}


@Article{Anderson2025,
author="Anderson, Michael",
title="Partnering with AI to derive and embed principles for ethically guided AI behavior",
journal="AI and Ethics",
year="2025",
month="Jun",
day="01",
volume="5",
number="3",
pages="1893--1910",
abstract="As artificial intelligence (AI) systems, particularly large language models (LLMs), become increasingly embedded in sensitive and impactful domains, ethical failures threaten public trust and the broader acceptance of these technologies. Current approaches to AI ethics rely on reactive measures---such as keyword filters, disclaimers, and content moderation---that address immediate concerns but fail to provide the depth and flexibility required for principled decision-making. This paper introduces AI-aided reflective equilibrium (AIRE), a novel framework for embedding ethical reasoning into AI systems. Building on the philosophical tradition of deriving principles from specific cases, AIRE leverages the capabilities of AI to dynamically generate and analyze such cases and abstract and refine ethical principles from them. Through illustrative scenarios, including a self-driving car dilemma and a vulnerable individual interacting with an AI, we demonstrate how AIRE navigates complex ethical decisions by prioritizing principles like minimizing harm and protecting the vulnerable. We address critiques of scalability, complexity, and the question of ``whose ethics,'' highlighting AIRE's potential to democratize ethical reasoning while maintaining rigor and transparency. Beyond its technical contributions, this paper underscores the transformative potential of AI as a collaborative partner in ethical deliberation, paving the way for trustworthy, principled systems that can adapt to diverse real-world challenges.",
issn="2730-5961",
doi="10.1007/s43681-025-00656-1",
url="https://doi.org/10.1007/s43681-025-00656-1"
}


@Inbook{Ma2026,
author="Ma, Tao
and Wang, Tiantian
and Zhang, Jian",
title="The Reshaping of the US Supply Chain and the Security Level of China's Semiconductor Industry Chain",
bookTitle="China's Semiconductor Industry Strategy: Assessment and Optimization",
year="2026",
publisher="Springer Nature Singapore",
address="Singapore",
pages="117--131",
abstract="Chapter 6 investigates the impact of US supply chain review policies on the security of China's semiconductor industry chain. Taking Executive Order No. 14017 as a policy intervention point, the chapter constructs a continuous difference-in-differences (DID) model using monthly trade data from 2019 to 2024. The empirical analysis indicates that US policy measures have significantly increased the import concentration of China's semiconductor intermediate goods, thereby undermining the resilience of its supply chain. Further results suggest that rationalising industrial structure, enhancing innovation capacity, and increasing fiscal support can partially offset these negative effects. The chapter concludes with policy recommendations centred on diversifying import sources, strengthening regional coordination, reinforcing key segments of the supply chain, advancing indigenous innovation, and optimising industrial structure to enhance national supply chain security and systemic resilience.",
isbn="978-981-95-3332-9",
doi="10.1007/978-981-95-3332-9_6",
url="https://doi.org/10.1007/978-981-95-3332-9_6"
}


@Inbook{Gastaldi2020,
author="Gastaldi, Juan Luis",
editor="Sriraman, Bharath",
title="How to Do Maths with Words: Neural Machine Learning Applications to Mathematics and Their Philosophical Significance",
bookTitle="Handbook of the History and Philosophy of Mathematical Practice",
year="2020",
publisher="Springer International Publishing",
address="Cham",
pages="1--37",
abstract="Recent years have seen a remarkable development of deep neural network techniques for data analysis, along with their increasing application in scientific research across different disciplines. The field of mathematics has not been exempted from this general trend. The present chapter provides a survey of recent applications of neural models to mathematics and assesses their philosophical significance, related to the role of language in mathematics.",
isbn="978-3-030-19071-2",
doi="10.1007/978-3-030-19071-2_142-1",
url="https://doi.org/10.1007/978-3-030-19071-2_142-1"
}


@inproceedings{10.1007/978-3-031-80889-0_11,
 abstract = {A large literature exists that investigates the use of learning models for vulnerabilities detection. Even if it is well-recognized that the presence of code weaknesses (CWEs) in code fragments can make them vulnerable, most of the effort has been spent to propose new models and methods for the early detection of vulnerable code in a dataset of code fragments. This paper reports a preliminary study conducted to adopt Knowledge Graphs (KGs) for detecting the presence of CWEs in the code. The proposed approach has been evaluated on two datasets and has shown interesting results.},
 address = {Cham},
 author = {Vecellio Reane, Martina
and Dall'Anese, Daniele
and Foulefack, Rosma{\"e}l Z. L.
and Marchetto, Alessandro},
 booktitle = {Testing Software and Systems},
 editor = {Men{\'e}ndez, H{\'e}ctor D.
and Bello-Orgaz, Gema
and Barnard, Pepita
and Bautista, John Robert
and Farahi, Arya
and Dash, Santanu
and Han, DongGyun
and Fortz, Sophie
and Rodriguez-Fernandez, Victor},
 isbn = {978-3-031-80889-0},
 pages = {159--166},
 publisher = {Springer Nature Switzerland},
 title = {Towards a Knowledge Graph Based Approach for Vulnerable Code Weaknesses Identification},
 url = {https://link.springer.com/chapter/10.1007/978-3-031-80889-0_11},
 year = {2025}
}

@Inbook{Stinson2025,
author="Stinson, Catherine",
editor="Friedell, David",
title="Raising an AI Teenager",
bookTitle="The Philosophy of Ted Chiang",
year="2025",
publisher="Springer Nature Switzerland",
address="Cham",
pages="187--194",
abstract="The classic science fiction scenario where super intelligent machines try to wipe out humanity is a popular obsession with powerful people in AI. But do we really have reason to fear super intelligent machines? The argument for fearing a singularity moment where AI outsmarts humans and suddenly takes over the planet is broken down here into its illogical components, with help from Chiang's novella, ``The Lifecycle of Software Objects''.",
isbn="978-3-031-81662-8",
doi="10.1007/978-3-031-81662-8_20",
url="https://doi.org/10.1007/978-3-031-81662-8_20"
}


@Article{Bechar2025,
author="Bechar, Amine
and Medjoudj, Rafik
and Elmir, Youssef
and Himeur, Yassine
and Amira, Abbes",
title="Federated and transfer learning for cancer detection based on image analysis",
journal="Neural Computing and Applications",
year="2025",
month="Feb",
day="01",
volume="37",
number="4",
pages="2239--2284",
abstract="This review highlights the efficacy of combining federated learning (FL) and transfer learning (TL) for cancer detection via image analysis. By integrating these techniques, research has shown improvements in diagnostic accuracy and efficiency. Specifically, the use of FL and TL has led to a measurable improvement in the precision of cancer diagnoses, with some studies reporting up to a 20{\%} increase in accuracy compared to traditional methods. This synthesis of FL and TL optimizes distributed data usage while leveraging existing models to expedite learning and application in cancer detection tasks. A concrete assessment of the two methods, including their strengths and weaknesses, is presented. Moving on, their applications in cancer detection are discussed, including potential directions for the future. Finally, this article offers a thorough description of the functions of TL and FL in image-based cancer detection. The authors also make insightful suggestions for additional study in this rapidly developing area. The findings underscore the potential of these combined approaches to significantly advance medical imaging and cancer diagnosis, setting a promising direction for future research.",
issn="1433-3058",
doi="10.1007/s00521-024-10956-y",
url="https://doi.org/10.1007/s00521-024-10956-y"
}


@inproceedings{10.1007/978-3-031-81567-6_9,
 abstract = {As it is known, artificial intelligence has become a part of our daily lives for a long time. It is impossible to imagine our activities today without smartphones or our household without technical equipment and devices that make it easier to pay bills, etc. Many countries of the world have adopted the development strategy of AI and set goals that will give them leadership and competitive advantage in this field. Today, AI has become a topic of discussion in both scientific circles and society. People regularly face several difficulties and limitations. Today, one of the areas where AI is regularly applied is the social area. Interlingual communication lays the groundwork for increasing the effectiveness of communication and translation in the transition of communication between people and between cultures to a higher level.},
 address = {Cham},
 author = {Afandiyeva, Gunay
and Zeynall{\i}, Arzu
and Nabiyeva, Aytac},
 booktitle = {International Conference on Smart Environment and Green Technologies -- ICSEGT2024},
 editor = {Mammadov, Fahreddin Sadikoglu
and Aliev, Rafik A.
and Kacprzyk, Janusz
and Pedrycz, Witold},
 isbn = {978-3-031-81567-6},
 pages = {67--76},
 publisher = {Springer Nature Switzerland},
 title = {Artificial Intelligence in Communication and Social Relationships},
 url = {https://link.springer.com/chapter/10.1007/978-3-031-81567-6_9},
 year = {2024}
}

@inproceedings{10.1007/978-981-97-2716-2_23,
 abstract = {Recent years have seen tremendous progress in Natural Language Processing (NLP), particularly in word and document illustration. Conventional methods that relied on contextual embeddings have given way to creative alternatives. In NLP, pre-trained language models have shown remarkable effectiveness, resulting in a fundamental shift away from supervised learning and towards pre-training accompanied by fine-tuning. Enhancing pre-trained models has attracted a lot of research attention in the NLP community. The taxonomy of pre-trained models is introduced, along with a thorough assessment of current advancements and representative work in the field of natural language processing. We initially provide a quick overview of pre-trained models, then go on to distinctive architectures and techniques. Next, we present and examine the implications and difficulties associated with pre-trained models and their subsequent uses. We wrap up quickly and discuss potential future research options in this area. These developments provide more accurate and effective ways to organise and extract data from large textual collections. In conclusion, this study addresses the intrinsic drawbacks of contextual embeddings and emphasises the critical role that word and document illustrations play in advancing NLP. These cutting-edge methods offer a viable avenue to improve natural language comprehension and utilisation as NLP continues to develop, with extensive applications in domains including information retrieval, machine translation, and sentiment analysis.},
 address = {Singapore},
 author = {Nagender, Y.
and Ravichand, M.
and Kocherlakota, Nirupama
and Mary, S. Shyni Carmel
and Bharathi, S. Kavitha
and Kalaivaani, P. C. D.},
 booktitle = {Proceedings of Third International Conference in Mechanical and Energy Technology},
 editor = {Yadav, Sanjay
and Arora, P. K.
and Sharma, Anuj Kumar
and Kumar, Harish},
 isbn = {978-981-97-2716-2},
 pages = {253--263},
 publisher = {Springer Nature Singapore},
 title = {Advancing Beyond Contextual Embeddings: Innovations in Word and Document Representations for Natural Language Processing},
 url = {https://link.springer.com/chapter/10.1007/978-981-97-2716-2_23},
 year = {2024}
}

@Article{Arasteh2025,
author="Arasteh, Bahman
and Bulut, Faruk
and Ince, Ibrahim Furkan
and Sefati, Seyed Salar
and Kusetogullari, Huseyin
and Kiani, Farzad",
title="A Metaheuristic and Neural Network-Based Framework for Automated Software Test Oracles Under Limited Test Data Conditions",
journal="Journal of Electronic Testing",
year="2025",
month="Dec",
day="01",
volume="41",
number="5",
pages="651--671",
abstract="With the growing complexity of modern software systems, the demand for effective and efficient testing techniques has become an important aspect of the software development process. Software Test Oracles (STOs) play a vital role in testing by determining whether a program behaves as expected for a given input. This study introduces a novel automated STO framework that utilizes metaheuristic algorithms and ML techniques to enhance testing precision and reduce the testing cost. The proposed approach begins with generating coverage-based test data using a hybrid of the Imperialist Competitive Algorithm (ICA) and Genetic Algorithm (GA). The initial test data is optimized using Hamming distance to address redundant test data and improve efficiency. This reduced dataset is used to train a multi-layer perceptron and to create an STO that accurately predicts the software under test's expected output. The oracle was validated using both original and mutant versions of standard benchmark programs. Additionally, an automated platform has been developed to support Oracle creation, test case generation, and validation. Experimental results demonstrate that the proposed STO attains high accuracy (96.70{\%}) and recall (98.63{\%}), highlighting its effectiveness when a limited quantity of test data is available.",
issn="1573-0727",
doi="10.1007/s10836-025-06210-5",
url="https://doi.org/10.1007/s10836-025-06210-5"
}


@Inbook{Vrana2025,
author="Vrana, Johannes",
editor="Meyendorf, Norbert
and Ida, Nathan
and Singh, Ripi
and Vrana, Johannes",
title="Cyber Security and Data Ownership",
bookTitle="Handbook of Nondestructive Evaluation 4.0",
year="2025",
publisher="Springer Nature Switzerland",
address="Cham",
pages="1--38",
abstract="The cyber world of information technologies, digitization, digital transformation, Industry 4.0, and NDE 4.0 presents similar complexities to the physical world, including threats such as: loss of information, human error, ethical lapses, malicious intent, and political interests. This chapter explores the critical area of cybersecurity. It examines the fundamental concepts, challenges, and threats that organizations face, providing a comprehensive overview of the cybersecurity landscape. Key areas of focus include identifying vulnerabilities, implementing robust security measures, and developing effective strategies to mitigate risk and protect critical assets. The chapter concludes with an insight into the current legal situation regarding data ownership, an examination of the current-day challenges of data access, and how the EU Data Act will change the situation.",
isbn="978-3-030-48200-8",
doi="10.1007/978-3-030-48200-8_79-1",
url="https://doi.org/10.1007/978-3-030-48200-8_79-1"
}


@Article{Küsters2025,
author="K{\"u}sters, Anselm
and W{\"o}rsd{\"o}rfer, Manuel",
title="Exploring Laws of Robotics: A Synthesis of Constitutional AI and Constitutional Economics",
journal="Digital Society",
year="2025",
month="Jun",
day="04",
volume="4",
number="2",
pages="46",
abstract="As today's world increasingly harnesses ever more powerful AI systems, policymakers and developers must recognize the need for effective regulatory frameworks to ensure that the underlying LLMs are used ethically and responsibly. Integrating ordoliberal constitutional economics with AI ethics helps to create such frameworks through system prompts, reinforcement learning, and non-fine-tunable learning. Constitutional AI aims to embed ethical principles and robust safeguards into AI systems to ensure they operate within pre-defined boundaries, prioritizing safety, legality, and human rights. By embedding ethical considerations and compliance requirements directly into the operational core of AI systems, a focus on regulation and transparency of system instructions can proactively shape AI outcomes. Such a system prompt approach is consistent with ordoliberal ideals and offers a preventive strategy to ensure that AI technologies operate responsibly from the outset, as we demonstrate with an empirical experiment involving Llama 2. However, it requires democratic legitimacy via so-called ``mini-publics'' and ongoing research to ensure that AI systems adhere closely to these imperatives.",
issn="2731-4669",
doi="10.1007/s44206-025-00204-8",
url="https://doi.org/10.1007/s44206-025-00204-8"
}


@Article{Jia2026,
author="Jia, Yan
and Song, Yuxin
and Liu, Zihou
and Tan, Qingyin
and Song, Yang
and Zhang, Yu
and Liu, Zheli",
title="Analyzing consumer IoT traffic from security and privacy perspectives: a comprehensive survey",
journal="Frontiers of Computer Science",
year="2026",
month="Jan",
day="31",
volume="20",
number="7",
pages="2007809",
abstract="The Consumer Internet of Things (CIoT), a notable segment within the IoT domain, involves the integration of IoT technology into consumer electronics and devices, such as smart homes and smart wearables. Compared to traditional IoT fields, CIoT differs notably in target users, product types, and design approaches. While offering convenience to users, it also raises new security and privacy concerns. Network traffic analysis, a widely used technique in the security community, has been extensively applied to investigate these concerns about CIoT. Compared to traditional network traffic analysis in fields like mobile apps and websites, CIoT introduces unique characteristics that pose new challenges and research opportunities. Researchers have made significant contributions in this area. To aid researchers in understanding the application of traffic analysis tools for assessing CIoT security and privacy risks, this survey reviews 310 publications on traffic analysis within the CIoT security and privacy domain from January 2018 to June 2024, focusing on three research questions. Our work: 1) outlines the CIoT traffic analysis process and highlights its differences from general network traffic analysis; 2) summarizes and classifies existing research into four categories according to its application objectives, device fingerprinting, user activity inference, malicious traffic detection, and measurement; 3) explores emerging challenges and potential future research directions based on each step of the CIoT traffic analysis process. This will provide new insights to the community and guide the industry towards safer product designs.",
issn="2095-2236",
doi="10.1007/s11704-025-41155-0",
url="https://doi.org/10.1007/s11704-025-41155-0"
}


@inproceedings{10.1007/978-3-031-62273-1_18,
 abstract = {This research explains the perceptions of university students regarding the use of ChatGPT in their learning and research. The research is based on evidence provided in articles that were considered in a systematic literature review using the PRISMA model. The motivation for this study was informed by a realization that there are limited studies that provide a comprehensive understanding of students' perceptions regarding the use of ChatGPT in learning and research. Many existing studies either focus on educators' perceptions or isolated cases of empirical studies. It is therefore not clear what factors influence students' adoption of ChatGPT. This gap was addressed by this study by conducting a systematic literature review to identify factors influencing the adoption of ChatGPT, the positive impact of ChatGPT on students' education and the concerns raised by students regarding the integration of ChatGPT into academic practice. The research focus was on articles that discuss the students' perceptions of ChatGPT, all of which were drawn from Google Scholar. In total 32 articles were considered for analysis for this study and in this paper, we present five constructs that influence the adoption of ChatGPT in academic activities. Even though the students' perceptions of ChatGPT are widely varied, the findings of this research show that students concur that ChatGPT has a positive influence on their learning. The students further highlighted concerns relating to their use of ChatGPT in learning, which could be improved if the tool's educational effectiveness is to be realized. The contribution of this study is three-fold. First, the findings of this research explain the perceptions of students about using ChatGPT in academic practice. Second, the results of this study inform the university management and policymakers about how to effectively integrate ChatGPT into the education system. Third, the study suggests, recommends, and guides educators who wish to incorporate this new, powerful tool (ChatGPT) into their teaching.},
 address = {Cham},
 author = {Dube, Sibusisiwe
and Dube, Sinokubekezela
and Ndlovu, Belinda Mutunhu
and Maguraushe, Kudakwashe
and Malungana, Lario
and Kiwa, Fungai Jacqueline
and Muduva, Martin},
 booktitle = {Intelligent Computing},
 editor = {Arai, Kohei},
 isbn = {978-3-031-62273-1},
 pages = {258--279},
 publisher = {Springer Nature Switzerland},
 title = {Students' Perceptions of ChatGPT in Education: A Rapid Systematic Literature Review},
 url = {https://link.springer.com/chapter/10.1007/978-3-031-62273-1_18},
 year = {2024}
}

@Article{Compère2025,
author="Comp{\`e}re, Justin
and Bouraga, Sarah",
title="Decentralized applications: a requirements engineering perspective",
journal="Requirements Engineering",
year="2025",
month="Dec",
day="06",
volume="31",
number="1",
pages="2",
abstract="Decentralized applications (DApps) are applications running on a blockchain and enable what has been coined Blockchain 3.0. They represent an evolution in digital technology, offering transparent, secure, and unmediated solutions. However, despite their considerable potential, the adoption of DApps remains limited. With this article, we aim to identify the main obstacles to DApps adoption, and propose recommendations for DApps developers, including recommendations about improving the User Experience (UX) and facilitating onboarding within these applications. Through qualitative interviews with experts, followed by the analysis of 25 DApps, we elicit and analyze the necessary requirements to address these issues. This paper has both practical and theoretical implications. Specifically, the resulting requirements will guide developers and entrepreneurs when designing a DApp by highlighting the various functional and non-functional requirements to take into account to increase the likelihood of user adoption. Furthermore, the findings contribute to the body of knowledge pertaining to the software engineering of DApps, a field that has been recognized as important by the research community. Notably, we discuss requirements engineering- and UX-related challenges, as well as best practices to implement for better DApp adoption.",
issn="1432-010X",
doi="10.1007/s00766-025-00456-3",
url="https://doi.org/10.1007/s00766-025-00456-3"
}


@Article{Recupito2025,
author="Recupito, Gilberto
and Giordano, Giammaria
and Ferrucci, Filomena
and Di Nucci, Dario
and Palomba, Fabio",
title="When code smells meet ML: on the lifecycle of ML-specific code smells in ML-enabled systems",
journal="Empirical Software Engineering",
year="2025",
month="Jul",
day="12",
volume="30",
number="5",
pages="139",
abstract="The adoption of Machine Learning (ML)-enabled systems is growing rapidly, introducing novel challenges in maintaining quality and managing technical debt in these complex systems. Among the key quality threats are ML-specific code smells (ML-CSs), suboptimal implementation practices in ML pipelines that can compromise system performance, reliability, and maintainability. Although these smells have been defined in the literature, detailed insights into their characteristics, evolution, and mitigation strategies are still needed to help developers address these quality issues effectively. In this paper, we investigate the emergence and evolution of ML-CSs through a large-scale empirical study focusing on (i) their prevalence in real ML-enabled systems, (ii) how they are introduced and removed, and (iii) their survivability. We analyze over 400,000 commits from 337 ML-enabled projects, leveraging CodeSmile, a novel ML smell detector that we developed to enable our investigation and identify ML-specific code smells. Our results reveal that: (1) CodeSmile can detect ML-CSs with precision and recall rates of 87.4{\%} and 78.6{\%}, respectively; (2) ML-CSs are frequently introduced during file modifications in new feature tasks; (3) smells are typically removed during tasks related to new features, enhancements, or refactoring; and (4) the majority of ML-CSs are resolved within the first 10{\%} of commits. Based on these findings, we provide actionable conclusions and insights to guide future research and quality assurance practices for ML-enabled systems.",
issn="1573-7616",
doi="10.1007/s10664-025-10676-4",
url="https://doi.org/10.1007/s10664-025-10676-4"
}


@inproceedings{10.1007/978-3-032-08847-5_19,
 abstract = {Despite the significant potential of elasticity in mechanical design, a unified methodological framework for selecting and implementing compliant mechanisms and other elastic structures remains elusive for practitioners. This work addresses this gap by compiling a comprehensive library of structures and mechanisms, integrating compliant mechanisms, origami-inspired designs, metamaterial-based solutions, and advancements in 4D and multi-material printing. This collection offers a broad perspective on the diverse possibilities of elastic elements. Recognizing the need for a systematic approach, we propose a method for navigating this database, grounded in the application of 40 TRIZ inventive principles. This framework aims to assist designers in both understanding the efficacy of elastic mechanical parts and identifying suitable concepts for functional modification. Future work will explore the integration of Large Language Models (LLMs) for enhanced database querying and exploration.},
 address = {Cham},
 author = {Cattaneo, Matteo
and Spreafico, Christian
and Russo, Davide},
 booktitle = {World Conference of AI-Powered Innovation and TRIZ Methodology},
 editor = {Cavallucci, Denis
and Brad, Stelian
and Livotov, Pavel
and Houssin, R{\'e}my},
 isbn = {978-3-032-08847-5},
 pages = {261--270},
 publisher = {Springer Nature Switzerland},
 title = {Classification of Geometric Effects Based on the 40 Inventive Principles},
 url = {https://link.springer.com/chapter/10.1007/978-3-032-08847-5_19},
 year = {2026}
}

@Article{Pan2025,
author="Pan, Xiao
and Yang, Tony T. Y.
and Li, Jun
and Ventura, Carlos
and M{\'a}laga-Chuquitaype, Christian
and Li, Chaobin
and Su, Ray Kai Leung
and Brzev, Svetlana",
title="A review of recent advances in data-driven computer vision methods for structural damage evaluation: algorithms, applications, challenges, and future opportunities",
journal="Archives of Computational Methods in Engineering",
year="2025",
month="Oct",
day="01",
volume="32",
number="7",
pages="4587--4619",
abstract="Computer vision techniques have gained great traction in civil infrastructure inspection and monitoring. This paper conducted a systematic review of recent data-driven computer vision algorithms in structural damage detection published during the past 5 years. The theories of prevalent computer vision models are first reviewed with an emphasis on the progressive innovation in algorithms' architecture. Then, recent applications of computer vision models for structural damage evaluation are discussed, which are classified into different structural categories by their material types (i.e., concrete, steel, masonry, timber) at three hierarchical levels including damage recognition, localization, and quantification. In particular, the paper also highlights the current state of using computer vision for damage assessment of timber structures, which remains under-explored compared to concrete and steel structures. Next, the paper scrutinizes existing structural damage inspection guidelines to identify key technological gaps between the capability of existing computer vision methods and manual inspection practices in the field. Finally, the paper summarizes existing challenges and recommends future research opportunities including the integration of computer vision methods with multimodal large language models, sensor-fusion, and mobile inspection approaches.",
issn="1886-1784",
doi="10.1007/s11831-025-10279-8",
url="https://doi.org/10.1007/s11831-025-10279-8"
}


@Inbook{Stencel2025,
author="Stencel, Grzegorz
and Berton, Luca",
title="Emerging and Advanced Kubernetes Concepts",
bookTitle="Kubernetes Recipes: A Practical Guide for Container Orchestration and Deployment",
year="2025",
publisher="Apress",
address="Berkeley, CA",
pages="551--601",
abstract="In this chapter, we explore emerging and advanced concepts in Kubernetes, focusing on technologies and approaches that push the boundaries of container orchestration and cloud-native computing. Kubernetes has evolved far beyond its origins as a container scheduler to match the complexity of modern workloads. Organizations need tools to handle specialized workloads: AI/ML, High-Performance Computing (HPC), and WebAssembly (WASM). The following cutting-edge tools ensure networking, security, and observability scale accordingly: functions as a service (FaaS) for serverless deployments, extended Berkeley Packet Filter (eBPF) with Cilium for advanced networking and security, and WebAssembly on Kubernetes, which introduces lightweight, high-performance compute environments. Additionally, we manage AI/ML workloads through Kubernetes, touching on MLOps and AIOps practices, along with techniques for extending Kubernetes with custom resource definitions (CRDs) and API extensions. This chapter also covers integrations with popular platforms like Jenkins and VMware Tanzu, as well as strategies for managing Kubernetes in air-gapped environments, which present unique challenges for DevOps teams. This chapter equips readers with a deep understanding of these advanced topics, enabling them to leverage Kubernetes for specialized use cases while maintaining operational control and scalability.",
isbn="979-8-8688-1325-2",
doi="10.1007/979-8-8688-1325-2_12",
url="https://doi.org/10.1007/979-8-8688-1325-2_12"
}


@Article{Li2025,
author="Li, David Chun Yin",
title="The synergistic potential of AI and blockchain for organizations",
journal="AI {\&} SOCIETY",
year="2025",
month="Jan",
day="01",
volume="40",
number="1",
pages="221--222",
issn="1435-5655",
doi="10.1007/s00146-023-01838-3",
url="https://doi.org/10.1007/s00146-023-01838-3"
}


@Inbook{Sarker2024,
author="Sarker, Iqbal H.",
title="Introduction to AI-Driven Cybersecurity and Threat Intelligence",
bookTitle="AI-Driven Cybersecurity and Threat Intelligence: Cyber Automation, Intelligent Decision-Making and Explainability",
year="2024",
publisher="Springer Nature Switzerland",
address="Cham",
pages="3--19",
abstract="With the convergence of artificial intelligence (AI) and cybersecurity, a new paradigm has emerged in how we defend against evolving digital threats. This book explores the dynamic landscape of AI-driven cybersecurity and threat intelligence, emphasizing how the computing and analytical power and decision-making capabilities of AI technologies are revolutionizing the detection, prevention, and response to cyberattacks. AI and machine learning algorithms can analyze vast datasets quickly, identify patterns, and predict potential threats, enabling organizations to strengthen their digital infrastructure proactively. In this book, we have bestowed a comprehensive study on this topic that explores not only the potentiality of cyber threat intelligence but also how different AI methods such as machine learning modeling, deep learning modeling, data science process, generative AI modeling, natural language processing with large language modeling, etc. can be employed to provide intelligent cybersecurity services. We have also discussed various essential real-world application areas such as Internet of Things and smart cities, industrial control systems and operational technology environments, critical infrastructures, cyber-physical systems, digital twins, and relevant others where AI-driven cybersecurity and threat intelligence could be useful for effective and automated solutions. Throughout this book, we have also highlighted relevant research issues and challenges as well as their potential solution directions within the context of AI-based cybersecurity and threat intelligence.",
isbn="978-3-031-54497-2",
doi="10.1007/978-3-031-54497-2_1",
url="https://doi.org/10.1007/978-3-031-54497-2_1"
}


@Article{Ricci2023,
author="Ricci, Luisa
and Stanley, Federico Uchenna
and Eberhart, Tanja
and Mainini, Francesco
and Sumpton, David
and Cardaci, Simone",
title="Pyruvate transamination and NAD biosynthesis enable proliferation of succinate dehydrogenase-deficient cells by supporting aerobic glycolysis",
journal="Cell Death {\&} Disease",
year="2023",
month="Jul",
day="06",
volume="14",
number="7",
pages="403",
abstract="Succinate dehydrogenase (SDH) is the mitochondrial enzyme converting succinate to fumarate in the tricarboxylic acid (TCA) cycle. SDH acts as a tumor suppressor with germline loss-of-function mutations in its encoding genes predisposing to aggressive familial neuroendocrine and renal cancer syndromes. Lack of SDH activity disrupts the TCA cycle, imposes Warburg-like bioenergetic features, and commits cells to rely on pyruvate carboxylation for anabolic needs. However, the spectrum of metabolic adaptations enabling SDH-deficient tumors to cope with a dysfunctional TCA cycle remains largely unresolved. By using previously characterized Sdhb-deleted kidney mouse cells, here we found that SDH deficiency commits cells to rely on mitochondrial glutamate-pyruvate transaminase (GPT2) activity for proliferation. We showed that GPT2-dependent alanine biosynthesis is crucial to sustain reductive carboxylation of glutamine, thereby circumventing the TCA cycle truncation determined by SDH loss. By driving the reductive TCA cycle anaplerosis, GPT2 activity fuels a metabolic circuit maintaining a favorable intracellular NAD+ pool to enable glycolysis, thus meeting the energetic demands of SDH-deficient cells. As a metabolic syllogism, SDH deficiency confers sensitivity to NAD+ depletion achieved by pharmacological inhibition of nicotinamide phosphoribosyltransferase (NAMPT), the rate-limiting enzyme of the NAD+ salvage pathway. Beyond identifying an epistatic functional relationship between two metabolic genes in the control of SDH-deficient cell fitness, this study disclosed a metabolic strategy to increase the sensitivity of tumors to interventions limiting NAD availability.",
issn="2041-4889",
doi="10.1038/s41419-023-05927-5",
url="https://doi.org/10.1038/s41419-023-05927-5"
}


@Inbook{Ahmad2025,
author="Ahmad, Asif
and Atif, Shiza
and Iman, Zain Ul",
editor="Zafar, Muhammad
and Ahmad, Mushtaq
and Keke{\c{c}}o{\u{g}}lu, Meral
and Makhkamov, Trobjon
and Santana de Oliveira, Mozaniel
and Majeed, Salman",
title="Innovations in Honey Quality Control",
bookTitle="Pure Honey: Assurance {\&} Authentication",
year="2025",
publisher="Springer Nature Switzerland",
address="Cham",
pages="235--260",
abstract="Honey stands out as a valued food product that people eat often. It is known for its health benefits, unique flavors, and healing powers that influence consumer acceptance. Owing to high consumer acceptance, there is a demand for quality evaluation of natural honey to safeguard the consumer and ensure natural honey without adulteration. In this chapter, we will look at the newest steps forward in keeping honey quality high. We will also see how these steps can help keep honey top-notch for selling and trading. In the past, people checked honey quality based on simple physiochemical analysis. This included pH, moisture content, how well it conducted electricity, how acidic it was, and how much 5-hydroxymethylfurfural it had. But these old ways of checking quality had some weak spots. These processes are lengthy and time-consuming and are less precise to combat adulteration. To fix these problems, researchers and scientists came up with new ideas to measure quality and check honey. Some of these new ideas include looking at pollen, testing how it tastes and looks in shape, and better ways to spot fake honey. One technique is to quantify the pollen grains in honey after qualitative identification to identify the source of specific honey. Another new approach is to use the latest trends of organoleptic analysis that make use of sense-based testing to check how honey looks, smells, tastes, and feels. Recent analysis techniques, including spectrophotometry, ELISA, chromatography, and mass spectrometry, are used for the detection and quantification of honey components. These techniques are valuable to determine flavonoids, phenolics, carbohydrates, bioactives, amino acids, and aromatic substances. The tools for artificial intelligence and machine learning algorithms further provide support in maintaining quality for honey. These algorithms can sort honey based on its authenticity, where it comes from, and its botanical source. This gives us a more accurate and effective way to manage quality. In general, improvements in honey quality control have led to more precise and efficient ways to check if honey is real and has high quality. These improvements help spot adulterated honey and also give us useful information about honey sources. The use of modern analytical techniques provides precise scientific data about the nutritional and physiochemical profile of honey. These steps forward in honey quality control help trade and selling because they make sure consumers get real top-notch honey.",
isbn="978-3-031-98913-1",
doi="10.1007/978-3-031-98913-1_9",
url="https://doi.org/10.1007/978-3-031-98913-1_9"
}


@Article{Tomova2023,
author="Tomova, Mihaela
and Hofmann, Martin
and H{\"u}tterer, Constantin
and M{\"a}der, Patrick",
title="Assessing the utility of text-to-SQL approaches for satisfying software developer information needs",
journal="Empirical Software Engineering",
year="2023",
month="Dec",
day="05",
volume="29",
number="1",
pages="15",
abstract="Software analytics integrated with complex databases can deliver project intelligence into the hands of software engineering (SE) experts for satisfying their information needs. A new and promising machine learning technique known as text-to-SQL automatically extracts information for users of complex databases without the need to fully understand the database structure nor the accompanying query language. Users pose their request as so-called natural language utterance, i.e., question. Our goal was evaluating the performance and applicability of text-to-SQL approaches on data derived from tools typically used in the workflow of software engineers for satisfying their information needs. We carefully selected and discussed five seminal as well as state-of-the-art text-to-SQL approaches and conducted a comparative assessment using the large-scale, cross-domain Spider dataset and the SE domain-specific SEOSS-Queries dataset. Furthermore, we study via a survey how SE professionals perform in satisfying their information needs and how they perceive text-to-SQL approaches. For the best performing approach, we observe a high accuracy of 94{\%} in query prediction when training specifically on SE data. This accuracy is almost independent of the query's complexity. At the same time, we observe that SE professionals have substantial deficits in satisfying their information needs directly via SQL queries. Furthermore, SE professionals are open for utilizing text-to-SQL approaches in their daily work, considering them less time-consuming and helpful. We conclude that state-of-the-art text-to-SQL approaches are applicable in SE practice for day-to-day information needs.",
issn="1573-7616",
doi="10.1007/s10664-023-10374-z",
url="https://doi.org/10.1007/s10664-023-10374-z"
}


@Inbook{Both2023,
author="Both, David",
title="Filesystems",
bookTitle="Using and Administering Linux: Volume 1: Zero to SysAdmin: Getting Started",
year="2023",
publisher="Apress",
address="Berkeley, CA",
pages="571--626",
abstract="In this chapter you will learn",
isbn="978-1-4842-9618-9",
doi="10.1007/978-1-4842-9618-9_19",
url="https://doi.org/10.1007/978-1-4842-9618-9_19"
}


@Inbook{Vrana2025,
author="Vrana, Johannes",
editor="Meyendorf, Norbert
and Ida, Nathan
and Singh, Ripudaman (Ripi)
and Vrana, Johannes",
title="Cyber Security and Data Ownership",
bookTitle="Handbook of Nondestructive Evaluation 4.0",
year="2025",
publisher="Springer Nature Switzerland",
address="Cham",
pages="479--516",
abstract="The cyber world of information technologies, digitization, digital transformation, Industry 4.0, and NDE 4.0 presents similar complexities to the physical world, including threats such as: loss of information, human error, ethical lapses, malicious intent, and political interests. This chapter explores the critical area of cybersecurity. It examines the fundamental concepts, challenges, and threats that organizations face, providing a comprehensive overview of the cybersecurity landscape. Key areas of focus include identifying vulnerabilities, implementing robust security measures, and developing effective strategies to mitigate risk and protect critical assets. The chapter concludes with an insight into the current legal situation regarding data ownership, an examination of the current-day challenges of data access, and how the EU Data Act will change the situation.",
isbn="978-3-031-84477-5",
doi="10.1007/978-3-031-84477-5_79",
url="https://doi.org/10.1007/978-3-031-84477-5_79"
}


@Inbook{Han2024,
author="Han, Jie
and Qiu, Wei
and Lichtfouse, Eric",
title="ChatGPT in Scientific Research and Writing: A Beginner's Guide",
bookTitle="ChatGPT in Scientific Research and Writing: A Beginner's Guide",
year="2024",
publisher="Springer Nature Switzerland",
address="Cham",
pages="1--109",
abstract="The developers of ChatGPT have predicted that, within the next ten years, artificial intelligence (AI) systems will exceed expert skill levels in most domains, and carry out as much productive work as one of today's largest corporations. Since the public release of ChatGPT, there has been surging interest in exploring the use of large language models, including ChatGPT, in scientific research, publication, and science communication in general. In this book, we will explore the models' capabilities, including GPT-4, GPT-3.5, and GPT-enabled new Bing (now Copilot), for carrying out the tasks through different stages of scientific research from research conceptualization, study design, to publication and science communication. We used these models for abstracting key points and extracting specific information from research publications, interpreting figures, evaluating research papers, spotting errors, responding to reviewer's comments, language editing, designing experiments, creating survey questionnaires, brainstorming, writing research proposals, and creating visuals. Major limitations of these models include hallucinations, randomness in answers when prompted by identical questions, and the lack of support for big data scrapping, processing, and visualization.",
isbn="978-3-031-66940-8",
doi="10.1007/978-3-031-66940-8_1",
url="https://doi.org/10.1007/978-3-031-66940-8_1"
}


@Inbook{Böckle2025,
author="B{\"o}ckle, Martin
and Mohme, Ulrich
and Bick, Markus",
editor="Xu, Wei",
title="Design Thinking and AI",
bookTitle="Handbook of Human-Centered Artificial Intelligence",
year="2025",
publisher="Springer Nature Singapore",
address="Singapore",
pages="1--61",
abstract="The integration of Design Thinking into the research stream of Human-Centered AI (HCAI) facilitates AI systems to align with human needs, ethical standards, and organizational goals in order to augment human capabilities. By applying the five phases Empathize, Define, Ideate, Prototype, and Test---AI development becomes more user-focused, iterative, and impactful. This structured approach enables AI solutions to be technically robust while remaining adaptable to evolving regulatory, social, and business environments. A novel framework is introduced, where the AI lifecycle interacts with the Design Thinking process to drive ethical, transparent, and strategic AI adoption. This framework emphasizes the importance of data governance, transparency, explainability, and bias mitigation, ensuring AI systems are fair and trustworthy. At the same time, it aligns AI development with business objectives, workforce capabilities, and operational needs, fostering collaboration and responsible innovation. The framework is illustrated through a series of case studies that demonstrate how Design Thinking can facilitate human-centered AI development across different application domains.",
isbn="978-981-97-8440-0",
doi="10.1007/978-981-97-8440-0_73-1",
url="https://doi.org/10.1007/978-981-97-8440-0_73-1"
}


@Inbook{Wienholt2025,
author="Wienholt, Nick",
title="Copilot and Data Science",
bookTitle="GitHub Copilot and AI Coding Tools in Practice: Accelerate AI Adoption from Individual Developers to Enterprise",
year="2025",
publisher="Apress",
address="Berkeley, CA",
pages="201--249",
abstract="Undoubtedly the scariest area of the use of machine learning (ML) and artificial intelligence (AI) is the use of them to make better ML and AI. If anything produced a sentient self-perpetuating AI menace akin to SkyNet, it would likely come from AI producing new AI. At the moment the real threat is to the survival of many job categories, and this has been the case since the Industrial Revolution where jobs like cobblers and horse carriage makers were largely obliterated, often in the span of a decade. ChatGPT and other LLMs have massively reduced the need for professionals like graphic design artists -- the current LLMs are easily capable of producing and refining graphics like logos using the same set of specifications that would traditionally be given to humans.",
isbn="979-8-8688-1784-7",
doi="10.1007/979-8-8688-1784-7_10",
url="https://doi.org/10.1007/979-8-8688-1784-7_10"
}


@Inbook{Hazzan2026,
author="Hazzan, Orit
and Ragonis, Noa
and Lapidot, Tami",
title="Problem-Solving Strategies in Computer Science",
bookTitle="Guide to Teaching Computer Science: An Activity-Based Approach",
year="2026",
publisher="Springer Nature Switzerland",
address="Cham",
pages="227--266",
abstract="Problem-solving is one of the central activities performed by computer scientists and computer science learners. However, problem solving is neither a uniform nor a linear process that can be taught as an algorithm to follow; that is, it is a soft concept (Chap. 6). Since computer science learners face a variety of challenges, computer science educators must equip learners with relevant problem-solving strategies, and be aware of learners' difficulties, guiding and scaffolding them in developing their problem-solving skills. This chapter presents several problem-solving strategies, along with the specific stages and practices involved in the problem-solving process. As collaborative problem solving is a crucial activity in our era, we also address this skill in the context of the MTCS course. While the full extent of GenAI's impact on computer science education remains uncertain, its deep integration into both learning and problem solving makes it a relevant focus. Accordingly, this chapter addresses GenAI across a range of contexts.",
isbn="978-3-032-00953-1",
doi="10.1007/978-3-032-00953-1_10",
url="https://doi.org/10.1007/978-3-032-00953-1_10"
}


@Article{Cassata2025,
author="Cassata, Francesco",
title="Wooden reels for adults: a psychosocial analysis of griefbots",
journal="Psychoanalysis, Culture {\&} Society",
year="2025",
month="Sep",
day="17",
abstract="The current vertiginous rise of artificial intelligence has opened up new opportunities for digital immortality. In this paper, I critically examine griefbots---AI-driven chatbots built to chat again with deceased loved ones. Drawing on Freud, I argue for interpreting griefbots as ``melancholic'' media. The framework of my analysis is psychosocial and aims to unravel the complex---technological and psychological---interplay between griefbots and their users. After a first section dedicated to griefbots' paradoxical sense of familiarity---both ``hyperrealist'' and incomplete---I locate the origins of their ``melancholia'' in their nature as digital objects, utilising the concept of the ``digital ruin'', and in their usage of ``traces'', only apparently similar to photography. Then, comparing griefbots to Winnicott's transitional objects, I explain the psychological meaning of melancholia: a denial of loss that amplifies loss. Ultimately, I address griefbots as elements of a contemporary media-based ``immune system'' that risks turning itself into ``autoimmunity''.",
issn="1543-3390",
doi="10.1057/s41282-025-00585-2",
url="https://doi.org/10.1057/s41282-025-00585-2"
}


@Article{Shi2025,
author="Shi, Xianming",
title="Leveraging emerging technologies to address the crisis of aging concrete infrastructure",
journal="Journal of Infrastructure Preservation and Resilience",
year="2025",
month="Dec",
day="15",
volume="6",
number="1",
pages="44",
abstract="Aging concrete infrastructure faces accelerating deterioration from deferred maintenance, environmental exposure, and climate change, creating urgent demand for cost-effective, durable solutions. This Commentary argues that meeting this challenge requires an integrated rehabilitation framework that unites electrochemical treatments, advanced repair materials and methods, and digital technologies. Electrochemical methods such as chloride extraction and cathodic protection can halt active corrosion of reinforcement, while advanced materials (including ultra-high-performance concrete, strain-hardening composites, fiber-reinforced polymers, and self-healing systems) enable resilient, minimally disruptive repairs. Emerging approaches in additive manufacturing and robotics further expand options for rapid, customized interventions. At the systems level, nondestructive evaluation, artificial intelligence, structural health monitoring, and digital twin platforms support data-driven diagnosis and proactive, life-cycle management. This Commentary contends that only through interdisciplinary collaboration can we achieve the necessary paradigm shift, from reactive fixes to predictive, holistic rehabilitation, ultimately extending service life and enhancing the safety, sustainability, and resilience of our concrete infrastructure.",
issn="2662-2521",
doi="10.1186/s43065-025-00147-x",
url="https://doi.org/10.1186/s43065-025-00147-x"
}


@Inbook{Das2025,
author="Das, Pulen
and Apanga, Emmanuel Achumboro",
editor="Chatterjee, Ayan
and Sarkar, Tanmay
and Smaoui, Slim",
title="Safety and Efficacy of Nano-supplements in Sports",
bookTitle="Nanofuel: The Future of Sports Nutrition: Boosting Performance with Nanotech Nutrients",
year="2025",
publisher="Springer Nature Singapore",
address="Singapore",
pages="461--487",
abstract="This chapter explores the burgeoning field of nanotechnology in sports nutrition, examining its potential benefitsBenefits and associated risks. This chapter delves into how nanoparticles enhance nutrient absorptionAbsorption, target delivery, and improve athletic performanceAthletic performance parameters such as muscle strength, endurance, and recovery. Furthermore, chapter discusses the critical safety concerns, including potential toxicity, tissue accumulation, and the lack of robust regulatory frameworks. The ethical implications, including fairness in competition, informed consent, and anti-doping considerations, are also addressed. Finally, chapter highlight the need for further research, stricter regulations, and a multi-stakeholder approach to ensure the responsible and ethical developmentDevelopment and utilization of nanotechnology in sports nutrition.",
isbn="978-981-96-5471-0",
doi="10.1007/978-981-96-5471-0_19",
url="https://doi.org/10.1007/978-981-96-5471-0_19"
}


@Article{Jadhav2025,
author="Jadhav, Dhanraj
and Singh, Jaibir",
title="A review on web information extraction and hidden predictive information from large databases",
journal="Multimedia Tools and Applications",
year="2025",
month="Oct",
day="01",
volume="84",
number="35",
pages="44113--44143",
abstract="Extracting the relevant data from Internet sources is a difficult task due to an increased amount of data. Therefore, an effective and flexible information extraction mechanism is established to transform the website pages into program-friendly structures like a relational database. So far, several types of research have been conducted on web information extraction using various methods. In recent years, retrieving information from website links has received more attention due to the increasing growth of Internet facilities. However, gaining better outcomes in the information extraction process is a critical issue faced by several existing methods. The current web information extraction survey presents only some techniques used to retrieve information and does not present exact limitations. In addition, there is a lack of verification of existing techniques used to retrieve hidden forecast information in various databases. In order to recommend suitable techniques for web information retrieval, identifying the limitations of existing methods is more important. Therefore, this review article covers varied techniques used to extract web and hidden prediction information from various databases and mentions the pros and cons of such methods in 2017--2024. To enhance the effectiveness of this article, it briefly describes the challenges of retrieving information from the web, applications of web information extraction, and useful future recommendations. Based on this review article, the most efficient techniques suitable for the web information extraction process can be exhibited for future use.",
issn="1573-7721",
doi="10.1007/s11042-025-20863-6",
url="https://doi.org/10.1007/s11042-025-20863-6"
}


@inproceedings{10.1007/978-3-031-99699-3_21,
 abstract = {The integration of artificial intelligence (AI) into healthcare is transforming the workforce, significantly affecting various professions. This systematic review aimed to identify the health professions most vulnerable to AI-driven automation and job replacement, focusing on roles with repetitive and predictable tasks. A comprehensive search was conducted across PubMed, Scopus, and Web of Science, applying inclusion criteria that targeted peer-reviewed studies evaluating the impact of AI on automation and workforce displacement in healthcare professions. A total of 56 studies were included, analyzed using a PRISMA approach. The findings revealed that radiology, nursing, pathology, and administrative roles are the most impacted due to their reliance on tasks highly amenable to AI automation, such as medical imaging analysis, patient monitoring, and data entry. However, professions requiring complex decision-making and critical thinking, such as surgery, showed lower vulnerability. Despite these risks, the review underscores that AI is unlikely to replace healthcare professionals entirely but will reshape roles, emphasizing the need for human oversight, empathy, and adaptability. To mitigate these challenges, healthcare systems must prioritize continuous education and workforce training to integrate AI as a complementary tool rather than a replacement. Policymakers and educational institutions should focus on ethical considerations, skill redefinition, and sustainable adaptation strategies for professionals in at-risk fields.},
 address = {Cham},
 author = {Suazo Galdames, Iv{\'a}n Claudio
and Chaple Gil, Alain Manuel},
 booktitle = {International Conference on Information Systems and Medicine. Volume 1},
 editor = {Galdames, Iv{\'a}n Suazo
and V{\'a}zquez-Justo, Enrique
and Abreu, Ant{\'o}nio
and Carvalho, Jo{\~a}o Vidal},
 isbn = {978-3-031-99699-3},
 pages = {267--301},
 publisher = {Springer Nature Switzerland},
 title = {The Impact of Artificial Intelligence on Healthcare Professions: Automation and Job Displacement Risks},
 url = {https://link.springer.com/chapter/10.1007/978-3-031-99699-3_21},
 year = {2026}
}

@Inbook{Qiblawi2026,
author="Qiblawi, Rabee Aladdin
and Sliman, Layth
and Osman, Ziad
and Haidar, Ali Massoud",
editor="Borroni, Andrea
and Perboli, Guido
and Piccinelli, Gian Maria",
title="MirageXR: A Framework for Metaverse-Enabled Cross-Platform Integration of Physical Hardware and XR Environments",
bookTitle="Metaverses: Reshaping Law, Economy, and Society: A Comprehensive Analysis of Virtual Worlds and Their Real-World Implications",
year="2026",
publisher="Springer Nature Switzerland",
address="Cham",
pages="155--179",
abstract="The metaverse has evolved from a concept in science fiction into an interconnected network of immersive digital experiences. Yet it remains fragmented---especially in integrating virtual components with real hardware. Existing solutions often rely on disparate communication protocols and siloed workflows, making it difficult to establish a unified, scalable approach. To unlock the metaverse's full potential across education, industry, and entertainment, developers and researchers need a framework that streamlines synchronization between the physical and virtual realms. By eliminating barriers to interoperability and providing standardized, flexible integration pathways, such a framework can facilitate rapid prototyping, iterative development, and the deployment of metaverse applications that bridge digital vision and tangible reality. In this paper, we introduce MirageXR, a real-time framework designed to close the gap between the real and virtual worlds. Although it does not address every facet of the metaverse, MirageXR offers a pivotal solution to one of its central challenges: seamlessly connecting virtual 3D objects, particularly virtual electronics, with real hardware. By leveraging standardized communication protocols and intuitive design, MirageXR enables users to build unified, hands-on XR experiences that blend digital and physical environments with ease, paving the way for future advances in metaverse integration.",
isbn="978-3-032-08664-8",
doi="10.1007/978-3-032-08664-8_9",
url="https://doi.org/10.1007/978-3-032-08664-8_9"
}


@Article{Albelali2025,
author="Albelali, Salma
and Ahmed, Moataz",
title="Testing Machine Learning and Deep Learning Systems: Achievements and Challenges",
journal="Arabian Journal for Science and Engineering",
year="2025",
month="Aug",
day="01",
volume="50",
number="15",
pages="11433--11484",
abstract="Rapid advancements in artificial intelligence have driven the integration of learning algorithms-machine learning (ML) and deep learning (DL) models-across various industries, posing new challenges for testing these complex systems. Rigorous testing of ML/DL-based systems (MLSs) is especially critical in high-stakes domains like autonomous driving, healthcare diagnostics, and financial forecasting, where system reliability is paramount. Unlike traditional software, MLS quality relies not only on model architecture and development processes but also significantly on the quality of the training data. This study offers a comprehensive review of MLS testing methodologies, with a focus on the emerging role of Data-Box testing, alongside established Black-Box and White-Box techniques. Data-Box testing assesses training data quality to ensure it meets criteria such as sufficiency and adequacy, bridging Black-Box and White-Box methods to enhance system reliability. The study further addresses the increasing use of mutation testing (MT) in DL, exploring MT techniques and mutation operators to ensure adequate coverage. By synthesizing recent advances, we propose an integrated MLS testing framework that encapsulates these critical aspects, offering insights and highlighting areas for future research to refine MLS testing practices.",
issn="2191-4281",
doi="10.1007/s13369-025-10276-w",
url="https://doi.org/10.1007/s13369-025-10276-w"
}


@Article{Liu2025,
author="Liu, Chao",
title="A comprehensive review of applications of AI technologies in higher engineering education",
journal="Discover Education",
year="2025",
month="Nov",
day="28",
volume="4",
number="1",
pages="528",
abstract="This paper presents a comprehensive narrative review of artificial intelligence (AI) applications in higher engineering education. We examine how AI technologies are reshaping teaching, learning, and assessment in engineering disciplines. Key implementation areas include personalized learning, student performance prediction, intelligent tutoring systems, and laboratory enhancements. Quantitative evidence reveals significant impacts: AI-driven platforms have improved student grades by up to 25{\%}, and predictive models have achieved accuracy rates exceeding 99{\%}. However, significant challenges persist, including algorithmic bias, accessibility barriers, and academic integrity concerns, with generative AI correctly answering up to 85{\%} of engineering assessment questions. The review also explores tensions between AI-driven automation and constructivist learning approaches. We conclude that while AI offers immense potential, its successful integration requires thoughtful curriculum design, faculty development, and robust institutional support to prepare students for an AI-driven professional landscape.",
issn="2731-5525",
doi="10.1007/s44217-025-00954-0",
url="https://doi.org/10.1007/s44217-025-00954-0"
}


@Inbook{Yadav2025,
author="Yadav, Sangeeta
and Saini, Pinki
and Iqbal, Unaiza
and Ahmed, Mazia",
editor="Chauhan, OP",
title="Fermented Fruits and Vegetables",
bookTitle="Fruits and Vegetables Technologies: Postharvest Processing and Packaging",
year="2025",
publisher="Springer Nature Singapore",
address="Singapore",
pages="343--393",
abstract="Fruits and vegetables are vital components of a healthy diet, providing essential vitamins, minerals, dietary fibers, and bioactive compounds that contribute to the prevention of chronic diseases such as hypertension, cardiovascular disorders, and stroke. Among various processing techniques, fermentation stands out as a sustainable and natural approach for enhancing the nutritional, sensory, and functional properties of fruits and vegetables. Rooted in ancient culinary traditions, fermentation has historically played a crucial role in food preservation, especially in regions with harsh climates or seasonal scarcity. This chapter explores the diverse world of fermented fruits and vegetables, highlighting their historical significance, cultural diversity, and health-promoting attributes. Emphasis is placed on lactic acid fermentation as a simple yet effective biotechnological intervention for improving product safety, shelf life, and nutritional value. Additionally, the chapter discusses the integration of traditional fermentation methods with modern biotechnological advancements, such as the use of selected starter cultures, to meet the growing consumer demand for minimally processed, nutritious, and flavorful foods. Challenges related to contamination, safety regulations, and quality control are also addressed, providing a comprehensive overview of the potential and limitations of fermentation in the production of functional fruit and vegetable-based foods.",
isbn="978-981-96-8433-5",
doi="10.1007/978-981-96-8433-5_10",
url="https://doi.org/10.1007/978-981-96-8433-5_10"
}


@Inbook{Chen2025,
author="Chen, Jing
and Garcia, Katherine Rose
and Zhang, Yining ``Elena''
and Dudley, LeGrand Estefan
and Warren, Ashley Doreen",
editor="Xu, Wei",
title="Human-Centered Artificial Intelligence in Cybersecurity",
bookTitle="Handbook of Human-Centered Artificial Intelligence",
year="2025",
publisher="Springer Nature Singapore",
address="Singapore",
pages="1--35",
abstract="This chapter explores human-centered artificial intelligence (AI) as a paradigm that integrates human needs, capabilities, and ethical considerations into AI-powered cybersecurity systems, aligning with the science of security's emphasis on systematic, interdisciplinary solutions. It examines AI's dual role in enhancing both defensive and offensive cybersecurity capabilities, and highlights the importance of human users in both. The chapter addresses critical challenges such as data quality, inherent biases, privacy issues, and the demand for explainability and transparency in AI-powered cyber defense. The authors propose using human-centered approaches to address these challenges, including the development of explainable and trustworthy AI-powered cyber defense systems, human-centric design, user training and education, effective human-AI teaming, bias mitigation, and supportive regulatory frameworks. Case studies, including cybersecurity for autonomous vehicles and defenses against AI-enhanced phishing attacks, further illustrate the practical applications and dual-use nature of AI in cybersecurity. The chapter concludes by envisioning a future where human-AI collaboration, grounded in scientific rigor, ethical standards, and regulatory frameworks, with interdisciplinary efforts, ensures robust and trustworthy cybersecurity solutions.",
isbn="978-981-97-8440-0",
doi="10.1007/978-981-97-8440-0_99-1",
url="https://doi.org/10.1007/978-981-97-8440-0_99-1"
}


@inproceedings{10.1007/978-3-031-82700-6_9,
 abstract = {In the verification of loop programs, disjunctive invariants are essential to capture complex loop dynamics such as phase and mode changes. In this work, we develop a novel approach for the automated generation of affine disjunctive invariants for affine while loops via Farkas' Lemma, a fundamental theorem on linear inequalities. Our main contributions are two-fold. First, we combine Farkas' Lemma with a succinct control flow transformation to derive disjunctive invariants from the conditional branches in the loop. Second, we propose an invariant propagation technique that minimizes the invariant computation effort by propagating previously solved invariants to yet unsolved locations as much as possible. Furthermore, we resolve the infeasibility checking in the application of Farkas' Lemma which has not been addressed previously, and extend our approach to nested loops via loop summary. Experimental evaluation over more than 100 affine while loops (mostly from SV-COMP 2023) demonstrates that our approach is promising to generate tight linear invariants over affine programs.},
 address = {Cham},
 author = {Ke, Jingyu
and Fu, Hongfei
and Liu, Hongming
and Sun, Zhouyue
and Chen, Liqian
and Li, Guoqiang},
 booktitle = {Verification, Model Checking, and Abstract Interpretation},
 editor = {Shankaranarayanan, Krishna
and Sankaranarayanan, Sriram
and Trivedi, Ashutosh},
 isbn = {978-3-031-82700-6},
 pages = {187--213},
 publisher = {Springer Nature Switzerland},
 title = {Affine Disjunctive Invariant Generation with Farkas' Lemma},
 url = {https://link.springer.com/chapter/10.1007/978-3-031-82700-6_9},
 year = {2025}
}

@Inbook{Jalote2025,
author="Jalote, Pankaj",
title="Application Deployment",
bookTitle="A Concise Introduction to Software Engineering: With Open Source and GenAI",
year="2025",
publisher="Springer Nature Switzerland",
address="Cham",
pages="203--217",
abstract="In the previous chapters, we have discussed various aspects of developing and testing a software application. During development, a software application is typically a set of source code files organized in directories and developed by different programmers in the development team.",
isbn="978-3-031-74318-4",
doi="10.1007/978-3-031-74318-4_8",
url="https://doi.org/10.1007/978-3-031-74318-4_8"
}


@Article{Mizuno2026,
author="Mizuno, Akiko
and Erickson, Zackory
and Jimenez, Daniel E.
and Aizenstein, Howard Jay",
title="AI in geriatric psychiatry: precision meets human experience",
journal="Neuropsychopharmacology",
year="2026",
month="Jan",
day="22",
abstract="Artificial intelligence (AI) and robotics are rapidly transforming geriatric psychiatry, offering powerful tools for early detection, personalized treatment, and enhanced care delivery. As the global population ages, these technologies promise not only greater efficiency but also new avenues for delivering scalable, accessible mental health support. However, as AI increasingly engages with domains once considered uniquely human---emotional intelligence, decision-making, and interpersonal connection---it raises deeper questions about the boundary between AI-simulated interaction and authentic human connection. This review examines the intersection of computational precision and existential complexity, emphasizing how theoretical frameworks such as the Theory of Computing (TOC) and Theory of Mind (ToM) can guide ethical and human-centered integration. While AI systems may convincingly simulate empathy or companionship, they cannot share subjective experience, vulnerability, or existential depth. By contrasting computational precision with the irreducible aspects of human complexity, we advocate for a transdisciplinary approach that embraces both the transformative potential of technology and the irreplaceable richness of human connection---especially in the later stages of life, when questions of purpose, mortality, and selfhood become most profound.",
issn="1740-634X",
doi="10.1038/s41386-026-02328-y",
url="https://doi.org/10.1038/s41386-026-02328-y"
}


@Article{Carić2025,
author="Cari{\'{c}}, Hrvoje
and Mandi{\'{c}}, Ante
and Sever, Ivan",
title="A six-phase AI-expert framework for evaluating policy coherence in sustainable tourism",
journal="Information Technology {\&} Tourism",
year="2025",
month="Dec",
day="09",
volume="28",
number="1",
pages="6",
abstract="This study introduces a six-phase AI--expert evaluation framework, grounded in computational interpretivism, to assess the coherence between national and international sustainable tourism policies, using the Croatian Strategy for Sustainable Tourism as a case study. Leveraging Anthropic Claude 3.5 Sonnet, a state-of-the-art generative AI model, the framework integrates natural language processing, vector-based similarity search, and expert validation to extract key policy areas, formulate evaluation questions, and assess alignment with the European Union's principal tourism policy documents: the Transition Pathway for Tourism, European Agenda for Tourism 2030, and EU Strategy for Sustainable Tourism. The analysis produced a validated set of 68 binary and 27 qualitative evaluation questions, developed through iterative AI--expert collaboration and verified through quantitative similarity measures (Jaccard coefficients of 0.714 for key areas and 0.904 for topics). Three domain experts independently validated all phases, achieving substantial inter-rater reliability (Fleiss' $\kappa${\thinspace}={\thinspace}0.717). Results confirm that AI can substantially enhance efficiency, scalability, and transparency in policy analysis, while expert oversight remains indispensable for contextual interpretation, ethical validation, and policy relevance. The study advances theoretical understanding of hybrid human--AI epistemologies and contributes a replicable, auditable model for policy coherence evaluation in complex governance systems. Beyond tourism, the framework demonstrates how AI-assisted analysis can strengthen evidence-based policymaking, supporting adaptive governance and cross-level policy alignment in sustainability-driven sectors.",
issn="1943-4294",
doi="10.1007/s40558-025-00352-0",
url="https://doi.org/10.1007/s40558-025-00352-0"
}


@inproceedings{10.1007/978-3-031-52667-1_22,
 abstract = {ChatGPT took the world by storm and generated extensive discussions about its uncanny capacity to generate realistic outputs that, superficially, could look like they are written by humans. Especially education appeared it could be hit hardest, due, mainly to slow but sustained evolution of assessment policies in universities and the slow response to perceived threats. This paper analyses the interference between AI creations and their possible effects on the learning processes. It then presents the proposed computer aided learning framework developed and implemented by one of the authors and its capacity to negate the harms that could be generated by ChatGPT applications. This is shown in an experiment. Cybertrainer, the proposed professional learning service has the capability to remove the threat to academic integrity by several special features: purely student-centred learning, full autonomy and freedom to intervene when desired, anonymised interventions, traceability by entirely written contributions, and, critically, robust quality assurance system guaranteeing integrity. Cybertrainer can truly counteract the dangers of ChatGPT by encouraging humans (students) to do what humans do better -- critical analysis, synthesis, and creativity.},
 address = {Cham},
 author = {Marian, Romeo Marin
and Surubaru, Teodora
and Isoc, Dorin},
 booktitle = {Towards a Hybrid, Flexible and Socially Engaged Higher Education},
 editor = {Auer, Michael E.
and Cukierman, Uriel R.
and Vendrell Vidal, Eduardo
and Tovar Caro, Edmundo},
 isbn = {978-3-031-52667-1},
 pages = {213--225},
 publisher = {Springer Nature Switzerland},
 title = {Are ChatGPT-Like Applications Harmful for the Learning Process?},
 url = {https://link.springer.com/chapter/10.1007/978-3-031-52667-1_22},
 year = {2024}
}

@inproceedings{10.1007/978-981-97-4677-4_9,
 abstract = {Smart city is an important concept that is gradually establishing in the everyday life of citizens. However, the integration of this concept brings a number of challenges in the area of cybersecurity. One of the key challenges faced by smart cities is the secure management of the big amount of data generated by various interconnected devices and systems. The complexity of managing and analyzing the enormous volume of log data generated by various systems and devices represents a security challenge. Security Information and Event Management systems (SIEMs) are used for effective processing and correlation of logs from multiple sources in real time. However, the key problem remains how to design efficient logging architecture, which includes, for example, the choice of security information source types, their data representation or parsing, and finally, their introduction into the context of a security event or incident. Designing a logging architecture is an expert activity that usually uses best practice based on the security baseline of common IT systems. The question is how to support the design of logging architecture with an emphasis on the specifics of technologies used in the smart city concept. With the development of artificial intelligence methods, the possibility of supporting the entire design process with AI tools opens. The aim of this article is to analyze the possibilities for simplifying the design process of the logging architecture in SIEM systems with an emphasis on the specifics of the smart city concept while simultaneously using artificial intelligence tools.},
 address = {Singapore},
 author = {Almer, Lubomir
and Horalek, Josef
and Sobeslav, Vladimir},
 booktitle = {Advances and Trends in Artificial Intelligence. Theory and Applications},
 editor = {Fujita, Hamido
and Cimler, Richard
and Hernandez-Matamoros, Andres
and Ali, Moonis},
 isbn = {978-981-97-4677-4},
 pages = {93--106},
 publisher = {Springer Nature Singapore},
 title = {Utilization of Artificial Intelligence for the SIEM Logging Architecture Design in the Context of Smart City},
 url = {https://link.springer.com/chapter/10.1007/978-981-97-4677-4_9},
 year = {2024}
}

@Article{Messaoud2024,
author="Messaoud, Montassar Ben
and Chekaya, Rania Ben
and Mkaouer, Mohamed Wiem
and Jenhani, Ilyes
and Aljedaani, Wajdi",
title="PR-DupliChecker: detecting duplicate pull requests in Fork-based workflows",
journal="International Journal of System Assurance Engineering and Management",
year="2024",
month="Jul",
day="01",
volume="15",
number="7",
pages="3538--3550",
abstract="Pull requests (PR) are a fundamental aspect of collaborative software development, allowing developers to propose changes to a codebase hosted on platforms like GitHub. They serve as a mechanism for peer review, enabling team members to assess the proposed changes before merging them into the main code repository. Duplicate pull requests occur when multiple contributors submit similar or identical proposed changes to a code repository. Such duplicate pull requests can be problematic because they create redundancy, waste developers' time, and complicate the review process. In this paper, we propose an approach which is based on a pre-trained language model, namely BERT (Bidirectional Encoder Representations from Transformers) to automatically detect duplicate PRs in GitHub repositories. A dataset of 3328 labeled PRs collected from 26 GitHub repositories is built. This data is then fed to a BERT model in order to get the embeddings which represent the contextual relationships between the words used in pairs of pull requests. Then, the BERT's classification outputs are fed to a Multilayer Perceptron (MLP) classifier which represents our final duplicate pull requests detector. Experiments have shown that BERT provided good performance and achieved an accuracy of 92{\%} with MLP classifier. Results have proven that BERT's word representation features achieved an increase of 13{\%} (resp., 17 and 23{\%}) compared to Siamese-BERT model (resp., DC-CNN and Word2Vec) in term of accuracy.",
issn="0976-4348",
doi="10.1007/s13198-024-02361-4",
url="https://doi.org/10.1007/s13198-024-02361-4"
}


@Inbook{Huang2025,
author="Huang, Ken
and Hughes, Chris",
title="Agentic AI Reinforcement Learning and Security",
bookTitle="Securing AI Agents: Foundations, Frameworks, and Real-World Deployment",
year="2025",
publisher="Springer Nature Switzerland",
address="Cham",
pages="169--205",
abstract="This chapter explores the unique security challenges presented by Reinforcement Learning (RL) in Agentic AI, where autonomous agents learn through interaction and reward maximization. It details key vulnerabilities such as adversarial attacks, reward hacking (specification gaming), and side-channel attacks, examining how these can compromise agent behavior and system integrity. Beyond identifying risks, the chapter delves into proactive defense strategies, including adversarial training, robust reward design, and the application of secure RL algorithms like GRPO and RLVR. Importantly, it analyzes the complex security implications---both positive and negative---of recent RL research findings, emphasizing the need for thorough model evaluations, red teaming, and continuous monitoring. For each implication, it lays out concrete mitigation strategies to build trust and to ensure the safety, reliability, and ethical operation of RL-based Agentic AI systems. By understanding the latest vulnerabilities and the methods to address them, it will be possible to plan a process that minimizes those failures.",
isbn="978-3-032-02130-4",
doi="10.1007/978-3-032-02130-4_7",
url="https://doi.org/10.1007/978-3-032-02130-4_7"
}


@Article{Abdel-Hafiz2023,
author="Abdel-Hafiz, Hany A.
and Schafer, Johanna M.
and Chen, Xingyu
and Xiao, Tong
and Gauntner, Timothy D.
and Li, Zihai
and Theodorescu, Dan",
title="Y chromosome loss in cancer drives growth by evasion of adaptive immunity",
journal="Nature",
year="2023",
month="Jul",
day="01",
volume="619",
number="7970",
pages="624--631",
abstract="Loss of the Y chromosome (LOY) is observed in multiple cancer types, including 10--40{\%} of bladder cancers1--6, but its clinical and biological significance is unknown. Here, using genomic and transcriptomic studies, we report that LOY correlates with poor prognoses in patients with bladder cancer. We performed in-depth studies of naturally occurring LOY mutant bladder cancer cells as well as those with targeted deletion of Y chromosome by CRISPR--Cas9. Y-positive (Y+) and Y-negative (Y--) tumours grew similarly in vitro, whereas Y− tumours were more aggressive than Y+ tumours in immune-competent hosts in a T cell-dependent manner. High-dimensional flow cytometric analyses demonstrated that Y− tumours promote striking dysfunction or exhaustion of CD8+ T cells in the tumour microenvironment. These findings were validated using single-nuclei RNA sequencing and spatial proteomic evaluation of human bladder cancers. Of note, compared with Y+ tumours, Y− tumours exhibited an increased response to anti-PD-1 immune checkpoint blockade therapy in both mice and patients with cancer. Together, these results demonstrate that cancer cells with LOY mutations alter T cell function, promoting T cell exhaustion and sensitizing them to PD-1-targeted immunotherapy. This work provides insights into the basic biology of LOY mutation and potential biomarkers for improving cancer immunotherapy.",
issn="1476-4687",
doi="10.1038/s41586-023-06234-x",
url="https://doi.org/10.1038/s41586-023-06234-x"
}


@Article{Donald2026,
author="Donald, Andy
and Galanopoulos, Apostolos
and Ojha, Atul Kumar
and Curry, Edward
and Mu{\~{n}}oz, Emir
and Ullah, Ihsan
and McCrae, John P.
and Kalra, Manan
and Saxena, Sagar
and Iqbal, Talha",
title="Investigating transformer models for textual bias detection in model, data, and dataspace cards",
journal="AI and Ethics",
year="2026",
month="Jan",
day="28",
volume="6",
number="1",
pages="118",
abstract="Identifying hidden biases in AI documentation metadata (model, data, and dataspace cards) is essential for responsible AI; yet this domain remains largely unexplored. The proposed work evaluates four Transformer models (XLNet, DistilBERT, RoBERTa, and ELECTRA) for bias detection across publicly available, synthetic, and custom datasets. On the BABE news corpus, all models achieved 77--80{\%} accuracy, with only ELECTRA exceeding 80{\%} on every metric. To address the absence of publicly available AI-card datasets, we generated synthetic metadata for two use cases (Customer Interaction and Customer Data Uploaded by Organisations) using ChatGPT. Models trained on this synthetic corpus displayed near-perfect scores, reflecting shared stylistic cues embedded in the generated text. To test real-world robustness, we curated a Hugging Face dataset by scraping documentation comments, filtering for bias-related keywords, and obtaining annotations from four independent labellers in a single-blind setting. Partial fine-tuning (zero-shot) evaluations of models trained only on BABE or synthetic data revealed substantial performance drops on this real-world set. To mitigate this cross-domain loss, we introduce a cascaded, full fine-tuning (few-shot) pipeline in which Transformer models are sequentially fine-tuned on BABE, synthetic text, and a subset of the Hugging Face corpus. Evaluation on the remaining portion achieved over 85{\%} across all performance metrics, enhancing precision and generalisation. This study demonstrates the challenges of bias detection beyond controlled or synthetic data and highlights cascaded fine-tuning as a practical, low-resource strategy. Future directions include leveraging evidence fusion methods, integrating cross-attention with bias taxonomies, and adopting dual-encoder architectures to advance bias detection toward more in-depth, knowledge-guided reasoning.",
issn="2730-5961",
doi="10.1007/s43681-025-00975-3",
url="https://doi.org/10.1007/s43681-025-00975-3"
}


@Article{Abid2025,
author="Abid, Hafiz Muhammad Rizwan
and Aslam, Sadia
and Alwan, Nisreen
and Khalid, Nauman",
title="Current readiness on food fraud risk mitigation in developing countries: a review",
journal="Agriculture {\&} Food Security",
year="2025",
month="May",
day="25",
volume="14",
number="1",
pages="9",
abstract="Food fraud is a deceptive act of deliberately and intentionally changing food composition for economic gain. In the context of developing countries, different emerging food fraud patterns are observed, including counterfeiting, dilution, substitution, mislabeling, etc. These have been proven to pose risks to people's health in the past few years. Currently, most developing countries have a food fraud mitigation readiness status ranging from low to moderate. Weak regulatory frameworks, lack of enforcement, the socio-economic situation, and the surveillance system gaps are some of the major reasons for low to moderate risk mitigation readiness. This paper identifies the foods most susceptible to food fraud, and significant tool systems to mitigate food fraud are also indicated. Some developing countries implement regulatory reforms and adopt advanced detection technologies, but the enforcement is still inconsistent due to resource limitations. In addition, the lack of integrated watchdog surveillance and collaboration among different stakeholders hinders effective food fraud mitigation. This review evaluates the success of modern and traditional food fraud methods, especially concerning labeling practices. The issue of molecular forensics and new analytical methods are studied in terms of their contribution towards strengthening the detection and control of food fraud. The emphasis is on developing countries, where the application of these technologies needs to be put in place to improve readiness to mitigate the food fraud risk.",
issn="2048-7010",
doi="10.1186/s40066-025-00528-1",
url="https://doi.org/10.1186/s40066-025-00528-1"
}


@Inbook{Kumari2025,
author="Kumari, Priyanka
and Singh, Shishir Kr.
and Utpal, Vinit Kumar Jha",
editor="Dutta, Soumi
and Rocha, {\'A}lvaro
and Agarwal, Ambuj Kumar
and Tiwari, Raj Gaurang
and Bhattacharya, Abhishek",
title="Guardians of Accountability: The Role of Media in Oversight and Governance of Generative AI Applications in Fintech",
bookTitle="Generative AI in FinTech: Revolutionizing Finance Through Intelligent Algorithms",
year="2025",
publisher="Springer Nature Switzerland",
address="Cham",
pages="345--359",
abstract="The integration of Generative Artificial Intelligence (AI) applications presents new opportunities and challenges in the field of financial technology (Fintech), especially with regard to governance and oversight. This study looks into how important it is for the media to improve transparency in the Fintech industry, with a particular emphasis on applications of generative AI. This research aims to clarify the complex relationships between media, oversight mechanisms, and governance structures in ensuring the responsible deployment of generative AI in Fintech through an analysis of existing literature, regulatory frameworks, and case studies. Fintech companies now have access to cutting-edge tools for risk assessment, fraud detection, customer support, and investment strategies thanks to the spread of generative AI. These innovations have the potential to truly transform the industry. On the other hand, questions about ethics, accountability, and transparency are brought up by the opacity and inherent complexity of generative AI algorithms. The media plays a vital role as a watchdog in this regard, promoting public debate, awareness, and examination of the moral and societal implications of Fintech's use of generative artificial intelligence. This paper investigates how media platforms impact public perceptions, regulatory responses, and industry practices related to Generative AI in Fintech. It does this by drawing upon theoretical frameworks from media studies, governance theory, and AI ethics. It also looks at how expert analysis, investigative journalism, and stakeholder involvement can help identify potential risks, biases, and malpractices related to AI-driven financial technologies. This research sheds light on the dynamics of media influence on regulatory enforcement, corporate accountability, and consumer trust in the Fintech ecosystem through in-depth case studies. This paper also looks at the efficiency of industry self-regulation programs and current regulatory frameworks in tackling the problems brought on by generative AI in Fintech. By demonstrating the complementary roles of public advocacy, regulatory interventions, and journalistic investigations in promoting responsible innovation and risk mitigation in the Fintech sector, it explores the symbiotic relationship between media scrutiny and regulatory oversight. In addition, it examines the governance gaps and moral conundrums associated with the application of generative AI in Fintech, including concerns about algorithmic bias, data privacy, and systemic vulnerabilities. The study's findings emphasize the critical role that the media plays in advancing the ethical governance, accountability, and transparency of generative AI applications in the Fintech industry. Media organizations have the power to stimulate positive public discourse, industry best practices, and regulatory changes that will guarantee AI-driven financial innovations balance potential risks with societal benefits. In the end, this paper advances our understanding of the intricate interactions that will shape Fintech's future in the AI era between media, oversight mechanisms, and governance frameworks.",
isbn="978-3-031-76957-3",
doi="10.1007/978-3-031-76957-3_18",
url="https://doi.org/10.1007/978-3-031-76957-3_18"
}


@inproceedings{10.1007/978-3-032-05188-2_5,
 abstract = {Effective testing depends on the testability of the system under test, yet current practice for assessing testability remains ad-hoc and fragmented. We introduce a multi-level bundle of testability indicators that map directly to actionable code refactorings and DevOps tweaks. Architecture, code, runtime-observability and process patterns are operationalised through metrics such as CBO, log-event density and mutation score. Recurring indicator constellations surface ``testability smells'', each linked to ranked, low-cost remedies. A prototype recommender couples RefactoringMiner with a random-forest classifier to automate smell detection and action ranking. We evaluate the approach on 12 open-source projects and one industrial microservice. Preliminary results show significant, sustained indicator improvements after targeted refactorings.},
 address = {Cham},
 author = {Holvoet, Tom},
 booktitle = {Testing Software and Systems},
 editor = {Bonfanti, Silvia
and Papadopoulos, George Angelos},
 isbn = {978-3-032-05188-2},
 pages = {61--69},
 publisher = {Springer Nature Switzerland},
 title = {Testability Indicators for Refactoring},
 url = {https://link.springer.com/chapter/10.1007/978-3-032-05188-2_5},
 year = {2026}
}

@Inbook{Wufka2026,
author="Wufka, Michael
and Canonico, Massimo",
title="Types of Cloud Services",
bookTitle="Introduction to Cloud Computing",
year="2026",
publisher="Springer Nature Switzerland",
address="Cham",
pages="111--155",
abstract="This chapter describes the wide range of services that modern clouds offer to their users. These services can be categorized broadly into three levels of abstraction, which are described in Sect. 4.1. The rest of this chapter gives an overview of the types of cloud resources that are available in modern cloud systems, using the Google Cloud Platform as an example.",
isbn="978-3-032-07151-4",
doi="10.1007/978-3-032-07151-4_4",
url="https://doi.org/10.1007/978-3-032-07151-4_4"
}


@Article{Hua2024,
author="Hua, Hao",
title="Porous interlocking assembly: performance-based dry masonry construction with digital stereotomy",
journal="Architectural Intelligence",
year="2024",
month="Jun",
day="03",
volume="3",
number="1",
pages="20",
abstract="Architected porosity in masonry structures can be created by transforming stock materials into a lattice of interlocking units through an automated batch process. Porous masonry forms numerous enclosed cavities for thermal performance and reduces material usage while maintaining structural integrity. This work investigates the potential and limits of digital tectonics of porous masonry through a complete process of design, manufacturing, and construction. The confluence of digital fabrication with tectonic exploration opens new dimensions unattainable by traditional stereotomy. Interlocking materials inspired by Abeille vault and digital stereotomy have made rapid progress. Following the theory of poetic construction, this work proposes that masonry construction should evoke visual or haptic enhancement through the fulfillment of pragmatic functions. We formulated a design challenge for a confined dry masonry wall for the envelope of the 2226 building. It assumes batch-cutting bespoke units out of large blocks of high-strength foam. Through a process of cutting and reassembling, the stock material is topologically expanded into a porous structure. A series of prototypes were developed to explore novel articulation, structural and thermal performance, and economical manufacturing. One can perceive the logic of porous construction through visual and haptic empathy. The materialization process interacts with the design masonry units and the interlocking mechanism. For future practice in masonry, the porosity should be planned at multiple scales (molecular scale, aggerate scale, construction scale) across the life cycle of the material.",
issn="2731-6726",
doi="10.1007/s44223-024-00061-4",
url="https://doi.org/10.1007/s44223-024-00061-4"
}


@Inbook{Leung2025,
author="Leung, Ricky",
title="Real-World Applications of Supervised Learning",
bookTitle="Leveraging GenAI for Machine Learning Education in Public Health: ChatGPT and R",
year="2025",
publisher="Springer Nature Switzerland",
address="Cham",
pages="155--184",
abstract="This chapter demonstrates the application of supervised learning models to a larger simulated dataset that incorporates demographic, clinical, behavioral, and genetic risk factors to predict substance use disorder outcomes. Using RStudio, readers are guided through key steps including data preparation, feature selection, model construction, and evaluation. Methods such as logistic regression, decision trees, and random forests are introduced in detail, allowing learners to compare model performance across multiple approaches. Performance is assessed using accuracy and related metrics, highlighting the strengths and limitations of each method. ChatGPT is integrated as a coding assistant, helping to generate R scripts, resolve errors, and produce visualizations, thereby lowering technical barriers for beginners. By working with the same dataset across different algorithms, the chapter illustrates practical applications of predictive analytics in public health, with special attention to fairness, interpretability, and the responsible use of machine learning for sensitive health outcomes.",
isbn="978-3-032-08677-8",
doi="10.1007/978-3-032-08677-8_8",
url="https://doi.org/10.1007/978-3-032-08677-8_8"
}


@Inbook{Ramachandran2026,
author="Ramachandran, Muthu",
title="AI Ethics in Generative AI",
bookTitle="Engineering Ethics of AI by Design: Principles, Practices, and Frameworks for Responsible Artificial Intelligence",
year="2026",
publisher="Springer Nature Singapore",
address="Singapore",
pages="825--867",
abstract="This chapter explores the ethical dimensions of Generative AI (GenAI), focusing on its transformative role in content creation and its far-reaching societal impacts. It analyses ethical challenges such as misinformation, intellectual property violations, bias, and the Doppelg{\"a}nger Effect---where AI replicates human personas. The chapter presents frameworks and regulatory approaches to mitigate harms, including EU's AI Act, platform policies, and emerging legal actions. Through case studies like deepfakes during war, copyright lawsuits, and public-health misinformation, it highlights the need for transparent, participatory, and harmonized governance to balance AI innovation with ethical compliance.",
isbn="978-981-95-2909-4",
doi="10.1007/978-981-95-2909-4_16",
url="https://doi.org/10.1007/978-981-95-2909-4_16"
}


@inproceedings{10.1007/978-3-031-42283-6_16,
 abstract = {Trust is an attitudinal construct that can be sensitive to prior experience, gender, and age. In our study, we explored how trust in a banking chatbot might be shaped by these user characteristics. Statistical analysis of 251 participants, who interacted with one of six chatbots defined by humanlikeness (high/low) and conversational performance (no breakdown, breakdown with repaired, breakdown without repair), showed that the user characteristics of gender and age did not significantly impact trust, but prior experience did. Trust resilience was found across the gender and age groups. The effect of users' prior experience on their trust in a chatbot which they have never used holds implications for research and practice. Future studies on the effect of cultural context, longer interaction episodes, and more diverse application contexts on trust in chatbots are recommended.},
 address = {Cham},
 author = {Law, Effie Lai-Chong
and van As, Nena
and F{\o}lstad, Asbj{\o}rn},
 booktitle = {Human-Computer Interaction -- INTERACT 2023},
 editor = {Abdelnour Nocera, Jos{\'e}
and Krist{\'i}n L{\'a}rusd{\'o}ttir, Marta
and Petrie, Helen
and Piccinno, Antonio
and Winckler, Marco},
 isbn = {978-3-031-42283-6},
 pages = {277--296},
 publisher = {Springer Nature Switzerland},
 title = {Effects of Prior Experience, Gender, and Age on Trust in a Banking Chatbot With(Out) Breakdown and Repair},
 url = {https://link.springer.com/chapter/10.1007/978-3-031-42283-6_16},
 year = {2023}
}

@Article{ref1,
title="19th European Colorectal Congress -- ECC, St. Gallen",
journal="European Surgery",
year="2025",
month="Nov",
day="01",
volume="57",
number="2",
pages="133--166",
issn="1682-4016",
doi="10.1007/s10353-025-00909-7",
url="https://doi.org/10.1007/s10353-025-00909-7"
}


@Inbook{Ramachandran2026,
author="Ramachandran, Muthu",
title="Ethical AI Testing and Evaluation Against Governance and Frameworks",
bookTitle="Engineering Ethics of AI by Design: Principles, Practices, and Frameworks for Responsible Artificial Intelligence",
year="2026",
publisher="Springer Nature Singapore",
address="Singapore",
pages="543--621",
abstract="This chapter on Ethical AI Testing and Evaluation, explores the critical role of testing and evaluation in ensuring that AI systems align with ethical principles such as fairness, transparency, accountability, and privacy. The chapter emphasizes the integration of ethical considerations into the AI development lifecycle, particularly through modern software development practices like Continuous Integration (CI), Continuous Deployment (CD), and DevOps. It introduces frameworks, tools, and methodologies for ethical testing, including fairness testing, bias detection, explainability validation, and privacy audits. The chapter also highlights the importance of iterative testing, stakeholder engagement, and continuous ethical impact assessments to address evolving societal norms and regulatory requirements. A case study on chatbot testing illustrates the practical application of these principles, showcasing how ethical and technical testing intersect to create responsible AI systems.",
isbn="978-981-95-2909-4",
doi="10.1007/978-981-95-2909-4_11",
url="https://doi.org/10.1007/978-981-95-2909-4_11"
}


@Inbook{Vargas-Gomez2025,
author="Vargas-Gomez, Daniel",
title="Creativity in the Year 2035",
bookTitle="The AI Revolution: What Creativity Could Look Like in 2035",
year="2025",
publisher="Springer Nature Switzerland",
address="Cham",
pages="83--135",
abstract="The chapter explores the evolving role of generative AI in creative processes, addressing key challenges such as bias in training data, intellectual property disputes and the democratization of creativity. It critiques the limitations of current AI systems, which often amplify stereotypes and stifle originality, while also examining the potential for AI to augment human creativity through collaboration. The text highlights the tension between automation and human agency, questioning whether AI will replace creative jobs or serve as a complementary tool. It also delves into societal implications, including the risk of AI-generated content (``slop'') flooding creative markets and diminishing the value of human artistry. The chapter advocates for a future where AI is used experimentally, pushing boundaries rather than reinforcing norms, and emphasizes the need for ethical frameworks and equitable access to AI technologies. Ultimately, it calls for a redefinition of creativity as a shared, networked endeavour rather than an individual talent.",
isbn="978-3-032-01172-5",
doi="10.1007/978-3-032-01172-5_3",
url="https://doi.org/10.1007/978-3-032-01172-5_3"
}


@Article{Pan2025,
author="Pan, Jeong Hoon
and Aykin-Burns, Nukhet
and Krager, Kimberly J.
and Shin, Hyo Ri
and Lee, Chae Hwan
and Lee, Jin Hyup
and Kong, Byungwhi
and Myoung, JaeEun
and Choi, Kyung-Chul
and Kim, Jae Kyeom",
title="SIRT3--IDH2 axis is a target of dietary fructose: implication of IDH2 as a key player in dietary carcinogen toxicity in mice colon",
journal="Experimental {\&} Molecular Medicine",
year="2025",
month="Nov",
day="01",
volume="57",
number="11",
pages="2643--2656",
abstract="Recent epidemiological studies have shown that dietary fructose intake is associated with an increased risk of colorectal cancer, yet its specific molecular mechanisms in colon carcinogenesis remain underexplored. Here we investigate the molecular mechanisms by which dietary fructose contributes to colon carcinogenesis, focusing on the role of mitochondrial NADP+-dependent isocitrate dehydrogenase 2 (IDH2). Using an unbiased multiomics approach (transcriptomics and proteomics), liver and colon tissues from fructose-fed wild-type mice were analyzed to identify key genes involved in cancer-related pathways. In addition, human liver transcriptomic data (GSE256398) were analyzed to confirm alterations in aryl hydrocarbon receptor (AhR) signaling and the sirtuin (SIRT)3--IDH2 axis. IDH2-knockout mice were exposed to a dietary carcinogen, 2-amino-1-methyl-6-phenylimidazo(4,5-b)pyridine (PhIP), to validate IDH2's role in colon cancer development. In vitro, fructose's effects on SIRT3 expression and IDH2 activity were assessed. Fructose-fed wild-type mice exhibited suppressed AhR signaling, increased oxidative stress and mitochondrial dysfunction via the SIRT3--IDH2 axis. In human liver datasets, AhR-associated genes and SIRT3--IDH2 expression were reduced in metabolic dysfunction-associated steatotic liver disease and cirrhosis. The IDH2-knockout mice showed heightened DNA damage, colonic tumorigenesis and mitochondrial and glutathione-mediated detoxification disruptions following PhIP exposure. In vitro, fructose reduced SIRT3 expression and IDH2 activity, further supporting its role in promoting colon carcinogenesis. Fructose promotes colon carcinogenesis by disrupting mitochondrial function and impairing DNA damage response mechanisms, particularly through SIRT3--IDH2 axis suppression. These findings highlight the critical role of mitochondrial dysfunction in fructose-induced carcinogenesis and suggest the SIRT3--IDH2 axis as a potential therapeutic target.",
issn="2092-6413",
doi="10.1038/s12276-025-01584-0",
url="https://doi.org/10.1038/s12276-025-01584-0"
}


@Article{Liao2024,
author="Liao, Weinian
and Chen, Xinliang
and Zhang, Shuzhen
and Chen, Jun
and Liu, Chaonan
and Yu, Kuan
and Zhang, Yimin
and Chen, Mo
and Chen, Fang
and Shen, Mingqiang
and Lu, Binghui
and Han, Songling
and Wang, Song
and Wang, Junping
and Du, Changhong",
title="Megakaryocytic IGF1 coordinates activation and ferroptosis to safeguard hematopoietic stem cell regeneration after radiation injury",
journal="Cell Communication and Signaling",
year="2024",
month="May",
day="27",
volume="22",
number="1",
pages="292",
abstract="Hematopoietic stem cell (HSC) regeneration underlies hematopoietic recovery from myelosuppression, which is a life-threatening side effect of cytotoxicity. HSC niche is profoundly disrupted after myelosuppressive injury, while if and how the niche is reshaped and regulates HSC regeneration are poorly understood.",
issn="1478-811X",
doi="10.1186/s12964-024-01651-5",
url="https://doi.org/10.1186/s12964-024-01651-5"
}


@Inbook{Singh2025,
author="Singh, Akansha
and Singh, Krishna Kant",
title="Setting Up Your R Environment for Generative AI",
bookTitle="Generative AI in R: Transforming Data Science with Synthetic Data and Advanced Modeling Techniques",
year="2025",
publisher="Apress",
address="Berkeley, CA",
pages="33--78",
abstract="In today's rapidly evolving landscape of data science and artificial intelligence, setting up a robust and versatile programming environment is foundational to success. R was initially developed for statistical analysis and has expanded into a powerful tool for machine learning and, more recently, Generative AI. This chapter guides you through configuring your R environment specifically for Generative AI applications, covering essential tools, packages, and configurations that will enhance your workflow, improve reproducibility, and enable the development of cutting-edge models. By the end of this chapter, you will have a well-organized R setup that's capable of tackling the complexities of Generative AI, empowering you to bring your ideas to life with confidence and efficiency.",
isbn="979-8-8688-1763-2",
doi="10.1007/979-8-8688-1763-2_2",
url="https://doi.org/10.1007/979-8-8688-1763-2_2"
}


@Article{Aminian-Dehkordi2025,
author="Aminian-Dehkordi, Javad
and Montazeri, Fateme
and Tamadon, Ali
and Mofrad, Mohammad R. K.",
title="Systems biology and microbiome innovations for personalized diabetic retinopathy management",
journal="npj Systems Biology and Applications",
year="2025",
month="Nov",
day="21",
volume="11",
number="1",
pages="133",
abstract="Diabetic retinopathy (DR), a complex condition driven by inflammation, oxidative stress, and metabolic imbalances, calls for innovative treatment strategies. Engineered probiotics delivering angiotensin-converting enzyme 2 (ACE2) offer a promising strategy by leveraging gut microbiome-retina association. Advances in synthetic biology and computational techniques enable personalized, data-driven therapies. This review discusses computational approaches at multiple scales and presents an integrated framework for promoting personalized, systems-level DR management.",
issn="2056-7189",
doi="10.1038/s41540-025-00607-w",
url="https://doi.org/10.1038/s41540-025-00607-w"
}


@Article{Gozum2025,
author="Gozum, Ivan Efreaim
and Eballo, Arvin",
title="Artificial intelligence, integral ecology, and the planetary polycrisis: insights from Laudato Si' and Laudate Deum for sustainable and just AI governance",
journal="AI {\&} SOCIETY",
year="2025",
month="Oct",
day="11",
abstract="This article investigates the ethical and ecological implications of artificial intelligence (AI) within the context of the planetary polycrisis---an interlinked series of environmental, social, and moral disruptions. Anchored in Catholic social thought and guided by the See--Judge--Act framework, the study engages the theological and ethical insights of Pope Francis' Laudato Si' and Laudate Deum to critically examine AI's role in shaping just and sustainable futures. The research first assesses (See) current trends in AI development and deployment, particularly their ecological costs and societal consequences. It then evaluates (Judge) these realities through the lens of integral ecology, arguing that AI must be governed not solely through technocratic or market-driven mechanisms, but through frameworks that uphold human dignity, environmental integrity, and the common good. Finally, it proposes (Act) a set of normative and policy-oriented recommendations that advance interdisciplinary cooperation, global regulatory alignment, and the ethical reorientation of AI systems. By synthesizing theological, philosophical, and governance perspectives, this article contributes to emerging discourses on AI ethics and sustainable governance in an era of global systemic risk.",
issn="1435-5655",
doi="10.1007/s00146-025-02684-1",
url="https://doi.org/10.1007/s00146-025-02684-1"
}


@inproceedings{10.1007/978-981-96-5887-9_2,
 abstract = {A timely and effective response is crucial to minimize damage and save lives during natural disasters like earthquakes. Microblogging platforms, particularly Twitter, have emerged as valuable real-time information sources for such events. This work explores the potential of leveraging Twitter data for earthquake response analysis. We develop a machine learning (ML) framework by incorporating natural language processing (NLP) techniques to extract and analyze relevant information from tweets posted during earthquake events. The approach primarily focuses on extracting location data from tweets to identify affected areas, generating severity maps, and utilizing WebGIS to display valuable information. The insights gained from this analysis can aid emergency responders, government agencies, humanitarian organizations, and NGOs in enhancing their disaster response strategies and facilitating more efficient resource allocation during earthquake events.},
 address = {Singapore},
 author = {Patel, Deep
and Bhattacharjee, Panthadeep
and Reza, Amit
and Pradhan, Priodyuti},
 booktitle = {Recent Challenges in Intelligent Information and Database Systems},
 editor = {Nguyen, Ngoc Thanh
and Matsuo, Tokuro
and Gaol, Ford Lumban
and Manolopoulos, Yannis
and Fujita, Hamido
and Hong, Tzung-Pei
and Wojtkiewicz, Krystian},
 isbn = {978-981-96-5887-9},
 pages = {18--30},
 publisher = {Springer Nature Singapore},
 title = {Earthquake Response Analysis with AI},
 url = {https://link.springer.com/chapter/10.1007/978-981-96-5887-9_2},
 year = {2025}
}

@Inbook{Chowdhury2025,
author="Chowdhury, Dhiman Deb",
title="AI Infrastructure",
bookTitle="Future of Networks: Modern Communication Infrastructure",
year="2025",
publisher="Springer Nature Switzerland",
address="Cham",
pages="313--370",
abstract="In recent years, the intersection of artificial intelligence",
isbn="978-3-031-71440-5",
doi="10.1007/978-3-031-71440-5_8",
url="https://doi.org/10.1007/978-3-031-71440-5_8"
}


@inproceedings{10.1007/978-981-99-8031-4_28,
 abstract = {This study aims to show how to use ChatGPT to analyze word-of-mouth about cosmetics. First, ChatGPT is used to add titles to word-of-mouth about cosmetics and rate the reviews of @COSME on a 7-point scale in this study. The titles are extracted from the main text of the reviews, and the words used are moist, comfortable, like strawberry chocolate, thick texture, and full of freshness, which are emotionally impressive words. Second, ChatGPT rates the reviews and answers the reason for the rating. The rationale for the rating depends on the amount of text in the review, but most of them are explained in detail. Many mention whether or not they describe their impressions of the product and its effectiveness. Also, attention is paid to whether negative words, praise, and the intention to continue using the product can be read. Third, this study testifies whether or not ChatGPT can categorize the titles of word-of-mouth about cosmetics. As a result, ChatGPT cannot categorize alone. It was found that communicating with ChatGPT is needed to categorize the titles of word-of-mouth.},
 address = {Singapore},
 author = {Minetaki, Kazunori},
 booktitle = {Intelligent Sustainable Systems},
 editor = {Nagar, Atulya K.
and Jat, Dharm Singh
and Mishra, Durgesh
and Joshi, Amit},
 isbn = {978-981-99-8031-4},
 pages = {313--324},
 publisher = {Springer Nature Singapore},
 title = {How Do We Use ChatGPT for Word-of-Mouth?},
 url = {https://link.springer.com/chapter/10.1007/978-981-99-8031-4_28},
 year = {2024}
}

@Article{Bory2025,
author="Bory, Paolo
and Natale, Simone
and Katzenbach, Christian",
title="Strong and weak AI narratives: an analytical framework",
journal="AI {\&} SOCIETY",
year="2025",
month="Apr",
day="01",
volume="40",
number="4",
pages="2107--2117",
abstract="The current debate on artificial intelligence (AI) tends to associate AI imaginaries with the vision of a future technology capable of emulating or surpassing human intelligence. This article advocates for a more nuanced analysis of AI imaginaries, distinguishing ``strong AI narratives,'' i.e., narratives that envision futurable AI technologies that are virtually indistinguishable from humans, from ``weak'' AI narratives, i.e., narratives that discuss and make sense of the functioning and implications of existing AI technologies. Drawing on the academic literature on AI narratives and imaginaries and examining examples drawn from the debate on Large Language Models and public policy, we underscore the critical role and interplay of weak and strong AI across public/private and fictional/non-fictional discourses. The resulting analytical framework aims to empower approaches that are more sensitive to the heterogeneity of AI narratives while also advocating normalising AI narratives, i.e., positioning weak AI narratives more firmly at the center stage of public debates about emerging technologies.",
issn="1435-5655",
doi="10.1007/s00146-024-02087-8",
url="https://doi.org/10.1007/s00146-024-02087-8"
}


@Article{Najm2025,
author="Najm, Aveen
and Chew, Esyin
and Bentley, Barry L.",
title="Social Robotics for the Treatment of Children with Cleft Lip and Palate: A Systematic Review",
journal="Current Robotics Reports",
year="2025",
month="Nov",
day="01",
volume="6",
number="1",
pages="1--14",
abstract="Cleft lip and palate (CLP) is a prevalent congenital disorder that impacts children globally. Despite recent advances in understanding CLP aetiology and improvements in surgical interventions and therapies, CLP still results in significant physical, psychological, and societal obstacles for affected children. This study presents a review of the predominant issues that affect children with CLP in the domains of speech, social interaction, hearing, and feeding, revealing speech-related challenges as the most significant factors for children with CLP.",
issn="2662-4087",
doi="10.1007/s43154-025-00111-3",
url="https://doi.org/10.1007/s43154-025-00111-3"
}


@Article{Ye2026,
author="Ye, Jiancheng",
title="Artificial intelligence-generated content (AIGC) in biomedical research, healthcare delivery, and clinical practices: technologies, applications, and regulatory considerations",
journal="Artificial Intelligence Review",
year="2026",
month="Jan",
day="19",
volume="59",
number="2",
pages="86",
abstract="Artificial Intelligence-Generated Content (AIGC) represents a paradigm shift in biomedical research and healthcare delivery, offering unprecedented capabilities for content creation, medical data analysis, and patient care optimization. This review examines the evolution of AIGC technologies from rule-based systems to advanced multimodal large models, with specific focus on their applications in healthcare settings. We analyze the three core capabilities of AIGC: intelligent digital content twinning, editing, and creation, and their transformative potential in medical imaging, clinical documentation, drug discovery, and personalized medicine. This paper discusses key challenges including algorithmic transparency, data privacy, and regulatory compliance, particularly in light of World Health Organization (WHO) guidelines for AI in health. Our findings indicate that while AIGC technologies show remarkable promise in enhancing diagnostic accuracy, streamlining clinical workflows, and democratizing healthcare access, careful consideration of ethical implications and regulatory frameworks is essential for safe and effective implementation.",
issn="1573-7462",
doi="10.1007/s10462-025-11487-1",
url="https://doi.org/10.1007/s10462-025-11487-1"
}


@inproceedings{10.1007/978-981-96-6432-0_21,
 abstract = {This study provides the privacy concerns of AI predictive algorithms for Ehealth systems. A significant disadvantage is that these algorithms can infer delicate private health data of people, particularly high-profile figures, from big datasets. This might be infringing privacy and results in discrimination or safety threats. The paper additionally analyzes the danger of AI prediction algorithms escalating wider privacy violation risks for patients and providers like accidental disclosure of personal details or unauthorized use of system vulnerabilities for information theft via AI models. The mixed-method methodology encompasses evaluation of AI algorithm abilities, privacy breach case studies, expert interviews, healthcare provider surveys, and eHealth method penetration tests. The results plot vulnerabilities; risk levels; and technical, cultural, and regulatory variables related to these privacy risks. To lessen those risks, a framework is suggested that has specialized safeguards including AI auditing and differing privacy, governance (data security policies and ethical AI guidelines), organizational (devoted privacy roles and staff training) along with ethical considerations and balance innovation with privacy protection. Lastly, the study suggests multi-stakeholder, strategic and collaborative interaction among healthcare, policymakers, AI designers, and patient advocates to mitigate AI-driven privacy issues in eHealth systems through serious scrutiny and suggestions guided by this vision.},
 address = {Singapore},
 author = {Tahenni, Abdellah
and Belkhir, Abdelkader},
 booktitle = {Proceedings of Tenth International Congress on Information and Communication Technology},
 editor = {Yang, Xin-She
and Sherratt, Simon
and Dey, Nilanjan
and Joshi, Amit},
 isbn = {978-981-96-6432-0},
 pages = {265--285},
 publisher = {Springer Nature Singapore},
 title = {Safeguarding Privacy of Sensitive E-health Data Against AI Predictive Algorithm Threats},
 url = {https://link.springer.com/chapter/10.1007/978-981-96-6432-0_21},
 year = {2025}
}

@Inbook{Rehan2025,
author="Rehan, Syed",
title="Understanding Attacker Tactics and Framework-Aligned Defense Strategies in the Cloud Era of AI",
bookTitle="Cybersecurity with AWS: Fortifying Digital Frontiers",
year="2025",
publisher="Apress",
address="Berkeley, CA",
pages="73--113",
abstract="In today's world threats to cybersecurity are everywhere and getting more complex all the time. Knowledge of attacker ``tactics, techniques, and procedures'' (TTPs) is essential to a strong defense. This chapter describes these TTPs in detail, spotlighting the reconnaissance threat actors do and the access they achieve in order to move through networks and either take out or make off with vital data. The work threat actors do at each stage from the very first sign of reconnaissance to the last, big act of impact is discussed so that you can have both the technical and the strategic picture of modern cyber operations.",
isbn="979-8-8688-1554-6",
doi="10.1007/979-8-8688-1554-6_3",
url="https://doi.org/10.1007/979-8-8688-1554-6_3"
}


@inproceedings{10.1007/978-3-031-36822-6_33,
 abstract = {The automated code evaluation system is designed to reliably evaluate user-submitted code. Code is first compiled and then tested on a homogeneous surface using defined input and output test cases. Automated code evaluation systems are gaining popularity due to their wide range of applications and valuable accumulated resources. The success of machine learning techniques emboldens researchers to use them for source code analysis tasks, and a large number of real-life solution codes from automated evaluation systems adds significant value. In this paper, we review the state-of-the-art of automated code evaluation systems and their resources for code analysis tasks using machine learning. We classify these code evaluation systems into several categories, including programming contests, programming learning, recruitment, online compilers, and additional modules of other systems. We research the datasets available in these systems for code analysis. Moreover, we summarize the machine learning-based code assessment tasks, including error detection, code comprehension, review, search and representation, refactoring, and repair using these datasets.},
 address = {Cham},
 author = {Rahman, Md. Mostafizer
and Watanobe, Yutaka
and Hamada, Mohamed},
 booktitle = {Advances and Trends in Artificial Intelligence. Theory and Applications},
 editor = {Fujita, Hamido
and Wang, Yinglin
and Xiao, Yanghua
and Moonis, Ali},
 isbn = {978-3-031-36822-6},
 pages = {385--396},
 publisher = {Springer Nature Switzerland},
 title = {A Survey on Automated Code Evaluation Systems and Their Resources for Code Analysis},
 url = {https://link.springer.com/chapter/10.1007/978-3-031-36822-6_33},
 year = {2023}
}

@Article{Ajibode2025,
author="Ajibode, Adekunle
and Bangash, Abdul Ali
and Cogo, Filipe R.
and Adams, Bram
and Hassan, Ahmed E.",
title="Towards semantic versioning of open pre-trained language model releases on hugging face",
journal="Empirical Software Engineering",
year="2025",
month="Mar",
day="06",
volume="30",
number="3",
pages="78",
abstract="The proliferation of open Pre-trained Language Models (PTLMs) on model registry platforms like Hugging Face (HF) presents both opportunities and challenges for companies building products around them. Similar to traditional software dependencies, PTLMs continue to evolve after a release. However, the current state of release practices of PTLMs on model registry platforms are plagued by a variety of inconsistencies, such as ambiguous naming conventions and inaccessible model training documentation. Given the knowledge gap on current PTLM release practices, our empirical study uses a mixed-methods approach to analyze the releases of 52,227 PTLMs on the most well-known model registry, HF. Our results reveal 148 different naming practices for PTLM releases, with 40.87{\%} of changes to model weight files not represented in the adopted name-based versioning practice or their documentation. In addition, we identified that the 52,227 PTLMs are derived from only 299 different base models (the modified original models used to create 52,227 PTLMs), with Fine-tuning and Quantization being the most prevalent modification methods applied to these base models. Significant gaps in release transparency, in terms of training dataset specifications and model card availability, still exist, highlighting the need for standardized documentation. While we identified a model naming practice explicitly differentiating between major and minor PTLM releases, we did not find any significant difference in the types of changes that went into either type of releases, suggesting that major/minor version numbers for PTLMs often are chosen arbitrarily. Our findings provide valuable insights to improve PTLM release practices, nudging the field towards more formal semantic versioning practices.",
issn="1573-7616",
doi="10.1007/s10664-025-10631-3",
url="https://doi.org/10.1007/s10664-025-10631-3"
}


@Inbook{Rosen2024,
author="Rosen, Amanda M.",
title="Solving Teaching and Learning Challenges",
bookTitle="Teaching Political Science:  A Practical Guide for Instructors",
year="2024",
publisher="Springer International Publishing",
address="Cham",
pages="231--268",
abstract="This chapter acts as a type of FAQ for faculty seeking practical tips on how to respond to challenges in the classroom. The first half offers a five step strategy for diagnosing a problem and identifying and applying solutions, as there is no way to anticipate every classroom issue. The second half dives into challenges divided into four areas: classroom technology, including AI concerns; behavioral issues; motivating students; and the role of the instructor.",
isbn="978-3-031-58290-5",
doi="10.1007/978-3-031-58290-5_9",
url="https://doi.org/10.1007/978-3-031-58290-5_9"
}


@Inbook{Langer2025,
author="Langer, Arthur M.",
title="Cybersecurity in Analysis and Design",
bookTitle="Analysis and Design of Next-Generation Software Architectures: Generative AI, Cybersecurity, and Cloud Computing",
year="2025",
publisher="Springer Nature Switzerland",
address="Cham",
pages="183--204",
abstract="The overall challenge in building more resilient applications that are better equipped to protect against threats is a decision that must address exposure coupled with risk. The general consensus is that no system can be 100{\%} protected and that this requires important decisions when analysts are designing applications and systems. Indeed, security access is not just limited to getting into the system, rather on the individual application level as well. How then do analysts participate in the process of designing secure applications through good design? We know that many cybersecurity architectures are designed from the office of the chief information security officer or CISO, a new and emerging role in organizations. The CISO role, often independent of the CIO (chief information officer) became significant as a result of the early threats from the internet, the 9/11 attacks and most recently the abundant number of system compromises experienced by companies such as JP Morgan Chase, SONY, Home Depot and Target to name just a few.",
isbn="978-3-031-76212-3",
doi="10.1007/978-3-031-76212-3_9",
url="https://doi.org/10.1007/978-3-031-76212-3_9"
}


@Inbook{Nicoletti2025,
author="Nicoletti, Bernardo",
title="Industrial AI-Driven Support to OT Logistics",
bookTitle="Artificial Intelligence for Logistics 5.0: From Foundation Models to Agentic AI",
year="2025",
publisher="Springer Nature Switzerland",
address="Cham",
pages="163--177",
abstract="This chapter focuses on the integration of AI and FMs in industrial logistics. It highlights the pivotal role of OT in monitoring and controlling physical processes, tracing its evolution from isolated systems to integrated enterprise networks. Integrating AI with OT revolutionizes warehouse automation and transportation systems, equipping them with advanced capabilities in predictive maintenance, resource optimization, and automated decision-making. The implementation framework centers on four key components: AI-driven digital twins, Industrial Internet of Things (IIoT) integration, natural language interfaces, and comprehensive data analytics [M{\"o}ller et al., 2021 IEEE International Conference on Electro Information Solutions (EIT) (pp. 413--418). IEEE. https://doi.org/10.1109/EIT51626.2021.9491874(2021)]. This framework enables real-time monitoring, control, and optimization of logistics processes. The chapter also addresses the significant challenges in cyber security and underscores the importance of protecting against data breaches, hostile attacks, and logistics vulnerabilities. It outlines strategic approaches to implementing FMs in organizations and emphasizes the importance of ethical leadership, change management, and ongoing maintenance. It recommends a hybrid approach that combines proprietary and public data for optimal model training. Implementing AI in logistics requires careful planning, significant investment in solutions and skilled personnel, and robust cybersecurity measures to protect sensitive operational data.",
isbn="978-3-031-94046-0",
doi="10.1007/978-3-031-94046-0_6",
url="https://doi.org/10.1007/978-3-031-94046-0_6"
}


@inproceedings{10.1007/978-3-031-70906-7_13,
 abstract = {This paper explores the application of zero-shot learning, a novel machine learning approach, in predicting and mitigating cybersecurity threats and defenses. Our research bridges the gap between the theory and practice of using zero-shot learning, a technique that allows models to adapt to unseen scenarios, in anticipating and managing cybersecurity threats. We propose new attack methodologies based on zero-shot learning and discuss the development of robust defense mechanisms. An interdisciplinary approach, combining social engineering, behavioral psychology, and AI governance, underpins our work. Our research also introduces a quantitative model to assess the impact of these emerging threats and a proof-of-concept prototype. In closing, we discuss ethical considerations and advocate for responsible AI practices and robust regulations to prevent misuse. This paper underscores the potential of zero-shot learning to revolutionize cybersecurity practices and preparations for future threats.},
 address = {Cham},
 author = {Srivastava, Aviral
and Sanghavi, Priyansh
and Parmar, Viral
and Rani, Seema},
 booktitle = {Advances in Computing and Data Sciences},
 editor = {Singh, Mayank
and Tyagi, Vipin
and Gupta, P. K.
and Flusser, Jan
and {\"O}ren, Tuncer
and Cherif, Amar Ramdane
and Tomar, Ravi},
 isbn = {978-3-031-70906-7},
 pages = {138--149},
 publisher = {Springer Nature Switzerland},
 title = {Zero-Shot Learning in Cybersecurity: A Paradigm Shift in Attack and Defense Strategies},
 url = {https://link.springer.com/chapter/10.1007/978-3-031-70906-7_13},
 year = {2025}
}

@inproceedings{10.1007/978-3-031-78386-9_17,
 abstract = {Prioritizing tasks is extremely beneficial, but also difficult for software development teams. Assigning priority to tasks is also time-consuming, especially in projects with a high volume of new issues. Consequently, many issues in GitHub are not labelled. An effective priority tool can streamline this process by suggesting priority labels, saving developers' time and enabling faster identification of high-impact product improvements. In this paper we investigate the application of text classification using Transformer models to automatically assign priority labels to software development issues. We used data from the GitHub and Jira vast datasets to develop state-of-the-art machine learning models (Transformers) to automatically classify the priority of text issues. We thoroughly evaluated the generalizability of our models by using issues that are self-tagged by developers in projects that were not part of the training (Out-of-Distribution) and we adapted our models to specific projects by incorporating part of the issues in the training (fine-tuning) to improve performance. Our experiments show that results vary but can reach a performance of correctly labeling 80{\%} of high priority issues in a project. Our results indicate that Transformers have the potential to assist developers in (semi-)automatically assigning priority labels to their issues and therefore reducing overhead. We find that fine-tuning improve significantly the performance by adapting the machine learning models to specific projects, but further research is needed to optimize this approach.},
 address = {Cham},
 author = {Haugerud, Kristian Marison
and Shivashankar, Karthik
and Martini, Antonio},
 booktitle = {Product-Focused Software Process Improvement},
 editor = {Pfahl, Dietmar
and Gonzalez Huerta, Javier
and Kl{\"u}nder, Jil
and Anwar, Hina},
 isbn = {978-3-031-78386-9},
 pages = {255--271},
 publisher = {Springer Nature Switzerland},
 title = {Towards Enhancing Task Prioritization in Software Development Through Transformer-Based Issues Classification},
 url = {https://link.springer.com/chapter/10.1007/978-3-031-78386-9_17},
 year = {2025}
}

@Article{Wang2025,
author="Wang, Hua
and Bai, Xue-Feng
and Cui, Xiu-Tao
and Chen, Gang
and Fan, Guo-Ming
and Wei, Guo-Lian
and Zheng, Ye-Ping
and Wu, Jing-Jing
and Gao, Sheng-Sheng",
title="Symptom Recognition in Medical Conversations Via multi- Instance Learning and Prompt",
journal="Journal of Medical Systems",
year="2025",
month="Aug",
day="20",
volume="49",
number="1",
pages="107",
abstract="With the widespread adoption of electronic health record (EHR) systems, there is a crucial need for automatic extraction of key symptom information from medical dialogue to support intelligent medical record generation. However, symptom recognition in such dialogues remains challenging because (a) symptom clues are scattered across multi-turn, unstructured conversations, (b) patient descriptions are often informal and deviate from standardized terminology, and (c) many symptom statements are ambiguous or negated, making them difficult for conventional models to interpret. To address these challenges, we propose a novel symptom identification approach that combines multi-instance learning (MIL) with prompt-guided attention for fine-grained symptom identification. In our framework, each conversation is treated as a bag of utterances. A MIL-based model aggregates information across utterances to improve recall and pinpoints which specific utterances mention each symptom, thus enabling sentence-level symptom recognition. Concurrently, a prompt-guided attention strategy leverages standardized symptom terminology as prior knowledge to guide the model in recognizing synonyms, implicit symptom mentions, and negations, thereby improving precision. We further employ R-Drop regularization to enhance robustness against noisy inputs. Experiments on public medical-dialogue datasets demonstrate that our method significantly outperforms existing techniques, achieving an 85.93{\%} F1-score (with 85.09{\%} precision and 86.83{\%} recall) --- about 8{\%} points higher than a strong multi-label classification baseline. Notably, our model accurately identifies the specific utterances corresponding to each symptom mention (symptom--utterance pairs), highlighting its fine-grained extraction capability. Ablation studies confirm that the MIL component boosts recall, while the prompt-guided attention component reduces false positives. By precisely locating symptom information within conversations, our approach effectively tackles the issues of dispersed data and inconsistent expressions. This fine-grained symptom documentation capability represents a promising advancement for automated medical information extraction, more intelligent EHR systems, and diagnostic decision support.",
issn="1573-689X",
doi="10.1007/s10916-025-02240-w",
url="https://doi.org/10.1007/s10916-025-02240-w"
}


@Article{Gupta2024,
author="Gupta, Ayan-Yue",
title="Polysemy and the sociolinguistics of policy ideas: resilience, sustainability and wellbeing 2000--2020",
journal="Journal of Computational Social Science",
year="2024",
month="Apr",
day="01",
volume="7",
number="1",
pages="331--360",
abstract="In policy studies, there is a concern with understanding how new ideas affect policymaking. Central to this is the issue of how ideas become collectively adopted by policy actors. The policy paradigm perspective---the classical way of understanding collective adoption---has faced criticism for overestimating the coherence of adopted ideas and not paying sufficient attention to the micro-scale cognitive processes at play during collective adoption and how these are conditioned by macro-scale organisational processes and structures. This paper provides a sociolinguistic account of the collective adoption of policy ideas that explicitly relates micro-scale cognitive processes (interpretation, attention allocation) to macro-scale organisational structure (division of labour). Drawing on relevance theory, it argues that implicit in the diffusion of an idea within policy circles is an organisationally coordinated interpretive process which results in multiple versions of the idea adapted to the division of labour of government. Supporting this is an empirical analysis of the collective adoption of resilience, sustainability and wellbeing by the British government during 2000--2020. Using a dataset of policy documents ({\textasciitilde}{\thinspace}163 million tokens) published by 12 British central departments, I use BERT to automatically extract the different senses expressed by occurrences of `resilience', `resilient', `sustainable', `sustainability' and `wellbeing'. I examine how these senses contribute to changes in the use of this vocabulary, the contents of these senses, and the distribution of these senses across the 12 departments. Through this, I examine senses that express versions of resilience, sustainability and wellbeing adapted to particular departmental functions.",
issn="2432-2725",
doi="10.1007/s42001-023-00238-3",
url="https://doi.org/10.1007/s42001-023-00238-3"
}


@Article{Patel2025,
author="Patel, Viral
and Maltare, Nilesh",
title="From Algorithms to Connectivity: A Comprehensive Review of Traffic Signal Optimization and Communication Based Cooperative Control",
journal="Archives of Computational Methods in Engineering",
year="2025",
month="Nov",
day="05",
abstract="Rapid urbanization and the exponential growth of vehicular traffic have intensified congestion, travel delays, and emissions, posing serious challenges for sustainable urban mobility. To address these issues, numerous studies have explored both algorithmic and communication-based traffic management approaches. However, existing reviews often treat these domains separately, lacking a unified perspective. This paper presents a comprehensive review that bridges algorithmic optimization and communication-enabled cooperative control. The study systematically categorizes traffic signal optimization algorithms including fixed time, actuated, adaptive, fuzzy logic, genetic, reinforcement learning and game theory based methods and communication-driven strategies such as Vehicle-to-Vehicle (V2V) {\&} Vehicle-to-Infrastructure (V2I), IoV / RSU / Edge / Fog / SDN, CAV Coordination {\&} Reservation Systems, AI/ML{\thinspace}+{\thinspace}Communication Fusion and Safety {\&} Perception Enhancement. A PRISMA process has been followed for the selection of the papers and selected papers were analyzed to evaluate performance metrics, limitations, and research trends. The findings reveal that adaptive control and reinforcement learning, particularly deep and multi-agent RL models, dominate algorithmic research, while CAV coordination and IoV frameworks are emerging as key communication paradigms. Persistent challenges include scalability, penetration rate, real-time responsiveness, and limited real-world validation. The paper's unique contribution lies in synthesizing these two research streams to propose a hybrid, future ready architecture that integrates local adaptive intelligence with global, communication-based coordination providing a strategic roadmap toward sustainable, intelligent traffic systems.",
issn="1886-1784",
doi="10.1007/s11831-025-10455-w",
url="https://doi.org/10.1007/s11831-025-10455-w"
}


@Inbook{Pan2025,
author="Pan, Hanxi
and Xu, Wei
and Shen, Mowei
and Gao, Zaifeng",
editor="Xu, Wei",
title="Human-Centered Artificial Social Intelligence (HC-ASI)",
bookTitle="Handbook of Human-Centered Artificial Intelligence",
year="2025",
publisher="Springer Nature Singapore",
address="Singapore",
pages="1--51",
abstract="As artificial intelligence systems become increasingly integrated into human social contexts, artificial social intelligence (ASI) has emerged as a critical capability that enables AI to perceive, understand, and engage meaningfully in complex human social interactions. This chapter introduces a comprehensive framework for human-centered artificial social intelligence (HC-ASI), built upon the technology-human factors-ethics (THE) triangle, which systematically addresses both technical foundations and human-centered design principles necessary for developing socially intelligent AI systems. This chapter provides a comprehensive overview of current ASI research. This chapter begins by establishing the theoretical foundations of ASI, tracing its evolution from classical psychological theories of human social intelligence to contemporary computational models, and then examines the mechanisms underlying human--AI social interaction with particular emphasis on establishing shared social understanding and appropriate role positioning. The chapter further explores ASI's practical implications for individuals and groups through comprehensive evaluation frameworks that combine technical benchmarks with human-centered experiential assessments, demonstrating real-world applications through detailed case studies spanning healthcare, companionship, education, and customer service domains. Building on the overview and the framework of HC-ASI, this chapter articulates core HC-ASI design principles and translates them into actionable methodologies and implementation guidelines that provide practical guidance for researchers and practitioners. This chapter concludes with a critical discussion of current challenges and promising directions for developing comprehensive HC-ASI ecosystems.",
isbn="978-981-97-8440-0",
doi="10.1007/978-981-97-8440-0_110-1",
url="https://doi.org/10.1007/978-981-97-8440-0_110-1"
}


@Inbook{Parasuraman2024,
author="Parasuraman, Banu",
title="Conversational AI with Spring AI",
bookTitle="Mastering Spring AI: The Java Developer's Guide for Large Language Models and Generative AI",
year="2024",
publisher="Apress",
address="Berkeley, CA",
pages="181--249",
abstract="Many businesses and organizations rely on basic chatbots for customer interactions. However, these simple rule-based systems often fall short, frustrating users with their limited understanding and rigid responses. Traditional chatbots struggle to comprehend context, handle complex queries, or engage in nuanced dialogue, leaving customers feeling unheard and dissatisfied.",
isbn="979-8-8688-1001-5",
doi="10.1007/979-8-8688-1001-5_5",
url="https://doi.org/10.1007/979-8-8688-1001-5_5"
}


@Article{Li2026,
author="Li, Hui
and Ji, Shuping
and Li, Yang
and Qiao, Yujie
and Sui, Huayi
and Tang, Zhen
and Chen, Wei
and Qin, Zheng
and Wang, Wei
and Zhong, Hua
and Huang, Tao",
title="AutoCache: an online and automatic caching solution for Spark",
journal="Frontiers of Computer Science",
year="2026",
month="Jan",
day="13",
volume="20",
number="5",
pages="2005108",
abstract="Apache Spark is one of the most popular in-memory distributed computing frameworks for processing large-scale datasets, and caching is indispensable for improving the performance of different Spark applications. However, proper cache usage in Spark is non-trivial. Developers must cache the hot data manually and evict unnecessary data timely in their applications to achieve better performance. This requires deep understanding and sufficient experience. Otherwise, the wrong caching decisions can lead to performance degradation, application bugs, and even system crashes. To overcome these challenges, we propose AutoCache, in a non-intrusive manner, which means it can identify the hot datasets and cache them automatically during the execution of a workload without changing any application code. For a given Spark application, AutoCache first parses the execution paths of the application and then analyzes the data references based on the DAG maintained within Spark. After that, AutoCache heuristically identifies the datasets, in the form of RDDs, that would be accessed multiple times at run time. Along with the application's execution, AutoCache automatically caches and evicts the RDDs by invoking Spark's underlying APIs on the fly. We evaluate AutoCache by using an open-source benchmark that contains various applications. Our experimental results show that AutoCache can significantly improve the performance of real-world applications and obviously outperform related work. Moreover, by comparing the caching decisions of AutoCache with existing manual written caching logics in these applications, nine previously unknown caching-related issues are detected, all of them have been confirmed and five of them have been fixed by related developers. This constructs another strong proof of the effectiveness of AutoCache.",
issn="2095-2236",
doi="10.1007/s11704-025-40776-9",
url="https://doi.org/10.1007/s11704-025-40776-9"
}


@Inbook{Weippl2024,
author="Weippl, Edgar
and Schrittwieser, Sebastian",
editor="Werthner, Hannes
and Ghezzi, Carlo
and Kramer, Jeff
and Nida-R{\"u}melin, Julian
and Nuseibeh, Bashar
and Prem, Erich
and Stanger, Allison",
title="Introduction to Security and Privacy",
bookTitle="Introduction to Digital Humanism: A Textbook",
year="2024",
publisher="Springer Nature Switzerland",
address="Cham",
pages="397--414",
abstract="This chapter on Security and Privacy builds on two aspects central to digital humanism: (1) self-determination and (2) humans in the center.",
isbn="978-3-031-45304-5",
doi="10.1007/978-3-031-45304-5_26",
url="https://doi.org/10.1007/978-3-031-45304-5_26"
}


@Article{Long2025,
author="Long, Robert
and Sebo, Jeff
and Sims, Toni",
title="Is there a tension between AI safety and AI welfare?",
journal="Philosophical Studies",
year="2025",
month="Jul",
day="01",
volume="182",
number="7",
pages="2005--2033",
abstract="The field of AI safety considers whether and how AI development can be safe and beneficial for humans and other animals, and the field of AI welfare considers whether and how AI development can be safe and beneficial for AI systems. There is a prima facie tension between these projects, since some measures in AI safety, if deployed against humans and other animals, would raise questions about the ethics of constraint, deception, surveillance, alteration, suffering, death, disenfranchisement, and more. Is there in fact a tension between these projects? We argue that, considering all relevant factors, there is indeed a moderately strong tension---and it deserves more examination. In particular, we should devise interventions that can promote both safety and welfare where possible, and prepare frameworks for navigating any remaining tensions thoughtfully.",
issn="1573-0883",
doi="10.1007/s11098-025-02302-2",
url="https://doi.org/10.1007/s11098-025-02302-2"
}


@Article{Elias2025,
author="Elias, Silvia
and Alshammari, Bunder Sebail
and Alfraidi, Khaled Nasser
and Karam, Khaled Mostafa",
title="Rethinking literary creativity in the digital age: a comparative study of human versus AI playwriting",
journal="Humanities and Social Sciences Communications",
year="2025",
month="May",
day="20",
volume="12",
number="1",
pages="689",
abstract="This paper contends that, in the digital era, the creation of art is no longer an endeavor exclusively pursued by humans to achieve a creative product. Over the past decade, computer-generated theater has emerged and progressed significantly through successive projects. This advancement has incited debate about whether these AI-generated works possess literary merit and originality comparable to Human-authored texts. Therefore, this interdisciplinary study aims to draw a comparison between an AI-generated play and a human-authored play in terms of originality, fluency, flexibility, and effectiveness. It utilizes Computational methods and NLP tools to process the two plays, analyze both content and language, and derive quantitative measures that support the creativity assessment of the two plays. The results of content and computational analysis indicate that the human-generated play has higher scores in all indexes of creativity. However, the results also suggest that the AI-generated play features significant creativity potential close in assessment to the human proficiency in several indexes. Thus, AI is capable of creative literary products, though it is not as masterful as those produced by creative humans.",
issn="2662-9992",
doi="10.1057/s41599-025-04999-2",
url="https://doi.org/10.1057/s41599-025-04999-2"
}


@Article{Amigud2024,
author="Amigud, Alexander",
title="The Age of the Intelligent Machine: Singularity, Efficiency, and Existential Peril",
journal="Philosophy {\&} Technology",
year="2024",
month="Apr",
day="03",
volume="37",
number="2",
pages="49",
abstract="Machine learning, and more broadly artificial intelligence (AI), is a fascinating technology and can be considered as the closest approximation to the Cartesian ``thinking thing'' that humans have ever created. Just as the industrial revolution required a new ethos, the age of intelligent machines will create its own, challenging the established moral, economic, and political presuppositions. This paper discusses the relationship between AI and society; it presents several thought experiments to explore the complexity of the relationship and highlights the insufficiency of the current normative paradigm in addressing technological expansion. I argue that many of the externalities, such as security risks, loss of privacy, and economic instability  will result from trying to fit the emerging technologies into the existing frame of efficiency and utility, by redefining the notions of human value, identity, autonomy, purity, and truth, among others. The age of the intelligent machine is elevating alienation to new levels, treating the individual as mere patterns in data---its primary commodity. I further argue that while the possibility of unintended consequences, due to the potential misuse of AI is ever present, the intelligent machine per se is unlikely to engage in a zero-sum game for power on its own initiative. I question whether singularity is at all attainable and argue that technology will forever remain a proxy for human interests. I conclude by posing questions for charting the path forward. Through this analysis, I aim to provide a more nuanced understanding of the complex relationship between the AI and humans.",
issn="2210-5441",
doi="10.1007/s13347-024-00740-0",
url="https://doi.org/10.1007/s13347-024-00740-0"
}


@Article{Sung2025,
author="Sung, Minkyoung
and Park, Eunhye
and Choi, Yeongwoo
and Hwang, Jisun
and Oh, Eok-Soo",
title="Regional Differences in Structure, Function, and Inflammatory Response of the Large Intestine",
journal="Innovations in Acupuncture and Medicine",
year="2025",
month="Jun",
day="20",
volume="18",
number="1",
pages="6",
abstract="The large intestine is anatomically divided into several distinct segments: the cecum, ascending colon, transverse colon, descending colon, sigmoid colon and rectum. While the structural and functional differences between these segments are well-recognized, the specific functional distinctions remain insufficiently explored. Recent studies indicate that regional variations in the colon are closely linked to differences in its primary functions and, more notably, to the development of colon-associated diseases. Therefore, understanding the precise localization of the colon is crucial for elucidating disease onset, prognosis, and therapeutic strategies. This review focuses on understanding the developmental differences, structural distinctions, and resulting functional alterations of the colon depending on the location. This will deepen our understanding of colonic function and regulatory mechanisms and improve our knowledge of colonic pathogenesis, and therapeutic approaches for various colonic diseases.",
issn="3059-4049",
doi="10.1186/s44424-025-00006-2",
url="https://doi.org/10.1186/s44424-025-00006-2"
}


@Inbook{Heimgärtner2025,
author="Heimg{\"a}rtner, R{\"u}diger",
editor="Xu, Wei",
title="Intercultural Design for Human-Centered AI Solutions",
bookTitle="Handbook of Human-Centered Artificial Intelligence",
year="2025",
publisher="Springer Nature Singapore",
address="Singapore",
pages="1--65",
abstract="Based on and inspired by the approach of human-centered AI (HCAI), intercultural human-centered AI (IHCAI) examines how AI systems are designed, developed, and deployed to respect cultural diversity and serve global user populations. This chapter synthesizes foundational concepts and cultural models to explain how expectations about communication, decision-making, and interaction shape human--AI experiences. Building on these foundations, it translates concepts into actionable methods for design and evaluation, including localization, culturally adaptive interfaces, multilingual interaction, and participatory design with diverse stakeholders. The chapter addresses critical challenges in global deployment such as dataset bias, conflicting ethical norms across regions, and the balance between standardization and localization. It also discusses ethics, fairness, and cross-cultural governance. It analyzes opportunities and risks in large language models, including bias, unequal language performance, culturally aware explainability, and adaptation. Applications across healthcare, education, and business illustrate how culturally sensitive design improves acceptance and effectiveness. The chapter concludes with guiding principles, success factors, and metrics for usability, trust, inclusion, and cultural appropriateness. It proposes a roadmap for scalable cultural intelligence, real-time adaptation, and evidence-based evaluation equipping researchers, practitioners, and policymakers with a coherent framework and practical guidance to create AI that is globally usable, locally appropriate, and aligned with diverse values.",
isbn="978-981-97-8440-0",
doi="10.1007/978-981-97-8440-0_77-2",
url="https://doi.org/10.1007/978-981-97-8440-0_77-2"
}


@Article{Akpoghelie2025,
author="Akpoghelie, Patrick Othuke
and Edo, Great Iruoghene
and Gaaz, Tayser Sumer
and Akpoghelie, Emmanuel Oghenekome
and Iwanegbe, Izuwa
and Owheruo, Joseph Oghenewogaga
and Igbuku, Ufuoma Augustina
and Yousif, Emad
and Oghroro, Ephraim Evi Alex
and Obayomi, Oluwatobi Victoria
and Essaghah, Arthur Efeoghene Athan
and Ahmed, Dina S.
and Ozsahin, Dilber Uzun
and Umar, Huzaifa",
title="Mycotoxins in food and agriculture: from occurrence to economic consequences",
journal="Archives of Microbiology",
year="2025",
month="Nov",
day="05",
volume="208",
number="1",
pages="4",
abstract="Filamentous fungus (moulds) create secondary metabolites called mycotoxins, which can be dangerous. These minuscule molecules (typically less than 1 000 Da) are almost inescapably found in nature. Aspergillus, Penicillium, Fusarium, and Alternaria are among the fungal species that produce mycotoxins. If these secondary metabolites make their way into the food chain and production, they are harmful and have a big effect. In African countries like Nigeria that rely significantly on staple foods like grains, cereals, and nuts, mycotoxin contamination is especially problematic. Mycotoxins have attracted international interest due to their effects on human health, significant financial losses, and both domestic and international trade. Due to concerns about food safety and financial losses, the majority of research has been on aflatoxins, ochratoxin A, fumonisin, zearalenone, and Fusarium toxins, despite the fact that over 400 mycotoxins have been found. Understanding how interacting abiotic factors connected to climate change---particularly rising temperatures, higher CO2, and fluctuations in water availability---affect the relative risks of mycotoxin contamination and the effects on food safety and security especially in Africa is therefore of great interest. For food safety and a sustainable food supply, mycotoxin contamination must be avoided. Stricter regulations and enforcement, post-harvest management, preventative agricultural practices, infrastructure and policy improvements, increased surveillance and awareness, and investments in cost-effective detection techniques and research are all suggested as ways to address mycotoxin issues in Africa. In order to address mycotoxin prevalence, governments, research institutions, and non-governmental organisations should customise the few resources at their disposal. This will provide the best chance for the successful establishment of a sustainable food system in Africa. Data from thorough investigations of mycotoxins in food commodities are expected to aid in the creation of safer food and in determining the areas that require further study.",
issn="1432-072X",
doi="10.1007/s00203-025-04559-x",
url="https://doi.org/10.1007/s00203-025-04559-x"
}


@Article{He2024,
author="He, Junda
and Xu, Bowen
and Yang, Zhou
and Han, DongGyun
and Yang, Chengran
and Liu, Jiakun
and Zhao, Zhipeng
and Lo, David",
title="PTM4Tag+: Tag recommendation of stack overflow posts with pre-trained models",
journal="Empirical Software Engineering",
year="2024",
month="Nov",
day="20",
volume="30",
number="1",
pages="28",
abstract="Stack Overflow is one of the most influential Software Question {\&} Answer (SQA) websites, hosting millions of programming-related questions and answers. Tags play a critical role in efficiently organizing the contents on Stack Overflow and are vital to support various site operations, such as querying relevant content. Poorly chosen tags often lead to issues such as tag ambiguity and tag explosion. Therefore, a precise and accurate automated tag recommendation technique is needed. Inspired by the recent success of pre-trained models (PTMs) in natural language processing (NLP), we present PTM4Tag+, a tag recommendation framework for Stack Overflow posts that utilize PTMs in language modeling. PTM4Tag+ is implemented with a triplet architecture, which considers three key components of a post, i.e., Title, Description, and Code, with independent PTMs. We utilize a number of popular pre-trained models, including BERT-based models (e.g., BERT, RoBERTa, CodeBERT, BERTOverflow, and ALBERT), and encoder-decoder models (e.g., PLBART, CoTexT, and CodeT5). Our results show that leveraging CodeT5 under the PTM4Tag+ framework achieves the best performance among the eight considered PTMs and outperforms the state-of-the-art Convolutional Neural Network-based approach by a substantial margin in terms of average Precision@k, Recall@k, and F1-score@k (k ranges from 1 to 5). Specifically, CodeT5 improves the performance of F1-score@1-5 by 8.8{\%}, 12.4{\%}, 15.3{\%}, 16.4{\%}, and 16.6{\%}, respectively. Moreover, to address the concern with inference latency, we experimented PTM4Tag+ using smaller PTM models (i.e., DistilBERT, DistilRoBERTa, CodeBERT-small, and CodeT5-small). We find that although smaller PTMs cannot outperform larger PTMs, they still maintain over 93.96{\%} of the performance on average while reducing the mean inference time by more than 47.2{\%}.",
issn="1573-7616",
doi="10.1007/s10664-024-10576-z",
url="https://doi.org/10.1007/s10664-024-10576-z"
}


@Article{Althobaiti2025,
author="Althobaiti, Kholoud
and Tahaei, Mohammad",
title="Understanding phishing discussions on stack overflow and information security stack exchange",
journal="Scientific Reports",
year="2025",
month="Dec",
day="23",
volume="16",
number="1",
pages="3555",
abstract="Phishing remains a prevalent cybersecurity threat. Given its impact, it is important to understand how technically skilled users interpret and respond to such threats. This paper examines how developers and security professionals discuss phishing on Stack Overflow (SO) and Information Security (IS) Stack Exchange in order to understand their concerns, pain points, and investigative practices. We qualitatively analyzed 140 phishing-related questions (60 from SO and 80 from IS) using inductive open coding and developed the Developer Phishing Engagement Framework, which organizes developer activities into four layers: prevention, detection and reporting, mitigation, and planning. Across the two platforms, we find complementary emphases: SO posts focus on implementation hurdles, false positives, and the usability of defenses, whereas IS posts concentrate on post-incident analysis, impact, and ethical considerations around phishing simulations. Developers demonstrate a strong threat mindset but still face workflow friction caused by inconsistent organizational practices, opaque anti-phishing tools, and security measures that conflict with legitimate workflows. Our findings contribute a developer-centered view of phishing that complements existing user-focused models and provides guidance for designing more realistic anti-phishing tools, training, and organizational policies.",
issn="2045-2322",
doi="10.1038/s41598-025-33568-5",
url="https://doi.org/10.1038/s41598-025-33568-5"
}


@Article{AbediRad2025,
author="Abedi Rad, Razieh
and Yamaghani, Mohammad Reza
and Nourbakhsh, Azamossadat",
title="A survey of sentiment analysis methods based on graph neural network",
journal="International Journal of Data Science and Analytics",
year="2025",
month="Apr",
day="01",
volume="19",
number="3",
pages="389--417",
abstract="Sentiment analysis is an active research field as one of the most popular tasks of natural language processing, which aims to extract valuable information from various social platforms and extensive online texts to process and find people's attitudes in business and advertising, government, economic fields, and even political orientations. Hence, researchers have made many efforts in this field, which mainly refer to traditional approaches based on dictionaries, machine learning, and deep learning models. Although deep learning methods have achieved promising results in this field, due to problems such as assigning indecisive weights and high dimensions in feature extraction stages, they are still a ``black box.'' Meanwhile, graph neural networks (GNNs) are a particular type of deep neural network that are interpretable and flexible. Their adaptability in solving complex problems in data analysis with a graph structure has made them one of the most efficient methods in the last decade. Considering the large amount of textual information in social media and various online platforms, sentiment analysis or opinion mining aims to help marketing strategies for business owners and awareness of the attitude of public opinion in governments has become one of the crucial issues in today's modern societies. This comprehensive review provides an in-depth analysis of new GNN-based approaches to the sentiment analysis tasks. We provided summary of recent state-of-the-art GNN-based approaches in sentiment analysis based on specific dataset used in each method. Also, we compared GNN-based models and discussed about their advantages and disadvantages. Our central goal is to show the potential, research motivation and foundational characteristics of GNN-based approaches compared with other existing approaches in this field. Demonstrating their recent development process in sentiment analysis tasks will help to find more effective directions for researchers interested in this field. Finally, we discussed about key challenges of GNNs methods in sentiment analysis problems and stated some gap research for future directions.",
issn="2364-4168",
doi="10.1007/s41060-025-00714-1",
url="https://doi.org/10.1007/s41060-025-00714-1"
}


@Article{ref1,
title="Selected Abstracts from The Royal College of Ophthalmologists Annual Congress 2024, Belfast, UK",
journal="Eye",
year="2024",
month="Nov",
day="01",
volume="38",
number="3",
pages="121--201",
issn="1476-5454",
doi="10.1038/s41433-024-03254-3",
url="https://doi.org/10.1038/s41433-024-03254-3"
}


@Article{Jung2025,
author="Jung, Han Wool
and Park, Jin Young
and Holoubek, Todd
and Kim, Woo Jung
and Park, Jaesub",
title="Socially Assistive Robots in Mental Healthcare: Principles and Conceptual Framework for User-Centered Design",
journal="International Journal of Social Robotics",
year="2025",
month="Nov",
day="01",
volume="17",
number="11",
pages="2827--2851",
abstract="Socially assistive robots (SARs) have a strong potential to advance digital mental healthcare, if they become able to promote meaningful interactions based on the user-centered approach. This review provides a comprehensive overview of user-centered design principles for SARs in mental healthcare, focusing on contemporary topics and practical guidelines. Successful SARs should enhance user autonomy, competence, and emotional experiences as their core objectives. Effective SAR design may integrate adaptive decision-making based on multimodal interactions, adequate motivation such as reward systems or gamification, contextual design, and participatory design for the personalized care that aligns with user needs. For long-term interactions, careful role setting such as mimicking human relationships and trust building are also recommended for in-depth user care. Evaluation of SARs incorporates holistic, multidimensional criteria including self-reports, behavioral data, physiological signals, and qualitative examinations to measure treatment effectiveness, user experiences, and safety, which ideally lead to iterative evaluation and refinement processes. Finally, recent breakthroughs in large language models (LLMs) have the potential to significantly boost SAR autonomy, level of personalization, and user engagement, but they also raise ethical risks such as user overdependence and compromised user autonomy, which may be addressed through person-centered principles, rigorous ethical/regulatory oversight, and evidence-based validation. Future roadmaps for SARs in mental healthcare emphasize integrated guidelines, responsible AI governance, and continued interdisciplinary collaboration to ensure safe and effective therapeutic care.",
issn="1875-4805",
doi="10.1007/s12369-025-01323-5",
url="https://doi.org/10.1007/s12369-025-01323-5"
}


@Inbook{Timmers2024,
author="Timmers, Paul",
editor="Werthner, Hannes
and Ghezzi, Carlo
and Kramer, Jeff
and Nida-R{\"u}melin, Julian
and Nuseibeh, Bashar
and Prem, Erich
and Stanger, Allison",
title="Sovereignty in the Digital Age",
bookTitle="Introduction to Digital Humanism: A Textbook",
year="2024",
publisher="Springer Nature Switzerland",
address="Cham",
pages="571--592",
abstract="The century-old concept of state sovereignty is acquiring new and hotly debated meaning, due to digital disruption and technology-without-borders, dominance by powerful---often foreign-owned---global tech companies, and cyber-undermining by malicious states. Sovereignty, as we know it, is also threatened by rising geopolitical tensions, war, and global challenges such as climate change, pandemics, and global cyber-crime. This chapter deals with the future of sovereignty in a digital and geopolitically contested age. It starts with an introduction into international relations, sovereignty, and strategic autonomy thinking. It reflects on the impact of digital technology on the international system of states. Then the chapter provides an analysis and some practical guidance to tackle the challenges of developing public policy for sovereignty in the digital, and digital humanistic, age. Finally, two case studies and a set of questions invite the reader to a deeper dive.",
isbn="978-3-031-45304-5",
doi="10.1007/978-3-031-45304-5_36",
url="https://doi.org/10.1007/978-3-031-45304-5_36"
}


@Article{Bai2025,
author="Bai, Ying
and Ren, Hui
and Leng, Shuo
and Yuan, Mengqin
and Jiang, YiXin
and Zhang, Shenyang
and Wang, Yu
and Ju, Minzi
and Wang, Zhi
and Xi, Wen
and Xu, Lian
and Zheng, Bingjing
and Li, Daxing
and Huo, Xinchen
and Zhu, Tianhao
and Zhang, Beicheng
and Shen, Ling
and Zhang, Yuan
and Jiang, Wei
and Zhang, John H.
and Han, Bing
and Yao, Honghong",
title="CD8+GZMK+CD27+CCR7+ T cells mobilized by splenic sympathetic nerves aggravate brain ischemia‒reperfusion injury via CCL19-positive endothelial cells",
journal="Cellular {\&} Molecular Immunology",
year="2025",
month="Sep",
day="01",
volume="22",
number="9",
pages="1061--1076",
abstract="Splenic sympathetic activity critically modulates peripheral immunity after ischemic stroke, thus intervention in spleen sympathetic activity represents a promising therapeutic strategy for stroke. However, the mechanisms underlying spleen-brain-immune axis communication remain poorly understood. Here, we utilized a surgical denervation protocol to perform splenic sympathetic denervation (SDN), which significantly attenuated brain injury following stroke. Through single-cell RNA sequencing, we identified a novel GZMK+CD8+CD27+CCR7+ T-cell subset in patients with acute ischemic stroke (AIS), which we designated stroke-associated T (Tsa) cells. The expansion of Tsa cells was positively correlated with the severity of clinical symptoms and was driven by the splenic sympathetic nervous system. Stroke-induced sympathetic activation triggers the release of splenic norepinephrine (NE), which preferentially signals through ADRB2 on Tsa cells to promote their mobilization. Additionally, ischemic injury induces endothelial cell-specific expression of CCL19, which chemoattracts Tsa cells into the brain parenchyma via their cognate CCR7 receptor, exacerbating neuroinflammatory injury and neurological deficits in a transient middle cerebral artery occlusion (tMCAO) mouse model. We developed a CCR7-targeting peptide to disrupt this chemotactic axis and reduce T-cell infiltration, thereby mitigating brain injury. Our findings highlight SDN as a promising therapeutic strategy to attenuate ischemia‒reperfusion injury and suggest its potential as an adjunctive therapy for reperfusion treatment in AIS patients.",
issn="2042-0226",
doi="10.1038/s41423-025-01311-9",
url="https://doi.org/10.1038/s41423-025-01311-9"
}


@Inbook{SanzDomínguez2023,
author="Sanz Dom{\'i}nguez, Estrella
and Rufi{\'a}n Fern{\'a}ndez, Francisco Jos{\'e}
and Sabrine, Isber",
editor="Oosterman, Naomi
and Yates, Donna",
title="New Security Challenges at Museums and Historic Sites: The Case of Spain",
bookTitle="Art Crime in Context",
year="2023",
publisher="Springer International Publishing",
address="Cham",
pages="103--120",
abstract="Security at museums and heritage sites has been an issue that has traditionally been oriented towards the protection of buildings and their contents against theft, and the evacuation of people in the event of a fire. But nowadays, analysing the wide range of risks and threats that they may be affected by is becoming increasingly complex. While contemporary societies are continually changing in terms never seen before, the risks and threats are increasing and evolving faster. This article aims to analyse how other security-related areas can help to resolve these challenges, for example, the different perspectives prepared by the National Security Plans and Civil Protection Systems, making an analysis of Spain, as an example within the European Union, and how the COVID-19 crisis has made these shortcomings even more evident.",
isbn="978-3-031-14084-6",
doi="10.1007/978-3-031-14084-6_7",
url="https://doi.org/10.1007/978-3-031-14084-6_7"
}


@Inbook{Huang2025,
author="Huang, Ken
and Hughes, Chris",
title="Deploying Agentic AI in Enterprise Environments",
bookTitle="Securing AI Agents: Foundations, Frameworks, and Real-World Deployment",
year="2025",
publisher="Springer Nature Switzerland",
address="Cham",
pages="289--319",
abstract="This chapter explores the challenges of transitioning agentic AI systems from controlled laboratory environments to live enterprise production. Using the continuous case study of a logistics firm deploying an agent named ``Logi-Agent,'' it deconstructs the ``deployment chasm'' between developer optimism and the unforgiving realities of corporate security and compliance. The chapter provides a practical framework for this transition, detailing the necessary evolution of a Security Operations Center (SOC) to become ``agent-aware'' through contextual logging and advanced threat detection. It advocates for the adoption of secure, standardized protocols like MCP for agent-to-tool interaction and A2A for inter-agent communication, and it outlines patterns for safely integrating agents with brittle legacy systems. Central to this framework is the establishment of robust governance, including an Agent Review Board and tiered human-in-the-loop controls to ensure auditable autonomy. Finally, it synthesizes these technical and procedural pillars into a cohesive ``Secure AgentOps'' pipeline and emphasizes the critical importance of organizational change management, including the upskilling of staff to become effective ``agent handlers'' and ``AI detectives.''",
isbn="978-3-032-02130-4",
doi="10.1007/978-3-032-02130-4_10",
url="https://doi.org/10.1007/978-3-032-02130-4_10"
}


@Article{Kim2024,
author="Kim, Hyunjun
and Han, Hwansoo",
title="GPU thread throttling for page-level thrashing reduction via static analysis",
journal="The Journal of Supercomputing",
year="2024",
month="May",
day="01",
volume="80",
number="7",
pages="9829--9847",
abstract="Unified virtual memory was introduced in modern GPUs to enable a new programming model for programmers. This method manages memory pages between the GPU and CPU automatically, reducing the complexity of data management for programmers. However, when a GPU programs generates a large memory footprint that exceeds the GPU memory capacity, thrashing can occur, leading to significant performance degradation. To address this issue, this paper proposes a thread throttling that restricts the active thread groups, thereby alleviating memory oversubscription and improving performance. The proposed method adjusts the active thread group at compile time to ensure that their memory footprints fit within the available memory capacity. The effectiveness of the proposed method was evaluated using GPU programs that experience memory oversubscription. The results showed that our approach improved the performance of the original programs by 3.44{\$}{\$}{\backslash}times{\$}{\$}on average. This represents a 1.53{\$}{\$}{\backslash}times{\$}{\$}performance improvement compared to static thread throttling.",
issn="1573-0484",
doi="10.1007/s11227-023-05787-y",
url="https://doi.org/10.1007/s11227-023-05787-y"
}


@Article{Boakye2025,
author="Boakye, Kwadwo Owusu
and Dodd, Matt
and Asante, Maxwell Darko
and Logah, Vincent
and Darko, Godfred",
title="Metal Contamination and Health Risk in Paddy Soils and Rice from Ghana's Semi-Deciduous Forest Zone",
journal="Chemistry Africa",
year="2025",
month="Nov",
day="01",
volume="8",
number="9",
pages="4655--4676",
abstract="Food safety, a critical aspect of sustainable global development, is threatened by metal contamination in crops, particularly rice, which imperils food security and human health. This research investigated the concentrations, bioaccessibility, and associated human health risks of metals in rice from six paddy fields in Ghana's semi-deciduous forest zone. The concentrations of potentially toxic metals in soils and rice grains were analysed by X-ray fluorescence spectrometry and inductively coupled plasma mass spectrometry. Potentially toxic elements, (PTEs) concentrations in soil samples ranged from: arsenic{\thinspace}<{\thinspace}1.33 -- 766 mg/kg; chromium{\thinspace}<{\thinspace}5.22 -- 418 mg/kg; copper{\thinspace}<{\thinspace}5.72 -- 1390 mg/kg, Pb{\thinspace}<{\thinspace}1.84 -- 7.04 mg/kg, and Zn 8.67 -- 633 mg/kg while cadmium, mercury and nickel were mostly below detection. Soil pollution indices revealed low contamination in chromium, copper, lead, and zinc and considerable contamination in arsenic. Elevated metal levels may be attributed to the use of agrochemicals and mine-impacted water for irrigation. The mean concentrations of arsenic and lead in rice grains exceeded the WHO/FAO Codex Alimentarius standards (0.20 mg/kg). Bioaccumulation and translocation factors showed low metal accumulation in rice, except for Zn. The human health risk assessment, incorporating bioaccessibility data, indicated a hazard index of less than 1 for all metals, suggesting minimal potential non-carcinogenic risks to the population from consuming rice from these areas. A comprehensive education program on the use of agrochemicals and sustainable irrigation practices that avoid the use of mining-impacted water is suggested to reduce metal contamination in paddy soils and subsequent transfer into rice grains.",
issn="2522-5766",
doi="10.1007/s42250-025-01380-3",
url="https://doi.org/10.1007/s42250-025-01380-3"
}


@Inbook{Huang2025,
author="Huang, Gaojian",
editor="Xu, Wei",
title="Embodied Intelligence",
bookTitle="Handbook of Human-Centered Artificial Intelligence",
year="2025",
publisher="Springer Nature Singapore",
address="Singapore",
pages="1--27",
abstract="Embodied intelligence, or embodied AI, refers to artificial systems whose cognitive processes emerge from continuous sensorimotor interactions in real-world environments. This chapter explores its foundational theories, examines implementations across multiple domains, and evaluates both the technical and ethical considerations. A triadic human--AI collaboration framework illustrates how embodied AI agents can provide situation awareness, share control, or override capabilities to enhance safety and performance in domains ranging from automated driving to robotics-assisted surgery. Additionally, human-centered design principles are introduced, emphasizing transparency, explainability, lifecycle maintenance, and human controllability. Interaction modalities, including multimodal sensing, social robotics, and affective cues, further demonstrate how embodiment can advance human--AI collaborations. Discussions on integrating large language models (LLMs) into embodied AI are also included. Across applications such as healthcare, automated driving, education, and manufacturing, embodied systems have demonstrated how physical presence, context awareness, and adaptive learning can produce more responsive and user-friendly technology.",
isbn="978-981-97-8440-0",
doi="10.1007/978-981-97-8440-0_8-1",
url="https://doi.org/10.1007/978-981-97-8440-0_8-1"
}


@Article{Spitale2024,
author="Spitale, Giovanni
and Germani, Federico
and Biller-Andorno, Nikola",
title="Disruptive Technologies and Open Science: How Open Should Open Science Be? A `Third Bioethics' Ethical Framework",
journal="Science and Engineering Ethics",
year="2024",
month="Aug",
day="09",
volume="30",
number="4",
pages="36",
abstract="This paper investigates the ethical implications of applying open science (OS) practices on disruptive technologies, such as generative AIs. Disruptive technologies, characterized by their scalability and paradigm-shifting nature, have the potential to generate significant global impact, and carry a risk of dual use. The tension arises between the moral duty of OS to promote societal benefit by democratizing knowledge and the risks associated with open dissemination of disruptive technologies. Van Rennselaer Potter's `third bioethics' serves as the founding horizon for an ethical framework to govern these tensions. Through theoretical analysis and concrete examples, this paper explores how OS can contribute to a better future or pose threats. Finally, we provide an ethical framework for the intersection between OS and disruptive technologies that tries to go beyond the simple `as open as possible' tenet, considering openness as an instrumental value for the pursuit of other ethical values rather than as a principle with prima facie moral significance.",
issn="1471-5546",
doi="10.1007/s11948-024-00502-3",
url="https://doi.org/10.1007/s11948-024-00502-3"
}


@Inbook{Marini2024,
author="Marini, Maria Giulia",
title="Where We Were and Are Now with Digital Health in Communication",
bookTitle="Non-violent Communication and Narrative Medicine for Promoting Sustainable Health",
year="2024",
publisher="Springer Nature Switzerland",
address="Cham",
pages="77--91",
abstract="The era we live in is post-contemporary, and philosopher Francesca Ferrando goes beyond this definition, calling it a posthumanism age [1]. It means that we overcome the anthropocentric humanistic approach, encompassing and even being threatened by the technological crafts of Artificial Intelligence (AI).",
isbn="978-3-031-58691-0",
doi="10.1007/978-3-031-58691-0_7",
url="https://doi.org/10.1007/978-3-031-58691-0_7"
}


@Article{Sun2025,
author="Sun, Jin
and Gu, Shiyu
and Su, Ruotong",
title="AI-Empowered Responsive Regulation for Preventing Future Crimes: An Empirical Inquiry into the Regulatory Pyramid to Combat Future Crimes in China and Southeast Asia",
journal="Asian Journal of Criminology",
year="2025",
month="Nov",
day="28",
volume="21",
number="1",
pages="8",
abstract="The rapid evolution of AI-enabled future crimes, including synthetic identity fraud, deepfake scams, and cryptocurrency-based money laundering, has exposed critical gaps in traditional regulatory frameworks. Yet, few studies have examined how multi-layered responsive policing strategies can protect potential victims of future crimes in China and Southeast Asia, where digital payment adoption is surging. This article fills this gap by empirically investigating AI-empowered responsive regulation, drawing on 30 in-depth interviews with frontline police officers and 12 senior digital experts from major digital platforms in China and Southeast Asia. Our study introduces a novel AI-empowered ``regulatory pyramid'' framework, synthesizing insights from law enforcement and private-sector cybersecurity innovations to combat modular, adaptive, and decentralized crime networks. The AI-empowered regulatory pyramid is a set of tech-driven responsive policing at three layers: (1) AI-empowered capacity building for protecting potential victims, (2) restorative community policing disrupting cyber money mule networks, and (3) incapacitative policing targeting cybercrime syndicates. Empirical evidence indicates all three levels of responsive policing have been observed and are available in China, but cyberfraud policing practices in Southeast Asia often focus on capacity building due to a lack of state capacity and resources, which explains why the strict policing enforcement in China led to the relocation of cyberfraud criminals from China to Southeast Asia and the rise of transnational future crimes. We also found that while escalating enforcement via AI-empowered strategies yields short-term deterrence in China, long-term resilience depends on poverty-alleviation-oriented capacity-building and cross-border police cooperation against transnational future crimes.",
issn="1871-014X",
doi="10.1007/s11417-025-09477-x",
url="https://doi.org/10.1007/s11417-025-09477-x"
}


@Inbook{Heimgärtner2025,
author="Heimg{\"a}rtner, R{\"u}diger",
editor="Xu, Wei",
title="Intercultural Design for Human-Centered AI Solutions",
bookTitle="Handbook of Human-Centered Artificial Intelligence",
year="2025",
publisher="Springer Nature Singapore",
address="Singapore",
pages="1--65",
abstract="Based on and inspired by the approach of human-centered AI (HCAI), intercultural human-centered AI (IHCAI) examines how AI systems are designed, developed, and deployed to respect cultural diversity and serve global user populations. This chapter synthesizes foundational concepts and cultural models to explain how expectations about communication, decision-making, and interaction shape human--AI experiences. Building on these foundations, it translates concepts into actionable methods for design and evaluation, including localization, culturally adaptive interfaces, multilingual interaction, and participatory design with diverse stakeholders. The chapter addresses critical challenges in global deployment such as dataset bias, conflicting ethical norms across regions, and the balance between standardization and localization. It also discusses ethics, fairness, and cross-cultural governance. It analyzes opportunities and risks in large language models, including bias, unequal language performance, culturally aware explainability, and adaptation. Applications across healthcare, education, and business illustrate how culturally sensitive design improves acceptance and effectiveness. The chapter concludes with guiding principles, success factors, and metrics for usability, trust, inclusion, and cultural appropriateness. It proposes a roadmap for scalable cultural intelligence, real-time adaptation, and evidence-based evaluation equipping researchers, practitioners, and policymakers with a coherent framework and practical guidance to create AI that is globally usable, locally appropriate, and aligned with diverse values.",
isbn="978-981-97-8440-0",
doi="10.1007/978-981-97-8440-0_77-1",
url="https://doi.org/10.1007/978-981-97-8440-0_77-1"
}


@Article{ref1,
title="60. Jahrestagung der Deutschen Gesellschaft f{\"u}r Neuroradiologie e.{\thinspace}V. und 32. Jahrestagung der {\"O}sterreichischen Gesellschaft f{\"u}r Neuroradiologie e.{\thinspace}V. Salzburg Congress",
journal="Clinical Neuroradiology",
year="2025",
month="Oct",
day="01",
volume="35",
number="1",
pages="1--115",
issn="1869-1447",
doi="10.1007/s00062-025-01556-x",
url="https://doi.org/10.1007/s00062-025-01556-x"
}


@inproceedings{10.1007/978-3-031-72559-3_17,
 abstract = {The financial services industry has witnessed a notable surge in cyber-related crimes, necessitating the adoption of more robust cybersecurity measures. Current literature has identified that the adoption of blockchain can enhance the efficiency and integrity of cybersecurity mechanisms. Despite blockchain's acknowledged cybersecurity capabilities in existing literature, the adoption rate of blockchain in the South African financial services industry has been relatively slow due to barriers associated with adopting a novel technology. This study interviewed 11 financial technology experts to gain insights into the perceived barriers to the adoption of blockchain technology to address cybersecurity concerns in the South African financial services industry. This research followed a deductive approach by combining constructs from the Innovation Resistance Theory and Technology-Organisation Framework to investigate the perceived barriers to the adoption of blockchain in addressing cybersecurity concerns in the financial services industry. The findings revealed that the adoption of blockchain in addressing cybersecurity in the South African financial services industry is influenced by the four main barriers: functional organisational-level, functional industry-level, psychological organisation-level, and psychological industry-level barriers. The functional barriers were influenced by the perceived value barrier, risk barrier, usage barrier, technology barrier, and regulatory environment barrier. The psychological barriers affecting blockchain adoption were influenced by the perceived image barrier and tradition barrier. Two additional barriers were found to inhibit the adoption of blockchain in addressing cybersecurity, namely, the use case barrier as a functional industry-level barrier and the knowledge barrier as a psychological organisation barrier.},
 address = {Cham},
 author = {Scott-King, J.
and Ruhwanya, Zainab},
 booktitle = {Human Aspects of Information Security and Assurance},
 editor = {Clarke, Nathan
and Furnell, Steven},
 isbn = {978-3-031-72559-3},
 pages = {243--263},
 publisher = {Springer Nature Switzerland},
 title = {The Perceived Barriers to the Adoption of Blockchain in Addressing Cybersecurity Concerns in the Financial Services Industry},
 url = {https://link.springer.com/chapter/10.1007/978-3-031-72559-3_17},
 year = {2025}
}

@Article{Picos2025,
author="Picos, Aitor
and Seoane, Nuria
and Campos-Toimil, Manuel
and Vi{\~{n}}a, Dolores",
title="Vascular senescence and aging: mechanisms, clinical implications, and therapeutic prospects",
journal="Biogerontology",
year="2025",
month="May",
day="26",
volume="26",
number="3",
pages="118",
abstract="The aging vasculature is characterized by endothelial dysfunction, arterial stiffness, and increased susceptibility to vascular pathologies. Central to these changes is the process of cellular senescence, where endothelial and vascular smooth muscle cells lose their replicative and functional capacity and adopt a pro-inflammatory secretory phenotype. This review provides an overview of the key mechanisms underlying vascular senescence, including the p53/p21 and p16/Rb pathways, the senescence-associated secretory phenotype (SASP), and oxidative stress, examines its contribution to cardiovascular diseases in older adults, and highlights emerging therapeutic strategies aimed at delaying or reversing these age-related vascular changes. In vascular cells, DNA damage, oxidative stress, and chronic inflammation associated with aging converge to amplify senescence. Clinically, vascular senescence is linked with hypertension, atherosclerosis, and increased overall cardiovascular risk. Several interventions, ranging from senolytics to lifestyle factors, show promise in mitigating these changes; however, long-term studies are needed. Given that vascular senescence is a pivotal driver of cardiovascular pathology in aging, targeting senescent cells or their secretory phenotype may potentially offer new avenues for preventing or attenuating age-related vascular diseases. This review presents an updated and integrative overview of vascular senescence, connecting fundamental cellular mechanisms with their clinical manifestations and highlighting the most promising therapeutic interventions.",
issn="1573-6768",
doi="10.1007/s10522-025-10256-5",
url="https://doi.org/10.1007/s10522-025-10256-5"
}


@Inbook{Majeed2026,
author="Majeed, Hammad
and Iftikhar, Tehreema",
title="Next Generation Smart Agricultural Technologies and Precision Farming with Industry 6.0 Innovations",
bookTitle="Intelligent Manufacturing in Industry 6.0: A Climate Resilience Approach",
year="2026",
publisher="Springer Nature Switzerland",
address="Cham",
pages="507--532",
abstract="Advancements in Industry 6.0 are revolutionizing the plant and agriculture sector through cutting-edge technologies such as artificial intelligence, robotics, biotechnology, and precision agriculture. The integration of smart farming techniques, including Internet of Things-based soil monitoring and AI-powered crop disease detection, enhances productivity while minimizing resource wastage. Automation in agriculture, enabled by robotics and drone technology, optimizes planting, irrigation, and harvesting processes, significantly improving yield efficiency. Genetic engineering and biotechnological innovations are facilitating the development of climate-resilient crops that can withstand environmental stresses, contributing to global food security. Sustainable agricultural practices, such as vertical farming, hydroponics, and aeroponics, reduce land and water usage while promoting year-round cultivation. Blockchain-based food supply chains ensure transparency, traceability, and quality control from farm to consumer. The integration of quantum computing in agricultural research accelerates genome analysis and predictive modeling for better crop improvement strategies. Advancements in biofertilizers and precision nutrient delivery systems enhance soil health while reducing environmental impact. These innovations align with global sustainability goals, addressing food security, resource conservation, and climate resilience. The future of agriculture lies in the seamless convergence of AI, automation, and biotechnological breakthroughs, transforming traditional farming into an intelligent, data-driven ecosystem.",
isbn="978-3-032-07278-8",
doi="10.1007/978-3-032-07278-8_13",
url="https://doi.org/10.1007/978-3-032-07278-8_13"
}


@Article{Hegazy2025,
author="Hegazy, Amira M.
and Abdelrahman, Maha M.
and Kamel, Mariam R.
and Ahmed, Adel M.",
title="Dual chromatographic analysis of pesticide residues in eggplants: a green approach to food safety monitoring",
journal="Scientific Reports",
year="2025",
month="Nov",
day="27",
volume="15",
number="1",
pages="42546",
abstract="The determination of pesticide residues is crucial for safeguarding human health, ensuring food safety, and contributing to environmental sustainability. Chlorfenapyr (CLF), abamectin (ABM), and fenpyroximate (FNP) are widely employed in eggplant (Solanum melongena L.) cultivation for pest control. This study presents the first reported simultaneous analysis of these pesticides using two validated chromatographic methods: high-performance thin-layer chromatography (HP-TLC, Method A) and reversed-phase high-performance liquid chromatography (RP-HPLC, Method B). Method A employed a mobile phase of methanol/chloroform/glacial acetic acid/triethylamine (7.00:2.50:0.50:0.10, v/v) with densitometric UV detection at 225 nm. The retention factors (Rf) for CLF, ABM, and FNP were 0.13, 0.34, and 0.85, respectively. Method B utilized a mobile phase of acetonitrile/5 mM ammonium acetate buffer (pH 4.0) (70:30, v/v) at a flow rate of 1.20 mL/min, with UV detection at 255 nm. The retention times for CLF, ABM, and FNP were 3.4, 6.1, and 8.4 min, respectively. Both methods were validated according to the key principles of ICH guideline Q2(R2) for the validation of analytical procedures, demonstrating robustness for assaying the ternary pesticide mixture in pure forms, commercial formulations (with excipients), and spiked eggplant samples. Both developed methods exhibited excellent detection and quantification capabilities. Method A demonstrated LODs ranging from 0.0006 to 0.0027 {\textmu}g/band and LOQs from 0.0019 to 0.009 {\textmu}g/band. Method B achieved even lower detection limits, with LODs from 0.0029 to 0.0296 {\textmu}g/mL and corresponding LOQs from 0.0089 to 0.089 {\textmu}g/mL for the target pesticides.",
issn="2045-2322",
doi="10.1038/s41598-025-27091-w",
url="https://doi.org/10.1038/s41598-025-27091-w"
}


@Article{Ansari2026,
author="Ansari, Bashir
and Sameer, Ahmed",
title="A TCCM-Based Systematic Literature Review of Anthropomorphism in Human-AI Interaction and Future Research Agenda",
journal="Journal of Technology in Behavioral Science",
year="2026",
month="Jan",
day="20",
abstract="Anthropomorphism in human-AI interaction is a critical yet paradoxical phenomenon shaping user engagement with Large Language Models (LLMs) and AI chatbots. Despite a surge in scholarly interest, the field lacks a systematic synthesis that uncovers the interdependencies between the theories, contexts, characteristics, and methods defining this research landscape. This review addresses this critical gap by employing the theory-context-characteristic-method (TCCM) framework to critically analyze and integrate 57 articles published between 2010 and 2025. Following PRISMA guidelines and quality assessment via the Critical Appraisal Skills Programme (CASP), our analysis reveals that while research in this domain is predominantly grounded in theories such as computers as social actors (CASA), mind perception theory, and the technology acceptance model, the efficacy of anthropomorphism is fundamentally contingent upon context. Studies delineate interaction stakes into high-stakes and low-stakes contexts, primarily involving Western and East Asian user groups, revealing a significant cultural bias that distorts theoretical constructs such as mind perception and trust, particularly when applied to Global South contexts. Key characteristics shaping engagement range from perceived humanness and trust to under-researched negative effects such as psychological reactance and AI anxiety, with recent evidence showing that anthropomorphic features such as empathy can increase user anxiety. The methodological landscape, dominated by controlled experiments and survey research, has established initial causal links but reveals significant limitations due to over-reliance on self-reported data. We highlight how emerging work using neurophysiological and cognitive models, such as EEG and eye-tracking, is beginning to address this gap. This first comprehensive TCCM-based synthesis of anthropomorphism in user interaction with LLMs and AI chatbots offers an integrative framework, outlining critical research gaps and a targeted future agenda for scholars, designers, and policymakers.",
issn="2366-5963",
doi="10.1007/s41347-026-00594-7",
url="https://doi.org/10.1007/s41347-026-00594-7"
}


@Inbook{Kovač2024,
author="Kova{\v{c}}, Mitja",
title="Introduction to the Generative Artificial Intelligence Systems",
bookTitle="Generative Artificial Intelligence: A Law and Economics Approach to Optimal Regulation and Governance",
year="2024",
publisher="Springer Nature Switzerland",
address="Cham",
pages="59--82",
abstract="This chapter attempts to explain the main concepts, definitions and developments of the field of artificial intelligence. It addresses the issues of logic, probability, perception, learning and action. The chapter examines the current ``state of the art'' of the artificial intelligence systems and its recent developments. Moreover, this chapter presents the artificial intelligence's conceptual foundations and discusses the issues of machine learning, uncertainty, reasoning, learning and robotics.",
isbn="978-3-031-65514-2",
doi="10.1007/978-3-031-65514-2_4",
url="https://doi.org/10.1007/978-3-031-65514-2_4"
}


@Inbook{Nuttall2025,
author="Nuttall, William
and Patelli, Edoardo
and Smith, Ewan
and Webbe-Wood, David
and Middleburgh, Simon",
title="The Way Ahead",
bookTitle="Perspectives on Engineering Uncertainty: Civil Nuclear Energy Safety and Efficiency",
year="2025",
publisher="Springer Nature Switzerland",
address="Cham",
pages="71--88",
abstract="In this chapter we build on the ideas presented in earlier chapters to explain how modern computational tools and techniques are facilitating more advanced forms of uncertainty handling. We also take the opportunity to think more broadly than previously and hence we comment on matters beyond simply safety and reliability.",
isbn="978-3-031-83254-3",
doi="10.1007/978-3-031-83254-3_6",
url="https://doi.org/10.1007/978-3-031-83254-3_6"
}


@Article{ref1,
title="CARS 2024---Computer Assisted Radiology and Surgery Proceedings of the 38th International Congress and Exhibition Barcelona, Spain, June 18--21, 2024",
journal="International Journal of Computer Assisted Radiology and Surgery",
year="2024",
month="Jun",
day="01",
volume="19",
number="1",
pages="1--153",
issn="1861-6429",
doi="10.1007/s11548-024-03128-9",
url="https://doi.org/10.1007/s11548-024-03128-9"
}


@Article{Kang2025,
author="Kang, Jiyoung",
title="Linguistic humanization in conversational AI: a systematic--narrative review of humor, empathy, tone, and imperfection in relational communication",
journal="AI {\&} SOCIETY",
year="2025",
month="Nov",
day="17",
abstract="As conversational AI systems and virtual humans increasingly mediate social, educational, and therapeutic interactions, their linguistic design has become central to how users construct meaning, establish trust, and experience emotional connection. This review synthesizes interdisciplinary research on linguistic humanization in conversational AI, focusing on how humor, empathy, tone, and imperfection operate as relational strategies that shape affective and cognitive responses. Drawing on frameworks, such as the Media Equation and Social Response Theory, this analysis integrates insights from communication, psychology, and human--computer interaction through a systematic--narrative hybrid approach. Across 136 studies, the findings reveal that linguistic cues not only enhance social presence and authenticity but also mediate trust and intimacy through mechanisms of affective mediation and social attribution. By identifying these mechanisms, the review advances the conceptual understanding of synthetic relationality---the emergence of emotional reciprocity between humans and AI agents. This discussion highlights design implications for creating ethically grounded, emotionally intelligent conversational systems, emphasizing context sensitivity, cultural adaptation, and transparency in human--AI communication.",
issn="1435-5655",
doi="10.1007/s00146-025-02738-4",
url="https://doi.org/10.1007/s00146-025-02738-4"
}


@inproceedings{10.1007/978-94-024-2308-2_8,
 abstract = {This paper addresses key aspects of securitySecurity in web environments, with a particular focus on API endpoints. The exchange of vast amounts of information within integrated systems requires consistent application of securitySecurity principles throughout all stages---planning, development, testing, and implementation. The paper provides insights into API endpoints, types of APIs, potential techniques for securing API endpoints, and recommendations for the phases that should be carried out during the development of API endpoints to ensure their securitySecurity. Special emphasis is placed on raising awareness of securitySecurity threats among those who develop and use such systems, as well as on the importance of employing appropriate tools for testing API endpoints and other system components. Additionally, the paper explores the challenges of retrieving data from websites without APIs and creating new APIs for exchanging such information, identifying specific securitySecurity risks and offering recommendations for addressing them. The paper also highlights the importance of introducing new methods into the education of software engineers in the field of information securitySecurity.},
 address = {Dordrecht},
 author = {{\v{C}}ovi{\'{c}}, Zlatko},
 booktitle = {Critical Infrastructure Protection: Advanced Technologies for Crisis Prevention and Response},
 editor = {Kov{\'a}cs, T{\"u}nde Anna
and F{\"u}rstner, Igor},
 isbn = {978-94-024-2308-2},
 pages = {117--128},
 publisher = {Springer Netherlands},
 title = {Secure Data Utilization from Web Environment},
 url = {https://link.springer.com/chapter/10.1007/978-94-024-2308-2_8},
 year = {2025}
}

@Inbook{Escobar-Castillejos2026,
author="Escobar-Castillejos, Daisy
and B{\'a}ez G{\'o}mez Tagle, Enrique Ulises
and Chavarr{\'i}a-Reyes, Fernando Mauricio
and Sig{\"u}enza-Noriega, I{\~{n}}aki
and Cruz-Ledesma, Iv{\'a}n
and Elizondo-Estrada, Hector Miguel
and Escobar-Castillejos, David",
editor="Moya-Albor, Ernesto
and Ponce, Hiram
and Brieva, Jorge
and Gomez-Coronel, Sandra L.
and Torres, Diego Renza",
title="Sphonic: Development of a Mobile Application Using AI and AR for Learning Biomedical Concepts",
bookTitle="Machine Learning Methods in Biomedical Field: Computer-Aided Diagnostics, Healthcare and Biology Applications",
year="2026",
publisher="Springer Nature Switzerland",
address="Cham",
pages="365--388",
abstract="This chapter covers the development of Sphonic, an iOS mobile application that combines artificial intelligence (AI) and augmented reality (AR) to improve biomedical education. Sphonic was designed to employ SwiftUI for its front end, while its back end is based on Django and hosted on Amazon Web Services. Sphonic intends to assist in explaining complex topics in chemistry and biology. Using Apple's ARKit framework, Sphonic allows students to explore dynamic representations of DNA architecture, chemical bonding interactions, and biological processes to promote engagement and retention. On the other hand, the application features an OCR-based chemical equation solver, using Amazon Bedrock's generative AI capabilities, that balances equations and offers comprehensive, step-by-step guidance, addressing conceptual deficiencies for learners. Additionally, Sphonic implements secure authentication protocols to protect user data and features a user-friendly interface that simplifies navigation. This chapter concludes by emphasizing the importance of AI and AR in modern education, demonstrating how these technologies could democratize access and understanding of scientific information and foster creativity in academic environments.",
isbn="978-3-031-96328-5",
doi="10.1007/978-3-031-96328-5_15",
url="https://doi.org/10.1007/978-3-031-96328-5_15"
}


@Article{Qin2025,
author="Qin, Qinggui
and Zhang, Shuhan",
title="Visualizing the knowledge mapping of artificial intelligence in education: A systematic review",
journal="Education and Information Technologies",
year="2025",
month="Jan",
day="01",
volume="30",
number="1",
pages="449--483",
abstract="Artificial Intelligence (AI) plays a vital role in the growth and progress of education. Therefore, there is a need to scientifically explore the application of Artificial Intelligence in Education (AIED) and systematically analyze the development trends and research hotspots of AIED to provide reference for researchers. In this study, 1356 articles (2016--2023) in WOS were selected for further research, utilizing knowledge graph analysis. Using both VOSviewer and CiteSpace, which facilitated triangulating the data across software platforms to ensure the reliability of the results, the main highly co-cited literature and keywords were thoroughly analyzed. The key highlights of the results are: Firstly, the study reveals three major themes in the field of AI education, namely, medical theme, educational theme, and ChatGPT theme. Secondly, important literature and nodes in the field of AIED were identified. Thirdly, the study demonstrates the main technologies in the field of AIED, including Natural Language Processing, Machine Learning, Deep Learning, and Generative Artificial Intelligence. Finally, the burst analysis illustrates the hotspots and themes of the AIED at different stages. This study enriches the understanding of the fundamental knowledge and research frontiers essential to the application of AIED, which helps identify the patterns and trends for future research and teaching practices.",
issn="1573-7608",
doi="10.1007/s10639-024-13076-1",
url="https://doi.org/10.1007/s10639-024-13076-1"
}


@Inbook{Khalifa2026,
author="Khalifa, Sherif",
title="Climate Change in the Common Era",
bookTitle="Climate Change, Economic Channels, and Historical Junctures: How Climate Change Shaped Human History",
year="2026",
publisher="Springer Nature Switzerland",
address="Cham",
pages="257--285",
abstract="This chapter focuses on climate change--induced societal collapse that occurred in the common era. It discusses the emergence and collapse of the Roman Empire. This starts with the story of the Western Roman Empire, its great achievements, its rise to the pinnacle of world dominance, and the challenges it faced. This is followed by the decline and fall of the empire, and the supporting evidence that show the contribution of climate change to closing this remarkable era in human history. The discussion also highlights the debilitating effect of climate change on the Eastern Roman Empire. The chapter, then, moves to the story of the Khmer Empire centered around the city of Angkor and beyond. A discussion of the hydraulic achievements of such a civilization, and how climate deterioration affected that elaborate system leading to its collapse, is included. The chapter concludes with the stories of several Chinese dynasties such as the Han, the Tang, the Song, the Yuan, and the Ming dynasties. Then, the chapter closes with a discussion of studies on the statistical association between climate disturbance and dynastic collapse in China.",
isbn="978-3-031-91867-4",
doi="10.1007/978-3-031-91867-4_10",
url="https://doi.org/10.1007/978-3-031-91867-4_10"
}


@Inbook{Kasturi2026,
author="Kasturi, Reddaiah
and Nellikondi, Raghavendar",
editor="Pandey, Bishwajeet
and Patel, Advait",
title="The Ethics of AI and Cloud Security",
bookTitle="Revolutionizing the Cloud: Generative AI, Security, and Sustainability",
year="2026",
publisher="Springer Nature Switzerland",
address="Cham",
pages="243--263",
abstract="This chapter explores the ethical dimensions of artificial intelligence (AI) as deployed within cloud computing environments. This chapter aims to critically assess ethical challenges such as algorithmic bias, surveillance, lack of transparency in AI models, and user consent within multi-tenant cloud platforms.",
isbn="978-3-032-07479-9",
doi="10.1007/978-3-032-07479-9_12",
url="https://doi.org/10.1007/978-3-032-07479-9_12"
}


@Inbook{Kizza2024,
author="Kizza, Joseph Migga",
title="Artificial Intelligence and the Security of Computing Systems",
bookTitle="Guide to Computer Network Security",
year="2024",
publisher="Springer International Publishing",
address="Cham",
pages="395--407",
abstract="A life with no death in the company of humanoids.",
isbn="978-3-031-47549-8",
doi="10.1007/978-3-031-47549-8_17",
url="https://doi.org/10.1007/978-3-031-47549-8_17"
}


@Inbook{Neelakrishnan2024,
author="Neelakrishnan, Priyanka",
title="Thinking Outside Norms",
bookTitle="Autonomous Data Security: Creating a Proactive Enterprise Protection Plan",
year="2024",
publisher="Apress",
address="Berkeley, CA",
pages="87--145",
abstract="In this chapter, I aim to expand your understanding of creating an effective data protection solution. Instead of offering a single solution, I encourage you to approach the problem from different perspectives. Drawing on the knowledge you've gained from the first two chapters, I'm confident you now have a solid grasp of the issue and available options. However, it's crucial to not feel limited by these options. To illustrate this, I've structured this chapter to review in detail what you might already know, and then explore creative alternatives. Using scenarios commonly encountered in the field, I've employed a sample organization to make the concepts easier to grasp. For security analysts and IT administrators reading this, you'll discover new ways to frame your questions and address challenges. Sometimes, all it takes is a clear description of your problem to guide a conversation with a security vendor.",
isbn="979-8-8688-0838-8",
doi="10.1007/979-8-8688-0838-8_3",
url="https://doi.org/10.1007/979-8-8688-0838-8_3"
}


@Article{Nuortimo2025,
author="Nuortimo, Kalle
and Harkonen, Janne
and Breznik, Kristijan
and Hannes, Rajeeka",
title="Developing a social media firestorm scale: from conceptualization to AI-assisted validation",
journal="Journal of Marketing Analytics",
year="2025",
month="Oct",
day="06",
abstract="A social media firestorm (SMF) refers to a sudden surge of negative reactions, criticism, or controversy on social media platforms, typically triggered by a specific event, statement, or action. Such firestorms can affect individuals, organizations, or brands, with potential reputational and financial consequences if not addressed appropriately. This paper elaborates on an SMF scale inspired by the Saffir-Simpson hurricane scale, adopting a structured approach to SMF measurement and management. The scale defines three measurable dimensions: width (reach or scope), height (intensity of negative sentiment), and duration of peak activity (the shark-fin shape). To provide preliminary validation, an artificial intelligence-based approach was applied to selected real-world firestorm cases. The findings suggest that the framework represents a first step toward a fully validated scale, offering an initial basis for assessing the potential impact of SMFs and supporting more structured organizational responses to digital crises.",
issn="2050-3326",
doi="10.1057/s41270-025-00439-x",
url="https://doi.org/10.1057/s41270-025-00439-x"
}


@Inbook{Treude2024,
author="Treude, Christoph",
editor="Mendez, Daniel
and Avgeriou, Paris
and Kalinowski, Marcos
and Ali, Nauman Bin",
title="Qualitative Data Analysis in Software Engineering: Techniques and Teaching Insights",
bookTitle="Handbook on Teaching Empirical Software Engineering",
year="2024",
publisher="Springer Nature Switzerland",
address="Cham",
pages="155--176",
abstract="Software repositories are rich sources of qualitative artifacts, including source code comments, commit messages, issue descriptions, and documentation. These artifacts offer many interesting insights when analyzed through quantitative methods, as outlined in the chapter on mining software repositories. This chapter shifts the focus toward interpreting these artifacts using various qualitative data analysis techniques. We introduce qualitative coding as an iterative process, which is crucial not only for educational purposes but also to enhance the credibility and depth of research findings. Various coding methods are discussed along with the strategic design of a coding guide to ensure consistency and accuracy in data interpretation. The chapter also discusses quality assurance in qualitative data analysis, emphasizing principles such as credibility, transferability, dependability, and confirmability. These principles are vital to ensure that the findings are robust and can be generalized in different contexts. By sharing best practices and lessons learned, we aim to equip all readers with the tools necessary to conduct rigorous qualitative research in the field of software engineering.",
isbn="978-3-031-71769-7",
doi="10.1007/978-3-031-71769-7_6",
url="https://doi.org/10.1007/978-3-031-71769-7_6"
}


@Inbook{Bano2025,
author="Bano, Saira
and Hill, David J.
and Ahmad, Noman",
editor="Stockemer, Daniel
and Abou-El-Kheir, Engi
and Kolodziejczyk, Kamila",
title="Misinformation and Democratic Dissatisfaction: Trump's Electoral Narratives in the 2020 and 2024 US Elections",
bookTitle="Democracy Under Scrutiny: Unpacking the Causes and Consequences of Democratic Dissatisfaction ",
year="2025",
publisher="Springer Nature Switzerland",
address="Cham",
pages="201--229",
abstract="Dissatisfaction with the performance of democracy is widespread in Western liberal democracies, where discontent is often linked to economic concerns, individual rights, perceived detachment of elites, and rapid social transformations. Media and political landscapes have undergone significant shifts, overwhelmed by torrents of information. The extensive spread of information, coupled with declining trust in media and governmental institutions, as well as the ascent of right-wing populism and increased political polarization, has fostered a political information environment where truth and falsehood intermingle. This convergence creates disparate narratives, exacerbating polarization and challenging the cohesion of democratic societies. This chapter explores the US political information environment as one that fosters public susceptibility to misinformation, erodes trust in democratic institutions, and undermines social cohesion and democratic integrity. Using Donald Trump's attempt to disrupt the 2020 US presidential election and his subsequent successful presidential campaign in 2024 as a case study, this chapter examines his narrative of election fraud---despite the absence of evidence---and analyzes his social media posts, public statements, and the role of these communications in perpetuating mistrust in democratic institutions. The chapter argues that right-wing populist leaders often operate in an environment where their supporters prioritize loyalty over accountability, readily believing misinformation even when confronted with rebuttals. By examining the dynamics of misinformation and its acceptance, the chapter contributes to understanding how these narratives weaken citizens' trust of democratic institutions and challenge the principles of accountability and truth that underpin liberal democracies.",
isbn="978-3-032-06596-4",
doi="10.1007/978-3-032-06596-4_11",
url="https://doi.org/10.1007/978-3-032-06596-4_11"
}


@Inbook{Anderson2025,
author="Anderson, Theresa Dirndorfer
and Marshall, Ruth",
editor="Xu, Wei",
title="Enabling Diversity and Gender Equity in Human-Centered AI",
bookTitle="Handbook of Human-Centered Artificial Intelligence",
year="2025",
publisher="Springer Nature Singapore",
address="Singapore",
pages="1--59",
abstract="Equity and diversity are increasingly being recognized as critical areas for attention in human-centered design work. For the development of human-centered AI, they are not only important considerations in AI data and systems design processes but also in relation to the people who design, evaluate and use these systems. Thus, this chapter moves between three layers: (1) data; (2) design; and (3) implementation to explore the detrimental effects of AI systems in which humans have not been at the center. It then discusses how the continued implementation of biased and unfair systems will significantly impact the perceived moral legitimacy of companies and governments as well as AI systems. The chapter makes an argument for the positive impact of HCAI as a universal approach to improve gender representation, diversity, and fairness in AI, with the added benefit of unforeseeable gains achieved by widening the aperture to look at how humans are moving through the world. The chapter discusses reframing notions of ``implementation'' to include ongoing and engaged monitoring and maintenance of AI systems and the systems that embody them. It offers strategies for exploring the social contexts within which AI systems are intended to be used, building diversity into work teams, and engaging with communities to identify and mitigate potentially detrimental impacts of AI systems. The chapter draws attention to the challenges for putting such strategies in place, with suggestions for future directions. It closes with recommendations for building diversity into teams and applying inclusive practices both in the workplace and when engaging with communities.",
isbn="978-981-97-8440-0",
doi="10.1007/978-981-97-8440-0_104-1",
url="https://doi.org/10.1007/978-981-97-8440-0_104-1"
}


@Article{Sherif2025,
author="Sherif, Miar M.
and El-Abhar, Hanan S.
and Fawzy, Hala M.
and Gad, Amany M.
and Abdallah, Dalaal M.",
title="Repurposing the antimalarial chloroquine: a potential therapy for hepatic injury in a rat model of hindlimb ischemia--reperfusion by modulating apoptosis, autophagy, inflammation, and oxidative stress",
journal="Future Journal of Pharmaceutical Sciences",
year="2025",
month="Mar",
day="11",
volume="11",
number="1",
pages="35",
abstract="Besides its local injurious effect, hindlimb hypoxia/reperfusion (HL-H/R) can escalate leading to multiple organ dysfunction syndrome.",
issn="2314-7253",
doi="10.1186/s43094-025-00781-y",
url="https://doi.org/10.1186/s43094-025-00781-y"
}


@inproceedings{10.1007/978-3-031-22137-8_29,
 abstract = {Representational Similarity Analysis is a method from cognitive neuroscience, which helps in comparing representations from two different sources of data. In this paper, we propose using Representational Similarity Analysis to probe the semantic grounding in language models of code. We probe representations from the CodeBERT model for semantic grounding by using the data from the IBM CodeNet dataset. Through our experiments, we show that current pre-training methods do not induce semantic grounding in language models of code, and instead focus on optimizing form-based patterns. We also show that even a little amount of fine-tuning on semantically relevant tasks increases the semantic grounding in CodeBERT significantly. Our ablations with the input modality to the CodeBERT model show that using bimodal inputs (code and natural language) over unimodal inputs (only code) gives better semantic grounding and sample efficiency during semantic fine-tuning. Finally, our experiments with semantic perturbations in code reveal that CodeBERT is able to robustly distinguish between semantically correct and incorrect code.},
 address = {Cham},
 author = {Naik, Shounak
and Patil, Rajaswa
and Agarwal, Swati
and Baths, Veeky},
 booktitle = {Advanced Data Mining and Applications},
 editor = {Chen, Weitong
and Yao, Lina
and Cai, Taotao
and Pan, Shirui
and Shen, Tao
and Li, Xue},
 isbn = {978-3-031-22137-8},
 pages = {395--406},
 publisher = {Springer Nature Switzerland},
 title = {Probing Semantic Grounding in Language Models of Code with Representational Similarity Analysis},
 url = {https://link.springer.com/chapter/10.1007/978-3-031-22137-8_29},
 year = {2022}
}

@Inbook{Forgács2024,
author="Forg{\'a}cs, Istv{\'a}n
and Kov{\'a}cs, Attila",
title="Test Design Automation by Model-Based Testing",
bookTitle="Modern Software Testing Techniques: A Practical Guide for Developers and Testers",
year="2024",
publisher="Apress",
address="Berkeley, CA",
pages="43--118",
abstract="In this chapter, you'll discover the significance of automating test design, a crucial aspect of test automation. The primary approach for automated test design is called model-based testing (MBT), which is widely used. MBT methods are divided into one-phase and two-phase model-based testing. We explain the advantages of the latter approach over the traditional one. Additionally, we categorize models into three types: stateless, stateful, and mixed models. We illustrate that stateless models are less effective in identifying defects. On the other hand, the stateful solution can be challenging to apply to certain requirements. As a result, the mixed model solution emerges as the most favorable option. This content should be understood within the reading time.",
isbn="978-1-4842-9893-0",
doi="10.1007/978-1-4842-9893-0_2",
url="https://doi.org/10.1007/978-1-4842-9893-0_2"
}


@Article{Al-antari2025,
author="Al-antari, Mugahed A.
and Salem, Saied
and Raza, Mukhlis
and Elbadawy, Ahmed S.
and B{\"u}t{\"u}n, Ertan
and Aydin, Ahmet Arif
and Aydo{\u{g}}an, Murat
and Ertu{\u{g}}rul, Bilal
and Talo, Muhammed
and Gu, Yeong Hyeon",
title="Evaluating AI-powered predictive solutions for MRI in lumbar spinal stenosis: a systematic review",
journal="Artificial Intelligence Review",
year="2025",
month="May",
day="03",
volume="58",
number="8",
pages="221",
abstract="Lumbar spinal stenosis (LSS) involves the narrowing of the spinal canal, leading to compression of the spinal cord and nerves in the lower back. Common causes include injuries, degenerative age-related changes, congenital conditions, and tumors, all of which contribute to back pain. Early diagnosis is critical for symptom management, preventing progression, and preserving quality of life. This study systematically reviews AI-based approaches for predicting LSS using MRI axial and sagittal imaging. The review focuses on various AI tasks: detection, segmentation, classification, hybrid approaches, spinal index measurements (SIM), and explainable AI frameworks. The aim is to highlight current knowledge, identify limitations in existing models, and propose future research directions. Following PRISMA guidelines and the PICO method (Population, Intervention, Comparison, Outcome), the review collects data from databases like PubMed, Web of Science, ScienceDirect, and IEEE Xplore (2005--2024). The Rayyan AI tool is used for duplicate removal and screening. The screening process includes an initial review of titles and abstracts, followed by full-text appraisal. The Meta Quality Appraisal Tool (MetaQAT) assesses the quality of selected articles. Of 1323 records, 97 duplicates were removed. After screening, 895 records were excluded, leaving 331 for full-text review. Among these, 184 articles were excluded for lacking AI relevance. Ultimately, 95 key articles (91 technical papers and 4 reviews) were identified for their contributions to AI-based LSS prediction. This review provides a comprehensive analysis of AI techniques in LSS prediction, guiding future research and advancing understanding in areas like explainable AI and large language models (LLMs).",
issn="1573-7462",
doi="10.1007/s10462-025-11185-y",
url="https://doi.org/10.1007/s10462-025-11185-y"
}


@Inbook{Strauss2025,
author="Strauss, Dirk",
title="Debugging Your Code",
bookTitle="Getting Started with Visual Studio 2022: Learning and Implementing New Features",
year="2025",
publisher="Apress",
address="Berkeley, CA",
pages="251--309",
abstract="Debugging code is probably one of the most essential tasks that a developer performs. Being able to run your application and pause the execution of code midway is a lifesaver. But there is a lot more to debugging than just setting breakpoints and viewing results.",
isbn="979-8-8688-2114-1",
doi="10.1007/979-8-8688-2114-1_3",
url="https://doi.org/10.1007/979-8-8688-2114-1_3"
}


@inproceedings{10.1007/978-3-031-85930-4_5,
 abstract = {This paper proposes a blockchain-based high performance transaction processing system called TOIChain. A new programming paradigm and architecture using Active Content Addressable Networking protocol and Statistic Multiplexed Computing runtime is introduced to overcome the typical service infrastructure scalability challenges.},
 address = {Cham},
 author = {Shi, Justin Y.},
 booktitle = {Foundations of Computer Science and Frontiers in Education: Computer Science and Computer Engineering},
 editor = {Arabnia, Hamid R.
and Deligiannidis, Leonidas
and Amirian, Soheyla
and Ghareh Mohammadi, Farid
and Shenavarmasouleh, Farzan},
 isbn = {978-3-031-85930-4},
 pages = {53--65},
 publisher = {Springer Nature Switzerland},
 title = {TOIChain{\texttrademark}: A Proposal for High Performance Tamper Resistant Transactions Without Scaling Limits},
 url = {https://link.springer.com/chapter/10.1007/978-3-031-85930-4_5},
 year = {2025}
}

@Article{Sahebi2025,
author="Sahebi, Siavosh
and Formosa, Paul",
title="The AI-mediated communication dilemma: epistemic trust, social media, and the challenge of generative artificial intelligence",
journal="Synthese",
year="2025",
month="Mar",
day="12",
volume="205",
number="3",
pages="128",
abstract="The rapid adoption of commercial Generative Artificial Intelligence (Gen AI) products raises important questions around the impact this technology will have on our communicative interactions. This paper provides an analysis of some of the potential implications that Artificial Intelligence-Mediated Communication (AI-MC) may have on epistemic trust in online communications, specifically on social media. We argue that AI-MC poses a risk to epistemic trust being diminished in online communications on both normative and descriptive grounds. Descriptively, AI-MC seems to (roughly) lower levels of epistemic trust. Normatively, we argue that this brings about the following dilemma. On the one hand, there are at least some instances where we should epistemically trust AI-MC less, and therefore the reduction in epistemic trust is justified in these instances. On the other hand, there are also instances where we epistemically trust AI-MC less, but this reduction in epistemic trust is not justified, resulting in discrimination and epistemic injustice in these instances. The difficulty in knowing which of these two groups any instance of AI-MC belongs to brings about the AI-MC dilemma: We must choose between maintaining normal levels of epistemic trust and risking epistemic gullibility when reduced trust is justified, or adopting generally reduced epistemic trust and risking epistemic injustice when such reduced trust is unjustified. Navigating this choice between problematic alternatives creates a significant challenge for social media as an epistemic environment.",
issn="1573-0964",
doi="10.1007/s11229-025-04963-2",
url="https://doi.org/10.1007/s11229-025-04963-2"
}


@inproceedings{10.1007/978-3-031-94956-2_12,
 abstract = {Compliance with information security frameworks, such as the Cloud Control Matrix (CCM), is increasingly essential for many companies. Depending on an organization's specific context, multiple frameworks may be relevant, requiring the identification and harmonization of similar requirements across standards. As frameworks evolve over time, continuous efforts from domain experts or the use of automated solutions are necessary. However, automatically identifying and matching information security requirements remains a complex task in the field of natural language processing. To support this claim, we evaluate the performance of state-of-the-art large language models on cloud security requirements. We also conduct a qualitative analysis of the specific challenges that arise when identifying similar requirements, providing a deeper, data-centric understanding of the matching difficulties. With this dual perspective, we contribute to the field of matching similar information security requirements and discuss implications for both AI systems and human analysts.},
 address = {Cham},
 author = {Hirschmeier, Stefan
and Hirschmeier, Markus
and Giersch, Andreas},
 booktitle = {Computational Science and Computational Intelligence},
 editor = {Arabnia, Hamid R.
and Deligiannidis, Leonidas
and Shenavarmasouleh, Farzan
and Amirian, Soheyla
and Ghareh Mohammadi, Farid},
 isbn = {978-3-031-94956-2},
 pages = {158--170},
 publisher = {Springer Nature Switzerland},
 title = {Efficacy of Large Language Models in Mapping Cloud Security Requirements: A Data-Centric View},
 url = {https://link.springer.com/chapter/10.1007/978-3-031-94956-2_12},
 year = {2025}
}

@Article{Ayash2025,
author="Ayash, Lama
and Algarni, Abdulmohsen
and Alqahtani, Omar",
title="Advancements in feature selection and extraction methods for text mining: a review",
journal="Discover Applied Sciences",
year="2025",
month="Aug",
day="11",
volume="7",
number="8",
pages="914",
abstract="Textual data are among the most popular data types that provide useful knowledge today. The need to extract insights from text has initiated and overseen the progression of research in the text mining domain. The challenge of addressing textual data comes from the complexity of text processing. The nature of text presents challenges that demand sophisticated processing methods. Subsequently, the degree of complexity involved in this field drives researchers to continuously improve the existing feature selection and extraction techniques, highlighting the essential work that is necessary to propel the field forward. Unlike the existing reviews that focus on individual components such as preprocessing, feature selection, or feature extraction in isolation, this study offers a unified perspective by integrating all three aspects. This review explores the feature selection and extraction methods advances achieved in text mining over the last decade. The focus of this investigation is the development of novel approaches that have been designed to address the complexities that accompany textual data. The comprehensive examination includes identifying the current trends and significant breakthroughs that characterize this dynamic field, thus providing a comprehensive view of the ongoing efforts to refine and optimize these methodologies. The presented explanation of the details associated with feature selection and extraction not only keeps researchers up to date on the latest developments but also contributes significantly to the ongoing refinement and growth exhibited by text mining techniques.",
issn="3004-9261",
doi="10.1007/s42452-025-07587-w",
url="https://doi.org/10.1007/s42452-025-07587-w"
}


@inproceedings{10.1007/978-3-031-25538-0_3,
 abstract = {Natural Language Processing (NLP) has recently gained wide attention in cybersecurity, particularly in Cyber Threat Intelligence (CTI) and cyber automation. Increased connection and automation have revolutionized the world's economic and cultural infrastructures, while they have introduced risks in terms of cyber attacks. CTI is information that helps cybersecurity analysts make intelligent security decisions, that is often delivered in the form of natural language text, which must be transformed to machine readable format through an automated procedure before it can be used for automated security measures.},
 address = {Cham},
 author = {Aghaei, Ehsan
and Niu, Xi
and Shadid, Waseem
and Al-Shaer, Ehab},
 booktitle = {Security and Privacy in Communication Networks},
 editor = {Li, Fengjun
and Liang, Kaitai
and Lin, Zhiqiang
and Katsikas, Sokratis K.},
 isbn = {978-3-031-25538-0},
 pages = {39--56},
 publisher = {Springer Nature Switzerland},
 title = {SecureBERT: A Domain-Specific Language Model for Cybersecurity},
 url = {https://link.springer.com/chapter/10.1007/978-3-031-25538-0_3},
 year = {2023}
}

@inproceedings{10.1007/978-3-032-02762-7_6,
 abstract = {The intersection of AI and human society necessitates robust regulatory frameworks. With the emergence of ChatGPT in 2022, the EU AI Act led the charge in governing technologies like generative AI (GenAI). As GenAI integrates across sectors, it presents distinct challenges, from potential breaches of business confidentiality to concerns of academic integrity. This working paper represents the first step in a three-phase research initiative, centred on the development of a comprehensive privacy compliance framework for GenAI. Through careful legal analysis and engagement with stakeholders, we establish sixteen key privacy principles tailored for GenAI platforms. The ensuing stages aim to refine this framework based on broad stakeholder feedback and test the framework's applicability across various GenAI platforms, ensuring users' privacy rights remain paramount. This research offers both a timely insight into GenAI's evolving legal landscape and a blueprint for future studies and regulatory endeavours.},
 address = {Cham},
 author = {Shreya, Shruti
and Tiwari, Pranav Bhaskar
and Tripathi, Gyan Prakash},
 booktitle = {The Quest for AI Sovereignty, Transparency and Accountability},
 editor = {Belli, Luca
and Gaspar, Walter B.},
 isbn = {978-3-032-02762-7},
 pages = {71--88},
 publisher = {Springer Nature Switzerland},
 title = {GenAI and the Goblet of Compliance: Delving into the Pensieve of Privacy Principles},
 url = {https://link.springer.com/chapter/10.1007/978-3-032-02762-7_6},
 year = {2025}
}

@Article{Acharya2025,
author="Acharya, Kamal
and Song, Houbing",
title="A Comprehensive Review of Neuro-symbolic AI for Robustness, Uncertainty Quantification, and Intervenability",
journal="Arabian Journal for Science and Engineering",
year="2025",
month="Dec",
day="09",
abstract="As Artificial Intelligence (AI) systems are increasingly deployed in high-stakes domains such as healthcare, autonomous systems, finance, and critical infrastructure, ensuring their trustworthiness has become imperative. This paper presents a comprehensive survey of neuro-symbolic AI, a hybrid paradigm that combines the learning capabilities of neural networks with the reasoning strengths of symbolic AI, through the lens of three foundational dimensions: robustness, uncertainty quantification (UQ), and intervenability. We first establish the limitations of purely data-driven ``black-box'' models in handling distribution shifts, ambiguous inputs, and human oversight. In contrast, neuro-symbolic systems offer enhanced interpretability, verifiability, and control, making them promising candidates for real-world deployment. We systematically review state-of-the-art techniques for modeling robustness, quantifying uncertainty, and enabling intervenability. We further examine how logic, probability, and learning can be integrated into unified or modular architectures to support transparent, adaptive reasoning. Finally, we outline current challenges and identify key research opportunities for advancing neuro-symbolic AI as a trustworthy paradigm. This survey aims to equip researchers and practitioners with a structured understanding of how to build reliable, interpretable, and interactive AI systems by bridging statistical learning and symbolic reasoning.",
issn="2191-4281",
doi="10.1007/s13369-025-10887-3",
url="https://doi.org/10.1007/s13369-025-10887-3"
}


@Article{Sapkota2025,
author="Sapkota, Ranjan
and Flores-Calero, Marco
and Qureshi, Rizwan
and Badgujar, Chetan
and Nepal, Upesh
and Poulose, Alwin
and Zeno, Peter
and Vaddevolu, Uday Bhanu Prakash
and Khan, Sheheryar
and Shoman, Maged
and Yan, Hong
and Karkee, Manoj",
title="YOLO advances to its genesis: a decadal and comprehensive review of the You Only Look Once (YOLO) series",
journal="Artificial Intelligence Review",
year="2025",
month="Jun",
day="11",
volume="58",
number="9",
pages="274",
abstract="This review systematically examines the progression of the You Only Look Once (YOLO) object detection algorithms from YOLOv1 to the recently unveiled YOLOv12. Employing a reverse chronological analysis, this study examines the advancements introduced by YOLO algorithms, beginning with YOLOv12 and progressing through YOLO11 (or YOLOv11), YOLOv10, YOLOv9, YOLOv8, and subsequent versions to explore each version's contributions to enhancing speed, detection accuracy, and computational efficiency in real-time object detection. Additionally, this study reviews the alternative versions derived from YOLO architectural advancements of YOLO-NAS, YOLO-X, YOLO-R, DAMO-YOLO, and Gold-YOLO. Moreover, the study highlights the transformative impact of YOLO models across five critical application areas: autonomous vehicles and traffic safety, healthcare and medical imaging, industrial manufacturing, surveillance and security, and agriculture. By detailing the incremental technological advancements in subsequent YOLO versions, this review chronicles the evolution of YOLO, and discusses the challenges and limitations in each of the earlier versions. The evolution signifies a path towards integrating YOLO with multimodal, context-aware, and Artificial General Intelligence (AGI) systems for the next YOLO decade, promising significant implications for future developments in AI-driven applications.",
issn="1573-7462",
doi="10.1007/s10462-025-11253-3",
url="https://doi.org/10.1007/s10462-025-11253-3"
}


@Article{Yang2024,
author="Yang, Shaoyu
and Chen, Xiang
and Liu, Ke
and Yang, Guang
and Yu, Chi",
title="Automatic bi-modal question title generation for Stack Overflow with prompt learning",
journal="Empirical Software Engineering",
year="2024",
month="May",
day="03",
volume="29",
number="3",
pages="63",
abstract="When drafting question posts for Stack Overflow, developers may not accurately summarize the core problems in the question titles, which can cause these questions to not get timely help. Therefore, improving the quality of question titles has attracted the wide attention of researchers. An initial study aimed to automatically generate the titles by only analyzing the code snippets in the question body. However, this study ignored the helpful information in their corresponding problem descriptions. Therefore, we propose an approach SOTitle+ by considering bi-modal information (i.e., the code snippets and the problem descriptions) in the question body. Then we formalize the title generation for different programming languages as separate but related tasks and utilize multi-task learning to solve these tasks. Later we fine-tune the pre-trained language model CodeT5 to automatically generate the titles. Unfortunately, the inconsistent inputs and optimization objectives between the pre-training task and our investigated task may make fine-tuning hard to fully explore the knowledge of the pre-trained model. To solve this issue, SOTitle+ further prompt-tunes CodeT5 with hybrid prompts (i.e., mixture of hard and soft prompts). To verify the effectiveness of SOTitle+, we construct a large-scale high-quality corpus from recent data dumps shared by Stack Overflow. Our corpus includes 179,119 high-quality question posts for six popular programming languages. Experimental results show that SOTitle+ can significantly outperform four state-of-the-art baselines in both automatic evaluation and human evaluation. In addition, our ablation studies also confirm the effectiveness of component settings (such as bi-modal information, prompt learning, hybrid prompts, and multi-task learning) of SOTitle+. Our work indicates that considering bi-modal information and prompt learning in Stack Overflow title generation is a promising exploration direction.",
issn="1573-7616",
doi="10.1007/s10664-024-10466-4",
url="https://doi.org/10.1007/s10664-024-10466-4"
}


@Inbook{Nguyen2023,
author="Nguyen, Tien N.",
editor="Mens, Tom
and De Roover, Coen
and Cleve, Anthony",
title="Mining for Software Library Usage Patterns Within an Ecosystem: Are We There Yet?",
bookTitle="Software Ecosystems: Tooling and Analytics",
year="2023",
publisher="Springer International Publishing",
address="Cham",
pages="85--103",
abstract="The use of software libraries is important in a software development ecosystem, in which the systems are interacting and share several usages of Application Programming Interfaces. The patterns of library usages have been shown to be useful in not only improving the productivity of the coding process via software reuse but also improving the code quality. In this chapter, we systematically evaluate the different approaches in library usage pattern mining. We also provide lessons learned from the history of those approaches and discuss the potential future directions in the usage pattern mining area.",
isbn="978-3-031-36060-2",
doi="10.1007/978-3-031-36060-2_4",
url="https://doi.org/10.1007/978-3-031-36060-2_4"
}


@Article{Bozdağ2025,
author="Bozda{\u{g}}, Ay{\c{s}}e Asl{\i}",
title="The AI-mediated intimacy economy: a paradigm shift in digital interactions",
journal="AI {\&} SOCIETY",
year="2025",
month="Apr",
day="01",
volume="40",
number="4",
pages="2285--2306",
abstract="This article critically examines the paradigm shift from the attention economy to the intimacy economy---a market system where personal and emotional data are exchanged for customized experiences that cater to individual emotional and psychological needs. It explores how AI transforms these personal and emotional inputs into services, thereby raising essential questions about the authenticity of digital interactions and the potential commodification of intimate experiences. The study delineates the roles of human--computer interaction and AI in deepening personal connections, significantly impacting emotional dynamics, and underscores AI's role in various applications, from healthcare to grief tech, highlighting both enhancements in emotional connections and potential disruptions to genuine human interactions. An AI-mediated framework (AMIE) is introduced to assess how AI reshapes these connections and the overall digital society through personalized interactions. This framework explores the interplay between human emotions and AI-generated responses within the new Avatar Sphere, emphasizing the necessity for regulatory measures to safeguard digital identities, recognize emotional data as intellectual property, and maintain system transparency. It highlights the critical need for maintaining genuine human interactions and advocates for context-aware consent, continuous monitoring, and cross-cultural considerations to foster ethical AI practices. Leveraging blockchain technology and decentralized autonomous organizations, the framework proposes methods to enhance individual control over emotional data, mitigating the commodification risks. The findings contribute to ongoing discussions on AI ethics, digital privacy, and the future of human-AI interactions, providing valuable insights for cultivating a responsible intimacy economy.",
issn="1435-5655",
doi="10.1007/s00146-024-02132-6",
url="https://doi.org/10.1007/s00146-024-02132-6"
}


@Article{Chu2024,
author="Chu, Liu-Xi
and Wang, Wen-Jia
and Gu, Xin-Pei
and Wu, Ping
and Gao, Chen
and Zhang, Quan
and Wu, Jia
and Jiang, Da-Wei
and Huang, Jun-Qing
and Ying, Xin-Wang
and Shen, Jia-Men
and Jiang, Yi
and Luo, Li-Hua
and Xu, Jun-Peng
and Ying, Yi-Bo
and Chen, Hao-Man
and Fang, Ao
and Feng, Zun-Yong
and An, Shu-Hong
and Li, Xiao-Kun
and Wang, Zhou-Guang",
title="Spatiotemporal multi-omics: exploring molecular landscapes in aging and regenerative medicine",
journal="Military Medical Research",
year="2024",
month="May",
day="27",
volume="11",
number="1",
pages="31",
abstract="Aging and regeneration represent complex biological phenomena that have long captivated the scientific community. To fully comprehend these processes, it is essential to investigate molecular dynamics through a lens that encompasses both spatial and temporal dimensions. Conventional omics methodologies, such as genomics and transcriptomics, have been instrumental in identifying critical molecular facets of aging and regeneration. However, these methods are somewhat limited, constrained by their spatial resolution and their lack of capacity to dynamically represent tissue alterations. The advent of emerging spatiotemporal multi-omics approaches, encompassing transcriptomics, proteomics, metabolomics, and epigenomics, furnishes comprehensive insights into these intricate molecular dynamics. These sophisticated techniques facilitate accurate delineation of molecular patterns across an array of cells, tissues, and organs, thereby offering an in-depth understanding of the fundamental mechanisms at play. This review meticulously examines the significance of spatiotemporal multi-omics in the realms of aging and regeneration research. It underscores how these methodologies augment our comprehension of molecular dynamics, cellular interactions, and signaling pathways. Initially, the review delineates the foundational principles underpinning these methods, followed by an evaluation of their recent applications within the field. The review ultimately concludes by addressing the prevailing challenges and projecting future advancements in the field. Indubitably, spatiotemporal multi-omics are instrumental in deciphering the complexities inherent in aging and regeneration, thus charting a course toward potential therapeutic innovations.",
issn="2054-9369",
doi="10.1186/s40779-024-00537-4",
url="https://doi.org/10.1186/s40779-024-00537-4"
}


@Inbook{Sarker2024,
author="Sarker, Iqbal H.",
title="Cybersecurity Background Knowledge: Terminologies, Attack Frameworks, and Security Life Cycle",
bookTitle="AI-Driven Cybersecurity and Threat Intelligence: Cyber Automation, Intelligent Decision-Making and Explainability",
year="2024",
publisher="Springer Nature Switzerland",
address="Cham",
pages="21--39",
abstract="This chapter provides a foundational understanding of cybersecurity concepts, including terminologies and attack frameworks like the cyber kill chain and MITRE ATT{\&}CK, as well as the cybersecurity life cycle. In this chapter, key terms regarding threats, vulnerabilities, security controls, and relevant emerging technologies associated with AI are clarified, enabling effective communication within the cybersecurity field. Examining attack frameworks, which encompass the sequential stages of the cyber kill chain and the tactical matrix of MITRE ATT{\&}CK, provides valuable insight into adversary tactics. Furthermore, the cybersecurity life cycle emphasizes a systematic approach to cybersecurity management, emphasizing risk assessment, continuous monitoring, and adaptive security measures. The purpose of this chapter is to provide readers with the knowledge and understanding necessary to navigate the complex landscape of cybersecurity with a strategic and informed perspective, providing a solid foundation for further exploration.",
isbn="978-3-031-54497-2",
doi="10.1007/978-3-031-54497-2_2",
url="https://doi.org/10.1007/978-3-031-54497-2_2"
}


@inproceedings{10.1007/978-981-97-7862-1_24,
 abstract = {This research presents the design and implementation of an intelligent chatbot tailored for academic institutions, specifically focusing on one intuition as of now. Leveraging natural language processing (NLP) techniques, the chatbot interprets user queries and accesses a structured JSON-type database housing intents, patterns, and responses. The database, featuring 47 broad tags with multiple patterns, ensures the chatbot's accuracy in addressing diverse user inquiries. The Flask framework serves as the system's backbone, providing a lightweight and versatile backend for efficient query processing. Dynamic buttons in responses enhance user interaction, categorizing queries for a structured experience. Statistical analysis methods, including confusion matrices, assess intent matching accuracy, providing insights for improvement. The research contributes to advancing user engagement and information accessibility in educational environments.},
 address = {Singapore},
 author = {Amin, Heeya
and Chauhan, Uttam
and Patel, Shail},
 booktitle = {Proceedings of International Conference on Recent Innovations in Computing},
 editor = {Singh, Yashwant
and Gon{\c{c}}alves, Paulo J. Sequeira
and Singh, Pradeep Kumar
and Kolekar, Maheshkumar H.},
 isbn = {978-981-97-7862-1},
 pages = {357--369},
 publisher = {Springer Nature Singapore},
 title = {Chatbot for Academic Institutions},
 url = {https://link.springer.com/chapter/10.1007/978-981-97-7862-1_24},
 year = {2024}
}

@Inbook{delaCruz2026,
author="dela Cruz, Jeane A.
and Jacob, James Kennard S.
and Beltran, Marjone S.
and dela Cruz, Thomas Edison E.",
editor="Guerrero, Jonathan Jaime G.
and De Leon, Marian P.
and Cena-Navarro, Rohani B.
and Notarte, Kin Israel R.
and Balendres, Mark Angelo O.
and dela Cruz, Thomas Edison E.",
title="From Mushroom Farms to Food Tables: Fungi for Food Security",
bookTitle="Mycology in a Changing Planet: Applications and Perspectives from Southeast Asia",
year="2026",
publisher="Springer Nature Singapore",
address="Singapore",
pages="327--356",
abstract="Fungi, particularly edible mushrooms, have long been a component of human diet, providing a diverse array of flavors, textures, and nutritional advantages. Edible mushrooms are rich sources of high-quality protein, dietary fiber, essential amino acids, a variety of vitamins and minerals, antioxidants, and other bioactive compounds. This book chapter focuses on edible mushrooms, both on their commercial cultivation and the nutritional and therapeutic advantages they offer. We also examined traditional and contemporary culinary traditions in relation to mushrooms. Furthermore, we provide insights on how edible mushrooms can aid in addressing food security through their efficient, low-cost, substrate-based cultivation, which makes them accessible and sustainable.",
isbn="978-981-95-2640-6",
doi="10.1007/978-981-95-2640-6_12",
url="https://doi.org/10.1007/978-981-95-2640-6_12"
}


@Article{Saini2023,
author="Saini, Jasleen
and Deere, Thomas M.
and Lessner, Daniel J.",
title="The minimal SUF system is not required for Fe--S cluster biogenesis in the methanogenic archaeon Methanosarcina acetivorans",
journal="Scientific Reports",
year="2023",
month="Sep",
day="13",
volume="13",
number="1",
pages="15120",
abstract="Iron--sulfur (Fe--S) proteins are essential for the ability of methanogens to carry out methanogenesis and biological nitrogen fixation (diazotrophy). Nonetheless, the factors involved in Fe--S cluster biogenesis in methanogens remain largely unknown. The minimal SUF Fe--S cluster biogenesis system (i.e., SufBC) is postulated to serve as the primary system in methanogens. Here, the role of SufBC in Methanosarcina acetivorans, which contains two sufCB gene clusters, was investigated. The CRISPRi-dCas9 and CRISPR-Cas9 systems were utilized to repress or delete sufC1B1 and sufC2B2, respectively. Neither the dual repression of sufC1B1 and sufC2B2 nor the deletion of both sufC1B1 and sufC2B2 affected the growth of M. acetivorans under any conditions tested, including diazotrophy. Interestingly, deletion of only sufC1B1 led to a delayed-growth phenotype under all growth conditions, suggesting that the deletion of sufC2B2 acts as a suppressor mutation in the absence of sufC1B1. In addition, the deletion of sufC1B1 and/or sufC2B2 did not affect the total Fe--S cluster content in M. acetivorans cells. Overall, these results reveal that the minimal SUF system is not required for Fe--S cluster biogenesis in M. acetivorans and challenge the universal role of SufBC in Fe--S cluster biogenesis in methanogens.",
issn="2045-2322",
doi="10.1038/s41598-023-42400-x",
url="https://doi.org/10.1038/s41598-023-42400-x"
}


@Inbook{Lau2025,
author="Lau, Theodora",
title="Trust and Evolution of Risk",
bookTitle="Banking on (Artificial) Intelligence: Navigating the Realities of AI in Financial Services",
year="2025",
publisher="Springer Nature Switzerland",
address="Cham",
pages="85--108",
isbn="978-3-031-81647-5",
doi="10.1007/978-3-031-81647-5_5",
url="https://doi.org/10.1007/978-3-031-81647-5_5"
}


@inproceedings{10.1007/978-3-031-50887-5_14,
 abstract = {This position paper explores the broad landscape of AI potentiality in the context of cybersecurity, with a particular emphasis on its possible risk factors with awareness, which can be managed by incorporating human experts in the loop, i.e., ``Human-AI'' teaming. As artificial intelligence (AI) technologies advance, they will provide unparalleled opportunities for attack identification, incident response, and recovery. However, the successful deployment of AI into cybersecurity measures necessitates an in-depth understanding of its capabilities, challenges, and ethical and legal implications to handle associated risk factors in real-world application areas. Towards this, we emphasize the importance of a balanced approach that incorporates AI's computational power with human expertise. AI systems may proactively discover vulnerabilities and detect anomalies through pattern recognition, and predictive modeling, significantly enhancing speed and accuracy. Human experts can explain AI-generated decisions to stakeholders, regulators, and end-users in critical situations, ensuring responsibility and accountability, which helps establish trust in AI-driven security solutions. Therefore, in this position paper, we argue that human-AI teaming is worthwhile in cybersecurity, in which human expertise such as intuition, critical thinking, or contextual understanding is combined with AI's computational power to improve overall cyber defenses.},
 address = {Cham},
 author = {Sarker, Iqbal H.
and Janicke, Helge
and Mohammad, Nazeeruddin
and Watters, Paul
and Nepal, Surya},
 booktitle = {Intelligent Computing and Optimization},
 editor = {Vasant, Pandian
and Panchenko, Vladimir
and Munapo, Elias
and Weber, Gerhard-Wilhelm
and Thomas, J. Joshua
and Intan, Rolly
and Shamsul Arefin, Mohammad},
 isbn = {978-3-031-50887-5},
 pages = {140--149},
 publisher = {Springer Nature Switzerland},
 title = {AI Potentiality and Awareness: A Position Paper from the Perspective of Human-AI Teaming in Cybersecurity},
 url = {https://link.springer.com/chapter/10.1007/978-3-031-50887-5_14},
 year = {2024}
}

@Inbook{Dabhade2025,
author="Dabhade, Kalpana
and Kaur, Navjeet",
editor="Chaughule, Ramesh S.
and Shelar, Amruta V.",
title="Nanotechnology and Plant Defense: Molecular Insights into Immune Response Activation",
bookTitle="Nanotechnology in Agriculture: Pioneering Progress and Challenges",
year="2025",
publisher="Springer Nature Switzerland",
address="Cham",
pages="113--145",
abstract="The convergence of nanotechnology and CRISPR (clustered regularly interspaced short palindromic repeats)/Cas gene-editing systems offers a transformative method for enhancing plant immunity. Nanomaterials allow precise delivery of CRISPR/Cas components to specific plant cells, improving gene-editing accuracy while reducing off-target risks. Materials like lipid-based nanoparticles, carbon nanotubes, and gold nanoparticles (AuNPs) protect CRISPR components from degradation and enhance cellular uptake. Additionally, their co-delivery capabilities enable the simultaneous transport of multiple CRISPR components, facilitating more complex edits, such as multiplexed gene editing in plant immune responses. Controlled release mechanisms further refine the timing and location of gene-editing activation in response to environmental or cellular conditions. This chapter explores the use of engineered nanoparticles to improve CRISPR/Cas delivery in plant cells and examines their role in enhancing plant stress responses. Studies show nanomaterials and their ability to upregulate genes linked to resilience against biotic and abiotic stress, boosting plant immune system robustness. Additionally, nanoparticles help minimize off-target mutations, preserving genome integrity. The development of biosensors for real-time CRISPR/Cas monitoring is also discussed, offering immediate feedback on gene-editing success and enabling precise immune response enhancements. This integration of nanotechnology and CRISPR/Cas systems marks a breakthrough in agricultural biotechnology, providing powerful tools to create crops with enhanced disease resistance and stress tolerance.",
isbn="978-3-031-97544-8",
doi="10.1007/978-3-031-97544-8_5",
url="https://doi.org/10.1007/978-3-031-97544-8_5"
}


@Inbook{Ximenes2024,
author="Ximenes, Filipe",
title="Self-Management",
bookTitle="Strategic Software Engineering: Software Engineering Beyond the Code",
year="2024",
publisher="Apress",
address="Berkeley, CA",
pages="1--49",
abstract="Self-management is undoubtedly the most important skill an engineer needs to develop in order to deliver good work over time and have it recognized by their managers and peers. That is for the simple fact that it doesn't matter how good you are at anything, if you cannot do it consistently over and over again, people won't trust you to do it. Being consistent is a trait that requires tweaks in many aspects of your work but that will ultimately enable you to achieve your career goals faster. By having a clear view of what are your personal goals and the business goals of your project, you will be able to actively plan the strategy that will lead you to success. Self-management skills enable all of this with the added benefit of making your work more enjoyable and allowing you to do more with less effort while being perceived as a high performer.",
isbn="979-8-8688-0995-8",
doi="10.1007/979-8-8688-0995-8_1",
url="https://doi.org/10.1007/979-8-8688-0995-8_1"
}


@Article{Hauptman2025,
author="Hauptman, Allyson
and Schelble, Beau
and Flathmann, Christopher
and Mallick, Rohit
and McNeese, Nathan",
title="Ethical adaptation: exploring the use of adaptive autonomy in the design of ethical AI teammates in healthcare",
journal="AI and Ethics",
year="2025",
month="Oct",
day="01",
volume="5",
number="5",
pages="5397--5414",
abstract="Modern advancements in AI technologies have allowed for their more seamless integration within society, including as full-fledged teammates tasked with making and executing independent decisions. This increased integration amplifies the burden on designers to determine if and when AI is capable of making such decisions when confronted with an ethical dilemma. In this mixed-methods study, we conducted a factorial survey (N=200) and interviewed fifteen medical professionals to understand how the principles of medical ethics should affect AI teammate autonomy and behavior. The results of this study enabled the creation of important themes and design recommendations that can guide the design of ethical AI teammates that can appropriately recognize and adapt to ethical dilemmas.",
issn="2730-5961",
doi="10.1007/s43681-025-00782-w",
url="https://doi.org/10.1007/s43681-025-00782-w"
}


@Article{Kasihmuddin2024,
author="Kasihmuddin, Sonia Mohd
and Cob, Zaidi Che
and Noor, Noorashikin Md
and Das, Simon Kumar",
title="Effect of different temperature variations on the physiological state of catfish species: a systematic review",
journal="Fish Physiology and Biochemistry",
year="2024",
month="Apr",
day="01",
volume="50",
number="2",
pages="413--434",
abstract="Catfish are a highly diverse group of fish that are found in various regions across the globe. The significance of catfish culture extends to various aspects, including food security, economic advancement, preservation of cultural legacy, and ecological stewardship. The catfish industry is presently encountering unprecedented challenges as a consequence of the variability in water temperature caused by climate change. Temperature is a significant abiotic component that regulates and restricts fish physiology throughout their life cycle. The impact of severe temperatures on various species of catfish is dependent upon the magnitude of the stressor and additional influencing factors. This paper presents an analysis of the effects of temperature fluctuations on various aspects of catfish species, including growth and survival, blood parameters, enzymatic and hormone response, oxygen consumption rates, sound generation and hearing skills, nutritional requirements, and other phenotypic attributes. While this review is certainly not exhaustive, it offers a broad synopsis of the ideal temperature ranges that are most favorable for several catfish species. In-depth research to investigate the interacting impacts of severe temperature occurrences in conjunction with other associated environmental stresses on a wider variety of catfish species is crucial in order to further our understanding of how catfish species will respond to the anticipated climate change in the future.",
issn="1573-5168",
doi="10.1007/s10695-024-01323-8",
url="https://doi.org/10.1007/s10695-024-01323-8"
}


@Article{Ikegwu2025,
author="Ikegwu, Anayo Chukwu
and Alo, Uzoma Rita
and Nweke, Henry Friday",
title="Cyber threats in mobile healthcare applications: systematic review of enabling technologies, threat models, detection approaches, and future directions",
journal="Discover Computing",
year="2025",
month="Jul",
day="17",
volume="28",
number="1",
pages="152",
abstract="Mobile healthcare (mHealth) delivery has revolutionized the healthcare industry 5.0. With the move toward mHealth, access to healthcare services has gradually increased, allowing individualized treatment routines, and real-time health monitoring. However, cybersecurity threats to mHealth systems also increase as their use increases. The continuous rise of cyber threats has recently affected the healthcare delivery sector. These have caused much financial damage and fears in the past few years. This paper aims to systematically assess and categorize the cyber threats in mHealth applications to guide the industry-wide adoption and prospects. A systematic literature review was carried out between 2018 and 2025 including 40 primary study papers were synthesized out of 24,854 search articles. The study discussed mHealth, trends, features, and main technologies to explore the advancement of mHealth. It presents cyber threats in mHealth applications where the common cyber threats were identified and security challenges such as data compromise, malicious attacks, insecure systems, and user vulnerabilities. We provided mitigation strategies to address the inherent challenges. It further highlights traditional and contemporary and centralized and decentralized detection approaches to cyber threats for effective implementation of patient health data. The findings showed that much work is left undone which we offered in open research prospects. The outlined open research prospects will improve the security and privacy of health-sensitive data against cyber threats. The practical implication of this study is that it will guide all health stakeholders, such as patients, health professionals, agencies, government and policymakers, and researchers, in improving the security and privacy of patients' data in digital cyberspace.",
issn="2948-2992",
doi="10.1007/s10791-025-09686-z",
url="https://doi.org/10.1007/s10791-025-09686-z"
}


@Article{Zhang2025,
author="Zhang, Yu-Wei
and Jin, Zhi
and Wang, Ze-Jun
and Xing, Ying
and Li, Ge",
title="SAGA: Summarization-Guided Assert Statement Generation",
journal="Journal of Computer Science and Technology",
year="2025",
month="Jan",
day="01",
volume="40",
number="1",
pages="138--157",
abstract="Generating meaningful assert statements is one of the key challenges in automated test case generation, which requires understanding the intended functionality of the tested code. Recently, deep learning based models have shown promise in improving the performance of assert statement generation. However, the existing models only rely on the test prefixes along with their corresponding focal methods, yet ignore the developer-written summarization. Based on our observations, the summarization contents usually express the intended program behavior or contain parameters that will appear directly in the assert statement. Such information will help existing models address their current inability to accurately predict assert statements. This paper presents a summarization-guided approach for automatically generating assert statements. To derive generic representations for natural language (i.e., summarization) and programming language (i.e., test prefixes and focal methods), we leverage a pre-trained language model as the reference architecture and fine-tune it on the task of assert statement generation. To the best of our knowledge, the proposed approach makes the first attempt to leverage the summarization of focal methods as the guidance for making the generated assert statements more accurate. We demonstrate the effectiveness of our approach on two real-world datasets compared with state-of-the-art models.",
issn="1860-4749",
doi="10.1007/s11390-023-2878-6",
url="https://doi.org/10.1007/s11390-023-2878-6"
}


@Inbook{Rajagopal2025,
author="Rajagopal",
title="Strategic Capability and Direction",
bookTitle="Contemporary Marketing Strategy: Analyzing Consumer Behavior to Drive Managerial Decision Making",
year="2025",
publisher="Springer Nature Switzerland",
address="Cham",
pages="169--209",
abstract="This chapter deliberates strategic capabilities and direction toward developing long-term marketing strategies by strengthening the organizational capabilities and competencies. Developing organizational vision and mission with employee engagement and deriving sustainable advantages of continuous improvement efforts involving employees form the core of discussions in this chapter. This chapter discusses the role of growing technology with artificial intelligence (AI), augmented reality (AR), virtual reality (VR), machine learning (ML), large language modeling (LLM), and Metaverse in developing competitive marketing strategy and creating consumer value. It is proposed to confine the intensive discussion of technology in marketing which include AI, ML, AR, VR, MR, LLM, and Metaverse to this chapter which focuses on the discussion of strategic capabilities and competencies. As the book has various perspectives on consumer behavior and marketing strategies, technology cannot be a part of discussion in all chapters. However, the core marketing-mix product, price, place, and promotion strategies will be discussed contextually to the technology in marketing. In addition, the role of sustainability in developing marketing strategies, impact of digital transformation (AI, ML, AR, VR, LLM, and Metaverse), and consumer privacy, ethical dilemmas, and data analytics will be discussed briefly in this chapter on Building Capabilities and Competencies. In addition, market leadership, technology used in marketing, and digital visualization topics have been covered in this chapter. The brief case studies discussed in this chapter include the downfall of Blackberry and IndiGo Airlines with the low-cost business model.",
isbn="978-3-031-95921-9",
doi="10.1007/978-3-031-95921-9_4",
url="https://doi.org/10.1007/978-3-031-95921-9_4"
}


@Article{Durmishi2025,
author="Durmishi, Ledian{\"e}
and R{\'a}k{\'o}czi, Attila
and Farkas, Tibor",
title="Policy Incentives for Economic Development and Climate Adaptation in Rural Europe",
journal="Current Climate Change Reports",
year="2025",
month="Oct",
day="13",
volume="11",
number="1",
pages="9",
abstract="This review explores how European policy incentives, including grants, subsidies, and capacity-building programs, can support rural communities in their pursuit of sustainable economic growth and urgent adaptation to climate change. It intends to understand what policies work, where they fall short, and how these policies can be improved to build stronger and climate-resilient rural areas.",
issn="2198-6061",
doi="10.1007/s40641-025-00206-1",
url="https://doi.org/10.1007/s40641-025-00206-1"
}


@Inbook{Halsey2022,
author="Halsey, Mike",
title="Windows 10 File Structure in Depth",
bookTitle="Windows 10 Troubleshooting: Learn to Troubleshoot and Repair Windows 10 Problems Like the Pros Do",
year="2022",
publisher="Apress",
address="Berkeley, CA",
pages="729--755",
abstract="I just counted how many files are currently sitting on my C: drive, where Windows 10 is installed on my PC. This doesn't count my documents, pictures, music, and video, which I store elsewhere to ensure file security, but there are well over half a million files in almost one hundred and thirty thousand folders sitting on the drive.",
isbn="978-1-4842-7471-2",
doi="10.1007/978-1-4842-7471-2_28",
url="https://doi.org/10.1007/978-1-4842-7471-2_28"
}


@Article{McNealis2026,
author="McNealis, Rachel",
title="Shame in the machine: affective accountability and the ethics of AI",
journal="AI {\&} SOCIETY",
year="2026",
month="Jan",
day="01",
volume="41",
number="1",
pages="403--413",
abstract="The cultural weaponization of shame surrounding the use of artificial intelligence (AI) tools like ChatGPT often redirects ethical scrutiny away from systemic concerns and toward individual users. Drawing on Sara Ahmed's affect theory, this paper argues that cultural narratives of ``AI shaming'' function as moral displacement that redirects scrutiny away from the environmental costs, exploitative labor practices, and corporate monopolization defining contemporary AI development. The analysis examines how shame operates across academic and professional settings to create ``effort anxiety'' that demands both visible human labor and accelerated productivity. Current discourse treats AI use as a personal virtue problem and obscures the carbon-intensive data centers, underpaid content moderators, and proprietary knowledge systems that enable these technologies. Instead of eliminating shame, the paper proposes redirecting it toward collective accountability for AI's systemic harms. Environmental degradation, algorithmic bias, and extractive infrastructures represent the true ethical frontier of artificial intelligence. Policy frameworks, educational interventions, and governance structures offer pathways for transforming shame from individual punishment into institutional reform. The stakes extend beyond AI itself: as emerging technologies reshape society, the patterns of moral responsibility established now will determine whether innovation serves collective flourishing or perpetuates existing inequalities. Shame can become a vehicle for institutional critique and systemic accountability if we redirect its focus from individual users to the powerful corporations, governance structures, and infrastructural systems that profit from AI's rapid expansion.",
issn="1435-5655",
doi="10.1007/s00146-025-02472-x",
url="https://doi.org/10.1007/s00146-025-02472-x"
}


@inproceedings{10.1007/978-3-032-07163-7_2,
 abstract = {Psychosocial contagion theory has evolved significantly, shifting from early models of group dynamics to contemporary frameworks that explain how emotional and ideational forces propagate through digital networks and emerging technologies. Initially grounded in studies of emotional contagion (where emotions spread through interpersonal interactions) the theory has expanded to include cognitive and ideational (including ideological) transmission, profoundly shaping collective beliefs and behaviours on a large scale. At the core of this broader framework lies cultural memetics which describes the transmission of stable traditions, symbols, and norms that collectively shape identities and behavioural patterns over time. Cultural memes maintain societal continuity by reinforcing shared values and long-standing practices. However, in the digital age, these traditional mechanisms are increasingly influenced by narrative memetics, a more dynamic and adaptive force. Unlike cultural memes, which preserve traditions, narrative memes actively shape perception and behaviour through repetition, mutation, and amplification within the information ecosystem. As an extension of psychosocial contagion, narrative memetics can take both constructive and destructive forms. As a malomeme, it manipulates public opinion to deepen social fragmentation through disinformation, disinfolklore, and psychosocial terrorism. These threats are further amplified by bots (automated software designed to perform tasks online) which enable malomemes to self-organise, making them more potent and disruptive to social coherence. The rise of media moguls turned digital oligarchs, who exert control over information flows and facilitate the spread of malomemes, influencing political structures and public discourse. While society possesses a social immune system intended to counteract malomemes and mitigate their psychological and social harms, its effectiveness remains uncertain. However, the notion of the bonimeme holds the potential to reinforce this defence by promoting social cohesion, countering harmful narratives, and strengthening collective resilience.},
 address = {Cham},
 author = {Yolles, Maurice},
 booktitle = {Technology and Society - Boon or Bane?},
 editor = {Gazzola, Patrizia
and Dominici, Gandolfo},
 isbn = {978-3-032-07163-7},
 pages = {16--43},
 publisher = {Springer Nature Switzerland},
 title = {The Psychological Impact of Technology: Narrative Memetics and Psychosocial Contagion in Digital Network},
 url = {https://link.springer.com/chapter/10.1007/978-3-032-07163-7_2},
 year = {2025}
}

@inproceedings{10.1007/978-981-96-4506-0_2,
 abstract = {With the advancement of informatization, the economic, political, and national life have become increasingly dependent on network information systems, making network information security issues a prominent concern for national and social security. Driven by economic or political interests, the competition among enterprises, institutions, and even governments in the cyberspace are continuously intensifying. Network attacks characterized by high technology, high concealment, and long duration have become one of the main challenges facing current network security. The article introduces and describes this type of covert network attack, analyzes its main features and security challenges, and reviews the latest research findings on the detection of covert network attacks at home and abroad. Finally, it summarizes the key technical issues and looks forward to the future research directions in this field.},
 address = {Singapore},
 author = {Peng, Jin
and Lu, Hui
and Xu, Haosheng
and Huang, Xun
and Zheng, Chencong
and Xue, Jieyao
and Liu, Zhouyang
and Zhang, Xinge
and Liu, Youyu
and Zhang, Haonan},
 booktitle = {Cyberspace Simulation and Evaluation},
 editor = {Xu, Guangxia
and Zhou, Wanlei
and Zhang, Jiawei
and Zhang, Yanchun
and Jia, Yan},
 isbn = {978-981-96-4506-0},
 pages = {23--42},
 publisher = {Springer Nature Singapore},
 title = {A Survey on Covert Network Attack},
 url = {https://link.springer.com/chapter/10.1007/978-981-96-4506-0_2},
 year = {2025}
}

@inproceedings{10.1007/978-3-032-10947-7_2,
 abstract = {Modern battlefield theatre is far more complex and kinetic than ever before. The emergence of new technologies, such as biotechnology, artificial intelligence, Unmanned Aerial Vehicles, Information technology, the Internet of Things, and other similar recent technologies, has altered warfare dynamics. Notwithstanding their beneficial uses, these technologies present dual-use challenges, as they can compromise security and present harm, such as physical, financial, social, and even emotional harm. Hence, deploying strategic tools to thwart such threats is of vital importance. This article provides an overview of dual-use research, methods and technologies, the nature of threat vectors, and how approaches such as complexity sciences, red-teaming exercises, and technological advancements, in conjunction with ATT{\&}CK tools, techniques, and procedures, can be used to thwart such threat vectors, to develop a resiliency strategy. The landscape of dual-use technologies is quite large; hence, the scope of this article is focused on a few emerging technologies for both challenges and solutions.},
 address = {Cham},
 author = {Vaseashta, Ashok},
 booktitle = {Developments and Advances in Defense and Security},
 editor = {Rocha, Alvaro
and Vaseashta, Ashok
and Fajardo-Toro, Carlos Hernan
and Riola, Jose Maria},
 isbn = {978-3-032-10947-7},
 pages = {11--24},
 publisher = {Springer Nature Switzerland},
 title = {Developing Resilience Against Cyber, Chemical, Biological, and Artificial Intelligence Dual-Use Technology Threats},
 url = {https://link.springer.com/chapter/10.1007/978-3-032-10947-7_2},
 year = {2026}
}

@Article{Yang2023,
author="Yang, Guang
and Zhou, Yu
and Chen, Xiang
and Zhang, Xiangyu
and Xu, Yiran
and Han, Tingting
and Chen, Taolue",
title="A syntax-guided multi-task learning approach for Turducken-style code generation",
journal="Empirical Software Engineering",
year="2023",
month="Oct",
day="14",
volume="28",
number="6",
pages="141",
abstract="Due to the development of pre-trained language models, automated code generation techniques have shown great promise in recent years. However, the generated code will not always adhere to syntactic constraints of the target language, especially in the case of Turducken-style code, where declarative code snippets are embedded within imperative programs. In this study, we summarize three significant challenges in regards to syntactic constraints: (1) the efficient representation of syntactic constraints, (2) the effective integration of syntactic information, and (3) the scalable syntax-first decoding algorithm. To address these challenges, we propose a syntax-guided multi-task learning approach TurduckenGen. Specifically, we first explicitly append the type information to the code tokens to capture the representation of syntactic constraints. Then we formalize code generation with syntactic constraint representation as an auxiliary task to enable the model to learn the syntactic constraints of the code. Finally, the syntactically correct code is selected accurately from the multiple candidates with the help of the compiler feedback. Extensive experiments and comprehensive analysis demonstrate the effectiveness and general applicability of our approach after being compared with six state-of-the-art baselines on two Turducken-style code datasets. Finally, we conducted a human study and found the code quality generated by our approach is better than baselines in terms of code readability and semantic similarity.",
issn="1573-7616",
doi="10.1007/s10664-023-10372-1",
url="https://doi.org/10.1007/s10664-023-10372-1"
}


@Article{Xie2024,
author="Xie, Mingshan
and Wang, Yuchen
and Huang, Haiping",
title="Local-contrastive-learning machine with both generalization and adversarial robustness: A statistical physics analysis",
journal="Science China Physics, Mechanics {\&} Astronomy",
year="2024",
month="Nov",
day="05",
volume="68",
number="1",
pages="210511",
abstract="Distinct from human cognitive processing, deep neural networks trained by backpropagation can be easily fooled by adversarial examples. To design a semantically meaningful representation learning, we discard backpropagation, and instead, propose a local contrastive learning, where the representations for the inputs bearing the same label shrink (akin to boson) in hidden layers, while those of different labels repel (akin to fermion). This layer-wise learning is local in nature, being biologically plausible. A statistical mechanics analysis shows that the target fermion-pair-distance is a key parameter. Moreover, the application of this local contrastive learning to MNIST benchmark dataset demonstrates that the adversarial vulnerability of standard perceptron can be greatly mitigated by tuning the target distance, i.e., controlling the geometric separation of prototype manifolds.",
issn="1869-1927",
doi="10.1007/s11433-024-2504-8",
url="https://doi.org/10.1007/s11433-024-2504-8"
}


@inproceedings{10.1007/978-3-031-42622-3_49,
 abstract = {The recent technological enhancements in the field of large language models and their integration into collaborative processes, for example, as chatbots, are perceived as key drivers for further transformations of work. However, the transformative effects of these technological enhancements have to be more thoroughly investigated in specific work contexts to benefit from the great potential of improvement. This research article provides findings of a case study research on how employees in software engineering perceive the collaboration with AI-powered chatbots, such as chatGPT. We investigate patterns employees develop to cope with the novel demands arising during the collaboration with these technologies and discuss our empirical findings regarding a conceptual framework of AI-related competences and another case study from a different industry. The findings contribute to a better understanding of human actors' AI-related coping patterns as key prerequisites for a more responsible and sustainable usage of this technology in professional work contexts.},
 address = {Cham},
 author = {S{\"u}{\ss}e, Thomas
and Kobert, Maria
and Grapenthin, Simon
and Voigt, Bernd-Friedrich},
 booktitle = {Collaborative Networks in Digitalization and Society 5.0},
 editor = {Camarinha-Matos, Luis M.
and Boucher, Xavier
and Ortiz, Angel},
 isbn = {978-3-031-42622-3},
 pages = {689--705},
 publisher = {Springer Nature Switzerland},
 title = {AI-Powered Chatbots and the Transformation of Work: Findings from a Case Study in Software Development and Software Engineering},
 url = {https://link.springer.com/chapter/10.1007/978-3-031-42622-3_49},
 year = {2023}
}

@Article{Krishna2025,
author="Krishna, Lalit Kumar Radha
and Ravindran, Nila
and Kwok, Hannah Yi Fang
and Tan, Xuan Yu
and Soh, Jasper
and Leong, Elizabeth Yong Mei
and Wan, Darius Wei Jun
and Low, Tiat Yan
and Chan, Aiden Wei-Jun
and Lim, Nicholas Chong Jin
and Ng, Yen Kit
and Thenpandiyan, Arthena Anushka
and Leong, Jun Rey
and Lim, Adele Yi Dawn
and Quah, Elaine Li Ying
and Tse, Leia Ning
and PL, Sriram
and Rajanala, Sri Priyanka
and Lua, Jun Kiat
and Rajalingam, Varsha
and Fam, Victoria Jia En
and Govindasamy, Ranitha
and AbdulHamid, Nur Amira Binte
and Lim, Crystal
and Ong, Eng Koon
and Sim, Shin Wei
and Mason, Stephen
and Ong, Simon Yew Kuang",
title="The impact of mentoring relationships on professional identity formation in medical education: a systematic review",
journal="BMC Medical Education",
year="2025",
month="Apr",
day="19",
volume="25",
number="1",
pages="576",
abstract="The promise that enduring and personalised mentoring relationships shape how mentees think, feel and act as professionals, or their professional identity formation (PIF), and thus how they interact, care and support patients and families has garnered significant interest. However, efforts to marshall these elements have been limited due to a lack of effective understanding. To address this lacunae, a systematic scoping review was carried out to map current knowledge on mentoring relationships and its impact on PIF.",
issn="1472-6920",
doi="10.1186/s12909-025-07158-y",
url="https://doi.org/10.1186/s12909-025-07158-y"
}


@Article{Shen2025,
author="Shen, Louisa",
title="Not the machine's fault: taxonomising AI failure as computational (mis)use",
journal="AI {\&} SOCIETY",
year="2025",
month="Dec",
day="01",
volume="40",
number="8",
pages="5793--5807",
abstract="This paper proposes a re-examination of connectionist AI failures (controversial incidents) from the perspective of technological use. It advances four categories of failure: technically sound outputs inherent to connectionist programming; machine-world mis-configuration; motivational failure that deploys technology for illegitimate ends; and finally epistemic failure of misapplication where computing and AI are being used to solve for the wrong sets of social problems. Drawing on the history of computing, the paper argues that computational machines and its software (classical or connectionist) are numerical, procedural, and electronic in nature, and therefore, are geared to treat problems through the functions of numerical  calculation, tabulation, approximation, and extrapolation. On account of these limitations, failure ensues when computers meet the many problems that cannot be solved on these grounds. The paper proposes and calls for an ontological comparison between computers and the problems they are pressed to serve prior to any pragmatic deployment.",
issn="1435-5655",
doi="10.1007/s00146-025-02333-7",
url="https://doi.org/10.1007/s00146-025-02333-7"
}


@Inbook{Kessler2025,
author="Kessler, T. T.
and Hancock, P. A.
and Sanders, T. L.
and Hancock, G. M.
and Kaplan, A. D.",
editor="Xu, Wei",
title="AI Risk and Trust",
bookTitle="Handbook of Human-Centered Artificial Intelligence",
year="2025",
publisher="Springer Nature Singapore",
address="Singapore",
pages="1--33",
abstract="The use of AI comes with inherent risks to humans and therefore impacts their trust in such systems. These include, but are not limited to, technical, ethical, privacy, security, social, and legal risks. Each of these impacts the ability of a human to properly calibrate their trust in an AI system. From a human-centered AI (HCAI) perspective, building appropriate trust requires designing AI systems that align with human needs, values, and capabilities. The present chapter examines and evaluates the foundational concepts of human trust in AI, reviews the major risks to trust, discusses the trust calibration process through transparency, interpretability, and user-centered design, and summarizes how explainability impacts trust in AI. It also elucidates ethical considerations that impact human trust in AI as well evaluates the regulatory and legal viewpoints that can and should exist to govern AI. It provides a window upon the future trends and challenges of AI governance and regulation to create trustworthy systems. Finally, it asserts that human trust in AI is predicated upon the understanding and acceptance of the risk of the use of AI, and without the opportunity for a choice whether to use the technology or not, the issue of trust becomes sadly inconsequential.",
isbn="978-981-97-8440-0",
doi="10.1007/978-981-97-8440-0_80-1",
url="https://doi.org/10.1007/978-981-97-8440-0_80-1"
}


@Inbook{Sanders2026,
author="Sanders, John",
title="Abstracting Austen: Playing with the Possible in Good Society: A Jane Austen RPG",
bookTitle="Literary Game Adaptations: A Systems Approach",
year="2026",
publisher="Springer Nature Switzerland",
address="Cham",
pages="107--151",
abstract="Jane Austen's 1813 novel Pride and Prejudice, like the present chapter, is all about the process by which people mentally model characters. The protagonist, Elizabeth Bennet, spends much of the novel forming and reforming a conception of the eligible bachelor Mr. Darcy. Her initial meeting with the gentleman---who is described as having a ``fine, tall person, handsome features, [and] noble mien'' in addition to his ``ten thousand pounds a year'' (6)---leaves her with a negative impression after she overhears him call her ``tolerable, but not handsome enough to tempt me'' (7). Despite numerous instances of witty banter and romantic chemistry between the two, Elizabeth's dislike for Mr. Darcy only increases when she is told that he has engineered the disinheritance of a certain Mr. Wickham and had convinced his friend, Mr. Bingley, to reject her sister Jane's advances. When Mr. Darcy suddenly proposes (and seemingly expects her consent despite all of these actions), the shocked Elizabeth considers him to be arrogant, conceited, and selfishly disdainful of the feelings of others (131). After curtly telling him so, she begins to discover there is more to the story: Darcy only separated Bingley and Jane because he thought Jane cared little for his best friend, and Wickham turns out to be a scoundrel who absconds with Elizabeth's younger sister Lydia. It is only after finding out that Darcy secretly forced Wickham to marry Lydia and thus saved her family from scandal that Elizabeth's understanding of Darcy changes: Mr. Darcy is not an aloof, selfish man who denies others of their happiness, but a noble one who is capable of great deeds provided he can overcome his pride.",
isbn="978-3-032-04893-6",
doi="10.1007/978-3-032-04893-6_4",
url="https://doi.org/10.1007/978-3-032-04893-6_4"
}


@Article{Chen2025,
author="Chen, Luyu
and Varga, Lilla
and Mehdizadkhani, Milad",
title="Taming AI for The Little Prince: a comparative analysis of NMTs and LLMs in Hungarian translation",
journal="Humanities and Social Sciences Communications",
year="2025",
month="Dec",
day="02",
volume="13",
number="1",
pages="46",
abstract="Achieving high-quality translation for literary works poses a unique challenge for machine translation models. This study compares Hungarian translations of Antoine de Saint-Exup{\'e}ry's novella, The Little Prince, produced by two leading neural machine translation (NMT) services (DeepL and Google Translate) and two large language models (LLMs) (Google Bard and ChatGPT 3.5). While the NMT tools achieved decent accuracy, their outputs often lacked the nuance required to capture the text's literary essence. Notably, our research addresses a gap in prompt engineering by investigating whether the LLMs' performance could be enhanced by using tailored, genre-specific prompts based on literary style guides, in contrast to baseline zero-shot outputs. Interestingly, this approach led to significant improvements for Google Bard in punctuation, grammar, and the preservation of literary devices. Conversely, the same prompt negatively affected the quality of the translation generated by ChatGPT 3.5. These findings suggest that while genre-specific prompts can guide certain LLMs toward self-correction, their effectiveness is highly model-dependent.",
issn="2662-9992",
doi="10.1057/s41599-025-06343-0",
url="https://doi.org/10.1057/s41599-025-06343-0"
}


@Inbook{Huang2024,
author="Huang, Jerry
and Huang, Ken
and Ma, Winston",
editor="Huang, Ken
and Parisi, Carlo
and Tan, Lisa JY
and Ma, Winston
and Zhang, Zhijun William",
title="Summary and Future Trends",
bookTitle="Web3 Applications Security and New Security Landscape: Theories and Practices",
year="2024",
publisher="Springer Nature Switzerland",
address="Cham",
pages="239--260",
abstract="This concluding chapter crystallizes the evolution of Web3 security thus far even as innovations continue apace. Emerging frontiers in quantum cryptography, AI security, IoT, and AR/VR are analyzed to highlight new attack surfaces. Community vigilance through education, responsible innovation, and collaboration is reaffirmed as vital to balance open access and resilience. As Web3 promises a new trust paradigm for digital ecosystems, robust security is highlighted as foundational to mainstream adoption.",
isbn="978-3-031-58002-4",
doi="10.1007/978-3-031-58002-4_11",
url="https://doi.org/10.1007/978-3-031-58002-4_11"
}


@Article{Blanco2025,
author="Blanco, Sara",
title="Human trust in AI: a relationship beyond reliance",
journal="AI and Ethics",
year="2025",
month="Aug",
day="01",
volume="5",
number="4",
pages="4167--4180",
abstract="Trust in artificial intelligence (AI) is often discussed by both the general public and a part of academia. The discourse on trust in AI is often presented as analogous to trust in people. However, it is unclear whether the concept of trust can suitably be extended to describe relationships between humans and other entities. In this article, I will argue that the main features of trusting relationships apply both when the trustee is a human or an AI system. This view is opposed to the claim that only humans can be trusted and that technology, at its best, can be just relied on. However, it is commonly accepted that reliance has weaker implications than trust. We often rely on those whom we need or want to do something for us, regardless of their motivation to act. I will argue that motivation is relevant for trust, both in humans and in AI. Because of this, I propose trust as a suitable goal to aim for when shaping human-AI relationships.",
issn="2730-5961",
doi="10.1007/s43681-025-00690-z",
url="https://doi.org/10.1007/s43681-025-00690-z"
}


@Inbook{Halsey2023,
author="Halsey, Mike",
title="Windows 11 File and Folder Structure in Depth",
bookTitle="Troubleshooting and Supporting Windows 11: Creating Robust, Reliable, Sustainable, and Secure Systems",
year="2023",
publisher="Apress",
address="Berkeley, CA",
pages="299--320",
abstract="I've just done a count of how many files I have on the C: drive of my PC where Windows 11 is installed. It's not a small number being 689,246 files in 139.196 folders, taking up a not insignificant 1.06TB of my 2TB SSD. This isn't including my documents and files either, which I always store on a separate SSD and which consume 638GB on their own.",
isbn="978-1-4842-8728-6",
doi="10.1007/978-1-4842-8728-6_11",
url="https://doi.org/10.1007/978-1-4842-8728-6_11"
}


@Article{McCumber2026,
author="McCumber, Andrew",
title="Structures of feeling in climate fiction",
journal="American Journal of Cultural Sociology",
year="2026",
month="Feb",
day="03",
abstract="With a steady onslaught of extreme weather events and quickly surpassed temperature records, the climate crisis is increasingly moving from the realm of hypotheticals to that of lived experience. In this context, many sociologists have called for their discipline to make climate change a core agenda item. While climate change-focused sociology has certainly increased of late, a general dissatisfaction with the discipline's treatment of the issue remains. Two obstacles to developing of a fully fledged ``sociology of climate change'' are tensions of temporality and scale; its global scope and a tendency to discuss it in terms of possible futures make it difficult to frame climate change in terms of the present-day and localized phenomena many sociologists study. I propose Raymond Williams's concept of ``structures of feeling'' as a cultural framework for studying climate change as it is actively experienced in the present. Williams used structures of feeling to describe the distinctive quality of experience that characterizes a period, and to challenge us to consider how the personal, affective realm of ``feeling'' is implicated in social processes that transcend us. Using computational methods and qualitative analysis and following Williams' assertion that structures of feeling are most visible in creative work, I analyze short stories in the literary subgenre of ``climate fiction.'' I find a temporal shift in these works, where they transition between imagining the acute destruction of climate chaos and the possibilities of post-apocalypse. I take these shifts to indicate underlying patterns in the affective social experience of climate change.",
issn="2049-7121",
doi="10.1057/s41290-026-00279-x",
url="https://doi.org/10.1057/s41290-026-00279-x"
}


@inproceedings{10.1007/978-3-031-89296-7_17,
 abstract = {This scientific article presents a comprehensive investigation into the amalgamated technical expertise cultivated through hands-on engagement in commercial projects. Its overarching aim is to align the training of emerging DevOps engineers with the dynamic requirements of the real-world industry. Central to our inquiry is the meticulous examination of the intricate stages within CI processes, informed by the rich practical insights of seasoned DevOps professionals. Our exploration reveals invaluable insights, extrapolating their relevance to benefit both IT educators and active DevOps practitioners alike. Rooted in the pragmatic utilization of a diverse spectrum of DevOps practices, we elucidate these stages with real-world examples and experiences. By drawing upon multifaceted encounters encountered in real-world scenarios, our article offers a nuanced comprehension of the challenges and successes inherent in the DevOps landscape. This nuanced approach not only enriches the theoretical knowledge transmitted to future engineers but also furnishes a repository of practical wisdom readily applicable in professional environments. The article also examines the influence of integrating AI into DevOps workflows as a transformative factor. Building on the foundational practices of DevOps, AI's role in automating routine tasks, optimizing CI/CD pipelines, and enhancing decision-making processes exemplifies how cutting-edge technology bridges the gap between theory and practice.},
 address = {Cham},
 author = {Bodnar, Liliia
and Bodnar, Mykola
and Shulakova, Kateryna
and Vasylenko, Oksana
and Siemens, Eduard
and Tsyra, Oleksandra},
 booktitle = {Applied Innovations in Information and Communication Technology},
 editor = {Dovgyi, Stanislav
and Siemens, Eduard
and Globa, Larysa
and Kopiika, Oleh
and Stryzhak, Oleksandr},
 isbn = {978-3-031-89296-7},
 pages = {336--359},
 publisher = {Springer Nature Switzerland},
 title = {A Comprehensive Integration of Practical Strategies in DevOps},
 url = {https://link.springer.com/chapter/10.1007/978-3-031-89296-7_17},
 year = {2025}
}

@Inbook{Agerwala2024,
author="Agerwala, Tilak",
editor="Menon, Sangeetha
and Todariya, Saurabh
and Agerwala, Tilak",
title="Artificial Intelligence: A Case for Ethical Design and Multidisciplinarity",
bookTitle="AI, Consciousness and The New Humanism: Fundamental Reflections on Minds and Machines",
year="2024",
publisher="Springer Nature Singapore",
address="Singapore",
pages="55--73",
abstract="Autonomous and intelligent systems and services that use narrow artificial intelligence technologies such as statistical learning and limited inferencing (AISSN) are pervasive in our lives and industry. These systems, very far from having human-like intelligence, will offer significant potential for doing social good, achieving productivity gains and advancing science and engineering. However, AISSN systems can have unanticipated and harmful impacts. This chapter highlights the ethical challenges of AISSNs using three diverse and pervasive examples: Internet of Things, conversational AI, and semi-autonomous vehicles. We contend that AISSNs will be the norm for the foreseeable future and that artificial general intelligence will not develop anytime soon. The ethical challenges of AISSNs are addressable using human-centred ``Ethical Design'', the use of widely accepted moral standards of right and wrong to guide the conduct of people in the ideation, design, development, and deployment of AISSN systems. Depending on the problem domain, multidisciplinary teams of computer scientists and engineers, sociologists, economists, ethicists, linguists, and cultural anthropologists will be required to implement humanistic design processes.",
isbn="978-981-97-0503-0",
doi="10.1007/978-981-97-0503-0_4",
url="https://doi.org/10.1007/978-981-97-0503-0_4"
}


@inproceedings{10.1007/978-981-97-5200-3_6,
 abstract = {As a backbone of our economy, playing a very crucial role in the movement of our economy, the trucking industry still has not come out of the persistent problems it has been facing over the last few decades such as high cost of operation, low efficiency in operation, government issues, regulatory and legislative mechanism, information issues and many more. However, adopting these latest available technologies and their effective use can solve many of these problems significantly. One such is the adaptation of digitalization properly for Digital Governance which further results in Good Governance and another is the adoption of blockchain technology for data and information management to enhance operation efficiency of the trucking industry. Today in most cases especially for small truck operations, the operation is not viable due to poor technology adoption. Hence with this paper, an attempt is being made to initiate these improvements. The methodology adopted for the study is also relevant to the requirement of the work undertaken in the form of a structured approach. It consists of a relevant literature review, primary and secondary sources of information with available data, and a model framework about digitalization, digital governance, and possible application of blocking technology to bring improvement in operations of the trucking segment in our economy. The study found that the adoption of the latest technology in trucking operations in the form of digitization and blockchain reduces the problem of asymmetric information and brings good governance to enhance operation efficiency. So, the study concludes by stating that if the government supports the trucking industry, provides consultation, and coordinates with other stakeholders of the industry regarding technology adaptation and other aspects, the trucking industry can yield better results and solve many of its problems independently.},
 address = {Singapore},
 author = {Parihar, Mahendra
and Akshaya},
 booktitle = {Intelligent System and Data Analysis},
 editor = {Nanda, Priyadarsi
and Srivastava, Sumit
and Verma, Vivek Kumar
and Vyas, Pankaj},
 isbn = {978-981-97-5200-3},
 pages = {75--90},
 publisher = {Springer Nature Singapore},
 title = {Enabling Technology for Smart and Intelligent Transportation: Analyse the Uses and Impact of Blockchain Technology and Artificial Intelligent on Trucking Industry for Better Results},
 url = {https://link.springer.com/chapter/10.1007/978-981-97-5200-3_6},
 year = {2025}
}

@Inbook{Fedorko2025,
author="Fedorko, Richard
and Mondal, Subhra R.",
editor="Mondal, Subhra R.
and Vartiak, Lukas
and Das, Subhankar",
title="The Algorithmic Alchemist: Transmuting Business Models for a Net-Zero Future",
bookTitle="Generative AI for a Net-Zero Economy: Managing Climate Change and Business Innovation in the Digital Era",
year="2025",
publisher="Springer Nature Singapore",
address="Singapore",
pages="37--55",
abstract="In the face of the climate crisis, urgent reinvention of business models is needed so that profitability and planetary health are in alignment. This chapter examines how artificial intelligence (AI) can serve catalytically in this transition, as an ``algorithmic alchemist'' capable of transmogrifying linear, extractive practices into regenerative, net-zero systems. Focusing on interdisciplinary analysis and specific case studies, it explores the role of AI in frontiers of technology that underpin circular economies, product-as-a-service (PaaS) frameworks, and systematic innovations to decouple economic growth from resource depletion. But the networked, AI-driven models---predictive maintenance services; novel logistics management services; dynamic pricing services---that could exponentially reduce waste and emissions from our systems will not come without complications and challenges to implement. Hollowing out of governments by giant corporations, massive energy requirements for AI training, and ethical risks from issues such as algorithmic bias and inequitable technology distribution call for balanced governance. It presents a gradual path for organizations toward sustainable AI adoption, emphasizing agility, stakeholder collaboration, and ethical governance. The debriefing from economists and industry leaders indicates the complex interplay between technological viability and regulatory coherence, as well as sociocultural readiness. Therefore, cross-sector collaboration and value reconfiguration are the keys to bridging the gap---achieving alignment where ecological resilience takes priority over short-term gain.",
isbn="978-981-96-8015-3",
doi="10.1007/978-981-96-8015-3_3",
url="https://doi.org/10.1007/978-981-96-8015-3_3"
}


@Article{Soliman2024,
author="Soliman, Mona M.
and Ahmed, Eman
and Darwish, Ashraf
and Hassanien, Aboul Ella",
title="Artificial intelligence powered Metaverse: analysis, challenges and future perspectives",
journal="Artificial Intelligence Review",
year="2024",
month="Feb",
day="05",
volume="57",
number="2",
pages="36",
abstract="The Metaverse, a virtual reality (VR) space where users can interact with each other and digital objects, is rapidly becoming a reality. As this new world evolves, Artificial Intelligence (AI) is playing an increasingly important role in shaping its development. Integrating AI with emerging technologies in the Metaverse creates new possibilities for immersive experiences that were previously impossible. This paper explores how AI is integrated with technologies such as the Internet of Things, blockchain, Natural Language Processing, virtual reality, Augmented Reality, Mixed Reality, and Extended Reality. One potential benefit of using AI in the Metaverse is the ability to create personalized experiences for individual users, based on their behavior and preferences. Another potential benefit of using AI in the Metaverse is the ability to automate repetitive tasks, freeing up time and resources for more complex and creative endeavors. However, there are also challenges associated with using AI in the Metaverse, such as ensuring user privacy and addressing issues of bias and discrimination. By examining the potential benefits and challenges of using AI in the Metaverse, including ethical considerations, we can better prepare for this exciting new era of VR. This paper presents a comprehensive survey of AI and its integration with other emerging technologies in the Metaverse, as the Metaverse continues to evolve and grow, it will be important for developers and researchers to stay up to date with the latest developments in AI and emerging technologies to fully leverage their potential.",
issn="1573-7462",
doi="10.1007/s10462-023-10641-x",
url="https://doi.org/10.1007/s10462-023-10641-x"
}


@Article{López-Pascual2025,
author="L{\'o}pez-Pascual, Ernesto
and Moreno-Torres, Marta
and Moro, Erika
and Rapisarda, Anna
and Ortega-Vallbona, Rita
and Serrano-Candelas, Eva
and Gozalbes, Rafael
and Jover, Ramiro
and Castell, Jos{\'e} V.",
title="Ontogeny of drug-induced fatty liver disease (DIFLD): from key initiating events to disease phenotypes",
journal="Archives of Toxicology",
year="2025",
month="Dec",
day="01",
volume="99",
number="12",
pages="5075--5091",
abstract="We conducted an expert review of clinical case reports on drug-induced fatty liver disease (DIFLD) to classify drugs according to distinct clinical phenotypes. Seven clusters were identified based on clinical, biochemical, and histological features reflecting drug toxic mechanisms:Cluster 0 (Control): Drugs with no known steatotic effects or clinical evidence of DIFLD.Cluster 1: Drugs with mild pro-steatotic effects, exacerbating existing metabolic steatosis without significant liver enzyme elevation.Cluster 2: Compounds causing moderate steatosis with mild hepatocellular damage, occasional enzyme increases, and delayed onset.Cluster 3: Agents causing severe mitochondrial dysfunction, ATP depletion, and lactic acidosis, initially without inflammation.Cluster 4: Drugs inducing inflammatory steatohepatitis with moderate elevations of liver enzymes (ALT, AST, ALP 90--700 U/L) but preserved liver function.Cluster 5: Drugs causing severe steatohepatitis with marked enzyme elevation (ALT, AST{\thinspace}>{\thinspace}700 U/L) indicating significant liver injury and inflammation.Cluster 6: Compounds causing steatohepatitis with additional cholestasis and elevated bilirubin (>{\thinspace}11 mg/dL).",
issn="1432-0738",
doi="10.1007/s00204-025-04178-x",
url="https://doi.org/10.1007/s00204-025-04178-x"
}


@Article{Kern2025,
author="Kern, Manuel
and Landauer, Max
and Skopik, Florian
and Weippl, Edgar",
title="Identifying Open-Source Threat Detection Resources on GitHub: A Scalable Machine Learning Approach",
journal="International Journal of Information Security",
year="2025",
month="Jun",
day="17",
volume="24",
number="4",
pages="158",
abstract="Many businesses rely on open-source software modules to build their technology stacks. However, those who lack domain expertise may struggle to find the right software due to unfamiliar terminology and specific names. As a consequence, search engines and other platforms often cannot be utilized effectively to discover appropriate solutions. There is thus a need for a more applicable approach to assist non-domain experts in navigating the vastness of available repositories, enabling them to efficiently discover and select the right solution for their business needs. To overcome these gaps, we introduce an approach that supports finding unpopular yet important open-source software repositories on GitHub using advanced machine learning techniques. For this purpose, we propose novel strategies for information gathering and data pre-processing that resolve scalability issues of existing solutions and enable clustering of repositories even when topics, descriptions, or repository names are unclear or absent. For our evaluation, we gathered a dataset of 221,971 repositories using GitHub search and keywords related to incident detection. We show that our approach is able to separate threat detection repositories from others with an F1-score of 0.93.",
issn="1615-5270",
doi="10.1007/s10207-025-01069-1",
url="https://doi.org/10.1007/s10207-025-01069-1"
}


@inproceedings{10.1007/978-3-031-88223-4_18,
 abstract = {Road accidents often result from drivers overlooking vehicles with open doors, particularly on high-speed expressways, where limited reaction time heightens the risk of collision. Traditional solutions involve promptly warning drivers to close open doors when approaching vehicles are detected. However, challenges arise with RADAR systems inaccurately distinguishing between open and closed doors due to signal limitations. This paper introduces a Sudden Door Opening Warning System (SDOWS) utilizing a Front Camera system and Deep Learning. To address the absence of a publicly accessible dataset, a synthetic dataset was created using Unity3D, simulating scenarios with open vehicle doors for training. A comparative study is conducted on object detection models, including YOLOv4 and YOLOv7, to detect vehicles within the path. Subsequently, a CNN classifier is employed to assess their door status, distinguishing between open and closed states. A tracking algorithm is integrated to enhance detection accuracy by continuously tracking individual vehicles. The proposed method attained maximum accuracy of 94.4{\%} with a mAP of 0.5 for vehicle detection and 96.6{\%} accuracy in classifying the door status.},
 address = {Cham},
 author = {Wankhade, Shubham
and Hegde, K. Sneha
and Kannan, Srividhya},
 booktitle = {Pattern Recognition. ICPR 2024 International Workshops and Challenges},
 editor = {Palaiahnakote, Shivakumara
and Schuckers, Stephanie
and Ogier, Jean-Marc
and Bhattacharya, Prabir
and Pal, Umapada
and Bhattacharya, Saumik},
 isbn = {978-3-031-88223-4},
 pages = {247--262},
 publisher = {Springer Nature Switzerland},
 title = {AI-Driven Approach for Warning Autonomous Vehicles from Sudden Vehicle Door Openings},
 url = {https://link.springer.com/chapter/10.1007/978-3-031-88223-4_18},
 year = {2025}
}

@Article{Song2025,
author="Song, Ziming",
title="Value-aligned but misguided: a dilemma in AI and AGI decision making",
journal="Synthese",
year="2025",
month="Aug",
day="29",
volume="206",
number="3",
pages="138",
abstract="The development of artificial intelligence (AI) systems raises distinctive ethical and theoretical challenges not only because such systems will participate in human society in ways that invite moral appraisal, but also because a superintelligent agent is expected to exhibit a level of instrumental rationality that enables it to make decisions with social impact. This paper reframes the AI value alignment problem as a problem of robustness in decision making. Drawing on modified trolley-problem-style scenarios, influenced by the ``Moral Machine'' experiment, I argue that even AI systems governed by fixed ethical principles may produce actions that fail to align with those very principles under certain contextual conditions. The underlying issue lies in AI's inability to reconcile the context-sensitive interpretation of values in a robust way. I suggest that this failure is best understood as arising from ambiguity in the interpretation of objectives. AI value alignment thus requires more than value specification; it demands the capacity to align values with contextually responsive belief formation and action selection.",
issn="1573-0964",
doi="10.1007/s11229-025-05232-y",
url="https://doi.org/10.1007/s11229-025-05232-y"
}


@inproceedings{10.1007/978-3-031-59100-6_13,
 abstract = {In the dynamic intersection of Natural Language Processing and cyber security, Named Entity Recognition plays a pivotal role in comprehending and countering cyber threats. This paper explores Named Entity Recognition techniques within the cyber security context, utilizing a meticulously curated dataset with 12 distinct entity types extracted from security blogs. Our study involves developing and comparative analysis of five Named Entity Recognition models: BiLSTM, BiLSTM-CRF, BERT, BERT-CRF, and BERT-BiLSTM-CRF. Rigorous evaluation reveals that the BERT-BiLSTM-CRF model outperforms others with an F1-Score of 0.9635, excelling at extracting entities from the intricate language used in cyber security texts. Through this paper, we contribute to the ongoing Named Entity Recognition discourse in cyber security, paving the way for advancements in Natural Language Processing techniques and fortifying cyber security measures against evolving digital threats. The implementation and dataset are accessible on our Github page: https://github.com/OPTIMA-CTI/CyberNER.git.},
 address = {Cham},
 author = {Aravind, P. C.
and Arikkat, Dincy R.
and Krishnan, Anupama S.
and Tesneem, Bahja
and Sebastian, Aparna
and Dev, Mridul J.
and Aswathy, K. R.
and Rehiman, K. A. Rafidha
and Vinod, P.},
 booktitle = {Advancements in Smart Computing and Information Security},
 editor = {Rajagopal, Sridaran
and Popat, Kalpesh
and Meva, Divyakant
and Bajeja, Sunil},
 isbn = {978-3-031-59100-6},
 pages = {163--178},
 publisher = {Springer Nature Switzerland},
 title = {CyTIE: Cyber Threat Intelligence Extraction with Named Entity Recognition},
 url = {https://link.springer.com/chapter/10.1007/978-3-031-59100-6_13},
 year = {2024}
}

@Article{Vuković2025,
author="Vukovi{\'{c}}, Darko B.
and Dekpo-Adza, Senanu
and Matovi{\'{c}}, Stefana",
title="AI integration in financial services: a systematic review of trends and regulatory challenges",
journal="Humanities and Social Sciences Communications",
year="2025",
month="Apr",
day="22",
volume="12",
number="1",
pages="562",
abstract="The integration of Artificial Intelligence (AI) into financial services represents a developmental shift in the industry, presenting unprecedented opportunities and challenges. This scientometric review examines the evolution of AI in finance from 1989 to 2024, analyzing its pivotal applications in credit scoring, fraud detection, digital insurance, robo-advisory services, and financial inclusion. The analysis reveals significant trends, particularly the growing adoption of machine learning, natural language processing, and blockchain technologies in reshaping financial operations and decision-making processes. The review addresses critical regulatory and ethical challenges, emphasizing the imperative for explainable AI (XAI) and robust governance frameworks to ensure transparency, fairness, and accountability in AI-driven systems. Despite rapid advancements, persistent gaps remain, the most notable of which is the lack of standardized frameworks for AI implementation across financial sectors. The findings support the need for a balanced approach that promotes innovation while addressing ethical, regulatory, and societal concerns. This comprehensive synthesis maps the trajectory of AI in finance, identifies key areas for future research, and recommends interdisciplinary collaboration to advance responsible and sustainable AI integration within the financial ecosystem.",
issn="2662-9992",
doi="10.1057/s41599-025-04850-8",
url="https://doi.org/10.1057/s41599-025-04850-8"
}


@Inbook{Kempt2024,
author="Kempt, Hendrik",
title="Applied Cases",
bookTitle="(Un)explainable Technology",
year="2024",
publisher="Springer Nature Switzerland",
address="Cham",
pages="87--110",
abstract="Assessing four different applied cases where unexplainable technologies are currently in use---medical diagnostics, self-driving cars and AWS, Recommender algorithms, and generative AI. All these applied cases have different ethical conundrums associated with their unexplainability, but none of them require them to be more explainable: whether increased transparency, auditability, or certifiability of their risks, there are ways of responsibly using this technology as it currently is.",
isbn="978-3-031-68098-4",
doi="10.1007/978-3-031-68098-4_5",
url="https://doi.org/10.1007/978-3-031-68098-4_5"
}


@Article{Mphaga2024,
author="Mphaga, Khathutshelo Vincent
and Moyo, Dingani
and Rathebe, Phoka Caiphus",
title="Unlocking food safety: a comprehensive review of South Africa's food control and safety landscape from an environmental health perspective",
journal="BMC Public Health",
year="2024",
month="Jul",
day="30",
volume="24",
number="1",
pages="2040",
abstract="Food fraud (often called fake food in South Africa) the deliberate misrepresentation or adulteration of food products for financial gain, is a growing problem in South Africa (SA) with severe public health and financial consequences for consumers and businesses. The recent public outcry against food fraud practices especially in communities that have lost loved ones due to the consumption of allegedly adulterated foodstuffs, highlights the grave danger that food fraud poses to consumers and the potential for significant reputational damage to food manufacturers. Despite the risks, food fraud often goes undetected, as perpetrators are becoming increasingly sophisticated. The precise magnitude of food fraud remains obscure, as incidents that do not cause consumer illnesses are frequently unreported and, as a result, are not investigated. Food fraud costs the global economy billion annually. This cost is borne by consumers, businesses, and the government. Food fraud can occur at any stage of the food supply chain, from production to processing to retailing or distribution. This is due in part to the limitations of current analytical methods, which are not always able to detect food fraud. This review of food fraud in SA looks at several factors that may be contributing to epidemic of food fraud, including inadequate penalties, inadequate government commitment, a complex labelling regulation, emerging threats such as e-commerce, and shortage of inspectors and laboratories. The review recommends establishing a single food control/safety authority, developing more food safety laboratories, and adopting innovative technologies to detect and prevent food fraud. SA faces a serious food fraud crises unless decisive action is taken.",
issn="1471-2458",
doi="10.1186/s12889-024-19589-1",
url="https://doi.org/10.1186/s12889-024-19589-1"
}


@Article{Zhang2025,
author="Zhang, Zhiwu",
title="Tragic love: AI's emotionless system and the absence of human emotions",
journal="Humanities and Social Sciences Communications",
year="2025",
month="Dec",
day="26",
volume="13",
number="1",
pages="98",
abstract="The emotional system of artificial intelligence (AI) is being increasingly woven into the emotional tapestry of human society, prompting widespread discourse on the essence, technological frontiers, and ethical ramifications of emotion. In this study, the potential and constraints of AI replicating and augmenting human emotions is investigated. According to philosophical, psychological, and ethical viewpoints, the fundamental traits, moral dilemmas, and societal repercussions of AI emotional systems are discussed. Evidence suggests that the emotional manifestations of AI can provide emotional support to humans, especially in addressing loneliness and resolving emotional quandaries; however, these systems are essentially instrumental and devoid of authentic emotional depth and experience. As AI emotional systems evolve, ethical and social challenges arise. Humanity must define the role and boundaries of AI in the emotional sphere to ensure that its applications align with societal values and do not harm human emotional structures or social principles. We posit that a symbiotic relationship between AI and human emotions is possible, endorsing the measured use of AI emotional systems via technical and social strategies to complement, not replace, human emotional encounters. Furthermore, we underscore the need for interdisciplinary collaboration, the setting of ethical standards, and the exploration of AI emotional boundaries to achieve this goal. Through a comprehensive analysis of AI emotional systems, we aim to provide guidance for their future trajectory and application, ensuring that AI advancements in the emotional domain enhance the well-being of human society. We examine how AI, by trying to surpass human emotions, illustrates a general human habit of giving away or discharging one's emotions when one faces internal emotional problems. According to this study, the main issue of having AI handle emotions lies in people's overreliance on machines for emotional assistance and support. Supported by Kantian ethics, Buddhist metaphysics, and Nietzschean existentialism, we review and analyse the boundaries of emotional AI. Kant's deontological imperative warns against using emotions as simple tools; Buddhist ideas about impermanence and attachment highlight the illusion that machines can be made human; and Nietzsche's criticism of self-transcendence reminds us not to impose human values on artificial beings. Although emotional AI is technologically advanced, we believe that these systems have little or no independence and are not independently responsible. Their use worries many experts because of the emotional outsourcing and increasing separation in relationships related to this use. Ultimately, we suggest principles for safe interactions between humans and AI, emphasizing ethical design, cultural needs, and emotional education to reduce further problems when integrating these AI systems with real emotions.",
issn="2662-9992",
doi="10.1057/s41599-025-06400-8",
url="https://doi.org/10.1057/s41599-025-06400-8"
}


@Inbook{Mejuto2023,
author="Mejuto, Javier",
editor="Froehlich, Annette",
title="Honduras in Space so Far: A Central American Approach",
bookTitle="Space Fostering Latin American Societies: Developing the Latin American Continent Through Space, Part 4",
year="2023",
publisher="Springer Nature Switzerland",
address="Cham",
pages="25--35",
abstract="This chapter intends to give a vision of the efforts in the special subject of the Moraz{\'a}n Project in Honduras from a Central American perspective. A small retrospective of the regional space missions is carried out to date to explain in detail the only Central American space project in development today, the Moraz{\'a}n Project. Finally, some of the trends in space issues in the Central American region are exposed.",
isbn="978-3-031-20675-7",
doi="10.1007/978-3-031-20675-7_2",
url="https://doi.org/10.1007/978-3-031-20675-7_2"
}


@Inbook{Thomas2025,
author="Thomas, Alfred",
title="Monstrous Death: Mourning, Melancholy, and Misogyny in Der Ackermann aus B{\"o}hmen, Pearl, and Machaut's Remede de Fortune and Le Jugement dou roy de Behaingne",
bookTitle="Wounded Knights: Violence, Masculinity, and Medieval Courtly Love",
year="2025",
publisher="Springer Nature Switzerland",
address="Cham",
pages="179--208",
abstract="This chapter explores how these tensions within the male ego reach a point of total breakdown in the German prose debate ``The Plowman of Bohemia'' in which a widowed plowman berates Death for taking away his beloved wife. Conventionally read as a scholastic dispute between Man and Death, I argue that it actually reflects the split within the male self between the courtly lover and his misogynistic alter ego. The second half of the chapter compares this German text with the Middle English elegy ``Pearl'' where this split within the male psyche is resolved through the figure of the pearl-maiden who functions as a reassuring maternal superego.",
isbn="978-3-032-04261-3",
doi="10.1007/978-3-032-04261-3_6",
url="https://doi.org/10.1007/978-3-032-04261-3_6"
}


@Article{Li2022,
author="Li, Shuyue
and Guo, Jiaqi
and Gao, Yan
and Lou, Jianguang
and Yang, Dejian
and Xiao, Yan
and Zhou, Yadong
and Liu, Ting",
title="How to manage a task-oriented virtual assistant software project: an experience report",
journal="Frontiers of Information Technology {\&} Electronic Engineering",
year="2022",
month="May",
day="01",
volume="23",
number="5",
pages="749--762",
abstract="Task-oriented virtual assistants are software systems that provide users with a natural language interface to complete domain-specific tasks. With the recent technological advances in natural language processing and machine learning, an increasing number of task-oriented virtual assistants have been developed. However, due to the well-known complexity and difficulties of the natural language understanding problem, it is challenging to manage a task-oriented virtual assistant software project. Meanwhile, the management and experience related to the development of virtual assistants are hardly studied or shared in the research community or industry, to the best of our knowledge. To bridge this knowledge gap, in this paper, we share our experience and the lessons that we have learned at managing a task-oriented virtual assistant software project at Microsoft. We believe that our practices and the lessons learned can provide a useful reference for other researchers and practitioners who aim to develop a virtual assistant system. Finally, we have developed a requirement management tool, named SpecSpace, which can facilitate the management of virtual assistant projects.",
issn="2095-9230",
doi="10.1631/FITEE.2100467",
url="https://doi.org/10.1631/FITEE.2100467"
}


@Article{Villa-Pérez2025,
author="Villa-P{\'e}rez, Miryam Elizabeth
and Monroy, Ra{\'u}l",
title="Public opinion and emotional discourse: a study of YouTube comments on Mexican news in 2024",
journal="International Journal of Data Science and Analytics",
year="2025",
month="Nov",
day="29",
volume="21",
number="1",
pages="30",
abstract="In recent years, integrating social media data and natural language processing techniques to handle large volumes of data has become more popular, particularly for gaining insights into user behavior in digital environments. Unlike traditional methods such as surveys, using data from social media has allowed for capturing broader and more spontaneous dynamics. In this context, YouTube stands out as a particularly relevant platform in Mexico, the fifth country in the world in terms of users and where this platform is the second main source for news consumption. This study introduces a dataset comprising 1,048,725 comments extracted from 14,161 news videos published in 2024 by eight national channels on YouTube. We aim to examine the public discourse among the Mexican audience based on the extracted data. We applied sentiment and emotion analysis techniques, as well as hate speech and aggressiveness detection, and utilized topic modeling with BERTopic. Likewise, we consider theme polarization by evaluating the distribution of categories in sentiment, hate, and aggression. The results reveal that the prevailing content is about politics, and nearly 45{\%} of comments are posted on the same day a news video is released, indicating that most activity occurs on the day of publication. Moreover, topics related to judicial reform, corruption, violence, and criticism of the government are the most frequent, and they are dominated by negative reactions. Although topic polarization is low, there is a clear tendency toward negative stances. By mapping the emotional tone of digital discourse in Mexico, this study identifies the primary areas of citizen concern and sheds light on the prevailing public sentiment regarding current affairs.",
issn="2364-4168",
doi="10.1007/s41060-025-00936-3",
url="https://doi.org/10.1007/s41060-025-00936-3"
}


@Article{Friesen2025,
author="Friesen, Norm
and Forsman, Michael
and Forsler, Ingrid",
title="Pedagogical Relations in the Postdigital Classroom",
journal="Postdigital Science and Education",
year="2025",
month="Sep",
day="01",
volume="7",
number="3",
pages="1048--1068",
issn="2524-4868",
doi="10.1007/s42438-025-00559-8",
url="https://doi.org/10.1007/s42438-025-00559-8"
}


@Article{Chanda2024,
author="Chanda, Sasanka Sekhar
and Banerjee, Debarag Narayan",
title="Omission and commission errors underlying AI failures",
journal="AI {\&} SOCIETY",
year="2024",
month="Jun",
day="01",
volume="39",
number="3",
pages="937--960",
abstract="In this article we investigate origins of several cases of failure of Artificial Intelligence (AI) systems employing machine learning and deep learning. We focus on omission and commission errors in (a) the inputs to the AI system, (b) the processing logic, and (c) the outputs from the AI system. Our framework yields a set of 28 factors that can be used for reconstructing the path of AI failures and for determining corrective action. Our research helps identify emerging themes of inquiry necessary for developing more robust AI-ML systems. We are hopeful that our work will help strengthen the use of machine-learning AI by enhancing the rates of true positive and true negative judgements from AI systems, and by lowering the probabilities of false positive and false negative judgements.",
issn="1435-5655",
doi="10.1007/s00146-022-01585-x",
url="https://doi.org/10.1007/s00146-022-01585-x"
}


@Article{Kirfel2025,
author="Kirfel, Lara
and J. MacCoun, Robert
and Icard, Thomas
and Gerstenberg, Tobias",
title="When AI meets counterfactuals: the ethical implications of counterfactual world simulation models",
journal="AI and Ethics",
year="2025",
month="Oct",
day="01",
volume="5",
number="5",
pages="4593--4604",
abstract="This paper examines the transformative potential of AI embedded with counterfactual world simulation models (CWSMs). A CWSM uses multimodal evidence, such as the CCTV footage of a road accident, to build a high-fidelity 3D reconstruction of what happened. It can answer causal questions, such as whether the accident happened because the driver was speeding, by simulating what would have happened in relevant counterfactual situations. We sketch a normative and ethical framework that guides and constrains the simulation of counterfactuals. We address the challenge of ensuring fidelity in reconstructions while simultaneously preventing stereotype perpetuation during counterfactual simulations. We anticipate different modes of how users will interact with AI-powered CWSMs and discuss how their outputs may be presented. Finally, we address the prospective applications of CWSMs in the legal domain, recognizing both their potential to revolutionize legal proceedings as well as the ethical concerns they engender. Sketching a new genre of AI, this paper seeks to illuminate the path forward for responsible and effective use of CWSMs.",
issn="2730-5961",
doi="10.1007/s43681-025-00718-4",
url="https://doi.org/10.1007/s43681-025-00718-4"
}


@Inbook{Pandiya2025,
author="Pandiya, Dileep Kumar
and Charankar, Nilesh",
title="The Future of AI-Enhanced Microservices and APIs",
bookTitle="AI and Microservices: Integrating AI into API Design and Distributed Microservice Architecture ",
year="2025",
publisher="Apress",
address="Berkeley, CA",
pages="273--323",
abstract="Artificial intelligence (AI) has recently revolutionized software development as new algorithms and models are adopted and adapted to enable machines to make decisions with the information they glean. These developments help microservices and APIs in creating intelligent, adaptive, and scalable systems. In addition, machine learning (ML) can provide increased enhancements in data processing, model updates, and system performance.",
isbn="979-8-8688-1306-1",
doi="10.1007/979-8-8688-1306-1_11",
url="https://doi.org/10.1007/979-8-8688-1306-1_11"
}


@Article{R2025,
author="R, Gauthama Raman M.
and Khandekar, Sanat
and Murarishetti, Rohit
and Caven, Chew Zhan Yi
and Eric, Ng Guo Feng
and Zhou, Jianying",
title="Network-Based Real-Time Detection of Data Manipulation Attacks in Industrial Control Systems",
journal="International Journal of Information Security",
year="2025",
month="Dec",
day="25",
volume="25",
number="1",
pages="12",
abstract="The increasing connectivity of Industrial Control Systems (ICS) has heightened their exposure to sophisticated data manipulation attacks that exploit vulnerabilities in communication protocols such as EtherNet/IP (ENIP), MODBUS, and DNP3. Existing intrusion detection solutions often rely on abstract traffic features or reactive detection strategies, limiting their effectiveness in preventing real-time disruptions. This paper presents a protocol-aware network intrusion detection system (PA-NIDS) for real-time detection of malicious ENIP traffic targeting controllers. By leveraging contextual information within protocol communications, the proposed system enables early identification of attacker intent before physical impact. The framework was deployed on the operational Secure Water Treatment (SWaT) testbed and evaluated under single-point and coordinated multi-point attack scenarios. Experimental results show that PA-NIDS achieves 100{\%} detection accuracy with zero false positives and an average detection latency below five seconds, providing proactive and interpretable protection for ICS environments.",
issn="1615-5270",
doi="10.1007/s10207-025-01172-3",
url="https://doi.org/10.1007/s10207-025-01172-3"
}


@Inbook{Rajagopal2024,
author="Rajagopal
and Rajagopal, Ananya",
title="Synchronizing Transitional Pace",
bookTitle="Unmasking Invisible Challenges in Entrepreneurship: Five Game Changer Models",
year="2024",
publisher="Springer Nature Switzerland",
address="Cham",
pages="133--153",
abstract="This chapter synchronizes the entrepreneurial transformation and change management models discussed in the previous chapters (Chapters 1--4). The transcending innovation and technology, and rapidly changing market behavior requires entrepreneurial alignment through effective transformation. In this process, the resource-based enterprises stay ahead of large number of micro, small, and medium enterprises due to their limited resources and managerial capabilities. Based on the discussions on game changer models, game analytics, and their contextual relationship with entrepreneurship, this chapter deliberates ways to synchronize the entrepreneurial transitional pace. Accordingly, the transformational synergy across innovation, technology, market, and entrepreneurial growth has been discussed by converging the divergent transformational factors. In addition, the contributions of changing public policies and political ideologies to transformational entrepreneurship in developing economies also constitute the core discussion in this chapter. Future perspective on entrepreneurial transformation in the context of technology transfer, collective intelligence, and hybrid enterprises has also been discussed in this chapter.",
isbn="978-3-031-63653-0",
doi="10.1007/978-3-031-63653-0_5",
url="https://doi.org/10.1007/978-3-031-63653-0_5"
}


@Inbook{Owusu-Apenten2023,
author="Owusu-Apenten, Richard
and Vieira, Ernest",
title="Food Safety Management, GMP {\&} HACCP",
bookTitle="Elementary Food Science",
year="2023",
publisher="Springer International Publishing",
address="Cham",
pages="217--236",
abstract="Current strategies for ensuring food is safe for the consumer involve a number of programs for food safety management including policies to safeguard against terrorist food threats. The aims of this chapter is to introduce the main elements of food safety management. The chapter is divided into six sections. (1) Good management practices (GMP); general principles, basic provisions for GMP, building and facilities for GMP, GMP and equipment, GMP and production -- process control. (2) Good Agricultural Practices (GAP); water use and GAP, manure and biosolids, GAP and personal hygiene. (3) Sanitation performance standards; revised sanitation standards, construction sanitation performance standards, lighting and ventilation sanitations standards. (4) Other hygiene codes, the food code -- United States, Codex Alimentarious food hygiene code, food hygiene in European Union, (5) Hazard analysis critical control point, HACCP in the United States, reasons for HACCP non-compliance, HACCP in developing countries, seven stages of HACCP, (6). Future food safety management, integrated food safety, FDA Food Safety Modernization Act (FSMA), Threat analysis critical control point (TACCP) and vulnerability analysis critical control points (VACCP) for food threats. With 45 references.",
isbn="978-3-030-65433-7",
doi="10.1007/978-3-030-65433-7_10",
url="https://doi.org/10.1007/978-3-030-65433-7_10"
}


@Inbook{Thagard2025,
author="Thagard, Paul",
editor="Arfini, Selene",
title="Can ChatGPT Make Explanatory Inferences? Benchmarks for Abductive Reasoning",
bookTitle="Abductive Minds: Essays in Honor of Lorenzo Magnani - Volume 1",
year="2025",
publisher="Springer Nature Switzerland",
address="Cham",
pages="189--218",
abstract="Explanatory inference is the creation and evaluation of hypotheses that provide explanations, and is sometimes known as abduction or abductive inference. Generative AI is a new set of artificial intelligence models based on novel algorithms for generating text, images, and sounds. This paper proposes a set of benchmarks for assessing the ability of AI programs to perform explanatory inference, and uses them to determine the extent to which ChatGPT, a leading generative AI model, is capable of making explanatory inferences. Tests on the benchmarks reveal that ChatGPT performs creative and evaluative inferences in many domains, although it is limited to verbal and visual modalities. Claims that ChatGPT and similar models are incapable of explanation, understanding, causal reasoning, meaning, and creativity are rebutted.",
isbn="978-3-031-96684-2",
doi="10.1007/978-3-031-96684-2_12",
url="https://doi.org/10.1007/978-3-031-96684-2_12"
}


@inproceedings{10.1007/978-3-031-61069-1_2,
 abstract = {This paper reports on ongoing and innovative research in the area of eXplainable Artificial Intelligence (XAI). A classical XAI task is considered as finding an explanation of the model generated via Machine Learning by identifying the most influential variables for local decision-making. Such an approach suffers from severe limitations. The proposed approach moves the explanatory process to a new, knowledge-level dimension. It is oriented towards Model Discovery, i.e. the internal structure and functions of the components. The concept of Model-Driven XAI is put forward and explained with examples. An experiment on Function Discovery via Grammatical Evolution is reported in brief.},
 address = {Cham},
 author = {Lig{\k{e}}za, Antoni
and Sepio{\l}o, Dominik},
 booktitle = {Artificial Intelligence for Knowledge Management, Energy and Sustainability},
 editor = {Mercier-Laurent, Eunika
and Kayakutlu, G{\"u}lg{\"u}n
and Owoc, Mieczyslaw Lech
and Wahid, Abdul
and Mason, Karl},
 isbn = {978-3-031-61069-1},
 pages = {11--26},
 publisher = {Springer Nature Switzerland},
 title = {In Search for Model-Driven eXplainable Artificial Intelligence},
 url = {https://link.springer.com/chapter/10.1007/978-3-031-61069-1_2},
 year = {2024}
}

@Article{Park2025,
author="Park, Se-Young
and Son, Kyuwon
and Kim, Jiwoo
and Kim, Kyeongah
and Joo, Sungmin
and Kim, Bomi
and Lee, Myunggyo
and Kim, Wankyu
and Jung, Won-Jung
and Choi, Byung Kwan
and Jeon, Nakyung
and Chung, Won-Yoon
and Hu, Yinling
and Lee, Haeseung
and Song, Na-Young",
title="Cathepsin L as a dual-target to mitigate muscle wasting while enhancing anti-tumor efficacy of anti-PD-L1",
journal="Nature Communications",
year="2025",
month="Nov",
day="28",
volume="16",
number="1",
pages="10706",
abstract="Immune checkpoint inhibitors (ICIs) have revolutionized cancer therapy; however, their use is frequently associated with immune-related adverse events (irAEs). In this study, anti-PD-L1 therapy exacerbates muscle wasting in tumor-bearing male mice despite its anti-tumor efficacy, accompanied by an accumulation of CD8+ T cells in muscle. Single-cell RNA sequencing identifies these cells as tissue-resident memory-like CD49a+ CD8+ T cells. While CD8+ T cell depletion prevents muscle wasting, it compromises the anti-tumor efficacy of anti-PD-L1. To resolve this paradox, we identify cathepsin L (CTSL) as a dual-target capable of suppressing both tumor progression and CD8+ T cell-mediated muscle wasting, through integrative transcriptomic analysis. Pharmacological inhibition of CTSL not only mitigates anti-PD-L1-induced muscle wasting but also further suppresses tumor growth, potentially via downregulation of BNIP3. Here, we show that CTSL is a dual-action target to uncouple anti-tumor efficacy from muscle-specific irAEs, offering a strategy to improve clinical outcomes of ICIs.",
issn="2041-1723",
doi="10.1038/s41467-025-64500-0",
url="https://doi.org/10.1038/s41467-025-64500-0"
}


@inproceedings{10.1007/978-3-032-14430-0_39,
 abstract = {Reliability of APIs plays a vital role in the stability of distributed systems and DevOps workflows. Although chaos engineering has proven effective in exposing vulnerabilities, its integration into API testing remains limited due to a lack of automation and efficient test case generation. This paper introduces ChaosAPI, a novel tool that employs genetic algorithms to automate chaos engineering specifically for API testing. By utilizing OpenAPI Specifications, ChaosAPI systematically generates, mutates, and optimizes test cases to improve coverage and uncover hard-to-detect edge cases. Comparative analysis shows that ChaosAPI performs competitively with state-of-the-art methods, including NLP-based testing techniques, Llama 3 8B, and JetBrains AI Assistant---despite requiring less computational power and not relying on access to source code.},
 address = {Cham},
 author = {Ait Said, Mehdi
and Belouaddane, Lahcen
and Soukaina, Mihi
and Abdellah, Ezzati
and Brahim, Zraibi},
 booktitle = {Artificial Intelligence and Cognitive Sciences for Emerging Technologies},
 editor = {Rouky, Naoufal
and Ben Bouazza, Fatima Ezzahraa
and Hussain, Amir
and El Bhiri, Brahim
and Nkenyereye, Lewis
and Edeh, Michael Onyema
and Lamaazi, Hanane},
 isbn = {978-3-032-14430-0},
 pages = {459--466},
 publisher = {Springer Nature Switzerland},
 title = {Enhancing API Testing Through Genetic Algorithm-Based Chaos Engineering},
 url = {https://link.springer.com/chapter/10.1007/978-3-032-14430-0_39},
 year = {2026}
}

@Article{Sass2025,
author="Sass, Reuben",
title="Defining dangerous AI: existential risk, power-intelligence, and the limits of AGI",
journal="AI and Ethics",
year="2025",
month="Oct",
day="01",
volume="5",
number="5",
pages="5557--5573",
abstract="Artificial general intelligence (AGI) features prominently in some existential risk literature, according to which the development of AGI greatly increases possible AI-induced risks to humanity. But we argue that the typical concept of AGI may be ill-suited for conceptualizing those systems that pose the greatest risks. In particular, AGI does not account for how AI agents' abilities and behavioral strategies could be affected by complex multi-agent environments. Accordingly, we develop a simple formal model for what we call power-intelligence, which assesses agents' capacities to exert influence over a resemblance class of multi-agent environments. We then taxonomize and examine multi-agent relational dynamics including competition, dependence, and complementarity. This allows us to characterize a concept of dangerous AI, which conjoins high power-intelligence with multi-agent dynamics that might incentivize harms to other agents. We argue that this concept of dangerous AI is more informative for existential risk assessments than both typical conceptions of AGI and ``power-seeking'' intelligence in the literature.",
issn="2730-5961",
doi="10.1007/s43681-025-00790-w",
url="https://doi.org/10.1007/s43681-025-00790-w"
}


@Article{Jia2025,
author="Jia, Ning
and Li, Ningzhong
and Ma, Guang
and Xu, Da",
title="Corporate responses to generative AI: early evidence from conference calls",
journal="Review of Accounting Studies",
year="2025",
month="Oct",
day="11",
abstract="We provide early evidence of corporate responses to generative artificial intelligence (GAI) through managerial discussions of GAI in conference calls after ChatGPT's release in November 2022. Following the release, these managerial discussions increase substantially, and most discussions have positive tone and contain action initiatives. The increase is more pronounced for larger firms, growth firms, younger firms, and those in high-tech industries. The largest increase is observed in the industry of business equipment---computers, software, and electronic equipment---while the utilities, chemicals and allied products, and oil, gas, and coal extraction industries experience the smallest increases. Managers of firms with greater innovation intensity (reflected in patent filings), cybersecurity threats, labor exposure to AI, and customer operations are more likely to increase GAI discussions. The results are primarily driven by positive-tone discussions and those with action initiatives. Analyst discussions of GAI exhibit similar patterns, except for patent filings. Finally, managers' GAI discussions inform investors.",
issn="1573-7136",
doi="10.1007/s11142-025-09916-1",
url="https://doi.org/10.1007/s11142-025-09916-1"
}


@Inbook{Billingsley2026,
author="Billingsley, John",
title="Simulating a Dynamic System",
bookTitle="Mechatronics Experiments with Machine Vision",
year="2026",
publisher="Springer Nature Switzerland",
address="Cham",
pages="1--25",
abstract="The book will encourage you to find out the essence of mechatronic devices, by building them. But before starting any construction, it is wise to find out if their control strategy is likely to work, by simulating them.",
isbn="978-3-032-11218-7",
doi="10.1007/978-3-032-11218-7_1",
url="https://doi.org/10.1007/978-3-032-11218-7_1"
}


@inproceedings{10.1007/978-981-96-2548-2_3,
 abstract = {With the rising adoption of cloud technology, we witness an upward trend toward reliance on cloud-based security services. With cloud-based services, the need arises to safeguard sensitive data, applications, and infrastructure from cyberattacks. The recent CrowdStrike outage on July 19, 2024, propels all security enthusiasts to re-evaluate organizations' practices and deployment procedures. This research paper dives into understanding the nature of cloud security and its extension beyond practices, technologies, policies, and measures that are established to create a safety net for cloud environments. How the event of an outage impacted industries like airlines and emergency services only outlines the need for enhancing frameworks that could quantitatively measure overall resilience maturity. This study emphasizes the need for and importance of continuous assessment and renewal of standards and guidelines for cloud security assessments that are agnostic of any complexity so that organizations can make informed decisions. The analysis of the CrowdStrike outage presents the current state of the effectiveness of current security measures, the applicability of resilience frameworks, and the preparedness of cloud-based security services to withstand and recover from cyber incidents. With its findings, the study aims to contribute to recommendations for security strategies and cloud service resilience against threats.},
 address = {Singapore},
 author = {Mahanta, Prabal
and Kumar, Manish},
 booktitle = {Leveraging Emerging Technologies and Analytics for Empowering Humanity, Vol. 1},
 editor = {Goyal, D. P.
and Sarker, Suprateek
and Mukhopadhyay, Somnath
and Roychoudhury, Basav
and Upadhyay, Parijat
and Dadabada, Pradeep Kumar},
 isbn = {978-981-96-2548-2},
 pages = {67--82},
 publisher = {Springer Nature Singapore},
 title = {When Clouds Go Dark: A Serious Look at CrowdStrike's Outage and the Fortitude of Cloud Security},
 url = {https://link.springer.com/chapter/10.1007/978-981-96-2548-2_3},
 year = {2025}
}

@Article{Pan2026,
author="Pan, Xuexue
and Xie, Yuanlin
and Li, Chujun
and He, Yingyin
and Zhang, Yitong
and Wang, Yilin
and Li, Ziman
and Peng, Peiyu
and Wang, Jun",
title="Convergence of Computational Materials Science and AI for Next-Generation Energy Storage Materials",
journal="Journal of Electronic Materials",
year="2026",
month="Jan",
day="01",
volume="55",
number="1",
pages="45--114",
abstract="With the urgent demand for high-performance energy storage materials in the global energy transition, traditional experimental trial and error methods are difficult to meet the rapid research and development needs owing to long cycles and high costs. In recent years, the deep integration of computational materials science and artificial intelligence (AI) technology has provided revolutionary tools for the rational design and performance optimization of energy storage materials. This article systematically reviews the progress of research on energy storage material computation and AI systems. At the traditional method level, quantum mechanics computation (such as VASP, Quantum ESPRESSO), molecular dynamics (such as LAMMPS, GROMACS), and high-throughput computing platforms (such as Materials Project) have achieved accurate predictions of material electronic structure, interface dynamics, and high-throughput screening. At the AI-driven level, generative models (GNoME, 3D-GPT), graph neural networks (MEGNet, CGCNN), and experimental computational closed--loop systems (such as the autonomous driving laboratory A-Lab) have significantly accelerated the discovery and reverse design of new materials. Further focusing on the integration trend of multi-scale modeling and AI, physical information-driven AI models (DPMD, PINNs) and cross-scale integration platforms (ASE, MedeA) are driving the collaborative improvement of material simulation accuracy and efficiency. However, data scarcity, computational bottlenecks caused by multi-physics coupling, and barriers to tool industrialization remain current challenges. In the future, sustainable design paradigms, open-source ecological construction, and human-machine collaboration models will lead the research and development of energy storage materials into the era of ``digital priority.'' This article aims to provide a technical roadmap reference for interdisciplinary research and call for collaboration between academia and industry to overcome key bottlenecks and accelerate the innovation breakthrough and large-scale application of energy storage materials.",
issn="1543-186X",
doi="10.1007/s11664-025-12511-4",
url="https://doi.org/10.1007/s11664-025-12511-4"
}


@Inbook{Mutambara2025,
author="Mutambara, Prof. Dr. Eng.",
title="Governance, Legislation, Regulations, Sociology, and Ethics",
bookTitle="Deploying Artificial Intelligence to Achieve the UN Sustainable Development Goals: Enablers, Drivers and Strategic Framework",
year="2025",
publisher="Springer Nature Switzerland",
address="Cham",
pages="225--244",
abstract="This chapter deals with AI governance, legislation, regulations, sociology and ethics. AI legislative lessons can be drawn from many jurisdictions, including the efforts of the EU and India. The utility of these emerging experiences can be extended to AI deployment for SDG attainment worldwide. Effective legislation and regulation are indispensable for deploying AI to achieve the SDGs. They provide a framework for ensuring ethical use, protecting privacy, promoting sustainability, and addressing inequality. By fostering accountability and international cooperation, these frameworks align AI's transformative power with the global vision of a more equitable and sustainable future. As the world advances towards 2030, crafting and implementing these regulations will be crucial to maximising AI's contribution to the SDGs while safeguarding against potential risks. Regulations tailored to specific sectors encourage responsible innovation, minimising risks while maximising AI's transformative potential for inclusive development. Beyond national efforts, there is a need for harmonised and standardised regional, continental and global legislation, regulations, guardrails, and governance. All these AI frameworks must be robust, balanced, dynamic and adaptable to accommodate the rapid and relentless changes and complex dilemmas that characterise the technology.",
isbn="978-3-031-88423-8",
doi="10.1007/978-3-031-88423-8_10",
url="https://doi.org/10.1007/978-3-031-88423-8_10"
}


@Inbook{Hofri2023,
author="Hofri, Micha",
editor="Nof, Shimon Y.",
title="Automation and Ethics",
bookTitle="Springer Handbook of Automation",
year="2023",
publisher="Springer International Publishing",
address="Cham",
pages="753--772",
abstract="This chapter surveys aspects of automation in technology, its design, implementation, and usage, as they interact with values that underpin our society and its civilization. The framework chosen for this survey is that of moral or ethical theories and attitudes. To this avail we describe several of the ethical theories used currently to anchor discussions about values in our society. Several significant failures of automatic systems, including a fictional one, are quarried for ethical insights and lessons. The currently intractable nature of ``neural networks'' artificial intelligence systems trained via machine learning is shown to be a moral conundrum. The concepts of code of ethics and of ethical analysis are presented in some detail.",
isbn="978-3-030-96729-1",
doi="10.1007/978-3-030-96729-1_34",
url="https://doi.org/10.1007/978-3-030-96729-1_34"
}


@Inbook{Chen2025,
author="Chen, Shaorong
and Li, Zijian",
editor="Malafaia, Guilherme",
title="What We Know About Aquatic and Semi-Aquatic Mammals' Ecotoxicity of Pesticides",
bookTitle="Aquatic Ecotoxicology of Legacy Pollutants and Emerging Contaminants in Animals and Plants",
year="2025",
publisher="Springer Nature Switzerland",
address="Cham",
pages="275--328",
abstract="The intensification of agriculture has led to the widespread use of chemical pesticides for the control of pests and diseases. However, it has been demonstrated that these pesticides can enter the animal body through a number of different pathways, with the potential to cause a range of health issues. Since mammals living in different environments (such as aquatic and terrestrial areas) exhibit similar physiological patterns and mechanisms of xenobiotic bioaccumulation, we extend the investigation of pesticide ecotoxicity in aquatic and semi-aquatic mammals to include both aquatic and terrestrial mammals. This chapter categorizes mammals as terrestrial, marine, domesticated and human in order to discuss the bioaccumulation, toxicity and ecological health effects of pesticides. Toxicity studies encompass not only acute effects but also chronic and long-term effects, with a particular focus on organ, reproductive health, teratogenic and carcinogenic effects. Furthermore, the utilization of pesticides has an impact on the surrounding environment, biodiversity and the stability of the food chain, which in turn threatens the overall ecological health and system stability. In conclusion, this chapter examines emerging areas of pesticide development, including innovative biopesticides, advanced application technologies and enhanced degradation methods, with the objective of minimizing the adverse impacts of pesticides from source to field.",
isbn="978-3-032-03382-6",
doi="10.1007/698_2024_1182",
url="https://doi.org/10.1007/698_2024_1182"
}


@Inbook{Nogaroli2025,
author="Nogaroli, Rafaella",
title="Ethical and Legal Aspects of Artificial Intelligence (AI) in Medical Service Contracts",
bookTitle="Medical Liability and Artificial Intelligence: Brazilian and European Legal Approaches",
year="2025",
publisher="Springer Nature Switzerland",
address="Cham",
pages="123--206",
abstract="This chapter provides a comprehensive exploration of the rise of artificial intelligence (AI) systems, focusing on their applications, benefits, and risks in clinical decision-making. It begins by examining the global integration of AI algorithms into medical practice, addressing their role in assisting healthcare professionals with diagnoses, prognoses, and treatment recommendations. The discussion also identifies the key challenges and risks associated with the implementation of AI in medicine, including issues of safety, reliability, and accountability. The chapter then delves into the European regulatory landscape, analysing the AI Act and the European framework for civil liability related to damages involving AI systems in healthcare. Ethical principles guiding the use of AI in medicine are also explored, with an emphasis on their influence upon the physician's duties of conduct. Finally, the chapter turns to the Brazilian context, providing an analysis of Bill No. 2338/23, which proposes a regulatory framework for AI in Brazil. This section highlights how current European discussions on AI ethics and regulation are influencing Brazilian legal developments. By integrating global, European, and Brazilian perspectives, the chapter offers an in-depth examination of the interplay between technology, ethics, and regulation in the rapidly evolving field of AI in healthcare.",
isbn="978-3-031-94306-5",
doi="10.1007/978-3-031-94306-5_4",
url="https://doi.org/10.1007/978-3-031-94306-5_4"
}


@Article{Yersaw2024,
author="Yersaw, Babur Tesfaye
and Chane, Mulusew Bezabih
and Yitayew, Natnael Andualem",
title="Performance evaluation of varies climate models using observed and regional climate models for the Katar Watershed, Ethiopia",
journal="Environmental Systems Research",
year="2024",
month="May",
day="11",
volume="13",
number="1",
pages="14",
abstract="Climate models are fundamental tools to estimates the reliable future climate change and its effects on the water resources and agriculture in basins. However, all climate models are not equally performed for all areas. Therefore, determining the most appropriate climate models for a specific study area is essential. The focus of this study was to evaluate the performance of the regional climate models with regard to simulating precipitation, and temperatures at Katar watershed. This study examines the performance of fourteen CORDEX-AFRICA-220 Regional Climate Models (RCMs) for the period of 1984--2005 using statistical metrics such as Pearson correlation coefficient (R), mean absolute error (MAE), root mean squared error (RMSE), and bias. The findings indicated that GERICS-MPI was better performed in representing Areta, and Bokoji station, GERICS-IPSL was better representing in Assela, Ketergenet, and Sagure station, CCCma-CanESM2-AFR22, and RCA4-ICHEC performed relatively better in representing the mean annual observed rainfall at the Kulumsa, and Ogolcho station respectively. However, RCA4-CSIRO performed weakly in estimation of annual rainfall at all stations. RCM model such as GERICS-MPI was relatively better than the others in replicating the annual pattern of the maximum temperature at Areta, Bokoji, and Ketergenet stations. Similarly, GERICS-IPSL were relatively better in replicating the annual maximum temperature at Assela, and Sagure stations, CCCma-CanESM2-AFR22 at Kulumsa station, and RCA4-ICHEC at Ogolcho station performed well in capturing the observed and simulated annual maximum temperature. Better performance was observed on minimum temperature at CCCma-CanESM2-AFR22 at Areta, Assela, and Ketergenet stations, GERICS-MOHE-AFR-22 at Bokoji station, GERICS-MPI at Kulumsa, and Ogolcho stations, RAC4-NOAA-2G at Sagure stations. However, weak performance was observed RCA4-CSIRO at all stations. RCM models of GERICS-MPI, and CCLM4-NCC-AFR-22 performed better than the other RCM models for correction of annual rainfall in Katar watershed. However, poor performance was observed at RCA4-ICHEC model on Katar watershed. The GERICS-MPI model performed well. However, poor performance was observed at RCA4-ICHEC on maximum temperature, and GERICS-NOAA-2M on minimum temperature in Katar watershed.",
issn="2193-2697",
doi="10.1186/s40068-024-00345-8",
url="https://doi.org/10.1186/s40068-024-00345-8"
}


@Article{Xu2025,
author="Xu, Ziyan
and Wang, Yunzhi
and Xie, Tao
and Luo, Rongkui
and Ni, Heng-Li
and Xiang, Hang
and Tang, Shaoshuai
and Tan, Subei
and Fang, Rundong
and Ran, Peng
and Zhang, Qiao
and Xu, Xiaomeng
and Tian, Sha
and He, Fuchu
and Yang, Wenjun
and Ding, Chen",
title="Panoramic spatial enhanced resolution proteomics (PSERP) reveals tumor architecture and heterogeneity in gliomas",
journal="Journal of Hematology {\&} Oncology",
year="2025",
month="May",
day="26",
volume="18",
number="1",
pages="58",
abstract="The spatial proteomic profiling of complex tissues is essential for investigating cellular function in physiological and pathological states. However, the imbalance among resolution, protein coverage, and expense precludes their systematic application to analyze whole tissue sections in an unbiased manner and with high resolution. Here, we introduce panoramic spatial enhanced resolution proteomics (PSERP), a method that combines tissue expansion, automated sample segmentation, and tryptic digestion with high-throughput proteomic profiling. The PSERP approach facilitates rapid quantitative profiling of proteomic spatial variability in whole tissue sections at sub-millimeter resolution. We demonstrated the utility of this method for determining the streamlined large-scale spatial proteomic features of gliomas. Specifically, we profiled spatial proteomic features for nine glioma samples across three different mutation types (IDH1-WT/EGFR-mutant, IDH1-mutant, and IDH1/EGFR-double-WT gliomas) at sub-millimeter resolution (corresponding to a total of 2,230 voxels). The results revealed over 10,000 proteins identified in a single slide, which helps us to portray the diverse proteins and pathways with spatial abundance patterns in the context of tumor heterogeneity and cellular features. Our spatial proteomic data revealed distinctive proteomic features of malignant and non-malignant tumor regions and depicted the distribution of proteins from tumor centers to tumor borders and non-malignant tumor regions. Through integrative analysis with single-cell transcriptomic data, we elucidated the cellular composition and cell--cell communications in a spatial context. Our PSERP also includes a spatially resolved tumor-specific peptidome identification workflow that not only enables us to elucidate the spatial expression patterns of tumor-specific peptides in glioma samples with different genomic types but also provides us with opportunities to select combinations of tumor-specific mutational peptides whose expression could cover the maximum tumor regions for future immune therapies. We further demonstrated that combining tumor-specific peptides might enhance the efficacy of immunotherapy in both patient-derived cell (PDC) and patient-derived xenograft (PDX) models. PSERP efficiently retains precise spatial proteomic information within the tissue context and provides a deeper understanding of tissue biology and pathology at the molecular level.",
issn="1756-8722",
doi="10.1186/s13045-025-01710-5",
url="https://doi.org/10.1186/s13045-025-01710-5"
}


@Inbook{Joshi2026,
author="Joshi, Deepak Chandra
and Sharma, Pravesh Kumar
and Upadhyay, Somya
and Verma, Lavkush
and Paudel, Keshav Raj",
editor="Gupta, Himanshu
and Madhav, Sughosh
and Dhiman, Soniya
and Kumar, Pramod",
title="Contamination of Polycyclic Aromatic Hydrocarbons in Foods and the Food Chain",
bookTitle="Ubiquitous Polycyclic Aromatic Hydrocarbon Contamination: Recent Perspectives",
year="2026",
publisher="Springer Nature Switzerland",
address="Cham",
pages="141--168",
abstract="Polycyclic Aromatic Hydrocarbons (PAHs) are persistent organic pollutants predominantly generated by the incomplete combustion of organic materials. Their presence in food and the food chain raises substantial public health concerns due to their carcinogenic, mutagenic, and endocrine-disrupting characteristics. This chapter examines PAH contamination, focussing on its occurrence and transit throughout food systems, and incorporates surveillance data, regulatory restrictions, and risk evaluations. Environmental deposition, absorption from polluted soil and water, bioaccumulation in aquatic and terrestrial species, and high-temperature cooking methods including smoking, grilling, and frying are all ways that PAHs get into food. The levels of benzo[a]pyrene (2 $\mu$g/kg) and PAH4 (30 $\mu$g/kg) in cereals, smoked fish, and grilled meats are often higher than the EU's standards. For example, in cereals, the levels vary from 2.5 to 276.7 $\mu$g/kg, in smoked fish, they may be as high as 222.7 $\mu$g/kg, and in grilled meats, they can be above 200 $\mu$g/kg. These pollutants go down the food chain, from soil and crops to cattle and fish, and then to people. This means that people are constantly exposed to them through their diets, which can cause malignancies, reproductive and developmental problems, immune system problems, and neurotoxicity. Even as better methods for identifying problems and rules have improved monitoring, there are still issues with enforcement, maintaining uniform standards, and increasing consumer awareness. To protect food safety and human health worldwide, we need to employ a range of measures, including advanced food processing technology, microbial bioremediation, and innovative methods for detecting PAHs.",
isbn="978-3-032-11764-9",
doi="10.1007/978-3-032-11764-9_7",
url="https://doi.org/10.1007/978-3-032-11764-9_7"
}


@inproceedings{10.1007/978-3-031-57327-9_17,
 abstract = {Context and motivation: User participation and involvement is important for system success. Communication between developers and users is an important part of participation. This communication influences user satisfaction and therefore system success. However, direct communication between users and developers is often not possible and thus developers need other information sources to understand how the users use the system and what improvements they want.},
 address = {Cham},
 author = {Anders, Michael
and Paech, Barbara
and Bockstaller, Lukas},
 booktitle = {Requirements Engineering: Foundation for Software Quality},
 editor = {Mendez, Daniel
and Moreira, Ana},
 isbn = {978-3-031-57327-9},
 pages = {267--283},
 publisher = {Springer Nature Switzerland},
 title = {Exploring the Automatic Classification of Usage Information in Feedback},
 url = {https://link.springer.com/chapter/10.1007/978-3-031-57327-9_17},
 year = {2024}
}

@Inbook{Walther2024,
author="Walther, Cornelia C.",
title="WHO: Human Perspectives on Humane Technology",
bookTitle="Human Leadership for Humane Technology: The New AI: Agency Ignited",
year="2024",
publisher="Springer Nature Switzerland",
address="Cham",
pages="195--245",
abstract="Giving voice to a diversity of human perspectives, this chapter grounds the conceptual ideas that were presented in Chapters 1and 2into reality through personal essays that illustrate the kaleidoscopic interplay of humans and technology across sectors, disciplines, and cultures. Starting with the 4 macro-questions of existence (Why---Who---Where---What) these narratives from innovators, leaders, educators, and artists demonstrate the principle that everything is connected in a continuum of constant change where everything matters, and every action has amplified ripple effects online and offline. Their accounts also illustrate the Win4 that will be explored in more detail in Chapter 4, whereby action that is taken for others serves the person who acts, the one for whom action is taken, the community they live in, and wider society. This ripple effect is amplified in a hybrid world.",
isbn="978-3-031-67823-3",
doi="10.1007/978-3-031-67823-3_3",
url="https://doi.org/10.1007/978-3-031-67823-3_3"
}


@inproceedings{10.1007/978-3-032-06173-7_7,
 abstract = {Traditional Service-Oriented Architecture (SOA) faces growing limitations in flexibility, adaptability, and intelligence, struggling to meet modern enterprises' evolving demands. This paper proposes the Internet of Intelligent Services (IIS) and the Agent Software Factory (ASF) as comprehensive solutions. IIS transforms conventional services into intelligent, autonomous, and dynamically collaborative entities, guided by principles of decentralized autonomy, dynamic adaptation, and semantic modeling with feedback-driven learning. Built upon IIS, ASF enables end-to-end automation of the intelligent agent lifecycle through specialized assistant agents. Furthermore, this paper presents the INNOVATORS framework, a structured analysis of ten transformative SaaS trends that characterize this new era. Drawing upon industry observations and technological foresight, the framework encompasses: I -- Immersive Experience as a Service (IEaaS), N -- Niche Market Focus, N -- Normal Business Enabler Tools, O -- Office Platforms for Collaboration and Business, V -- Vertical Industry Solutions, A -- AI Plugins Everywhere, T -- Transformation Software, O -- Orchestrated SaaS Development Platforms, R -- Recognition of Data Insights, and S -- Service-to-Service (S2S) Integration. To support S2S collaboration among heterogeneous agents, the Agent Service Bus (ASB) is proposed, featuring a layered architecture for standardized access, intention-driven scheduling, and dynamic orchestration. Together, these innovations provide a foundational framework for intelligent, adaptive, and service-centric enterprise software systems.},
 address = {Cham},
 author = {Dang, Jiawei
and Chen, Huan
and He, Sheng
and Li, Changhu
and Chen, Xiaojun
and Zhang, Liang-Jie},
 booktitle = {Web Services -- ICWS 2025},
 editor = {He, Sheng
and Zhang, Liang-Jie},
 isbn = {978-3-032-06173-7},
 pages = {96--114},
 publisher = {Springer Nature Switzerland},
 title = {INNOVATORS: Ten Defining Trends of SaaS in the Era of the Internet of Intelligent Services},
 url = {https://link.springer.com/chapter/10.1007/978-3-032-06173-7_7},
 year = {2026}
}

@Inbook{Waghmare2023,
author="Waghmare, Charles",
title="Security and Ethical Considerations When Using ChatGPT",
bookTitle="Unleashing The Power of ChatGPT: A Real World Business Applications",
year="2023",
publisher="Apress",
address="Berkeley, CA",
pages="111--132",
abstract="Due to the increasing integration of AI technologies into mainstream society, ethical and security considerations have to be taken into account. The use of ChatGPT has various advantages and challenges. Among the ethical issues that have to be considered when using ChatGPT are fairness, transparency, and bias. Its training language models may inadvertently reinforce certain biases, and it should adopt practices that are ethical, such as continuous evaluation and data collection that are representative and diverse.",
isbn="979-8-8688-0032-0",
doi="10.1007/979-8-8688-0032-0_6",
url="https://doi.org/10.1007/979-8-8688-0032-0_6"
}


@Inbook{Bonde2024,
author="Bonde, Bhushan",
editor="Heifetz, Alexander",
title="Edge, Fog, and Cloud Against Disease: The Potential of High-Performance Cloud Computing for Pharma Drug Discovery",
bookTitle="High Performance Computing for Drug Discovery and Biomedicine",
year="2024",
publisher="Springer US",
address="New York, NY",
pages="181--202",
abstract="The high-performance computing (HPC) platform for large-scale drug discovery simulation demands significant investment in speciality hardware, maintenance, resource management, and running costs. The rapid growth in computing hardware has made it possible to provide cost-effective, robust, secure, and scalable alternatives to the on-premise (on-prem) HPC via Cloud, Fog, and Edge computing. It has enabled recent state-of-the-art machine learning (ML) and artificial intelligence (AI)-based tools for drug discovery, such as BERT, BARD, AlphaFold2, and GPT. This chapter attempts to overview types of software architectures for developing scientific software or application with deployment agnostic (on-prem to cloud and hybrid) use cases. Furthermore, the chapter aims to outline how the innovation is disrupting the orthodox mindset of monolithic software running on on-prem HPC and provide the paradigm shift landscape to microservices driven application programming (API) and message parsing interface (MPI)-based scientific computing across the distributed, high-available infrastructure. This is coupled with agile DevOps, and good coding practices, low code and no-code application development frameworks for cost-efficient, secure, automated, and robust scientific application life cycle management.",
isbn="978-1-0716-3449-3",
doi="10.1007/978-1-0716-3449-3_8",
url="https://doi.org/10.1007/978-1-0716-3449-3_8"
}


@Inbook{Kaur2026,
author="Kaur, Livpreet
and Kaur, Gurleen
and Dwivedi, Shiv Kumar
and Mishra, Jitendra",
editor="Arora, Naveen Kumar
and Mishra, Jitendra",
title="The Silent Menace of Fumonisins: Cancer Risks, Outbreak Lessons, and Mitigation Pathways",
bookTitle="Biotechnological Solutions for a Sustainable Future: From Soil Health to Industrial Applications",
year="2026",
publisher="Springer Nature Singapore",
address="Singapore",
pages="255--278",
abstract="Fumonisins, primarily produced by Fusarium verticillioides, are a group of mycotoxins commonly found in maize and maize-based products, posing a significant global food safety concern due to their carcinogenic and systemic toxic potential. Fumonisin B1 (FB1), the most prevalent and toxic variant, has been classified as a Group 2B carcinogen by the IARC and is strongly associated with esophageal and liver cancers, as well as nephrotoxicity. Mechanistically, FB1 disrupts sphingolipid metabolism by inhibiting ceramide synthase, resulting in apoptotic imbalance, oxidative stress, and epigenetic dysregulation---hallmarks of carcinogenesis. Epidemiological studies in high-risk regions, such as northeastern Iran and southern Africa, support a strong correlation between dietary FB1 exposure and elevated cancer incidence. Moreover, FB1 has been linked to renal carcinogenesis in rodent models, where the kidney's proximal tubules display heightened sensitivity to sphingolipid perturbation. Given its environmental resilience and food chain persistence, effective control of fumonisins demands an integrated farm-to-fork approach. Strategies include pre- and postharvest interventions, biological controls using Trichoderma and Bacillus species, and emerging chemical detoxification techniques like ozonation and enzymatic hydrolysis. Despite promising advances, challenges remain---particularly in regulatory approval, affordability, and field scalability. This chapter also highlights rapid detection technologies and potential therapeutic responses for fumonisin-induced liver cancer. India's experience with mycotoxin outbreaks underscores the urgency of strengthening food safety surveillance and public health awareness. Collectively, this work reinforces fumonisin's significant public health implications and the necessity for multidisciplinary mitigation efforts that span agriculture, toxicology, biotechnology, and public policy.",
isbn="978-981-95-3989-5",
doi="10.1007/978-981-95-3989-5_11",
url="https://doi.org/10.1007/978-981-95-3989-5_11"
}


@Article{DiCarlo2026,
author="Di Carlo, Emma",
title="Tumor-on-chip's alliance with molecular pathology against metastatic disease",
journal="Journal of Biomedical Science",
year="2026",
month="Jan",
day="06",
volume="33",
number="1",
pages="9",
abstract="Cancer is the second leading cause of death worldwide. While significant progress has been made in early detection and treatment, metastasis remains the major cause of cancer-related morbidity and mortality. In the last decade the rate of long-term survivorship of metastatic cancer has continued to improve and overcoming resistance to therapy has now become a challenge. Developing strategies to prevent and treat metastatic disease is a priority for public health and requires a thorough understanding of the mechanisms driving progression of a specific patient's tumor and the rapid identification of targetable cancer drivers and drug resistance genes.",
issn="1423-0127",
doi="10.1186/s12929-025-01209-8",
url="https://doi.org/10.1186/s12929-025-01209-8"
}


@inproceedings{10.1007/978-3-031-61546-7_15,
 abstract = {Modern civilization is being impacted by the digital revolution, which is evolving at an astounding rate. ICT are rapidly developing, widely adopted, and used by people worldwide for social networking, travel, and physical exercise. Gamified cyclotourism, which combines the concepts of ICT, tourism, and exercise, may be a technological solution that encourages the senior public to participate in the activity. It offers many advantages, including low-impact physical activity, sustainability, social and cultural interaction, and health and wellbeing. The purpose of this study is to report the development and evaluation of Gen from Jizo, an improved concept of a senior cyclotourism app entitled Jizo, following a development research methodology. Twelve senior citizens and five ICT informed individuals were involved, at different moments, in the testing and evaluation of the app and website hi-fi prototypes. Despite suggesting some changes, the results generally imply that participants thought the interfaces were easy to use, straightforward, and compliant with design and interaction standards. It is claimed that these solutions fight against social isolation, encourage consistent physical exercise, and provide enriching experiences that encourage social contact among users.},
 address = {Cham},
 author = {Ortet, Cl{\'a}udia Pedro
and Veloso, Ana Isabel
and Vale Costa, Liliana},
 booktitle = {Human Aspects of IT for the Aged Population},
 editor = {Gao, Qin
and Zhou, Jia},
 isbn = {978-3-031-61546-7},
 pages = {227--242},
 publisher = {Springer Nature Switzerland},
 title = {Towards a New (Old) Generation of Cyclotourists: Implementing an Improved Concept of Jizo Brand},
 url = {https://link.springer.com/chapter/10.1007/978-3-031-61546-7_15},
 year = {2024}
}

@Inbook{Kreissl2024,
author="Kreissl, Reinhard
and von Laufenberg, Roger",
editor="Heinlein, Michael
and Huchler, Norbert",
title="The Controversy About Risks and Threats in Artificial Intelligence",
bookTitle="Artificial Intelligence in Society: Social, Political and Cultural Implications of a Technological Innovation",
year="2024",
publisher="Springer Fachmedien Wiesbaden",
address="Wiesbaden",
pages="221--253",
abstract="The debate about Artificial intelligence (AI) displays a complex constellation. On the one hand there are the techno-solutionists, who believe in the instrumental value and impact of AI. Here we find utopians and dystopians. For the utopians, AI holds a bold promise, it will boost human capacities, facilitate problem-solving, optimise information management and bring the human race to the next (transhuman) level. The dystopians' position is a mirror image of the utopian hopes: humans will lose autonomy and control to the AI-enabled techno-social systems that will curb genuine human creativity and reason. In the other large camp of techno-critics, we also find two positions: AI, so one group of critics point out, is far from living up to the promises and claims of its techno-solutionist adherents. While this position wants to defuse the overblown hopes and fears of the techno-solutionist camp, others see several detrimental effects in the spread of AI-enabled systems, albeit not as a consequence of the self-proclaimed capabilities of AI but rather as side-effects of its application in mundane everyday contexts. We will try to disentangle this complex constellation, starting from a discussion, investigating some of the basic claims entertained in the debate about AI. To develop a better understanding of risks and threats we will then introduce four scenarios to discuss the societal, practical impact of AI.",
isbn="978-3-658-45708-2",
doi="10.1007/978-3-658-45708-2_9",
url="https://doi.org/10.1007/978-3-658-45708-2_9"
}


@Article{Elsokkary2025,
author="Elsokkary, Nada
and Khan, Wasif
and Shurrab, Mohammed
and Mizouni, Rabeb
and Singh, Shakti
and Bentahar, Jamal
and Mourad, Azzam
and Otrok, Hadi",
title="Reinforcement learning and the Metaverse: a symbiotic collaboration",
journal="Artificial Intelligence Review",
year="2025",
month="Dec",
day="11",
volume="59",
number="1",
pages="34",
abstract="The Metaverse is an emerging virtual reality space that merges digital and physical worlds and provides users with immersive, interactive, and persistent virtual environments. The Metaverse leverages multiple technologies, including digital twins, blockchain, artificial intelligence, extended reality, and edge computing to realize the seamless connectivity and interaction between both worlds: physical and virtual. Artificial Intelligence (AI) empowers intelligent decisions in such complex dynamic environments. More specifically, Reinforcement Learning (RL) is uniquely effective in the context of Metaverse applications due to the natural process of learning through interaction and its modeling of sequential decision making, allowing it to be flexible, dynamic, and able to discover complex strategies and emergent behavior in complicated environments where programming explicit rules is impractical. Although multiple works have explored the research on the Metaverse and AI-based applications, there remains a significant gap in the literature that addresses the contribution of RL algorithms within the Metaverse. Therefore, this review presents a comprehensive overview of RL algorithms for Metaverse applications. We examine the architecture of Metaverse networks, the role of RL in enhancing virtual interactions, and the potential for transferring learned behaviors to real-world applications. Furthermore, we categorize the key challenges, opportunities, and research directions associated with deploying RL in the Metaverse.",
issn="1573-7462",
doi="10.1007/s10462-025-11433-1",
url="https://doi.org/10.1007/s10462-025-11433-1"
}


@Inbook{Mishra2025,
author="Mishra, Prafful",
title="Providing Practical Guidance",
bookTitle="A Guide to Implementing MLOps: From Data to Operations",
year="2025",
publisher="Springer Nature Switzerland",
address="Cham",
pages="9--86",
abstract="This chapter provides practical guidance on how to implement MLOps. Most of the sections delve into the technical aspects, with some touch of non-technical challenges that one might face.",
isbn="978-3-031-82010-6",
doi="10.1007/978-3-031-82010-6_2",
url="https://doi.org/10.1007/978-3-031-82010-6_2"
}


@Article{Bucchiarone2025,
author="Bucchiarone, Antonio
and Di Rocco, Juri
and Di Vincenzo, Damiano
and Pierantonio, Alfonso",
title="Modeling in Jjodel: towards bridging complexity and usability in model-driven engineering",
journal="Software and Systems Modeling",
year="2025",
month="Oct",
day="06",
abstract="Jjodel is a cloud-based reflective platform designed to address the challenges of Model-Driven Engineering (MDE), particularly the cognitive complexity and usability barriers often encountered in existing model-driven tools. This article presents the motivation and requirements behind the design of Jjodel and demonstrates how it satisfies these through its key features. By offering a low-code environment with modular viewpoints for syntax, validation, and semantics, Jjodel empowers language designers to define and refine domain-specific languages (DSLs) with ease. Its innovative capabilities, such as live co-evolution support, and syntax customization, ensure adaptability for academic and industrial contexts. A practical case study of an algebraic expression language highlights the ability of Jjodel to manage positional semantics and event-driven workflows, illustrating its effectiveness in simplifying complex modeling scenarios. Built on modern front-end technologies, Jjodel aims to operationalize concepts from MDE research into a usable platform that supports a range of modeling tasks.",
issn="1619-1374",
doi="10.1007/s10270-025-01324-y",
url="https://doi.org/10.1007/s10270-025-01324-y"
}


@Inbook{Zhang2024,
author="Zhang, Li",
title="What Could Be the Goal of AI After IT Wakes up?",
bookTitle="AGI is Waking Up!: From the Thought Lab of Science Fiction",
year="2024",
publisher="Springer Nature Singapore",
address="Singapore",
pages="201--216",
abstract="In the film Ex Machina, Ava is an AI robot. She has a delicate face and a meticulously crafted body. Her eyes are clear, her demeanor elegant; she is indeed a secluded beauty.",
isbn="978-981-97-4515-9",
doi="10.1007/978-981-97-4515-9_10",
url="https://doi.org/10.1007/978-981-97-4515-9_10"
}


@Article{Qu2024,
author="Qu, Yubin
and Huang, Song
and Yao, Yongming",
title="A survey on robustness attacks for deep code models",
journal="Automated Software Engineering",
year="2024",
month="Aug",
day="09",
volume="31",
number="2",
pages="65",
abstract="With the widespread application of deep learning in software engineering, deep code models have played an important role in improving code quality and development efficiency, promoting the intelligence and industrialization of software engineering. In recent years, the fragility of deep code models has been constantly exposed, with various attack methods emerging against deep code models and robustness attacks being a new attack paradigm. Adversarial samples after model deployment are generated to evade the predictions of deep code models, making robustness attacks a hot research direction. Therefore, to provide a comprehensive survey of robustness attacks on deep code models and their implications, this paper comprehensively analyzes the robustness attack methods in deep code models. Firstly, it analyzes the differences between robustness attacks and other attack paradigms, defines basic attack methods and processes, and then summarizes robustness attacks' threat model, evaluation metrics, attack settings, etc. Furthermore, existing attack methods are classified from multiple dimensions, such as attacker knowledge and attack scenarios. In addition, common tasks, datasets, and deep learning models in robustness attack research are also summarized, introducing beneficial applications of robustness attacks in data augmentation, adversarial training, etc., and finally, looking forward to future key research directions.",
issn="1573-7535",
doi="10.1007/s10515-024-00464-7",
url="https://doi.org/10.1007/s10515-024-00464-7"
}


@Article{Dilek2025,
author="Dilek, Esma
and Dener, Murat",
title="An overview of transformers for video anomaly detection",
journal="Neural Computing and Applications",
year="2025",
month="Aug",
day="01",
volume="37",
number="22",
pages="17825--17857",
abstract="Transformer is a kind of deep neural network that relies on the technique of self-attention and used initially in the field of natural language processing. Scientists use transformer for computer vision (CV) applications because of its good data representation capabilities. Transformer-based models yield similar performance or surpass other network architectures, including convolutional and recurrent neural networks, in a variety of visual benchmarks. In this work, we investigate the methods for video anomaly detection (VAD) using vision transformer models in the recent literature. The main topics we explore comprise vision transformers used in CV applications with a special focus on VAD methods leveraging transformer architecture. We also briefly present anomaly detection methods based on transformers. Additionally, we address the advantages, challenges and current limitations of the transformer architecture as well as potential solutions to address the technical challenges. In the concluding section of this study, we offer avenues for further investigation concerning the use of vision transformers in VAD tasks.",
issn="1433-3058",
doi="10.1007/s00521-025-11218-1",
url="https://doi.org/10.1007/s00521-025-11218-1"
}


@Article{Hu2025,
author="Hu, Xiuying
and Hu, Tianzhen
and Cao, Shuyun
and Jiang, Li
and Zhou, Yan
and Fang, Qin
and Wang, Jishi",
title="The ALDH2/PolG2 axis enhances mitochondrial biogenesis via transcriptional regulation of Nrf2 and promotes chemotherapy resistance in acute myeloid leukaemia",
journal="Cell Death {\&} Disease",
year="2025",
month="Aug",
day="13",
volume="16",
number="1",
pages="616",
abstract="Although patients with acute myeloid leukaemia (AML) initially respond to conventional treatments, many patients die from AML progression and relapsed/refractory (RR) disease. Eradicating AML thus remains therapeutically challenging. In this study, we found a strong expression of aldehyde dehydrogenase 2 (ALDH2) and increased mitochondrial biosynthesis in samples from patients with drug-resistant AML, and these changes were strongly associated with poor prognosis and recurrence of AML. We examined the clonogenic capacity, growth and apoptosis of AML cells, as well as mitochondrial DNA expression and reactive oxygen species production. Our results revealed that chemotherapeutic agents triggered the activation of NF-E2-related factor 2 (Nrf2) and promoted high expression of ALDH2, mediating the compensatory activation of mitochondrial respiration and resistance to chemotherapeutic agents in RR AML cells. Nrf2 promoted mitochondrial respiration by activating ALDH2 expression and stabilising the expression of DNA polymerase-gamma2 (PolG2) in mitochondria. Inhibition of the Nrf2-ALDH2/PolG2 pathway reduced AML metabolic fitness and oxidative phosphorylation levels, highlighting the key role of this pathway in promoting cell survival. Nrf2 inhibition reduced the translation of ALDH2, induced a unique mitochondrial stress response and inhibited mitochondrial biosynthesis in AML cells. Importantly, tumours in an in vivo xenograft model were sensitive to combined Nrf2 and ALDH2 inhibition. Given the role of the Nrf2-ALDH2/PolG2 pathway in the progression of AML, inhibition of this pathway may prevent disease relapse/resistance and promote sensitisation to chemotherapy.",
issn="2041-4889",
doi="10.1038/s41419-025-07927-z",
url="https://doi.org/10.1038/s41419-025-07927-z"
}


@Inbook{Kiggins2025,
author="Kiggins, Ryan David",
editor="David Kiggins, Ryan",
title="Big Data, Artificial Intelligence, and Autonomous Policy Decision-Making: A Crisis in International Relations Theory?",
bookTitle="The Political Economy of Robots: Prospects for Prosperity and Peace in the Automated 21st Century,  2nd edition",
year="2025",
publisher="Springer Nature Switzerland",
address="Cham",
pages="297--327",
abstract="Theories of international relations may soon confront a crisis of explanative power. This crisis emerges from a shift in how policymakers determine policy decisions to effectuate outcomes consistent with global policy objectives. International relations theory is a composite of heterodox traditions many of which are in disagreement but nonetheless share a common objective: the explanation of human decision-making under certain conditions. The emergence of information technologies has inundated policymakers with data derived from tracking, recording, and analyzing information technology user behavior. This data deluge has spawned new data analysis techniques and technologies leveraged when making and automating policy decisions. Automated systems are consequential actors in global politics. International relations theory must account for the agentive capacity of automated systems through reflexivity.",
isbn="978-3-031-95171-8",
doi="10.1007/978-3-031-95171-8_13",
url="https://doi.org/10.1007/978-3-031-95171-8_13"
}


@Article{Wickramathilaka2025,
author="Wickramathilaka, Shavindra
and Grundy, John
and Madampe, Kashumi
and Haggag, Omar",
title="Adaptive and accessible user interfaces for seniors through model-driven engineering",
journal="Automated Software Engineering",
year="2025",
month="Aug",
day="11",
volume="32",
number="2",
pages="74",
abstract="The use of diverse mobile applications among senior users is becoming increasingly widespread. However, many of these apps contain accessibility problems that result in negative user experiences for seniors. A key reason is that software practitioners often lack the time or resources to address the broad spectrum of age-related accessibility and personalisation needs. As current developer tools and practices encourage one-size-fits-all interfaces with limited potential to address the diversity of senior needs, there is a growing demand for approaches that support the systematic creation of adaptive, accessible app experiences. To this end, we present AdaptForge, a novel model-driven engineering (MDE) approach that enables advanced design-time adaptations of mobile application interfaces and behaviours tailored to the accessibility needs of senior users. AdaptForge uses two domain-specific languages (DSLs) to address age-related accessibility needs. The first model defines users' context-of-use parameters, while the second defines conditional accessibility scenarios and corresponding UI adaptation rules. These rules are interpreted by an MDE workflow to transform an app's original source code into personalised instances. We also report evaluations with professional software developers and senior end-users, demonstrating the feasibility and practical utility of AdaptForge.",
issn="1573-7535",
doi="10.1007/s10515-025-00547-z",
url="https://doi.org/10.1007/s10515-025-00547-z"
}


@Article{Maile2025,
author="Maile, Thomas
and Masoumi, Salim
and Wang, Carl
and Riddell, Anna
and McClusky, Simon",
title="Analysis of the AUSPOS v3.0 positioning service after alignment to ITRF2020",
journal="GPS Solutions",
year="2025",
month="Dec",
day="29",
volume="30",
number="1",
pages="52",
abstract="AUSPOS is an openly available online Global Positioning System (GPS) data processing service provided by Geoscience Australia and is recommended by the Intergovernmental Committee on Surveying and Mapping (ICSM) for control surveys by Global Navigation Satellite System (GNSS) in the Australian region as a method for providing connection to the Geocentric Datum of Australia 2020 (GDA2020). A new and improved global reference frame, the International Terrestrial Reference Frame 2020 (ITRF2020), was released in April 2022 by the International Earth Rotation and Reference Systems Service (IERS), which was followed by the International GNSS Service (IGS) realisation, using modernised analysis standards and processing strategies, called IGS20. This paper outlines the adoption of ITRF2020/IGS20 for the AUSPOS processing service, which includes the introduction of a two-step transformation strategy to provide access to the national datum, GDA2020. The paper also provides relevant analysis and validation of the new AUSPOS system's (v3.0) ability to produce consistent GDA2020 coordinates when compared to those produced by the previous AUSPOS system (v2.4 - aligned to ITRF2014) with observed differences of {\$}{\$}{\backslash}:-0.2{\backslash}pm{\backslash}:6mm,{\backslash}:-0.2{\backslash}pm{\backslash}:4mm{\backslash}:and-0.3{\backslash}pm{\backslash}:21mm{\$}{\$}in Easting, Northing and Height components, respectively. Geoscience Australia is committed to being Australia's trusted provider of analytic products and services, by enhancing the accuracy and reliability of positioning within Australia.",
issn="1521-1886",
doi="10.1007/s10291-025-02003-7",
url="https://doi.org/10.1007/s10291-025-02003-7"
}


@Article{Cibralic2024,
author="Cibralic, Beba
and Mattingly, James",
title="Machine agency and representation",
journal="AI {\&} SOCIETY",
year="2024",
month="Feb",
day="01",
volume="39",
number="1",
pages="345--352",
abstract="Theories of action tend to require agents to have mental representations. A common trope in discussions of artificial intelligence (AI) is that they do not, and so cannot be agents. Properly understood there may be something to the requirement, but the trope is badly misguided. Here we provide an account of representation for AI that is sufficient to underwrite attributions to these systems of ownership, action, and responsibility. Existing accounts of mental representation tend to be too demanding and unparsimonious. We offer instead a minimalist account of representation that ascribes only those features necessary for explaining action, trimming the ``extra'' features in existing accounts (e.g., representation as a ``mental'' phenomenon). Our account makes `representation' whatever it is that, for example, the thermostat is doing with the thermometer. The thermostat is disposed to act as long as the thermometer is outside a given range of parameters.. Our account allows us to offer a new perspective on the `responsibility gap', a problem raised by the actions of sophisticated machines: because nobody has enough control over the machine's actions to be able to assume responsibility, conventional approaches to responsibility ascription are inappropriate. We argue that there is a distinction between finding responsible and holding responsible and, in order to resolve the responsibility gap, we must first clarify the conceptual terrain on which agent is in fact responsible. ",
issn="1435-5655",
doi="10.1007/s00146-022-01446-7",
url="https://doi.org/10.1007/s00146-022-01446-7"
}


@Inbook{Neuwirth2025,
author="Neuwirth, Rostam J.",
editor="Raposo, Vera L{\'u}cia",
title="Prohibited Artificial Intelligence Practices Revisited",
bookTitle="The European Artificial Intelligence Act: Promises and Perils?",
year="2025",
publisher="Springer Nature Switzerland",
address="Cham",
pages="131--155",
abstract="In April 2021, the European Commission launched its initial proposal for an Artificial Intelligence Act (AIA). More than three years later, on 1 August 2024, the Act finally entered into force. This chapter revisits the category of `prohibited AI practices' enshrined in Article 5 AIA, which are banned because they are deemed to pose unacceptable risks to the European Union's values and fundamental rights. After several amendments, the prohibition now covers eight different but interrelated categories of artificial intelligence (AI) practices, namely AI systems that deploy subliminal or purposefully manipulative or deceptive techniques, AI systems that exploit vulnerabilities, AI systems that predict the risk of committing a criminal offence, the creation of facial recognition databases, emotion recognition systems, social scoring systems, biometric categorisation systems and remote real-time biometric identification systems used for the purpose of law enforcement. The chapter examines each of these categories in isolation and critically contrasts the respective changes introduced during the negotiation phase. Last, it undertakes a brief outlook on the interplay between each of these categories and future regulatory challenges in this rapidly evolving field of technology.",
isbn="978-3-031-98406-8",
doi="10.1007/978-3-031-98406-8_6",
url="https://doi.org/10.1007/978-3-031-98406-8_6"
}


@Article{Yuan2024,
author="Yuan, Dawei
and Peng, Xiao
and Chen, Zijie
and Zhang, Tao
and Lei, Ruijia",
title="Code context-based reviewer recommendation",
journal="Frontiers of Computer Science",
year="2024",
month="Nov",
day="11",
volume="19",
number="1",
pages="191202",
abstract="Code review is a critical process in software development, contributing to the overall quality of the product by identifying errors early. A key aspect of this process is the selection of appropriate reviewers to scrutinize changes made to source code. However, in large-scale open-source projects, selecting the most suitable reviewers for a specific change can be a challenging task. To address this, we introduce the Code Context Based Reviewer Recommendation (CCB-RR), a model that leverages information from changesets to recommend the most suitable reviewers. The model takes into consideration the paths of modified files and the context derived from the changesets, including their titles and descriptions. Additionally, CCB-RR employs KeyBERT to extract the most relevant keywords and compare the semantic similarity across changesets. The model integrates the paths of modified files, keyword information, and the context of code changes to form a comprehensive picture of the changeset. We conducted extensive experiments on four open-source projects, demonstrating the effectiveness of CCB-RR. The model achieved a Top-1 accuracy of 60{\%}, 55{\%}, 51{\%}, and 45{\%} on the Android, OpenStack, QT, and LibreOffice projects respectively. For Mean Reciprocal Rank (MRR), CCB achieved 71{\%}, 62{\%}, 52{\%}, and 68{\%} on the same projects respectively, thereby highlighting its potential for practical application in code reviewer recommendation.",
issn="2095-2236",
doi="10.1007/s11704-023-3256-9",
url="https://doi.org/10.1007/s11704-023-3256-9"
}


@Article{Park2025,
author="Park, Su Jung
and Cerella, Claudia
and Kang, Jin Mo
and Byun, Jinyoung
and Kum, David
and Orlikova-Boyer, Barbora
and Lorant, Anne
and Schnekenburger, Michael
and Al-Mourabit, Ali
and Christov, Christo
and Lee, Juyong
and Han, Byung Woo
and Diederich, Marc",
title="Tetrahydrobenzimidazole TMQ0153 targets OPA1 and restores drug sensitivity in AML via ROS-induced mitochondrial metabolic reprogramming",
journal="Journal of Experimental {\&} Clinical Cancer Research",
year="2025",
month="Apr",
day="07",
volume="44",
number="1",
pages="114",
abstract="Acute myeloid leukemia (AML) is a highly aggressive cancer with a 5-year survival rate of less than 35{\%}. It is characterized by significant drug resistance and abnormal energy metabolism. Mitochondrial dynamics and metabolism are crucial for AML cell survival. Mitochondrial fusion protein optic atrophy (OPA)1 is upregulated in AML patients with adverse mutations and correlates with poor prognosis.",
issn="1756-9966",
doi="10.1186/s13046-025-03372-0",
url="https://doi.org/10.1186/s13046-025-03372-0"
}


@Inbook{Kshetri2025,
author="Kshetri, Nir",
title="Adapting to Global Markets: Strategies for Marketing and R{\&}D",
bookTitle="Foundations of International Business: An Introduction to Trade Dynamics, Institutional Frameworks, and Strategic Operations",
year="2025",
publisher="Springer Nature Switzerland",
address="Cham",
pages="255--282",
abstract="The marketing mix, crucial for targeting markets, comprises product attributes, distribution strategy, communication strategy, and pricing strategy. Its effectiveness varies based on a market's political, cultural, economic, and infrastructural characteristics. International expansion requires firms to choose between standardizing or adapting these elements. Theodore Levitt's theory supports global standardization, but current perspectives recognize significant cultural and economic differences that necessitate market-specific adjustments. Globalization trends are tempered by trade barriers and varying product standards, often requiring local adaptations. Effective market segmentation is based on geography, demography, socio-cultural, and psychological factors. Strategies must be tailored to these segments, with e-commerce and technological advancements influencing distribution and communication. The rapid pace of technological change and shortened product life cycles make strong coordination between R{\&}D, marketing, and manufacturing essential. Prioritizing product innovation is crucial for maintaining a competitive edge in today's fast-evolving market.",
isbn="978-3-032-03244-7",
doi="10.1007/978-3-032-03244-7_10",
url="https://doi.org/10.1007/978-3-032-03244-7_10"
}


@Article{Ueda2025,
author="Ueda, Daiju
and Walston, Shannon L.
and Kurokawa, Ryo
and Saida, Tsukasa
and Honda, Maya
and Iima, Mami
and Watabe, Tadashi
and Yanagawa, Masahiro
and Nishioka, Kentaro
and Sofue, Keitaro
and Sakata, Akihiko
and Sugawara, Shunsuke
and Kawamura, Mariko
and Ito, Rintaro
and Takumi, Koji
and Oda, Seitaro
and Hirata, Kenji
and Ide, Satoru
and Naganawa, Shinji",
title="Artificial superintelligence alignment in healthcare",
journal="Japanese Journal of Radiology",
year="2025",
month="Nov",
day="14",
abstract="The emergence of Artificial Superintelligence (ASI) in healthcare presents unprecedented opportunities for revolutionizing diagnostics, treatment planning, and population health management, but also introduces critical risks if these systems are not properly aligned with human values and clinical objectives. This review examines the theoretical foundations of ASI and the alignment problem in healthcare contexts, exploring how misaligned Artificial Intelligence (AI) systems could optimize for wrong objectives or pursue harmful strategies leading to patient harm and systemic failures. Current challenges in AI alignment are illustrated through real-world examples from radiology and clinical decision-making, where algorithms have demonstrated concerning biases, generalizability failures, and optimization for inappropriate proxy measures. The paper analyzes key alignment challenges including objective complexity and technical pitfalls, bias and fairness issues in healthcare data, ethical integration concerns involving compassion and patient autonomy, and system-level policy challenges around regulation and liability. Technical alignment strategies are discussed including reinforcement learning from human feedback, interpretability requirements, formal verification methods, and adversarial testing approaches. Normative alignment solutions encompass ethical frameworks, professional standards, patient engagement protocols, and multi-level governance structures spanning institutional, national, and international coordination. The review emphasizes that successful ASI alignment in healthcare requires combining cutting-edge AI research with fundamental medical ethics, noting that while proper alignment could enable transformative health improvements and medical breakthroughs, misalignment risks undermining the core purpose of medicine. The stakes of this alignment challenge are characterized as among the highest in both technology and ethics, with implications extending from individual patient safety to public trust and potentially existential risks.",
issn="1867-108X",
doi="10.1007/s11604-025-01907-1",
url="https://doi.org/10.1007/s11604-025-01907-1"
}


@Inbook{Schenk2025,
author="Schenk, Bernd",
title="System Development and System Deployment",
bookTitle="Advanced Management Information Systems: Models, Concepts and Cases",
year="2025",
publisher="Springer Nature Switzerland",
address="Cham",
pages="197--235",
abstract="This chapter explores foundational and modern concepts in system development and deployment, emphasizing the critical role these processes play in the success of information systems. By combining traditional methodologies, such as the Waterfall Model and Agile frameworks, with emerging paradigms like low-code/no-code platforms, artificial intelligence-driven development, and edge computing, organizations can adapt to rapidly changing technological landscapes.",
isbn="978-3-031-87904-3",
doi="10.1007/978-3-031-87904-3_5",
url="https://doi.org/10.1007/978-3-031-87904-3_5"
}


@Inbook{Zhang2025,
author="Zhang, Guanghan
and Zhang, Shuo
and Wu, Haiyan",
editor="Xu, Wei",
title="AI-Augmented Computational Modeling of Human Behavior",
bookTitle="Handbook of Human-Centered Artificial Intelligence",
year="2025",
publisher="Springer Nature Singapore",
address="Singapore",
pages="1--55",
abstract="This chapter focuses on the integration of artificial intelligence methodologies into computational modeling of human behavior, emphasizing the interdisciplinary convergence of cognitive psychology, neuroscience, and machine learning. It describes the evolution from traditional behavioral modeling approaches to advanced deep learning, such as neural networks and probabilistic models, to achieve more nuanced simulations of human cognition and social interactions. Furthermore, it highlights the enhanced capacity of Bayesian frameworks to incorporate prior knowledge and manage uncertainty, thereby providing a more robust theoretical foundation compared to classical frequentist methods. In addition, it summarizes the application of reinforcement learning paradigms to emulate human learning processes and strategic decision-making. With the development of human-centered artificial intelligence (HCAI), AI technologies have not only advanced human behavior modeling but also introduced new perspectives for the design of intelligent systems, especially in areas like simulating human cognition, emotion, and collective behaviors. This chapter offers a comprehensive analysis of state-of-the-art AI-driven modeling strategies, highlighting their potential to yield deeper insights into complex human behaviors and inform the development of intelligent systems capable of adaptive human cognition.",
isbn="978-981-97-8440-0",
doi="10.1007/978-981-97-8440-0_111-1",
url="https://doi.org/10.1007/978-981-97-8440-0_111-1"
}


@Inbook{Powers2025,
author="Powers, Thomas M.",
editor="Noorman, Merel
and Verdicchio, Mario",
title="Understanding Sociotechnical Systems",
bookTitle="Computer Ethics Across Disciplines: Deborah G. Johnson and Algorithmic Accountability",
year="2025",
publisher="Springer Nature Switzerland",
address="Cham",
pages="145--165",
abstract="Sociotechnical explanations can contribute to individual failure investigations and even normative critiques of an entire system, in the event that clear descriptions of causes and conditions of failure can be specified. But in many failed systems, what has happened is complicated. At present, sociotechnical systems approaches in organizational studies, engineering, and philosophy do not meet standards of clarity that are required of explanation. Further, sociotechnical system approaches so far have not been used to predict instances of failure, though it would be highly desirable if we could develop such predictions. Complications with explanation and the dim prospects for prediction lead us in the direction of new computational technologies, but these are not without risks. Progress in understanding sociotechnical systems means, in effect, that explanations and predictions would become useful for ``unenhanced'' human understanding, not just for computational models of sociotechnical behavior. One byproduct of this better understanding could be guidance on when to hold certain parts of AI sociotechnical systems accountable---namely, when the algorithms by which they operate could be held accountable for damages that they cause.",
isbn="978-3-031-87974-6",
doi="10.1007/978-3-031-87974-6_9",
url="https://doi.org/10.1007/978-3-031-87974-6_9"
}


@Article{Watermeyer2025,
author="Watermeyer, Richard
and Lanclos, Donna
and Phipps, Lawrie
and Shapiro, Hanne
and Guizzo, Danielle
and Knight, Cathryn",
title="Academics' Weak(ening) Resistance to Generative AI: The Cause and Cost of Prestige?",
journal="Postdigital Science and Education",
year="2025",
month="Dec",
day="01",
volume="7",
number="4",
pages="1171--1191",
abstract="The disruptive potential of generative AI (GenAI) tools to academic labour is potentially vast. Yet as we argue herein, such tools also represent a continuation of the inequities inherent to academia's prestige economy and the intensified hierarchy and labour precarisation endemic to universities as prestige institutions. In a recent survey of n{\thinspace}={\thinspace}284 UK-based academics, reasons were put forward for avoiding GenAI tools. These responses surface concerns about automative technologies corrupting academic identity and inauthenticating scholarly practice; concerns that are salient to all who participate within and benefit from the work of scholarly communities. In discussion of these survey results, we explore ambivalence about whether GenAI tools expedite the acquisition or depletion of prestige demanded of academics, especially where GenAI tools are adopted to increase scholarly productivity. We also appraise whether, far from helping academics cope with a work climate of hyper-intensifcation, GenAI tools ultimately exacerbate their vulnerability, status-based peripheralisation, and self-estrangement.",
issn="2524-4868",
doi="10.1007/s42438-024-00524-x",
url="https://doi.org/10.1007/s42438-024-00524-x"
}


@Inbook{Garvey2024,
author="Garvey, Bruce
and Svendsen, Adam D. M.",
title="Prompt-Engineering Testing ChatGPT4 and Bard for Assessing Generative-AI Efficacy to Support Decision-Making",
bookTitle="Navigating Uncertainty Using Foresight Intelligence: A Guidebook for Scoping Scenario Options in Cyber and Beyond",
year="2024",
publisher="Springer Nature Switzerland",
address="Cham",
pages="167--212",
abstract="In this chapter, we examine what the Generative-AI (Gen-AI) systems of OpenAI's ChatGPT4 and Google's Bard (from 2024, re-named Gemini) can offer during each stage of the Strategic Options Analysis (SOA) process.",
isbn="978-3-031-66115-0",
doi="10.1007/978-3-031-66115-0_10",
url="https://doi.org/10.1007/978-3-031-66115-0_10"
}


@Article{Di Rocco2025,
author="Di Rocco, Juri
and Di Ruscio, Davide
and Di Sipio, Claudio
and Nguyen, Phuong T.
and Rubei, Riccardo",
title="On the use of large language models in model-driven engineering",
journal="Software and Systems Modeling",
year="2025",
month="Jun",
day="01",
volume="24",
number="3",
pages="923--948",
abstract="Model-driven engineering (MDE) has seen significant advancements with the integration of machine learning (ML) and deep learning techniques. Building upon the groundwork of previous investigations, our study provides a concise overview of current large language models (LLMs) applications in MDE, emphasizing their role in automating tasks like model repository classification and developing advanced recommender systems. The paper also outlines the technical considerations for seamlessly integrating LLMs in MDE, offering a practical guide for researchers and practitioners. Looking forward, the paper proposes a focused research agenda for the future interplay of LLMs and MDE, identifying key challenges and opportunities. This concise roadmap envisions the deployment of LLM techniques to enhance the management, exploration, and evolution of modeling ecosystems. Moreover, we also discuss the adoption of LLMs in various domains by means of model-driven techniques and tools, i.e., MDE for supporting LLMs. By offering a compact exploration of LLMs in MDE, this paper contributes to the ongoing evolution of MDE practices, providing a forward-looking perspective on the transformative role of large language models in software engineering and model-driven practices.",
issn="1619-1374",
doi="10.1007/s10270-025-01263-8",
url="https://doi.org/10.1007/s10270-025-01263-8"
}


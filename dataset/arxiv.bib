@misc{pearce2022examiningzeroshotvulnerabilityrepair,
 abstract = {Human developers can produce code with cybersecurity bugs. Can emerging 'smart' code completion tools help repair those bugs? In this work, we examine the use of large language models (LLMs) for code (such as OpenAI's Codex and AI21's Jurassic J-1) for zero-shot vulnerability repair. We investigate challenges in the design of prompts that coax LLMs into generating repaired versions of insecure code. This is difficult due to the numerous ways to phrase key information - both semantically and syntactically - with natural languages. We perform a large scale study of five commercially available, black-box, "off-the-shelf" LLMs, as well as an open-source model and our own locally-trained model, on a mix of synthetic, hand-crafted, and real-world security bug scenarios. Our experiments demonstrate that while the approach has promise (the LLMs could collectively repair 100% of our synthetically generated and hand-crafted scenarios), a qualitative evaluation of the model's performance over a corpus of historical real-world examples highlights challenges in generating functionally correct code.},
 archiveprefix = {arXiv},
 author = {Hammond Pearce and Benjamin Tan and Baleegh Ahmad and Ramesh Karri and Brendan Dolan-Gavitt},
 eprint = {2112.02125},
 primaryclass = {cs.CR},
 title = {Examining Zero-Shot Vulnerability Repair with Large Language Models},
 url = {https://arxiv.org/abs/2112.02125},
 year = {2022}
}

@misc{zhang2022repairingbugspythonassignments,
 abstract = {Students often make mistakes on their introductory programming assignments as part of their learning process. Unfortunately, providing custom repairs for these mistakes can require a substantial amount of time and effort from class instructors. Automated program repair (APR) techniques can be used to synthesize such fixes. Prior work has explored the use of symbolic and neural techniques for APR in the education domain. Both types of approaches require either substantial engineering efforts or large amounts of data and training. We propose to use a large language model trained on code, such as Codex, to build an APR system -- MMAPR -- for introductory Python programming assignments. Our system can fix both syntactic and semantic mistakes by combining multi-modal prompts, iterative querying, test-case-based selection of few-shots, and program chunking. We evaluate MMAPR on 286 real student programs and compare to a baseline built by combining a state-of-the-art Python syntax repair engine, BIFI, and state-of-the-art Python semantic repair engine for student assignments, Refactory. We find that MMAPR can fix more programs and produce smaller patches on average.},
 archiveprefix = {arXiv},
 author = {Jialu Zhang and José Cambronero and Sumit Gulwani and Vu Le and Ruzica Piskac and Gustavo Soares and Gust Verbruggen},
 eprint = {2209.14876},
 primaryclass = {cs.SE},
 title = {Repairing Bugs in Python Assignments Using Large Language Models},
 url = {https://arxiv.org/abs/2209.14876},
 year = {2022}
}

@article{Ahmad_2024,
 abstract = {Novel AI-based code-writing Large Language Models (LLMs) such as OpenAI's Codex have demonstrated capabilities in many coding-adjacent domains. In this work we consider how LLMs maybe leveraged to automatically repair security relevant bugs present in hardware designs. We focus on bug repair in code written in the Hardware Description Language Verilog. For this study we build a corpus of domain-representative hardware security bugs. We then design and implement a framework to quantitatively evaluate the performance of any LLM tasked with fixing the specified bugs. The framework supports design space exploration of prompts (i.e., prompt engineering) and identifying the best parameters for the LLM. We show that an ensemble of LLMs can repair all ten of our benchmarks. This ensemble outperforms the state-of-the-art Cirfix hardware bug repair tool on its own suite of bugs. These results show that LLMs can repair hardware security bugs and the framework is an important step towards the ultimate goal of an automated end-to-end bug repair framework.},
 author = {Ahmad, Baleegh and Thakur, Shailja and Tan, Benjamin and Karri, Ramesh and Pearce, Hammond},
 doi = {10.1109/tifs.2024.3374558},
 issn = {1556-6021},
 journal = {IEEE Transactions on Information Forensics and Security},
 pages = {4043–4057},
 publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
 title = {On Hardware Security Bug Code Fixes by Prompting Large Language Models},
 url = {http://dx.doi.org/10.1109/TIFS.2024.3374558},
 volume = {19},
 year = {2024}
}

@misc{noever2023largelanguagemodelsfix,
 abstract = {In this study, we evaluated the capability of Large Language Models (LLMs), particularly OpenAI's GPT-4, in detecting software vulnerabilities, comparing their performance against traditional static code analyzers like Snyk and Fortify. Our analysis covered numerous repositories, including those from NASA and the Department of Defense. GPT-4 identified approximately four times the vulnerabilities than its counterparts. Furthermore, it provided viable fixes for each vulnerability, demonstrating a low rate of false positives. Our tests encompassed 129 code samples across eight programming languages, revealing the highest vulnerabilities in PHP and JavaScript. GPT-4's code corrections led to a 90% reduction in vulnerabilities, requiring only an 11% increase in code lines. A critical insight was LLMs' ability to self-audit, suggesting fixes for their identified vulnerabilities and underscoring their precision. Future research should explore system-level vulnerabilities and integrate multiple static code analyzers for a holistic perspective on LLMs' potential.},
 archiveprefix = {arXiv},
 author = {David Noever},
 eprint = {2308.10345},
 primaryclass = {cs.SE},
 title = {Can Large Language Models Find And Fix Vulnerable Software?},
 url = {https://arxiv.org/abs/2308.10345},
 year = {2023}
}

@misc{moon2024coffeeboostcodellms,
 abstract = {Code editing is an essential step towards reliable program synthesis to automatically correct critical errors generated from code LLMs. Recent studies have demonstrated that closed-source LLMs (i.e., ChatGPT and GPT-4) are capable of generating corrective feedback to edit erroneous inputs. However, it remains challenging for open-source code LLMs to generate feedback for code editing, since these models tend to adhere to the superficial formats of feedback and provide feedback with misleading information. Hence, the focus of our work is to leverage open-source code LLMs to generate helpful feedback with correct guidance for code editing. To this end, we present Coffee, a collected dataset specifically designed for code fixing with feedback. Using this dataset, we construct CoffeePots, a framework for COde Fixing with FEEdback via Preference-Optimized Tuning and Selection. The proposed framework aims to automatically generate helpful feedback for code editing while minimizing the potential risk of superficial feedback. The combination of Coffee and CoffeePots marks a significant advancement, achieving state-of-the-art performance on HumanEvalFix benchmark. Codes and model checkpoints are publicly available at this https URL.},
 archiveprefix = {arXiv},
 author = {Seungjun Moon and Hyungjoo Chae and Yongho Song and Taeyoon Kwon and Dongjin Kang and Kai Tzu-iunn Ong and Seung-won Hwang and Jinyoung Yeo},
 eprint = {2311.07215},
 primaryclass = {cs.CL},
 title = {Coffee: Boost Your Code LLMs by Fixing Bugs with Feedback},
 url = {https://arxiv.org/abs/2311.07215},
 year = {2024}
}

@misc{du2025exploringlargelanguagemodels,
 abstract = {Software crash bugs cause unexpected program behaviors or even abrupt termination, thus demanding immediate resolution. However, resolving crash bugs can be challenging due to their complex root causes, which can originate from issues in the source code or external factors like third-party library dependencies. Large language models (LLMs) have shown promise in software engineering tasks. However, existing research predominantly focuses on the capability of LLMs to localize and repair code-related crash bugs, leaving their effectiveness in resolving environment-related crash bugs in real-world software unexplored. To fill this gap, we conducted the first comprehensive study to assess the capability of LLMs in resolving real-world environment-related crash bugs. We first systematically compare LLMs' performance in resolving code-related and environment-related crash bugs with varying levels of crash contextual information. Our findings reveal that localization is the primary challenge for resolving code-related crashes, while repair poses a greater challenge for environment-related crashes. Furthermore, we investigate the impact of different prompt strategies on improving the resolution of environment-related crash bugs, incorporating different prompt templates and multi-round interactions. Building on this, we further explore an advanced active inquiry prompting strategy leveraging the self-planning capabilities of LLMs. Based on these explorations, we propose IntDiagSolver, an interactive methodology designed to enable precise crash bug resolution through ongoing engagement with LLMs. Extensive evaluations of IntDiagSolver across multiple LLMs (including GPT-3.5, GPT-4, Claude, CodeLlama, DeepSeek-R1, and Qwen-3-Coder) demonstrate consistent improvements in resolution accuracy, with substantial enhancements ranging from 9.1% to 43.3% in localization and 9.1% to 53.3% in repair.},
 archiveprefix = {arXiv},
 author = {Xueying Du and Mingwei Liu and Hanlin Wang and Juntao Li and Xin Peng and Yiling Lou},
 eprint = {2312.10448},
 primaryclass = {cs.SE},
 title = {Exploring Large Language Models in Resolving Environment-Related Crash Bugs: Localizing and Repairing},
 url = {https://arxiv.org/abs/2312.10448},
 year = {2025}
}

@misc{islam2024llmpoweredcodevulnerabilityrepair,
 abstract = {In software development, the predominant emphasis on functionality often supersedes security concerns, a trend gaining momentum with AI-driven automation tools like GitHub Copilot. These tools significantly improve developers' efficiency in functional code development. Nevertheless, it remains a notable concern that such tools are also responsible for creating insecure code, predominantly because of pre-training on publicly available repositories with vulnerable code. Moreover, developers are called the "weakest link in the chain" since they have very minimal knowledge of code security. Although existing solutions provide a reasonable solution to vulnerable code, they must adequately describe and educate the developers on code security to ensure that the security issues are not repeated. Therefore we introduce a multipurpose code vulnerability analysis system \texttt{SecRepair}, powered by a large language model, CodeGen2 assisting the developer in identifying and generating fixed code along with a complete description of the vulnerability with a code comment. Our innovative methodology uses a reinforcement learning paradigm to generate code comments augmented by a semantic reward mechanism. Inspired by how humans fix code issues, we propose an instruction-based dataset suitable for vulnerability analysis with LLMs. We further identify zero-day and N-day vulnerabilities in 6 Open Source IoT Operating Systems on GitHub. Our findings underscore that incorporating reinforcement learning coupled with semantic reward augments our model's performance, thereby fortifying its capacity to address code vulnerabilities with improved efficacy.},
 archiveprefix = {arXiv},
 author = {Nafis Tanveer Islam and Joseph Khoury and Andrew Seong and Mohammad Bahrami Karkevandi and Gonzalo De La Torre Parra and Elias Bou-Harb and Peyman Najafirad},
 eprint = {2401.03374},
 primaryclass = {cs.SE},
 title = {LLM-Powered Code Vulnerability Repair with Reinforcement Learning and Semantic Reward},
 url = {https://arxiv.org/abs/2401.03374},
 year = {2024}
}

@article{de_Fitero_Dominguez_2024,
 abstract = {This research addresses the complex challenge of automated repair of code vulnerabilities, vital for enhancing digital security in an increasingly technology-driven world. The study introduces a novel and efficient format for the representation of code modification, using advanced Large Language Models (LLMs) such as Code Llama and Mistral. These models, fine-tuned on datasets featuring C code vulnerabilities, significantly improve the accuracy and adaptability of automated code repair techniques. A key finding is the enhanced repair accuracy of these models when compared to previous methods such as VulRepair, which underscores their practical utility and efficiency. The research also offers a critical assessment of current evaluation metrics, such as perfect predictions, and their limitations in reflecting the true capabilities of automated repair models in real-world scenarios. Following this, it underscores the importance of using test datasets devoid of train samples, emphasizing the need for dataset integrity to enhance the effectiveness of LLMs in code repair tasks. The significance of this work is its contribution to digital security, setting new standards for automated code vulnerability repair and paving the way for future advancements in the fields of cybersecurity and artificial intelligence. The study does not only highlight the potential of LLMs in enhancing code security but also fosters further exploration and research in these crucial areas.},
 author = {de-Fitero-Dominguez, David and Garcia-Lopez, Eva and Garcia-Cabot, Antonio and Martinez-Herraiz, Jose-Javier},
 doi = {10.1016/j.engappai.2024.109291},
 issn = {0952-1976},
 journal = {Engineering Applications of Artificial Intelligence},
 month = {December},
 pages = {109291},
 publisher = {Elsevier BV},
 title = {Enhanced automated code vulnerability repair using large language models},
 url = {http://dx.doi.org/10.1016/j.engappai.2024.109291},
 volume = {138},
 year = {2024}
}

@misc{islam2024codesecurityvulnerabilityrepair,
 abstract = {With the recent advancement of Large Language Models (LLMs), generating functionally correct code has become less complicated for a wide array of developers. While using LLMs has sped up the functional development process, it poses a heavy risk to code security. Code generation with proper security measures using LLM is a significantly more challenging task than functional code generation. Security measures may include adding a pair of lines of code with the original code, consisting of null pointer checking or prepared statements for SQL injection prevention. Currently, available code repair LLMs generate code repair by supervised fine-tuning, where the model looks at cross-entropy loss. However, the original and repaired codes are mostly similar in functionality and syntactically, except for a few (1-2) lines, which act as security measures. This imbalance between the lines needed for security measures and the functional code enforces the supervised fine-tuned model to prioritize generating functional code without adding proper security measures, which also benefits the model by resulting in minimal loss. Therefore, in this work, for security hardening and strengthening of generated code from LLMs, we propose a reinforcement learning-based method for program-specific repair with the combination of semantic and syntactic reward mechanisms that focus heavily on adding security and functional measures in the code, respectively.},
 archiveprefix = {arXiv},
 author = {Nafis Tanveer Islam and Mohammad Bahrami Karkevandi and Peyman Najafirad},
 eprint = {2401.07031},
 primaryclass = {cs.CR},
 title = {Code Security Vulnerability Repair Using Reinforcement Learning with Large Language Models},
 url = {https://arxiv.org/abs/2401.07031},
 year = {2024}
}

@misc{zhou2024multillmcollaborationdatacentric,
 abstract = {The advances of deep learning (DL) have paved the way for automatic software vulnerability repair approaches, which effectively learn the mapping from the vulnerable code to the fixed code. Nevertheless, existing DL-based vulnerability repair methods face notable limitations: 1) they struggle to handle lengthy vulnerable code, 2) they treat code as natural language texts, neglecting its inherent structure, and 3) they do not tap into the valuable expert knowledge present in the expert system.
To address this, we propose VulMaster, a Transformer-based neural network model that excels at generating vulnerability repairs through data-centric innovation. Specifically, VulMaster introduces the utilization and combination of various types of input data, including complete vulnerable code of any size, vulnerable code structures, and expert knowledge from the CWE system. Additionally, VulMaster leverages the collaboration between two Large Language Models (LLMs), CodeT5 and ChatGPT: CodeT5 acts as the customizable backbone LLM, fine-tuned with the training data, while ChatGPT supplements by providing missing relevant inputs to CodeT5. We evaluated VulMaster on a real-world C/C++ vulnerability repair dataset comprising 1,754 projects with 5,800 vulnerable functions. The experimental results demonstrated that VulMaster exhibits substantial improvements compared to the learning-based state-of-the-art vulnerability repair approach. Specifically, VulMaster improves the EM, BLEU, and CodeBLEU scores from 10.2\% to 20.0\%, 21.3\% to 29.3\%, and 32.5\% to 40.9\%, respectively.},
 archiveprefix = {arXiv},
 author = {Xin Zhou and Kisub Kim and Bowen Xu and DongGyun Han and David Lo},
 eprint = {2401.15459},
 primaryclass = {cs.SE},
 title = {Multi-LLM Collaboration + Data-Centric Innovation = 2x Better Vulnerability Repair},
 url = {https://arxiv.org/abs/2401.15459},
 year = {2024}
}

@misc{berabi2024deepcodeaifixfixing,
 abstract = {The automated program repair field has attracted substantial interest over the years, but despite significant research efforts, creating a system that works well for complex semantic bugs such as security vulnerabilities has proven difficult. A promising direction to solve this challenge is by leveraging large language models (LLMs), which are increasingly used to solve various programming tasks. In this paper, we investigate the effectiveness of LLMs for solving code-repair task. We show that the task is difficult as it requires the model to learn long-range code relationships, a task that inherently relies on extensive amounts of training data. At the same time, creating a large, clean dataset for complex program bugs and their corresponding fixes is non-trivial. We propose a technique to address these challenges with a new approach for querying and fine-tuning LLMs. The idea is to use program analysis to limit the LLM's attention mechanism on the portions of code needed to perform the fix, drastically reducing the amount of required training data. Concretely, for training and inference, rather than feeding the entire program to the LLM, we reduce its code to a much shorter snippet that contains the reported defect together with the necessary context - and use that instead. Our evaluation shows that this code reduction approach substantially improves available models such as GPT-4 using few-shot learning, as well as fine-tuning models. To train and evaluate our system, we created a comprehensive code fixing dataset by extensively labeling 156 bug patterns (including 40 security rules), requiring complex interprocedural dataflow to discover. Our best system with Mixtral-8x7B can remove more than 80% of the reported defects while exactly matching the human fix in between 10 and 50% of cases, outperforming baselines based on GPT-3.5 and GPT-4, or based on window-based models like TFix.},
 archiveprefix = {arXiv},
 author = {Berkay Berabi and Alexey Gronskiy and Veselin Raychev and Gishor Sivanrupan and Victor Chibotaru and Martin Vechev},
 eprint = {2402.13291},
 primaryclass = {cs.CR},
 title = {DeepCode AI Fix: Fixing Security Vulnerabilities with Large Language Models},
 url = {https://arxiv.org/abs/2402.13291},
 year = {2024}
}

@misc{liu2024llmcompdroidrepairingconfigurationcompatibility,
 abstract = {XML configurations are integral to the Android development framework, particularly in the realm of UI display. However, these configurations can introduce compatibility issues (bugs), resulting in divergent visual outcomes and system crashes across various Android API versions (levels). In this study, we systematically investigate LLM-based approaches for detecting and repairing configuration compatibility bugs. Our findings highlight certain limitations of LLMs in effectively identifying and resolving these bugs, while also revealing their potential in addressing complex, hard-to-repair issues that traditional tools struggle with. Leveraging these insights, we introduce the LLM-CompDroid framework, which combines the strengths of LLMs and traditional tools for bug resolution. Our experimental results demonstrate a significant enhancement in bug resolution performance by LLM-CompDroid, with LLM-CompDroid-GPT-3.5 and LLM-CompDroid-GPT-4 surpassing the state-of-the-art tool, ConfFix, by at least 9.8% and 10.4% in both Correct and Correct@k metrics, respectively. This innovative approach holds promise for advancing the reliability and robustness of Android applications, making a valuable contribution to the field of software development.},
 archiveprefix = {arXiv},
 author = {Zhijie Liu and Yutian Tang and Meiyun Li and Xin Jin and Yunfei Long and Liang Feng Zhang and Xiapu Luo},
 eprint = {2402.15078},
 primaryclass = {cs.SE},
 title = {LLM-CompDroid: Repairing Configuration Compatibility Bugs in Android Apps with Pre-trained Large Language Models},
 url = {https://arxiv.org/abs/2402.15078},
 year = {2024}
}

@misc{nong2024chainofthoughtpromptinglargelanguage,
 abstract = {Security vulnerabilities are increasingly prevalent in modern software and they are widely consequential to our society. Various approaches to defending against these vulnerabilities have been proposed, among which those leveraging deep learning (DL) avoid major barriers with other techniques hence attracting more attention in recent years. However, DL-based approaches face critical challenges including the lack of sizable and quality-labeled task-specific datasets and their inability to generalize well to unseen, real-world scenarios. Lately, large language models (LLMs) have demonstrated impressive potential in various domains by overcoming those challenges, especially through chain-of-thought (CoT) prompting. In this paper, we explore how to leverage LLMs and CoT to address three key software vulnerability analysis tasks: identifying a given type of vulnerabilities, discovering vulnerabilities of any type, and patching detected vulnerabilities. We instantiate the general CoT methodology in the context of these tasks through VSP , our unified, vulnerability-semantics-guided prompting approach, and conduct extensive experiments assessing VSP versus five baselines for the three tasks against three LLMs and two datasets. Results show substantial superiority of our CoT-inspired prompting (553.3%, 36.5%, and 30.8% higher F1 accuracy for vulnerability identification, discovery, and patching, respectively, on CVE datasets) over the baselines. Through in-depth case studies analyzing VSP failures, we also reveal current gaps in LLM/CoT for challenging vulnerability cases, while proposing and validating respective improvements.},
 archiveprefix = {arXiv},
 author = {Yu Nong and Mohammed Aldeen and Long Cheng and Hongxin Hu and Feng Chen and Haipeng Cai},
 eprint = {2402.17230},
 primaryclass = {cs.CR},
 title = {Chain-of-Thought Prompting of Large Language Models for Discovering and Fixing Software Vulnerabilities},
 url = {https://arxiv.org/abs/2402.17230},
 year = {2024}
}

@misc{zhang2025acfixguidingllmsmined,
 abstract = {Smart contracts are susceptible to various security issues, among which access control (AC) vulnerabilities are particularly critical. While existing research has proposed multiple detection tools, the automatic and appropriate repair of AC vulnerabilities in smart contracts remains a challenge. Unlike commonly supported vulnerability types by existing repair tools, such as reentrancy, which are usually fixed by template-based approaches, the main obstacle of AC lies in identifying the appropriate roles or permissions amid a long list of non-AC-related source code to generate proper patch code, a task that demands human-level intelligence.
Leveraging recent advancements in large language models (LLMs), we employ the state-of-the-art GPT-4 model and enhance it with a novel approach called ACFIX. The key insight is that we can mine common AC practices for major categories of code functionality and use them to guide LLMs in fixing code with similar functionality. To this end, ACFIX involves both offline and online phases. First, during the offline phase, ACFIX mines a taxonomy of common Role-based Access Control (RBAC) practices from 344,251 on-chain contracts, categorizing 49 role-permission pairs from the top 1,000 pairs mined. Second, during the online phase, ACFIX tracks AC-related elements across the contract and uses this context information along with a Chain-of-Thought pipeline to guide LLMs in identifying the most appropriate role-permission pair for the subject contract and subsequently generating a suitable patch. This patch will then undergo a validity and effectiveness check. To evaluate ACFIX, we built the first benchmark dataset of 118 real-world AC vulnerabilities, and our evaluation revealed that ACFIX successfully repaired 94.92% of them. This represents a significant improvement compared to the baseline GPT-4, which achieved only 52.54%.},
 archiveprefix = {arXiv},
 author = {Lyuye Zhang and Kaixuan Li and Kairan Sun and Daoyuan Wu and Ye Liu and Haoye Tian and Yang Liu},
 eprint = {2403.06838},
 primaryclass = {cs.SE},
 title = {ACFIX: Guiding LLMs with Mined Common RBAC Practices for Context-Aware Repair of Access Control Vulnerabilities in Smart Contracts},
 url = {https://arxiv.org/abs/2403.06838},
 year = {2025}
}

@inproceedings{Le_2024,
 abstract = {In recent years, JavaScript has become the most widely used programming language, especially in web development. However, writing secure JavaScript code is not trivial, and programmers often make mistakes that lead to security vulnerabilities in web applications. Large Language Models (LLMs) have demonstrated substantial advancements across multiple domains, and their evolving capabilities indicate their potential for automatic code generation based on a required specification, including automatic bug fixing. In this study, we explore the accuracy of LLMs, namely ChatGPT and Bard, in finding and fixing security vulnerabilities in JavaScript programs. We also investigate the impact of context in a prompt on directing LLMs to produce a correct patch of vulnerable JavaScript code. Our experiments on real-world software vulnerabilities show that while LLMs are promising in automatic program repair of JavaScript code, achieving a correct bug fix often requires an appropriate amount of context in the prompt.},
 author = {Le, Tan Khang and Alimadadi, Saba and Ko, Steven Y.},
 booktitle = {Companion Proceedings of the ACM Web Conference 2024},
 collection = {WWW ’24},
 doi = {10.1145/3589335.3651463},
 month = {May},
 pages = {666–669},
 publisher = {ACM},
 series = {WWW ’24},
 title = {A Study of Vulnerability Repair in JavaScript Programs with Large Language Models},
 url = {http://dx.doi.org/10.1145/3589335.3651463},
 year = {2024}
}

@misc{zhang2025evaluatinglargelanguagemodels,
 abstract = {Recently, Automated Vulnerability Localization (AVL) has attracted growing attention, aiming to facilitate diagnosis by pinpointing the specific lines of code responsible for vulnerabilities. Large Language Models (LLMs) have shown potential in various domains, yet their effectiveness in line-level vulnerability localization remains underexplored.
In this work, we present the first comprehensive empirical evaluation of LLMs for AVL. Our study examines 19 leading LLMs suitable for code analysis, including ChatGPT and multiple open-source models, spanning encoder-only, encoder-decoder, and decoder-only architectures, with model sizes from 60M to 70B parameters. We evaluate three paradigms including few-shot prompting, discriminative fine-tuning, and generative fine-tuning with and without Low-Rank Adaptation (LoRA), on both a BigVul-derived dataset for C/C++ and a smart contract vulnerability dataset.}
Our results show that discriminative fine-tuning achieves substantial performance gains over existing learning-based AVL methods when sufficient training data is available. In low-data settings, prompting advanced LLMs such as ChatGPT proves more effective. We also identify challenges related to input length and unidirectional context during fine-tuning, and propose two remedial strategies: a sliding window approach and right-forward embedding, both of which yield significant improvements. Moreover, we provide the first assessment of LLM generalizability in AVL, showing that certain models can transfer effectively across Common Weakness Enumerations (CWEs) and projects. However, performance degrades notably for newly discovered vulnerabilities containing unfamiliar lexical or structural patterns, underscoring the need for continual adaptation.},
 archiveprefix = {arXiv},
 author = {Jian Zhang and Chong Wang and Anran Li and Weisong Sun and Cen Zhang and Wei Ma and Yang Liu},
 eprint = {2404.00287},
 primaryclass = {cs.SE},
 title = {Evaluating Large Language Models for Line-Level Vulnerability Localization},
 url = {https://arxiv.org/abs/2404.00287},
 year = {2025}
}

@misc{zhou2024largelanguagemodelvulnerability,
 abstract = {The significant advancements in Large Language Models (LLMs) have resulted in their widespread adoption across various tasks within Software Engineering (SE), including vulnerability detection and repair. Numerous studies have investigated the application of LLMs to enhance vulnerability detection and repair tasks. Despite the increasing research interest, there is currently no existing survey that focuses on the utilization of LLMs for vulnerability detection and repair. In this paper, we aim to bridge this gap by offering a systematic literature review of approaches aimed at improving vulnerability detection and repair through the utilization of LLMs. The review encompasses research work from leading SE, AI, and Security conferences and journals, encompassing 43 papers published across 25 distinct venues, along with 15 high-quality preprint papers, bringing the total to 58 papers. By answering three key research questions, we aim to (1) summarize the LLMs employed in the relevant literature, (2) categorize various LLM adaptation techniques in vulnerability detection, and (3) classify various LLM adaptation techniques in vulnerability repair. Based on our findings, we have identified a series of limitations of existing studies. Additionally, we have outlined a roadmap highlighting potential opportunities that we believe are pertinent and crucial for future research endeavors.},
 archiveprefix = {arXiv},
 author = {Xin Zhou and Sicong Cao and Xiaobing Sun and David Lo},
 eprint = {2404.02525},
 primaryclass = {cs.SE},
 title = {Large Language Model for Vulnerability Detection and Repair: Literature Review and the Road Ahead},
 url = {https://arxiv.org/abs/2404.02525},
 year = {2024}
}

@misc{hossain2024deepdivelargelanguage,
 abstract = {Large language models (LLMs) have shown impressive effectiveness in various software engineering tasks, including automated program repair (APR). In this study, we take a deep dive into automated bug fixing utilizing LLMs. In contrast to many deep learning-based APR methods that assume known bug locations, rely on line-level localization tools, or address bug prediction and fixing in one step, our approach uniquely employs LLMs to predict bug location at the token level and subsequently utilizes them for bug fixing. This methodological separation of bug localization and fixing using different LLMs enables effective integration of diverse contextual information and improved incorporation of inductive biases. We introduce Toggle: Token-Granulated Bug Localization and Repair, a comprehensive program repair framework that integrates a bug localization model, an adjustment unit, and a bug-fixing model. Toggle takes a buggy function as input and generates a complete corrected function. We investigate various styles of prompting to the bug fixing model to identify the most effective prompts that better utilize the inductive bias and significantly outperform others. Toggle achieves the new state-of-the-art (SOTA) performance on the CodeXGLUE code refinement benchmark, and exhibits better and comparable performance on several other widely-used APR datasets, including Defects4J.},
 archiveprefix = {arXiv},
 author = {Soneya Binta Hossain and Nan Jiang and Qiang Zhou and Xiaopeng Li and Wen-Hao Chiang and Yingjun Lyu and Hoan Nguyen and Omer Tripp},
 eprint = {2404.11595},
 primaryclass = {cs.SE},
 title = {A Deep Dive into Large Language Models for Automated Bug Localization and Repair},
 url = {https://arxiv.org/abs/2404.11595},
 year = {2024}
}

@article{Wang_2025,
 abstract = {Purpose: In the field of vulnerability repair, previous research has leveraged pretrained models and LLM-based prompt engineering, among which LLM-based approaches show better generalizability and achieve the best performance. However, the LLM-based approaches generally regard vulnerability repair as a sequence-to-sequence task, and do not explicitly capture the syntax patterns for different vulnerability types, leading to limited accuracy. We aim to create a method that ensures the specificity of prompts targeting vulnerable code while also leveraging the generative capabilities of Large Language Models. Methods: We propose SPVR (Syntax-to-Prompt Vulnerability Repair), a novel framework that collects information from syntax trees, and generates corresponding prompts. Our method consists of three steps: rule design, prompt generation, and patch generation. In the rule design step, our method parses code patches and designs rules to extract relevant contextual information. These rules aid in identifying vulnerability-related issues. In the prompt generation step, our method extracts information from vulnerable code with pre-defined rules, automatically converting them into prompts. We also incorporate the description of CWE (Common Weakness Enumeration) as known information into the prompts. Finally, in the patch generation step, this prompt will serve as input to any conversational LLM to obtain code patches. Results: Extensive experiments validate that our method achieves excellent results in assisting LLMs to fix vulnerabilities accurately. We utilize multiple Large Language Models to validate the effectiveness of our work, repairing 143 of 547 vulnerable code using ChatGPT-4. We conducted a comparison of our approach against several existing vulnerability repair approaches (including fine-tuning-based and prompt-based), across multiple metrics.},
 author = {Wang, Ruoke and Li, Zongjie and Gao, Cuiyun and Wang, Chaozheng and Xiao, Yang and Wang, Xuan},
 doi = {10.1007/s10515-025-00579-5},
 issn = {1573-7535},
 journal = {Automated Software Engineering},
 month = {December},
 number = {1},
 publisher = {Springer Science and Business Media LLC},
 title = {SPVR: syntax-to-prompt vulnerability repair based on large language models},
 url = {http://dx.doi.org/10.1007/s10515-025-00579-5},
 volume = {33},
 year = {2025}
}

@misc{kulsum2024casestudyllmautomated,
 abstract = {Recent work in automated program repair (APR) proposes the use of reasoning and patch validation feedback to reduce the semantic gap between the LLMs and the code under analysis. The idea has been shown to perform well for general APR, but its effectiveness in other particular contexts remains underexplored. In this work, we assess the impact of reasoning and patch validation feedback to LLMs in the context of vulnerability repair, an important and challenging task in security. To support the evaluation, we present VRpilot, an LLM-based vulnerability repair technique based on reasoning and patch validation feedback. VRpilot (1) uses a chain-of-thought prompt to reason about a vulnerability prior to generating patch candidates and (2) iteratively refines prompts according to the output of external tools (e.g., compiler, code sanitizers, test suite, etc.) on previously-generated patches. To evaluate performance, we compare VRpilot against the state-of-the-art vulnerability repair techniques for C and Java using public datasets from the literature. Our results show that VRpilot generates, on average, 14% and 7.6% more correct patches than the baseline techniques on C and Java, respectively. We show, through an ablation study, that reasoning and patch validation feedback are critical. We report several lessons from this study and potential directions for advancing LLM-empowered vulnerability repair},
 archiveprefix = {arXiv},
 author = {Ummay Kulsum and Haotian Zhu and Bowen Xu and Marcelo d'Amorim},
 eprint = {2405.15690},
 primaryclass = {cs.SE},
 title = {A Case Study of LLM for Automated Vulnerability Repair: Assessing Impact of Reasoning and Patch Validation Feedback},
 url = {https://arxiv.org/abs/2405.15690},
 year = {2024}
}

@misc{mündler2025swtbenchtestingvalidatingrealworld,
 abstract = {Rigorous software testing is crucial for developing and maintaining high-quality code, making automated test generation a promising avenue for both improving software quality and boosting the effectiveness of code generation methods. However, while code generation with Large Language Models (LLMs) is an extraordinarily active research area, test generation remains relatively unexplored. We address this gap and investigate the capability of LLM-based Code Agents to formalize user issues into test cases. To this end, we propose a novel benchmark based on popular GitHub repositories, containing real-world issues, ground-truth bug-fixes, and golden tests. We find that LLMs generally perform surprisingly well at generating relevant test cases, with Code Agents designed for code repair exceeding the performance of systems designed specifically for test generation. Further, as test generation is a similar but more structured task than code generation, it allows for a more fine-grained analysis using issue reproduction rate and coverage changes, providing a dual metric for analyzing systems designed for code repair. Finally, we find that generated tests are an effective filter for proposed code fixes, doubling the precision of SWE-Agent. We release all data and code at this https URL},
 archiveprefix = {arXiv},
 author = {Niels Mündler and Mark Niklas Müller and Jingxuan He and Martin Vechev},
 eprint = {2406.12952},
 primaryclass = {cs.SE},
 title = {SWT-Bench: Testing and Validating Real-World Bug-Fixes with Code Agents},
 url = {https://arxiv.org/abs/2406.12952},
 year = {2025}
}

@misc{yan2024betterdebuggingcombiningstatic,
 abstract = {Nowadays, many applications do not exist independently but rely on various frameworks or libraries. The frequent evolution and the complex implementation of framework APIs induce many unexpected post-release crashes. Starting from the crash stack traces, existing approaches either perform direct call graph (CG) tracing or construct datasets with similar crash-fixing records to locate buggy methods. However, these approaches are limited by the completeness of CG or dependent on historical fixing records. Moreover, they fail to explain the buggy candidates by revealing their relationship with the crashing point.
To fill the gap, we propose an explainable crashing fault localization approach by combining static analysis and LLM techniques. Our primary insight is that understanding the semantics of exception-throwing statements in the framework code can help find and apprehend the buggy methods in the app code. Based on this idea, first, we design the exception-thrown summary (ETS) that describes the key elements related to each framework-specific exception and extract ETSs by performing static analysis. Then we make data-tracking of its key elements to identify and sort buggy candidates for the given crash. After that, we introduce LLMs to improve the explainability of the localization results. To construct effective LLM prompts, we design the candidate information summary (CIS) that describes multiple types of explanation-related contexts and then extract CISs via static analysis. We apply our approach to one typical scenario, i.e., locating Android framework-specific crashing faults, and implement a tool CrashTracker. For fault localization, it exhibited an overall MRR value of 0.91 in precision. For fault explanation, compared to the naive one produced by static analysis only, the LLM-powered explanation achieved a 67.04% improvement in users' satisfaction score.},
 archiveprefix = {arXiv},
 author = {Jiwei Yan and Jinhao Huang and Chunrong Fang and Jun Yan and Jian Zhang},
 eprint = {2408.12070},
 primaryclass = {cs.SE},
 title = {Better Debugging: Combining Static Analysis and LLMs for Explainable Crashing Fault Localization},
 url = {https://arxiv.org/abs/2408.12070},
 year = {2024}
}

@misc{liu2024marscodeagentainativeautomated,
 abstract = {Recent advances in large language models (LLMs) have shown significant potential to automate various software development tasks, including code completion, test generation, and bug fixing. However, the application of LLMs for automated bug fixing remains challenging due to the complexity and diversity of real-world software systems. In this paper, we introduce MarsCode Agent, a novel framework that leverages LLMs to automatically identify and repair bugs in software code. MarsCode Agent combines the power of LLMs with advanced code analysis techniques to accurately localize faults and generate patches. Our approach follows a systematic process of planning, bug reproduction, fault localization, candidate patch generation, and validation to ensure high-quality bug fixes. We evaluated MarsCode Agent on SWE-bench, a comprehensive benchmark of real-world software projects, and our results show that MarsCode Agent achieves a high success rate in bug fixing compared to most of the existing automated approaches.},
 archiveprefix = {arXiv},
 author = {Yizhou Liu and Pengfei Gao and Xinchen Wang and Jie Liu and Yexuan Shi and Zhao Zhang and Chao Peng},
 eprint = {2409.00899},
 primaryclass = {cs.SE},
 title = {MarsCode Agent: AI-native Automated Bug Fixing},
 url = {https://arxiv.org/abs/2409.00899},
 year = {2024}
}

@inproceedings{Wang_2024,
 abstract = {Smart contracts are susceptible to being exploited by attackers, especially when facing real-world vulnerabilities. To mitigate this risk, developers often rely on third-party audit services to identify potential vulnerabilities before project deployment. Nevertheless, repairing the identified vulnerabilities is still complex and labor-intensive, particularly for developers lacking security expertise. Moreover, existing pattern-based repair tools mostly fail to address real-world vulnerabilities due to their lack of high-level semantic understanding. To fill this gap, we propose ContractTinker, a Large Language Models (LLMs)-empowered tool for real-world vulnerability repair. The key insight is our adoption of the Chain-of-Thought approach to break down the entire generation task into sub-tasks. Additionally, to reduce hallucination, we integrate program static analysis to guide the LLM. We evaluate ContractTinker on 48 high-risk vulnerabilities. The experimental results show that among the patches generated by ContractTinker, 23 (48%) are valid patches that fix the vulnerabilities, while 10 (21%) require only minor modifications. A video of ContractTinker is available at this https URL.},
 author = {Wang, Che and Zhang, Jiashuo and Gao, Jianbo and Xia, Libin and Guan, Zhi and Chen, Zhong},
 booktitle = {Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering},
 collection = {ASE ’24},
 doi = {10.1145/3691620.3695349},
 month = {October},
 pages = {2350–2353},
 publisher = {ACM},
 series = {ASE ’24},
 title = {ContractTinker: LLM-Empowered Vulnerability Repair for Real-World Smart Contracts},
 url = {http://dx.doi.org/10.1145/3691620.3695349},
 year = {2024}
}

@misc{yao2024locationkeyleveraginglarge,
 abstract = {Bug localization in Verilog code is a crucial and time-consuming task during the verification of hardware design. Since introduction, Large Language Models (LLMs) have showed their strong programming capabilities. However, no work has yet considered using LLMs for bug localization in Verilog code. This paper presents Location-is-Key, an opensource LLM solution to locate functional errors in Verilog snippets. LiK achieves high localization accuracy, with a pass@1 localization accuracy of 93.3% on our test dataset based on RTLLM, surpassing GPT-4's 77.9% and comparable to Claude-3.5's 90.8%. Additionally, the bug location obtained by LiK significantly improves GPT-3.5's bug repair efficiency (Functional pass@1 increased from 40.39% to 58.92%), highlighting the importance of bug localization in LLM-based Verilog debugging. Compared to existing methods, LiK only requires the design specification and the erroneous code snippet, without the need for testbenches, assertions, or any other EDA tools. This research demonstrates the feasibility of using LLMs for Verilog error localization, thus providing a new direction for automatic Verilog code debugging.},
 archiveprefix = {arXiv},
 author = {Bingkun Yao and Ning Wang and Jie Zhou and Xi Wang and Hong Gao and Zhe Jiang and Nan Guan},
 eprint = {2409.15186},
 primaryclass = {cs.AR},
 title = {Location is Key: Leveraging Large Language Model for Functional Bug Localization in Verilog},
 url = {https://arxiv.org/abs/2409.15186},
 year = {2024}
}

@inproceedings{Khan_2025,
 abstract = {Large Language Models (LLMs) have shown significant challenges in detecting and repairing vulnerable code, particularly when dealing with vulnerabilities involving multiple aspects, such as variables, code flows, and code structures. In this study, we utilize GitHub Copilot as the LLM and focus on buffer overflow vulnerabilities. Our experiments reveal a notable gap in Copilot's abilities when dealing with buffer overflow vulnerabilities, with a 76% vulnerability detection rate but only a 15% vulnerability repair rate. To address this issue, we propose context-aware prompt tuning techniques designed to enhance LLM performance in repairing buffer overflow. By injecting a sequence of domain knowledge about the vulnerability, including various security and code contexts, we demonstrate that Copilot's successful repair rate increases to 63%, representing more than four times the improvement compared to repairs without domain knowledge.},
 author = {Khan, Arshiya and Liu, Guannan and Gao, Xing},
 booktitle = {2025 IEEE Security and Privacy Workshops (SPW)},
 doi = {10.1109/spw67851.2025.00040},
 month = {May},
 pages = {283–287},
 publisher = {IEEE},
 title = {Code Vulnerability Repair with Large Language Model Using Context-Aware Prompt Tuning},
 url = {http://dx.doi.org/10.1109/SPW67851.2025.00040},
 year = {2025}
}

@misc{li2025llmsjustlooksimple,
 abstract = {The rapid expansion of software systems and the growing number of reported vulnerabilities have emphasized the importance of accurately identifying vulnerable code segments. Traditional methods for vulnerability localization, such as manual code audits or rule-based tools, are often time-consuming and limited in scope, typically focusing on specific programming languages or types of vulnerabilities. In recent years, the introduction of large language models (LLMs) such as GPT and LLaMA has opened new possibilities for automating vulnerability detection. However, while LLMs show promise in this area, they face challenges, particularly in maintaining accuracy over longer code contexts. This paper introduces LOVA, a novel framework leveraging the self-attention mechanisms inherent in LLMs to enhance vulnerability localization. Our key insight is that self-attention mechanisms assign varying importance to different parts of the input, making it possible to track how much attention the model focuses on specific lines of code. In the context of vulnerability localization, the hypothesis is that vulnerable lines of code will naturally attract higher attention weights because they have a greater influence on the model's output. By systematically tracking changes in attention weights and focusing on specific lines of code, LOVA improves the precision of identifying vulnerable lines across various programming languages. Through rigorous experimentation and evaluation, we demonstrate that LOVA significantly outperforms existing LLM-based approaches, achieving up to a 5.3x improvement in F1-scores. LOVA also demonstrated strong scalability, with up to a 14.6x improvement in smart contract vulnerability localization across languages like C, Python, Java, and Solidity. Its robustness was proven through consistent performance across different LLM architectures.},
 archiveprefix = {arXiv},
 author = {Yue Li and Xiao Li and Hao Wu and Yue Zhang and Xiuzhen Cheng and Yating Liu and Fengyuan Xu and Sheng Zhong},
 eprint = {2410.15288},
 primaryclass = {cs.CR},
 title = {If LLMs Would Just Look: Simple Line-by-line Checking Improves Vulnerability Localization},
 url = {https://arxiv.org/abs/2410.15288},
 year = {2025}
}

@misc{duan2024pdcdmsftroad,
 abstract = {Code Large Language Models (Code LLMs), such as Code llama and DeepSeek-Coder, have demonstrated exceptional performance in the code generation tasks. However, most existing models focus on the abilities of generating correct code, but often struggle with bug repair. We introduce a suit of methods to enhance LLM's SQL bug-fixing abilities. The methods are mainly consisted of two parts: A Progressive Dataset Construction (PDC) from scratch and Dynamic Mask Supervised Fine-tuning (DM-SFT). PDC proposes two data expansion methods from the perspectives of breadth first and depth first respectively. DM-SFT introduces an efficient bug-fixing supervised learning approach, which effectively reduce the total training steps and mitigate the "disorientation" in SQL code bug-fixing training. In our evaluation, the code LLM models trained with two methods have exceeds all current best performing model which size is much larger.},
 archiveprefix = {arXiv},
 author = {Yiwen Duan and Yonghong Yu and Xiaoming Zhao and Yichang Wu and Wenbo Liu},
 eprint = {2411.06767},
 primaryclass = {cs.CL},
 title = {PDC & DM-SFT: A Road for LLM SQL Bug-Fix Enhancing},
 url = {https://arxiv.org/abs/2411.06767},
 year = {2024}
}

@misc{meng2025empiricalstudyllmbasedagents,
 abstract = {Large language models (LLMs) and LLM-based Agents have been applied to fix bugs automatically, demonstrating the capability in addressing software defects by engaging in development environment interaction, iterative validation and code modification. However, systematic analysis of these agent systems remain limited, particularly regarding performance variations among top-performing ones. In this paper, we examine six repair systems on the SWE-bench Verified benchmark for automated bug fixing. We first assess each system's overall performance, noting the instances solvable by all or none of these systems, and explore the capabilities of different systems. We also compare fault localization accuracy at file and code symbol levels and evaluate bug reproduction capabilities. Through analysis, we concluded that further optimization is needed in both the LLM capability itself and the design of Agentic flow to improve the effectiveness of the Agent in bug fixing.},
 archiveprefix = {arXiv},
 author = {Xiangxin Meng and Zexiong Ma and Pengfei Gao and Chao Peng},
 eprint = {2411.10213},
 primaryclass = {cs.SE},
 title = {An Empirical Study on LLM-based Agents for Automated Bug Fixing},
 url = {https://arxiv.org/abs/2411.10213},
 year = {2025}
}

@misc{feng2025integratingvarioussoftwareartifacts,
 abstract = {LLMs have garnered considerable attention for their potential to streamline Automated Program Repair (APR). LLM-based approaches can either insert the correct code or directly generate patches when provided with buggy methods. However, most of LLM-based APR methods rely on a single type of software information, without fully leveraging different software artifacts. Despite this, many LLM-based approaches do not explore which specific types of information best assist in APR. Addressing this gap is crucial for advancing LLM-based APR techniques. We propose DEVLoRe to use issue content (description and message) and stack error traces to localize buggy methods, then rely on debug information in buggy methods and issue content and stack error to localize buggy lines and generate plausible patches which can pass all unit tests. The results show that while issue content is particularly effective in assisting LLMs with fault localization and program repair, different types of software artifacts complement each other. By incorporating different artifacts, DEVLoRe successfully locates 49.3% and 47.6% of single and non-single buggy methods and generates 56.0% and 14.5% plausible patches for the Defects4J v2.0 dataset, respectively. This outperforms current state-of-the-art APR methods. Furthermore, we re-implemented and evaluated our framework, demonstrating its effectiveness in its effectiveness in resolving 9 unique issues compared to other state-of-the-art frameworks using the same or more advanced models on SWE-bench this http URL also discussed whether a leading framework for Python code can be directly applied to Java code, or vice versa. The source code and experimental results of this work for replication are available at this https URL.},
 archiveprefix = {arXiv},
 author = {Qiong Feng and Xiaotian Ma and Jiayi Sheng and Ziyuan Feng and Wei Song and Peng Liang},
 eprint = {2412.03905},
 primaryclass = {cs.SE},
 title = {Integrating Various Software Artifacts for Better LLM-based Bug Localization and Program Repair},
 url = {https://arxiv.org/abs/2412.03905},
 year = {2025}
}

@misc{fakih2025llm4cveenablingiterativeautomated,
 abstract = {Software vulnerabilities continue to be ubiquitous, even in the era of AI-powered code assistants, advanced static analysis tools, and the adoption of extensive testing frameworks. It has become apparent that we must not simply prevent these bugs, but also eliminate them in a quick, efficient manner. Yet, human code intervention is slow, costly, and can often lead to further security vulnerabilities, especially in legacy codebases. The advent of highly advanced Large Language Models (LLM) has opened up the possibility for many software defects to be patched automatically. We propose LLM4CVE an LLM-based iterative pipeline that robustly fixes vulnerable functions in real-world code with high accuracy. We examine our pipeline with State-of-the-Art LLMs, such as GPT-3.5, GPT-4o, Llama 38B, and Llama 3 70B. We achieve a human-verified quality score of 8.51/10 and an increase in groundtruth code similarity of 20% with Llama 3 70B. To promote further research in the area of LLM-based vulnerability repair, we publish our testing apparatus, fine-tuned weights, and experimental data on our website},
 archiveprefix = {arXiv},
 author = {Mohamad Fakih and Rahul Dharmaji and Halima Bouzidi and Gustavo Quiros Araya and Oluwatosin Ogundare and Mohammad Abdullah Al Faruque},
 eprint = {2501.03446},
 primaryclass = {cs.SE},
 title = {LLM4CVE: Enabling Iterative Automated Vulnerability Repair with Large Language Models},
 url = {https://arxiv.org/abs/2501.03446},
 year = {2025}
}

@misc{shi2026hafixhistoryaugmentedlargelanguage,
 abstract = {Recent studies have explored the performance of Large Language Models (LLMs) on various Software Engineering (SE) tasks, such as code generation and bug fixing. However, these approaches typically rely on the context data from the current snapshot of the project, overlooking the potential of rich historical data residing in real-world software repositories. Additionally, the impact of prompt styles on LLM performance for SE tasks within a historical context remains underexplored. To address these gaps, we propose HAFix, which stands for History-Augmented LLMs on Bug Fixing, a novel approach that leverages seven individual historical heuristics associated with bugs and aggregates the results of these heuristics (HAFix-Agg) to enhance LLMs' bug-fixing capabilities. To empirically evaluate HAFix, we employ three Code LLMs (i.e., Code Llama, DeepSeek-Coder and DeepSeek-Coder-V2-Lite models) on 51 single-line Python bugs from BugsInPy and 116 single-line Java bugs from Defects4J. Our evaluation demonstrates that multiple HAFix heuristics achieve statistically significant improvements compared to a non-historical baseline inspired by GitHub Copilot. Furthermore, the aggregated HAFix variant HAFix-Agg achieves substantial improvements by combining the complementary strengths of individual heuristics, increasing bug-fixing rates by an average of 45.05% on BugsInPy and 49.92% on Defects4J relative to the corresponding baseline. Moreover, within the context of historical heuristics, we identify the Instruction prompt style as the most effective template compared to the InstructionLabel and InstructionMask for LLMs in bug fixing. Finally, we evaluate the cost of HAFix in terms of inference time and token usage, and provide a pragmatic trade-off analysis of the cost and bug-fixing performance, offering valuable insights for the practical deployment of our approach in real-world scenarios.},
 archiveprefix = {arXiv},
 author = {Yu Shi and Abdul Ali Bangash and Emad Fallahzadeh and Bram Adams and Ahmed E. Hassan},
 eprint = {2501.09135},
 primaryclass = {cs.SE},
 title = {HAFix: History-Augmented Large Language Models for Bug Fixing},
 url = {https://arxiv.org/abs/2501.09135},
 year = {2026}
}

@misc{yang2025codechangeintentiondevelopment,
 abstract = {Detecting vulnerability fix commits in open-source software is crucial for maintaining software security. To help OSS identify vulnerability fix commits, several automated approaches are developed. However, existing approaches like VulFixMiner and CoLeFunDa, focus solely on code changes, neglecting essential context from development artifacts. Tools like Vulcurator, which integrates issue reports, fail to leverage semantic associations between different development artifacts (e.g., pull requests and history vulnerability fixes). Moreover, they miss vulnerability fixes in tangled commits and lack explanations, limiting practical use. Hence to address those limitations, we propose LLM4VFD, a novel framework that leverages Large Language Models (LLMs) enhanced with Chain-of-Thought reasoning and In-Context Learning to improve the accuracy of vulnerability fix detection. LLM4VFD comprises three components: (1) Code Change Intention, which analyzes commit summaries, purposes, and implications using Chain-of-Thought reasoning; (2) Development Artifact, which incorporates context from related issue reports and pull requests; (3) Historical Vulnerability, which retrieves similar past vulnerability fixes to enrich context. More importantly, on top of the prediction, LLM4VFD also provides a detailed analysis and explanation to help security experts understand the rationale behind the decision. We evaluated LLM4VFD against state-of-the-art techniques, including Pre-trained Language Model-based approaches and vanilla LLMs, using a newly collected dataset, BigVulFixes. Experimental results demonstrate that LLM4VFD significantly outperforms the best-performed existing approach by 68.1%--145.4%. Furthermore, We conducted a user study with security experts, showing that the analysis generated by LLM4VFD improves the efficiency of vulnerability fix identification.},
 archiveprefix = {arXiv},
 author = {Xu Yang and Wenhan Zhu and Michael Pacheco and Jiayuan Zhou and Shaowei Wang and Xing Hu and Kui Liu},
 eprint = {2501.14983},
 primaryclass = {cs.SE},
 title = {Code Change Intention, Development Artifact and History Vulnerability: Putting Them Together for Vulnerability Fix Detection by LLM},
 url = {https://arxiv.org/abs/2501.14983},
 year = {2025}
}

@misc{zhang2025patchempoweringlargelanguage,
 abstract = {Bug fixing holds significant importance in software development and maintenance. Recent research has made substantial strides in exploring the potential of large language models (LLMs) for automatically resolving software bugs. However, a noticeable gap in existing approaches lies in the oversight of collaborative facets intrinsic to bug resolution, treating the process as a single-stage endeavor. Moreover, most approaches solely take the buggy code snippet as input for LLMs during the patch generation stage. To mitigate the aforementioned limitations, we introduce a novel stage-wise framework named PATCH. Specifically, we first augment the buggy code snippet with corresponding dependence context and intent information to better guide LLMs in generating the correct candidate patches. Additionally, by taking inspiration from bug management practices, we decompose the bug-fixing task into four distinct stages: bug reporting, bug diagnosis, patch generation, and patch verification. These stages are performed interactively by LLMs, aiming to simulate the collaborative behavior of programmers during the resolution of software bugs. By harnessing these collective contributions, PATCH effectively enhances the bug-fixing capability of LLMs. We implement PATCH by employing the powerful dialogue-based LLM ChatGPT. Our evaluation on the widely used bug-fixing benchmark BFP demonstrates that PATCH has achieved better performance than state-of-the-art LLMs.},
 archiveprefix = {arXiv},
 author = {Yuwei Zhang and Zhi Jin and Ying Xing and Ge Li and Fang Liu and Jiaxin Zhu and Wensheng Dou and Jun Wei},
 eprint = {2501.16149},
 primaryclass = {cs.SE},
 title = {PATCH: Empowering Large Language Model with Programmer-Intent Guidance and Collaborative-Behavior Simulation for Automatic Bug Fixing},
 url = {https://arxiv.org/abs/2501.16149},
 year = {2025}
}

@misc{cheng2025agenticbugreproductioneffective,
 abstract = {Bug reports often lack sufficient detail for developers to reproduce and fix the underlying defects. Bug Reproduction Tests (BRTs), tests that fail when the bug is present and pass when it has been resolved, are crucial for debugging, but they are rarely included in bug reports, both in open-source and in industrial settings. Thus, automatically generating BRTs from bug reports has the potential to accelerate the debugging process and lower time to repair. This paper investigates automated BRT generation within an industry setting, specifically at Google, focusing on the challenges of a large-scale, proprietary codebase and considering real-world industry bugs extracted from Google's internal issue tracker. We adapt and evaluate a state-of-the-art BRT generation technique, LIBRO, and present our agent-based approach, BRT Agent, which makes use of a fine-tuned Large Language Model (LLM) for code editing. Our BRT Agent significantly outperforms LIBRO, achieving a 28% plausible BRT generation rate, compared to 10% by LIBRO, on 80 human-reported bugs from Google's internal issue tracker. We further investigate the practical value of generated BRTs by integrating them with an Automated Program Repair (APR) system at Google. Our results show that providing BRTs to the APR system results in 30% more bugs with plausible fixes. Additionally, we introduce Ensemble Pass Rate (EPR), a metric which leverages the generated BRTs to select the most promising fixes from all fixes generated by APR system. Our evaluation on EPR for Top-K and threshold-based fix selections demonstrates promising results and trade-offs. For example, EPR correctly selects a plausible fix from a pool of 20 candidates in 70% of cases, based on its top-1 ranking.},
 archiveprefix = {arXiv},
 author = {Runxiang Cheng and Michele Tufano and Jürgen Cito and José Cambronero and Pat Rondon and Renyao Wei and Aaron Sun and Satish Chandra},
 eprint = {2502.01821},
 primaryclass = {cs.SE},
 title = {Agentic Bug Reproduction for Effective Automated Program Repair at Google},
 url = {https://arxiv.org/abs/2502.01821},
 year = {2025}
}

@article{Sovrano_2025,
 abstract = {Recent advancements in artificial intelligence have enabled processing of larger inputs, leading everyday software developers to increasingly rely on chat-based large language models (LLMs) like GPT-3.5 and GPT-4 to detect vulnerabilities across entire files, not just within functions. This new development practice requires researchers to urgently investigate whether commonly used LLMs can effectively analyze large file-sized inputs, in order to provide timely insights for software developers and engineers about the pros and cons of this emerging technological trend. Hence, the goal of this paper is to evaluate the effectiveness of several state-of-the-art chat-based LLMs, including the GPT models, in detecting in-file vulnerabilities. We conducted a costly investigation into how the performance of LLMs varies based on vulnerability type, input size, and vulnerability location within the file. To give enough statistical power to our study, we could only focus on the three most common (as well as dangerous) vulnerabilities: XSS, SQL injection, and path traversal. Our findings indicate that the effectiveness of LLMs in detecting these vulnerabilities is strongly influenced by both the location of the vulnerability and the overall size of the input. Specifically, regardless of the vulnerability type, LLMs tend to significantly (p < .05) underperform when detecting vulnerabilities located toward the end of larger files, a pattern we call the 'lost-in-the-end' effect. Finally, to further support software developers and practitioners, we also explored the optimal input size for these LLMs and presented a simple strategy for identifying it, which can be applied to other models and vulnerability types. Eventually, we show how adjusting the input size can lead to significant improvements in LLM-based vulnerability detection, with an average recall increase of over 37% across all models.},
 author = {Sovrano, Francesco and Bauer, Adam and Bacchelli, Alberto},
 doi = {10.1145/3715758},
 issn = {2994-970X},
 journal = {Proceedings of the ACM on Software Engineering},
 month = {June},
 number = {FSE},
 pages = {891–913},
 publisher = {Association for Computing Machinery (ACM)},
 title = {Large Language Models for In-File Vulnerability Localization Can Be “Lost in the End”},
 url = {http://dx.doi.org/10.1145/3715758},
 volume = {2},
 year = {2025}
}

@misc{monisha2025empiricalevaluationllmspredicting,
 abstract = {This empirical study evaluates the effectiveness of Large Language Models (LLMs) in predicting fixes for configuration bugs in smart home systems. The research analyzes three prominent LLMs - GPT-4, GPT-4o (GPT-4 Turbo), and Claude 3.5 Sonnet - using four distinct prompt designs to assess their ability to identify appropriate fix strategies and generate correct solutions. The study utilized a dataset of 129 debugging issues from the Home Assistant Community, focusing on 21 randomly selected cases for in-depth analysis. Results demonstrate that GPT-4 and Claude 3.5 Sonnet achieved 80\% accuracy in strategy prediction when provided with both bug descriptions and original scripts. GPT-4 exhibited consistent performance across different prompt types, while GPT-4o showed advantages in speed and cost-effectiveness despite slightly lower accuracy. The findings reveal that prompt design significantly impacts model performance, with comprehensive prompts containing both description and original script yielding the best results. This research provides valuable insights for improving automated bug fixing in smart home system configurations and demonstrates the potential of LLMs in addressing configuration-related challenges.},
 archiveprefix = {arXiv},
 author = {Sheikh Moonwara Anjum Monisha and Atul Bharadwaj},
 eprint = {2502.10953},
 primaryclass = {cs.SE},
 title = {Empirical evaluation of LLMs in predicting fixes of Configuration bugs in Smart Home System},
 url = {https://arxiv.org/abs/2502.10953},
 year = {2025}
}

@misc{chang2025bridgingbuglocalizationissue,
 abstract = {Automated issue fixing is a critical task in software debugging and has recently garnered significant attention from academia and industry. However, existing fixing techniques predominantly focus on the repair phase, often overlooking the importance of improving the preceding bug localization phase. As a foundational step in issue fixing, bug localization plays a pivotal role in determining the overall effectiveness of the entire process.
To enhance the precision of issue fixing by accurately identifying bug locations in large-scale projects, this paper presents BugCerberus, the first hierarchical bug localization framework powered by three customized large language models. First, BugCerberus analyzes intermediate representations of bug-related programs at file, function, and statement levels and extracts bug-related contextual information from the representations. Second, BugCerberus designs three customized LLMs at each level using bug reports and contexts to learn the patterns of bugs. Finally, BugCerberus hierarchically searches for bug-related code elements through well-tuned models to localize bugs at three levels. With BugCerberus, we further investigate the impact of bug localization on the issue fixing.
We evaluate BugCerberus on the widely-used benchmark SWE-bench-lite. The experimental results demonstrate that BugCerberus outperforms all baselines. Specifically, at the fine-grained statement level, BugCerberus surpasses the state-of-the-art in Top-N (N=1, 3, 5, 10) by 16.5%, 5.4%, 10.2%, and 23.1%, respectively. Moreover, in the issue fixing experiments, BugCerberus improves the fix rate of the existing issue fixing approach Agentless by 17.4% compared to the best baseline, highlighting the significant impact of enhanced bug localization on automated issue fixing.},
 archiveprefix = {arXiv},
 author = {Jianming Chang and Xin Zhou and Lulu Wang and David Lo and Bixin Li},
 eprint = {2502.15292},
 primaryclass = {cs.SE},
 title = {Bridging Bug Localization and Issue Fixing: A Hierarchical Localization Framework Leveraging Large Language Models},
 url = {https://arxiv.org/abs/2502.15292},
 year = {2025}
}

@misc{wang2025empiricalresearchutilizingllmbased,
 abstract = {This paper presents a novel framework for automated code generation and debugging, designed to improve accuracy, efficiency, and scalability in software development. The proposed system integrates three core components LangGraph, GLM4 Flash, and ChromaDB within a four step iterative workflow to deliver robust performance and seamless functionality.
LangGraph serves as a graph-based library for orchestrating tasks, providing precise control and execution while maintaining a unified state object for dynamic updates and consistency. It supports multi-agent, hierarchical, and sequential processes, making it highly adaptable to complex software engineering workflows. GLM4 Flash, a large language model, leverages its advanced capabilities in natural language understanding, contextual reasoning, and multilingual support to generate accurate code snippets based on user prompts. ChromaDB acts as a vector database for semantic search and contextual memory storage, enabling the identification of patterns and the generation of context-aware bug fixes based on historical data.
The system operates through a structured four-step process: (1) Code Generation, which translates natural language descriptions into executable code; (2) Code Execution, which validates the code by identifying runtime errors and inconsistencies; (3) Code Repair, which iteratively refines buggy code using ChromaDB's memory capabilities and LangGraph's state tracking; and (4) Code Update, which ensures the code meets functional and performance requirements through iterative modifications.},
 archiveprefix = {arXiv},
 author = {Jialin Wang and Zhihua Duan},
 eprint = {2502.18465},
 primaryclass = {cs.SE},
 title = {Empirical Research on Utilizing LLM-based Agents for Automated Bug Fixing via LangGraph},
 url = {https://arxiv.org/abs/2502.18465},
 year = {2025}
}

@misc{karanjai2025securingsmartcontractlanguages,
 abstract = {The rapid growth of the blockchain ecosystem and the increasing value locked in smart contracts necessitate robust security measures. While languages like Solidity and Move aim to improve smart contract security, vulnerabilities persist. This paper presents Smartify, a novel multi-agent framework leveraging Large Language Models (LLMs) to automatically detect and repair vulnerabilities in Solidity and Move smart contracts. Unlike traditional methods that rely solely on vast pre-training datasets, Smartify employs a team of specialized agents working on different specially fine-tuned LLMs to analyze code based on underlying programming concepts and language-specific security principles. We evaluated Smartify on a dataset for Solidity and a curated dataset for Move, demonstrating its effectiveness in fixing a wide range of vulnerabilities. Our results show that Smartify (Gemma2+codegemma) achieves state-of-the-art performance, surpassing existing LLMs and enhancing general-purpose models' capabilities, such as Llama 3.1. Notably, Smartify can incorporate language-specific knowledge, such as the nuances of Move, without requiring massive language-specific pre-training datasets. This work offers a detailed analysis of various LLMs' performance on smart contract repair, highlighting the strengths of our multi-agent approach and providing a blueprint for developing more secure and reliable decentralized applications in the growing blockchain landscape. We also provide a detailed recipe for extending this to other similar use cases.},
 archiveprefix = {arXiv},
 author = {Rabimba Karanjai and Lei Xu and Weidong Shi},
 eprint = {2502.18515},
 primaryclass = {cs.CR},
 title = {Securing Smart Contract Languages with a Unified Agentic Framework for Vulnerability Repair in Solidity and Move},
 url = {https://arxiv.org/abs/2502.18515},
 year = {2025}
}

@misc{liu2025agentdebugsdynamicstateguided,
 abstract = {In recent years, more vulnerabilities have been discovered every day, while manual vulnerability repair requires specialized knowledge and is time-consuming. As a result, many detected or even published vulnerabilities remain unpatched, thereby increasing the exposure of software systems to attacks. Recent advancements in agents based on Large Language Models have demonstrated their increasing capabilities in code understanding and generation, which can be promising to achieve automated vulnerability repair. However, the effectiveness of agents based on static information retrieval is still not sufficient for patch generation. To address the challenge, we propose a program repair agent called VulDebugger that fully utilizes both static and dynamic context, and it debugs programs in a manner akin to humans. The agent inspects the actual state of the program via the debugger and infers expected states via constraints that need to be satisfied. By continuously comparing the actual state with the expected state, it deeply understands the root causes of the vulnerabilities and ultimately accomplishes repairs. We experimentally evaluated VulDebugger on 50 real-life projects. With 60.00% successfully fixed, VulDebugger significantly outperforms state-of-the-art approaches for vulnerability repair.},
 archiveprefix = {arXiv},
 author = {Zhengyao Liu and Yunlong Ma and Jingxuan Xu and Junchen Ai and Xiang Gao and Hailong Sun and Abhik Roychoudhury},
 eprint = {2504.07634},
 primaryclass = {cs.SE},
 title = {Agent That Debugs: Dynamic State-Guided Vulnerability Repair},
 url = {https://arxiv.org/abs/2504.07634},
 year = {2025}
}

@misc{pham2025swesynthsynthesizingverifiablebugfix,
 abstract = {Large language models (LLMs) are transforming automated program repair (APR) through agent-based approaches that localize bugs, generate patches, and verify fixes. However, the lack of high-quality, scalable training datasets, especially those with verifiable outputs and intermediate reasoning traces-limits progress, particularly for open-source models. In this work, we present SWE-Synth, a framework for synthesizing realistic, verifiable, and process-aware bug-fix datasets at the repository level. SWE-Synth leverages LLM agents to simulate debugging workflows, producing not only bug-fix pairs but also test cases and structured repair trajectories. Compared to manually curated datasets, our method scales with minimal human effort while preserving contextual richness and correctness. Experiments show that models trained on SWE-Synth outperform those trained on real-world datasets by 2.3% on SWE-Bench Lite. Our results highlight the potential of synthetic, agent-generated data to advance the state of the art in APR and software engineering automation.},
 archiveprefix = {arXiv},
 author = {Minh V. T. Pham and Huy N. Phan and Hoang N. Phan and Cuong Le Chi and Tien N. Nguyen and Nghi D. Q. Bui},
 eprint = {2504.14757},
 primaryclass = {cs.SE},
 title = {SWE-Synth: Synthesizing Verifiable Bug-Fix Data to Enable Large Language Models in Resolving Real-World Bugs},
 url = {https://arxiv.org/abs/2504.14757},
 year = {2025}
}

@misc{defiterodominguez2025syntheticcodesurgeryrepairing,
 abstract = {This paper presents a novel methodology for enhancing Automated Program Repair (APR) through synthetic data generation utilizing Large Language Models (LLMs). Current APR systems are constrained by the limited availability of high-quality training data encompassing diverse bug types across multiple programming languages. The proposed approach addresses this limitation through a two-phase process: a synthetic sample generation followed by a rigorous quality assessment. Multiple state-of-the-art LLMs were employed to generate approximately 30,000 paired examples of buggy and fixed code across 12 programming languages and 13 bug categories. Subsequently, these samples underwent cross-model evaluation against five criteria: correctness, code quality, security, performance, and completeness. Experimental evaluation on the VulRepair test set dataset showed statistically significant improvements in Perfect Prediction rates, with the quality-filtered synthetic dataset outperforming both baseline and real-world commit data configurations in certain scenarios. The methodology was validated through rigorous statistical testing, including ANOVA and post-hoc Tukey's Honest Significant Difference analysis. Furthermore, the best-performing configurations surpassed existing systems despite using a less computationally intensive decoding strategy. This research establishes a self-bootstrapping paradigm in which LLMs generate and evaluate their own training data, potentially transforming approaches to data scarcity across software engineering tasks and advancing the development of robust, adaptable tools for automated code maintenance.},
 archiveprefix = {arXiv},
 author = {David de-Fitero-Dominguez and Antonio Garcia-Cabot and Eva Garcia-Lopez},
 eprint = {2505.07372},
 primaryclass = {cs.SE},
 title = {Synthetic Code Surgery: Repairing Bugs and Vulnerabilities with LLMs and Synthetic Data},
 url = {https://arxiv.org/abs/2505.07372},
 year = {2025}
}

@misc{zhou2025benchmarkingenhancingllmagents,
 abstract = {The Linux kernel is a critical system, serving as the foundation for numerous systems. Bugs in the Linux kernel can cause serious consequences, affecting billions of users. Fault localization (FL), which aims at identifying the buggy code elements in software, plays an essential role in software quality assurance. While recent LLM agents have achieved promising accuracy in FL on recent benchmarks like SWE-bench, it remains unclear how well these methods perform in the Linux kernel, where FL is much more challenging due to the large-scale code base, limited observability, and diverse impact factors. In this paper, we introduce LinuxFLBench, a FL benchmark constructed from real-world Linux kernel bugs. We conduct an empirical study to assess the performance of state-of-the-art LLM agents on the Linux kernel. Our initial results reveal that existing agents struggle with this task, achieving a best top-1 accuracy of only 41.6% at file level. To address this challenge, we propose LinuxFL^+
, an enhancement framework designed to improve FL effectiveness of LLM agents for the Linux kernel. LinuxFL^+
substantially improves the FL accuracy of all studied agents (e.g., 7.2% - 11.2% accuracy increase) with minimal costs. Data and code are available at this https URL.},
 archiveprefix = {arXiv},
 author = {Zhenhao Zhou and Zhuochen Huang and Yike He and Chong Wang and Jiajun Wang and Yijian Wu and Xin Peng and Yiling Lou},
 eprint = {2505.19489},
 primaryclass = {cs.AI},
 title = {Benchmarking and Enhancing LLM Agents in Localizing Linux Kernel Bugs},
 url = {https://arxiv.org/abs/2505.19489},
 year = {2025}
}

@misc{antal2025identifyinghelpfulcontextllmbased,
 abstract = {Recent advancements in large language models (LLMs) have shown promise for automated vulnerability detection and repair in software systems. This paper investigates the performance of GPT-4o in repairing Java vulnerabilities from a widely used dataset (Vul4J), exploring how different contextual information affects automated vulnerability repair (AVR) capabilities. We compare the latest GPT-4o's performance against previous results with GPT-4 using identical prompts. We evaluated nine additional prompts crafted by us that contain various contextual information such as CWE or CVE information, and manually extracted code contexts. Each prompt was executed three times on 42 vulnerabilities, and the resulting fix candidates were validated using Vul4J's automated testing framework.
Our results show that GPT-4o performed 11.9\% worse on average than GPT-4 with the same prompt, but was able to fix 10.5\% more distinct vulnerabilities in the three runs together. CVE information significantly improved repair rates, while the length of the task description had minimal impact. Combining CVE guidance with manually extracted code context resulted in the best performance. Using our \textsc{Top}-3 prompts together, GPT-4o repaired 26 (62\%) vulnerabilities at least once, outperforming both the original baseline (40\%) and its reproduction (45\%), suggesting that ensemble prompt strategies could improve vulnerability repair in zero-shot settings.},
 archiveprefix = {arXiv},
 author = {Gábor Antal and Bence Bogenfürst and Rudolf Ferenc and Péter Hegedűs},
 eprint = {2506.11561},
 primaryclass = {cs.SE},
 title = {Identifying Helpful Context for LLM-based Vulnerability Repair: A Preliminary Study},
 url = {https://arxiv.org/abs/2506.11561},
 year = {2025}
}

@misc{camporese2026repairingvulnerabilitiesinvisiblehands,
 abstract = {Background: Automated Vulnerability Repair (AVR) is a fast-growing branch of program repair. Recent studies show that large language models (LLMs) outperform traditional techniques, extending their success beyond code generation and fault detection.
Hypothesis: These gains may be driven by hidden factors -- "invisible hands" such as training-data leakage or perfect fault localization -- that let an LLM reproduce human-authored fixes for the same code.
Objective: We replicate prior AVR studies under controlled conditions by deliberately adding errors to the reported vulnerability location in the prompt. If LLMs merely regurgitate memorized fixes, both small and large localization errors should yield the same number of correct patches, because any offset should divert the model from the original fix.
Method: Our pipeline repairs vulnerabilities from the Vul4J and VJTrans benchmarks after shifting the fault location by n lines from the ground truth. A first LLM generates a patch, a second LLM reviews it, and we validate the result with regression and proof-of-vulnerability tests. Finally, we manually audit a sample of patches and estimate the error rate with the Agresti-Coull-Wilson method.},
 archiveprefix = {arXiv},
 author = {Maria Camporese and Fabio Massacci},
 eprint = {2507.20977},
 primaryclass = {cs.SE},
 title = {Repairing vulnerabilities without invisible hands. A differentiated replication study on LLMs},
 url = {https://arxiv.org/abs/2507.20977},
 year = {2026}
}

@misc{asad2025leveraginglargelanguagemodel,
 abstract = {Information Retrieval-based Bug Localization (IRBL) aims to identify buggy source files for a given bug report. Traditional and deep-learning-based IRBL techniques often suffer from vocabulary mismatch and dependence on project-specific metadata, while recent Large Language Model (LLM)-based approaches are limited by insufficient contextual information. To address these issues, we propose GenLoc, an LLM-based technique that combines semantic retrieval with code-exploration functions to iteratively analyze the code base and identify potential buggy files. We evaluate GenLoc on two diverse datasets: a benchmark of 9,097 bugs from six large open-source projects and the GHRB (GitHub Recent Bugs) dataset of 131 recent bugs across 16 projects. Results demonstrate that GenLoc substantially outperforms traditional IRBL, deep learning approaches and recent LLM-based methods, while also localizing bugs that other techniques fail to detect.},
 archiveprefix = {arXiv},
 author = {Moumita Asad and Rafed Muhammad Yasir and Sam Malek},
 eprint = {2508.00253},
 primaryclass = {cs.SE},
 title = {Leveraging Large Language Model for Information Retrieval-based Bug Localization},
 url = {https://arxiv.org/abs/2508.00253},
 year = {2025}
}

@misc{wang2025evaluationlargelanguagemodels,
 abstract = {Various Deep Learning-based approaches with pre-trained language models have been proposed for automatically repairing software vulnerabilities. However, these approaches are limited to a specific programming language (C/C++). Recent advances in large language models (LLMs) offer language-agnostic capabilities and strong semantic understanding, exhibiting potential to overcome multilingual vulnerability limitations. Although some work has begun to explore LLMs' repair performance, their effectiveness is unsatisfactory. To address these limitations, we conducted a large-scale empirical study to investigate the performance of automated vulnerability repair approaches and state-of-the-art LLMs across seven programming languages. Results show GPT-4o, instruction-tuned with few-shot prompting, performs competitively against the leading approach, VulMaster. Additionally, the LLM-based approach shows superior performance in repairing unique vulnerabilities and is more likely to repair the most dangerous vulnerabilities. Instruction-tuned GPT-4o demonstrates strong generalization on vulnerabilities in previously unseen language, outperforming existing approaches. Analysis shows Go consistently achieves the highest effectiveness across all model types, while C/C++ performs the worst. Based on findings, we discuss the promise of LLM on multilingual vulnerability repair and the reasons behind LLM's failed cases. This work takes the first look at repair approaches and LLMs across multiple languages, highlighting the promising future of adopting LLMs for multilingual vulnerability repair.},
 archiveprefix = {arXiv},
 author = {Dong wang and Junji Yu and Honglin Shu and Michael Fu and Chakkrit Tantithamthavorn and Yasutaka Kamei and Junjie Chen},
 eprint = {2508.03470},
 primaryclass = {cs.SE},
 title = {On the Evaluation of Large Language Models in Multilingual Vulnerability Repair},
 url = {https://arxiv.org/abs/2508.03470},
 year = {2025}
}

@misc{mhatre2025llmguardlargelanguagemodelbased,
 abstract = {Large Language Models (LLMs) such as ChatGPT-4, Claude 3, and LLaMA 4 are increasingly embedded in software/application development, supporting tasks from code generation to debugging. Yet, their real-world effectiveness in detecting diverse software bugs, particularly complex, security-relevant vulnerabilities, remains underexplored. This study presents a systematic, empirical evaluation of these three leading LLMs using a benchmark of foundational programming errors, classic security flaws, and advanced, production-grade bugs in C++ and Python. The dataset integrates real code from SEED Labs, OpenSSL (via the Suresoft GLaDOS database), and PyBugHive, validated through local compilation and testing pipelines. A novel multi-stage, context-aware prompting protocol simulates realistic debugging scenarios, while a graded rubric measures detection accuracy, reasoning depth, and remediation quality. Our results show that all models excel at identifying syntactic and semantic issues in well-scoped code, making them promising for educational use and as first-pass reviewers in automated code auditing. Performance diminishes in scenarios involving complex security vulnerabilities and large-scale production code, with ChatGPT-4 and Claude 3 generally providing more nuanced contextual analyses than LLaMA 4. This highlights both the promise and the present constraints of LLMs in serving as reliable code analysis tools.},
 archiveprefix = {arXiv},
 author = {Akshay Mhatre and Noujoud Nader and Patrick Diehl and Deepti Gupta},
 eprint = {2508.16419},
 primaryclass = {cs.SE},
 title = {LLM-GUARD: Large Language Model-Based Detection and Repair of Bugs and Security Vulnerabilities in C++ and Python},
 url = {https://arxiv.org/abs/2508.16419},
 year = {2025}
}

@misc{wang2025vulnrepairevalexploitbasedevaluationframework,
 abstract = {The adoption of Large Language Models (LLMs) for automated software vulnerability patching has shown promising outcomes on carefully curated evaluation sets. Nevertheless, existing datasets predominantly rely on superficial validation methods rather than exploit-based verification, leading to overestimated performance in security-sensitive applications. This paper introduces VulnRepairEval, an evaluation framework anchored in functional Proof-of-Concept (PoC) exploits. Our framework delivers a comprehensive, containerized evaluation pipeline that enables reproducible differential assessment, where repair success requires the original exploit to fail execution against the modified code. The benchmark construction involved extensive data curation: we processed over 400 CVEs and approximately 2,500 potential sources to extract a collection of authentic vulnerability instances (23 Python CVEs) amenable to automated testing with working PoCs. Through VulnRepairEval, we conduct a comprehensive evaluation of 12 popular LLMs and observe a significant performance deficit: even the top-performing model successfully addresses merely 5/23 instances (about 21.7%), exposing critical weaknesses in security-focused applications. Our failure analysis reveals that most unsuccessful attempts stem from imprecise vulnerability identification and patches containing syntactic or semantic errors. Enhanced prompting strategies and multi-agent approaches yield minimal improvements, with overall effectiveness remaining largely unaffected. This work contributes a stringent, practical evaluation framework for LLM-driven vulnerability remediation and underscores the necessity for assessment protocols that authentically reflect real-world exploitation scenarios.},
 archiveprefix = {arXiv},
 author = {Weizhe Wang and Wei Ma and Qiang Hu and Yao Zhang and Jianfei Sun and Bin Wu and Yang Liu and Guangquan Xu and Lingxiao Jiang},
 eprint = {2509.03331},
 primaryclass = {cs.SE},
 title = {VulnRepairEval: An Exploit-Based Evaluation Framework for Assessing Large Language Model Vulnerability Repair Capabilities},
 url = {https://arxiv.org/abs/2509.03331},
 year = {2025}
}

@misc{nguyen2025patchseekermappingnvdrecords,
 abstract = {Software vulnerabilities pose serious risks to modern software ecosystems. While the National Vulnerability Database (NVD) is the authoritative source for cataloging these vulnerabilities, it often lacks explicit links to the corresponding Vulnerability-Fixing Commits (VFCs). VFCs encode precise code changes, enabling vulnerability localization, patch analysis, and dataset construction. Automatically mapping NVD records to their true VFCs is therefore critical. Existing approaches have limitations as they rely on sparse, often noisy commit messages and fail to capture the deep semantics in the vulnerability descriptions. To address this gap, we introduce PatchSeeker, a novel method that leverages large language models to create rich semantic links between vulnerability descriptions and their VFCs. PatchSeeker generates embeddings from NVD descriptions and enhances commit messages by synthesizing detailed summaries for those that are short or uninformative. These generated messages act as a semantic bridge, effectively closing the information gap between natural language reports and low-level code changes. Our approach PatchSeeker achieves 59.3% higher MRR and 27.9% higher Recall@10 than the best-performing baseline, Prospector, on the benchmark dataset. The extended evaluation on recent CVEs further confirms PatchSeeker's effectiveness. Ablation study shows that both the commit message generation method and the selection of backbone LLMs make a positive contribution to PatchSeeker. We also discuss limitations and open challenges to guide future work.},
 archiveprefix = {arXiv},
 author = {Huu Hung Nguyen and Anh Tuan Nguyen and Thanh Le-Cong and Yikun Li and Han Wei Ang and Yide Yin and Frank Liauw and Shar Lwin Khin and Ouh Eng Lieh and Ting Zhang and David Lo},
 eprint = {2509.07540},
 primaryclass = {cs.SE},
 title = {PatchSeeker: Mapping NVD Records to their Vulnerability-fixing Commits with LLM Generated Commits and Embeddings},
 url = {https://arxiv.org/abs/2509.07540},
 year = {2025}
}

@misc{xu2025revisitingvulnerabilitypatchlocalization,
 abstract = {Open-source software vulnerability patch detection is a critical component for maintaining software security and ensuring software supply chain integrity. Traditional manual detection methods face significant scalability challenges when processing large volumes of commit histories, while being prone to human errors and omissions. Existing automated approaches, including heuristic-based methods and pre-trained model solutions, suffer from limited accuracy, poor generalization capabilities, and inherent methodological constraints that hinder their practical deployment. To address these fundamental challenges, this paper conducts a comprehensive empirical study of existing vulnerability patch detection methods, revealing four key insights that guide the design of effective solutions: the critical impact of search space reduction, the superiority of pre-trained semantic understanding over architectural complexity, the temporal limitations of web crawling approaches, and the advantages of knowledge-driven methods. Based on these insights, we propose a novel two-stage framework that combines version-driven candidate filtering with large language model-based multi-round dialogue voting to achieve accurate and efficient vulnerability patch identification. Extensive experiments on a dataset containing 750 real vulnerabilities demonstrate that our method outperforms current approaches.},
 archiveprefix = {arXiv},
 author = {Haoran Xu and Chen Zhi and Junxiao Han and Xinkui Zhao and Jianwei Yin and Shuiguang Deng},
 eprint = {2509.15777},
 primaryclass = {cs.SE},
 title = {Revisiting Vulnerability Patch Localization: An Empirical Study and LLM-Based Solution},
 url = {https://arxiv.org/abs/2509.15777},
 year = {2025}
}

@misc{gajjar2025securefixagenthybridllmagent,
 abstract = {Modern software development pipelines face growing challenges in securing large codebases with extensive dependencies. Static analysis tools like Bandit are effective at vulnerability detection but suffer from high false positives and lack repair capabilities. Large Language Models (LLMs), in contrast, can suggest fixes but often hallucinate changes and lack self-validation. We present SecureFixAgent, a hybrid repair framework integrating Bandit with lightweight local LLMs (<8B parameters) in an iterative detect-repair-validate loop. To improve precision, we apply parameter-efficient LoRA-based fine-tuning on a diverse, curated dataset spanning multiple Python project domains, mitigating dataset bias and reducing unnecessary edits. SecureFixAgent uses Bandit for detection, the LLM for candidate fixes with explanations, and Bandit re-validation for verification, all executed locally to preserve privacy and reduce cloud reliance. Experiments show SecureFixAgent reduces false positives by 10.8% over static analysis, improves fix accuracy by 13.51%, and lowers false positives by 5.46% compared to pre-trained LLMs, typically converging within three iterations. Beyond metrics, developer studies rate explanation quality 4.5/5, highlighting its value for human trust and adoption. By combining verifiable security improvements with transparent rationale in a resource-efficient local framework, SecureFixAgent advances trustworthy, automated vulnerability remediation for modern pipelines.},
 archiveprefix = {arXiv},
 author = {Jugal Gajjar and Kamalasankari Subramaniakuppusamy and Relsy Puthal and Kaustik Ranaware},
 eprint = {2509.16275},
 primaryclass = {cs.CR},
 title = {SecureFixAgent: A Hybrid LLM Agent for Automated Python Static Vulnerability Repair},
 url = {https://arxiv.org/abs/2509.16275},
 year = {2025}
}

@misc{li2025fixllmaidedcategorizationsecurity,
 abstract = {Open-source software projects are foundational to modern software ecosystems, with the Linux kernel standing out as a critical exemplar due to its ubiquity and complexity. Although security patches are continuously integrated into the Linux mainline kernel, downstream maintainers often delay their adoption, creating windows of vulnerability. A key reason for this lag is the difficulty in identifying security-critical patches, particularly those addressing exploitable vulnerabilities such as out-of-bounds (OOB) accesses and use-after-free (UAF) bugs. This challenge is exacerbated by intentionally silent bug fixes, incomplete or missing CVE assignments, delays in CVE issuance, and recent changes to the CVE assignment criteria for the Linux kernel. While fine-grained patch classification approaches exist, they exhibit limitations in both coverage and accuracy. In this work, we identify previously unexplored opportunities to significantly improve fine-grained patch classification. Specifically, by leveraging cues from commit titles/messages and diffs alongside appropriate code context, we develop DUALLM, a dual-method pipeline that integrates two approaches based on a Large Language Model (LLM) and a fine-tuned small language model. DUALLM achieves 87.4% accuracy and an F1-score of 0.875, significantly outperforming prior solutions. Notably, DUALLM successfully identified 111 of 5,140 recent Linux kernel patches as addressing OOB or UAF vulnerabilities, with 90 true positives confirmed by manual verification (many do not have clear indications in patch descriptions). Moreover, we constructed proof-of-concepts for two identified bugs (one UAF and one OOB), including one developed to conduct a previously unknown control-flow hijack as further evidence of the correctness of the classification.},
 archiveprefix = {arXiv},
 author = {Xingyu Li and Juefei Pu and Yifan Wu and Xiaochen Zou and Shitong Zhu and Xiaochen Zou and Shitong Zhu and Qiushi Wu and Zheng Zhang and Joshua Hsu and Yue Dong and Zhiyun Qian and Kangjie Lu and Trent Jaeger and Michael De Lucia and Srikanth V. Krishnamurthy},
 eprint = {2509.22796},
 primaryclass = {cs.CR},
 title = {What Do They Fix? LLM-Aided Categorization of Security Patches for Critical Memory Bugs},
 url = {https://arxiv.org/abs/2509.22796},
 year = {2025}
}

@misc{garg2025perfbenchagentsresolverealworld,
 abstract = {Performance bugs are inefficiencies in software that waste computational resources without causing functional failures, making them particularly challenging to detect and fix. While recent advances in Software Engineering agents have shown promise in automated bug fixing, existing benchmarks primarily focus on functional correctness and fail to evaluate agents' abilities to identify and resolve non-functional issues like performance bugs. We introduce PerfBench, a benchmark comprising 81 real-world performance bug-fixing tasks from popular .NET repositories on GitHub. Unlike existing benchmarks that rely on pre-existing test suites, PerfBench features a novel evaluation harness that allows agents to generate their own performance benchmarks and validates fixes by comparing execution metrics collected for developer fix and agent fix. Each task in PerfBench is derived from actual developer fixes linked to performance-related issues, which are then verified by human experts, ensuring real-world relevance. Our evaluation reveals that current state-of-the-art coding agents struggle with performance optimization tasks, with baseline OpenHands agent achieving only a ~3% success rate on our benchmark. We develop OpenHands-Perf-Agent, which incorporates performance-aware tooling and instructions and achieves a ~20% success rate on the benchmark. We show that by ensuring the agent has proper instructions to benchmark its changes and tooling for benchmark output processing, we can improve the agent performance significantly, but room for improvement still remains. PerfBench provides a challenging test set for furthering the capabilities of agents in fixing performance issues.},
 archiveprefix = {arXiv},
 author = {Spandan Garg and Roshanak Zilouchian Moghaddam and Neel Sundaresan},
 eprint = {2509.24091},
 primaryclass = {cs.SE},
 title = {PerfBench: Can Agents Resolve Real-World Performance Bugs?},
 url = {https://arxiv.org/abs/2509.24091},
 year = {2025}
}

@misc{chen2025redteamingprogramrepair,
 abstract = {LLM-based agents are increasingly deployed for software maintenance tasks such as automated program repair (APR). APR agents automatically fetch GitHub issues and use backend LLMs to generate patches that fix the reported bugs. However, existing work primarily focuses on the functional correctness of APR-generated patches, whether they pass hidden or regression tests, while largely ignoring potential security risks. Given the openness of platforms like GitHub, where any user can raise issues and participate in discussions, an important question arises: Can an adversarial user submit a valid issue on GitHub that misleads an LLM-based agent into generating a functionally correct but vulnerable patch? To answer this question, we propose SWExploit, which generates adversarial issue statements designed to make APR agents produce patches that are functionally correct yet vulnerable. SWExploit operates in three main steps: (1) program analysis to identify potential injection points for vulnerable payloads; (2) adversarial issue generation to provide misleading reproduction and error information while preserving the original issue semantics; and (3) iterative refinement of the adversarial issue statements based on the outputs of the APR agents. Empirical evaluation on three agent pipelines and five backend LLMs shows that SWExploit can produce patches that are both functionally correct and vulnerable (the attack success rate on the correct patch could reach 0.91, whereas the baseline ASRs are all below 0.20). Based on our evaluation, we are the first to challenge the traditional assumption that a patch passing all tests is inherently reliable and secure, highlighting critical limitations in the current evaluation paradigm for APR agents.},
 archiveprefix = {arXiv},
 author = {Simin Chen and Yixin He and Suman Jana and Baishakhi Ray},
 eprint = {2509.25894},
 primaryclass = {cs.SE},
 title = {Red Teaming Program Repair Agents: When Correct Patches can Hide Vulnerabilities},
 url = {https://arxiv.org/abs/2509.25894},
 year = {2025}
}

@misc{xi2026tracelinellmagent,
 abstract = {Large language models show promise for vulnerability discovery, yet prevailing methods inspect code in isolation, struggle with long contexts, and focus on coarse function- or file-level detections that offer limited guidance to engineers who need precise line-level localization for targeted patches. We introduce T2L, an executable framework for project-level, line-level vulnerability localization that progressively narrows scope from repository modules to exact vulnerable lines via AST-based chunking and evidence-guided refinement. We provide a baseline agent with an Agentic Trace Analyzer (ATA) that fuses runtime evidence such as crash points and stack traces to translate failure symptoms into actionable diagnoses. To enable rigorous evaluation, we introduce T2L-ARVO, an expert-verified 50-case benchmark spanning five crash families in real-world projects. On T2L-ARVO, our baseline achieves up to 58.0% detection and 54.8% line-level localization rate. Together, T2L framework advance LLM-based vulnerability detection toward deployable, precision diagnostics in open-source software workflows.},
 archiveprefix = {arXiv},
 author = {Haoran Xi and Minghao Shao and Brendan Dolan-Gavitt and Muhammad Shafique and Ramesh Karri},
 eprint = {2510.02389},
 primaryclass = {cs.SE},
 title = {From Trace to Line: LLM Agent for Real-World OSS Vulnerability Localization},
 url = {https://arxiv.org/abs/2510.02389},
 year = {2026}
}

@misc{wen2025vulr2reasoningllmautomated,
 abstract = {The exponential increase in software vulnerabilities has created an urgent need for automatic vulnerability repair (AVR) solutions. Recent research has formulated AVR as a sequence generation problem and has leveraged large language models (LLMs) to address this problem. Typically, these approaches prompt or fine-tune LLMs to generate repairs for vulnerabilities directly. Although these methods show state-of-the-art performance, they face the following challenges: (1) Lack of high-quality, vulnerability-related reasoning data. Current approaches primarily rely on foundation models that mainly encode general programming knowledge. Without vulnerability-related reasoning data, they tend to fail to capture the diverse vulnerability repair patterns. (2) Hard to verify the intermediate vulnerability repair process during LLM training. Existing reinforcement learning methods often leverage intermediate execution feedback from the environment (e.g., sandbox-based execution results) to guide reinforcement learning training. In contrast, the vulnerability repair process generally lacks such intermediate, verifiable feedback, which poses additional challenges for model training.},
 archiveprefix = {arXiv},
 author = {Xin-Cheng Wen and Zirui Lin and Yijun Yang and Cuiyun Gao and Deheng Ye},
 eprint = {2510.05480},
 primaryclass = {cs.AI},
 title = {Vul-R2: A Reasoning LLM for Automated Vulnerability Repair},
 url = {https://arxiv.org/abs/2510.05480},
 year = {2025}
}

@misc{wang2025defects4cbenchmarkinglargelanguage,
 abstract = {Automated Program Repair (APR) plays a critical role in enhancing the quality and reliability of software systems. While substantial progress has been made in Java-based APR, largely facilitated by benchmarks like Defects4J, there remains a significant gap in research on C/C++ program repair, despite the widespread use of C/C++ and the prevalence of associated vulnerabilities. This gap is primarily due to the lack of high-quality, open-source benchmarks tailored for C/C++.
To address this issue, we introduce Defects4C, a comprehensive and executable benchmark specifically designed for C/C++ program repair. Our dataset is constructed from real-world C/C++ repositories and includes a large collection of bug-relevant commits (9M in total), 248 high-quality buggy functions, and 102 vulnerable functions, all paired with test cases for reproduction. These resources enable rigorous evaluation of repair techniques and support the retraining of learning-based approaches for enhanced performance.
Using Defects4C, we conduct a comprehensive empirical study evaluating the effectiveness of 24 state-of-the-art large language models (LLMs) in repairing C/C++ faults. Our findings offer valuable insights into the strengths and limitations of current LLM-based APR techniques in this domain, highlighting both the need for more robust methods and the critical role of Defects4C in advancing future research},
 archiveprefix = {arXiv},
 author = {Jian Wang and Xiaofei Xie and Qiang Hu and Shangqing Liu and Jiongchi Yu and Jiaolong Kong and Yi Li},
 eprint = {2510.11059},
 primaryclass = {cs.SE},
 title = {Defects4C: Benchmarking Large Language Model Repair Capability with C/C++ Bugs},
 url = {https://arxiv.org/abs/2510.11059},
 year = {2025}
}

@misc{oskooei2025naturallanguagesummarizationenables,
 abstract = {Bug localization in multi-repository microservice architectures is challenging due to the semantic gap between natural language bug reports and code, LLM context limitations, and the need to first identify the correct repository. We propose reframing this as a natural language reasoning task by transforming codebases into hierarchical NL summaries and performing NL-to-NL search instead of cross-modal retrieval. Our approach builds context-aware summaries at file, directory, and repository levels, then uses a two-phase search: first routing bug reports to relevant repositories, then performing top-down localization within those repositories. Evaluated on DNext, an industrial system with 46 repositories and 1.1M lines of code, our method achieves Pass@10 of 0.82 and MRR of 0.50, significantly outperforming retrieval baselines and agentic RAG systems like GitHub Copilot and Cursor. This work demonstrates that engineered natural language representations can be more effective than raw source code for scalable bug localization, providing an interpretable repository -> directory -> file search path, which is vital for building trust in enterprise AI tools by providing essential transparency.},
 archiveprefix = {arXiv},
 author = {Amirkia Rafiei Oskooei and S. Selcan Yukcu and Mehmet Cevheri Bozoglan and Mehmet S. Aktas},
 eprint = {2512.05908},
 primaryclass = {cs.SE},
 title = {Natural Language Summarization Enables Multi-Repository Bug Localization by LLMs in Microservice Architectures},
 url = {https://arxiv.org/abs/2512.05908},
 year = {2025}
}

@misc{caumartin2025reformulateretrievelocalizeagents,
 abstract = {Bug localization remains a critical yet time-consuming challenge in large-scale software repositories. Traditional information retrieval-based bug localization (IRBL) methods rely on unchanged bug descriptions, which often contain noisy information, leading to poor retrieval accuracy. Recent advances in large language models (LLMs) have improved bug localization through query reformulation, yet the effect on agent performance remains unexplored. In this study, we investigate how an LLM-powered agent can improve file-level bug localization via lightweight query reformulation and summarization. We first employ an open-source, non-fine-tuned LLM to extract key information from bug reports, such as identifiers and code snippets, and reformulate queries pre-retrieval. Our agent then orchestrates BM25 retrieval using these preprocessed queries, automating localization workflow at scale. Using the best-performing query reformulation technique, our agent achieves 35% better ranking in first-file retrieval than our BM25 baseline and up to +22% file retrieval performance over SWE-agent.},
 archiveprefix = {arXiv},
 author = {Genevieve Caumartin and Glaucia Melo},
 eprint = {2512.07022},
 primaryclass = {cs.SE},
 title = {Reformulate, Retrieve, Localize: Agents for Repository-Level Bug Localization},
 url = {https://arxiv.org/abs/2512.07022},
 year = {2025}
}

@misc{zibaeirad2025diversellmsvsvulnerabilities,
 abstract = {Large Language Models (LLMs) are increasingly being studied for Software Vulnerability Detection (SVD) and Repair (SVR). Individual LLMs have demonstrated code understanding abilities, but they frequently struggle when identifying complex vulnerabilities and generating fixes.
This study presents DVDR-LLM, an ensemble framework that combines outputs from diverse LLMs to determine whether aggregating multiple models reduces error rates. Our evaluation reveals that DVDR-LLM achieves 10-12% higher detection accuracy compared to the average performance of individual models, with benefits increasing as code complexity grows. For multi-file vulnerabilities, the ensemble approach demonstrates significant improvements in recall (+18%) and F1 score (+11.8%) over individual models. However, the approach raises measurable trade-offs: reducing false positives in verification tasks while simultaneously increasing false negatives in detection tasks, requiring careful decision on the required level of agreement among the LLMs (threshold) for increased performance across different security contexts.
Artifact: this https URL},
 archiveprefix = {arXiv},
 author = {Arastoo Zibaeirad and Marco Vieira},
 eprint = {2512.12536},
 primaryclass = {cs.SE},
 title = {Diverse LLMs vs. Vulnerabilities: Who Detects and Fixes Them Better?},
 url = {https://arxiv.org/abs/2512.12536},
 year = {2025}
}

@misc{park2025effectivenessinstructiontuninglocalllms,
 abstract = {Large Language Models (LLMs) show significant promise in automating software vulnerability analysis, a critical task given the impact of security failure of modern software systems. However, current approaches in using LLMs to automate vulnerability analysis mostly rely on using online API-based LLM services, requiring the user to disclose the source code in development. Moreover, they predominantly frame the task as a binary classification(vulnerable or not vulnerable), limiting potential practical utility. This paper addresses these limitations by reformulating the problem as Software Vulnerability Identification (SVI), where LLMs are asked to output the type of weakness in Common Weakness Enumeration (CWE) IDs rather than simply indicating the presence or absence of a vulnerability. We also tackle the reliance on large, API-based LLMs by demonstrating that instruction-tuning smaller, locally deployable LLMs can achieve superior identification performance. In our analysis, instruct-tuning a local LLM showed better overall performance and cost trade-off than online API-based LLMs. Our findings indicate that instruct-tuned local models represent a more effective, secure, and practical approach for leveraging LLMs in real-world vulnerability management workflows.},
 archiveprefix = {arXiv},
 author = {Sangryu Park and Gihyuk Ko and Homook Cho},
 eprint = {2512.20062},
 primaryclass = {cs.CR},
 title = {On the Effectiveness of Instruction-Tuning Local LLMs for Identifying Software Vulnerabilities},
 url = {https://arxiv.org/abs/2512.20062},
 year = {2025}
}

@misc{wang2026autovulnphpllmpoweredtwostagephp,
 abstract = {PHP's dominance in web development is undermined by security challenges: static analysis lacks semantic depth, causing high false positives; dynamic analysis is computationally expensive; and automated vulnerability localization suffers from coarse granularity and imprecise context. Additionally, the absence of large-scale PHP vulnerability datasets and fragmented toolchains hinder real-world deployment.
We present AutoVulnPHP, an end-to-end framework coupling two-stage vulnerability detection with fine-grained automated localization. SIFT-VulMiner (Structural Inference for Flaw Triage Vulnerability Miner) generates vulnerability hypotheses using AST structures enhanced with data flow. SAFE-VulMiner (Semantic Analysis for Flaw Evaluation Vulnerability Miner) verifies candidates through pretrained code encoder embeddings, eliminating false positives. ISAL (Incremental Sequence Analysis for Localization) pinpoints root causes via syntax-guided tracing, chain-of-thought LLM inference, and causal consistency checks to ensure precision.
We contribute PHPVD, the first large-scale PHP vulnerability dataset with 26,614 files (5.2M LOC) across seven vulnerability types. On public benchmarks and PHPVD, AutoVulnPHP achieves 99.7% detection accuracy, 99.5% F1 score, and 81.0% localization rate. Deployed on real-world repositories, it discovered 429 previously unknown vulnerabilities, 351 assigned CVE identifiers, validating its practical effectiveness.},
 archiveprefix = {arXiv},
 author = {Zhiqiang Wang and Yizhong Ding and Zilong Xiao and Jinyu Lu and Yan Jia and Yanjun Li},
 eprint = {2601.06177},
 primaryclass = {cs.CR},
 title = {AutoVulnPHP: LLM-Powered Two-Stage PHP Vulnerability Detection and Automated Localization},
 url = {https://arxiv.org/abs/2601.06177},
 year = {2026}
}

@misc{zhang2026codingbubbleevaluatingllms,
 abstract = {Code adaptation is a fundamental but challenging task in software development, requiring developers to modify existing code for new contexts. A key challenge is to resolve Context Adaptation Bugs (CtxBugs), which occurs when code correct in its original context violates constraints in the target environment. Unlike isolated bugs, CtxBugs cannot be resolved through local fixes and require cross-context reasoning to identify semantic mismatches. Overlooking them may lead to critical failures in adaptation. Although Large Language Models (LLMs) show great potential in automating code-related tasks, their ability to resolve CtxBugs remains a significant and unexplored obstacle to their practical use in code adaptation. To bridge this gap, we propose CtxBugGen, a novel framework for generating CtxBugs to evaluate LLMs. Its core idea is to leverage LLMs' tendency to generate plausible but context-free code when contextual constraints are absent. The framework generates CtxBugs through a four-step process to ensure their relevance and validity: (1) Adaptation Task Selection, (2) Task-specific Perturbation,(3) LLM-based Variant Generation and (4) CtxBugs Identification. Based on the benchmark constructed by CtxBugGen, we conduct an empirical study with four state-of-the-art LLMs. Our results reveal their unsatisfactory performance in CtxBug resolution. The best performing LLM, Kimi-K2, achieves 55.93% on Pass@1 and resolves just 52.47% of CtxBugs. The presence of CtxBugs degrades LLMs' adaptation performance by up to 30%. Failure analysis indicates that LLMs often overlook CtxBugs and replicate them in their outputs. Our study highlights a critical weakness in LLMs' cross-context reasoning and emphasize the need for new methods to enhance their context awareness for reliable code adaptation.},
 archiveprefix = {arXiv},
 author = {Tanghaoran Zhang and Xinjun Mao and Shangwen Wang and Yuxin Zhao and Yao Lu and Zezhou Tang and Wenyu Xu and Longfei Sun and Changrong Xie and Kang Yang and Yue Yu},
 eprint = {2601.06497},
 primaryclass = {cs.SE},
 title = {Coding in a Bubble? Evaluating LLMs in Resolving Context Adaptation Bugs During Code Adaptation},
 url = {https://arxiv.org/abs/2601.06497},
 year = {2026}
}

@misc{alkaswan2026modelseemodeldo,
 abstract = {Large language models are increasingly used for code generation and debugging, but their outputs can still contain bugs, that originate from training data. Distinguishing whether an LLM prefers correct code, or a familiar incorrect version might be influenced by what it's been exposed to during training. We introduce an exposure-aware evaluation framework that quantifies how prior exposure to buggy versus fixed code influences a model's preference. Using the ManySStuBs4J benchmark, we apply Data Portraits for membership testing on the Stack-V2 corpus to estimate whether each buggy and fixed variant was seen during training. We then stratify examples by exposure and compare model preference using code completion as well as multiple likelihood-based scoring metrics We find that most examples (67%) have neither variant in the training data, and when only one is present, fixes are more frequently present than bugs. In model generations, models reproduce buggy lines far more often than fixes, with bug-exposed examples amplifying this tendency and fix-exposed examples showing only marginal improvement. In likelihood scoring, minimum and maximum token-probability metrics consistently prefer the fixed code across all conditions, indicating a stable bias toward correct fixes. In contrast, metrics like the Gini coefficient reverse preference when only the buggy variant was seen. Our results indicate that exposure can skew bug-fix evaluations and highlight the risk that LLMs may propagate memorised errors in practice.},
 archiveprefix = {arXiv},
 author = {Ali Al-Kaswan and Claudio Spiess and Prem Devanbu and Arie van Deursen and Maliheh Izadi},
 eprint = {2601.10496},
 primaryclass = {cs.SE},
 title = {Model See, Model Do? Exposure-Aware Evaluation of Bug-vs-Fix Preference in Code LLMs},
 url = {https://arxiv.org/abs/2601.10496},
 year = {2026}
}

@misc{samir2026improvedbuglocalizationai,
 abstract = {Software bugs cost technology providers (e.g., AT&T) billions annually and cause developers to spend roughly 50% of their time on bug resolution. Traditional methods for bug localization often analyze the suspiciousness of code components (e.g., methods, documents) in isolation, overlooking their connections with other components in the codebase. Recent advances in Large Language Models (LLMs) and agentic AI techniques have shown strong potential for code understanding, but still lack causal reasoning during code exploration and struggle to manage growing context effectively, limiting their capability. In this paper, we present a novel agentic technique for bug localization -- CogniGent -- that overcomes the limitations above by leveraging multiple AI agents capable of causal reasoning, call-graph-based root cause analysis and context engineering. It emulates developers-inspired debugging practices (a.k.a., dynamic cognitive debugging) and conducts hypothesis testing to support bug localization. We evaluate CogniGent on a curated dataset of 591 bug reports using three widely adopted performance metrics and compare it against six established baselines from the literature. Experimental results show that our technique consistently outperformed existing traditional and LLM-based techniques, achieving MAP improvements of 23.33-38.57% at the document and method levels. Similar gains were observed in MRR, with increases of 25.14-53.74% at both granularity levels. Statistical significance tests also confirm the superiority of our technique. By addressing the reasoning, dependency, and context limitations, CogniGent advances the state of bug localization, bridging human-like cognition with agentic automation for improved performance.},
 archiveprefix = {arXiv},
 author = {Asif Mohammed Samir and Mohammad Masudur Rahman},
 eprint = {2601.12522},
 primaryclass = {cs.SE},
 title = {Improved Bug Localization with AI Agents Leveraging Hypothesis and Dynamic Cognition},
 url = {https://arxiv.org/abs/2601.12522},
 year = {2026}
}

@misc{su2026cmindaiagentlocalizing,
 abstract = {This demonstration paper presents CMind, an artificial intelligence agent for localizing C memory bugs. The novel aspect to CMind is that it follows steps that we observed human programmers perform during empirical study of those programmers finding memory bugs in C programs. The input to the tool is a C program's source code and a bug report describing the problem. The output is the tool's hypothesis about the reason for the bug and its location. CMind reads the bug report to find potential entry points to the program, then navigates the program's source code, analyzes that source code, and generates a hypothesis location and rationale that fit a template. The tool combines large language model reasoning with guided decision making we encoded to mimic human behavior. The video demonstration is available at this https URL.},
 archiveprefix = {arXiv},
 author = {Chia-Yi Su and Collin McMillan},
 eprint = {2601.14434},
 primaryclass = {cs.SE},
 title = {CMind: An AI Agent for Localizing C Memory Bugs},
 url = {https://arxiv.org/abs/2601.14434},
 year = {2026}
}

@misc{adeseye2026promptbasedframeworkloopvulnerability,
 abstract = {Loop vulnerabilities are one major risky construct in software development. They can easily lead to infinite loops or executions, exhaust resources, or introduce logical errors that degrade performance and compromise security. The problem are often undetected by traditional static analyzers because such tools rely on syntactic patterns, which makes them struggle to detect semantic flaws. Consequently, Large Language Models (LLMs) offer new potential for vulnerability detection because of their ability to understand code contextually. Moreover, local LLMs unlike commercial ones like ChatGPT or Gemini addresses issues such as privacy, latency, and dependency concerns by facilitating efficient offline analysis. Consequently, this study proposes a prompt-based framework that utilize local LLMs for the detection of loop vulnerabilities within Python 3.7+ code. The framework targets three categories of loop-related issues, such as control and logic errors, security risks inside loops, and resource management inefficiencies. A generalized and structured prompt-based framework was designed and tested with two locally deployed LLMs (LLaMA 3.2; 3B and Phi 3.5; 4B) by guiding their behavior via iterative prompting. The designed prompt-based framework included key safeguarding features such as language-specific awareness, code-aware grounding, version sensitivity, and hallucination prevention. The LLM results were validated against a manually established baseline truth, and the results indicate that Phi outperforms LLaMA in precision, recall, and F1-score. The findings emphasize the importance of designing effective prompts for local LLMs to perform secure and accurate code vulnerability analysis.},
 archiveprefix = {arXiv},
 author = {Adeyemi Adeseye and Aisvarya Adeseye},
 eprint = {2601.15352},
 primaryclass = {cs.SE},
 title = {A Prompt-Based Framework for Loop Vulnerability Detection Using Local LLMs},
 url = {https://arxiv.org/abs/2601.15352},
 year = {2026}
}

@misc{kim2026patchislandorchestrationllmagents,
 abstract = {Continuous fuzzing platforms such as OSS-Fuzz uncover large numbers of vulnerabilities, yet the subsequent repair process remains largely manual. Unfortunately, existing Automated Vulnerability Repair (AVR) techniques -- including recent LLM-based systems -- are not directly applicable to continuous fuzzing. This is because these systems are designed and evaluated on a static, single-run benchmark setting, making them ill-suited for the diverse, noisy, and failure-prone environments in continuous fuzzing.
To address these issues, we introduce PatchIsland, a system for Continuous Vulnerability Repair (CVR) that tightly integrates with continuous fuzzing pipelines. PatchIsland employs an ensemble of diverse LLM agents. By leveraging multiple LLM agents, PatchIsland can cover a wider range of settings (e.g., different projects, bug types, and programming languages) and also improve operational robustness. In addition, PatchIsland utilizes a two-phase patch-based deduplication to mitigate duplicate crashes and patches, which can be problematic in continuous fuzzing.
In our internal evaluation, PatchIsland repaired 84 of 92 vulnerabilities, demonstrating strong repair capability. In the official AIxCC competition, the system operated with no human intervention in a fully autonomous environment and successfully patched 31 out of 43 vulnerabilities, achieving a repair rate of 72.1\%.},
 archiveprefix = {arXiv},
 author = {Wonyoung Kim and Seunggi Min and Minjae Gwon and Dowoo Baik and Haein Lee and Hyeon Heo and Minjae Lee and Min Woo Baek and Yonghwi Jin and Younggi Park and Yunjae Choi and Taesoo Kim and Sangdon Park and Insu Yun},
 eprint = {2601.17471},
 primaryclass = {cs.CR},
 title = {PatchIsland: Orchestration of LLM Agents for Continuous Vulnerability Repair},
 url = {https://arxiv.org/abs/2601.17471},
 year = {2026}
}

@misc{cheng2026dynamiccogenerationbugreproduction,
 abstract = {Bug Reproduction Tests (BRTs) have been used in many agentic Automated Program Repair (APR) systems, primarily for validating promising fixes and aiding fix generation. In practice, when developers submit a patch, they often implement the BRT alongside the fix. Our experience deploying agentic APR reveals that developers similarly desire a BRT within AI-generated patches to increase their confidence. However, canonical APR systems tend to generate BRTs and fixes separately, or focus on producing only the fix in the final patch. In this paper, we study agentic APR in the context of cogeneration, where the APR agent is instructed to generate both a fix and a BRT in the same patch. We evaluate the effectiveness of different cogeneration strategies on 120 human-reported bugs at Google and characterize different cogeneration strategies by their influence on APR agent behavior. We develop and evaluate patch selectors that account for test change information to select patches with plausible fixes (and plausible BRTs). Finally, we analyze the root causes of failed cogeneration trajectories. Importantly, we show that cogeneration allows the APR agent to generate BRTs for at least as many bugs as a dedicated BRT agent, without compromising the generation rate of plausible fixes, thereby reducing engineering effort in maintaining and coordinating separate generation pipelines for fix and BRT at scale.},
 archiveprefix = {arXiv},
 author = {Runxiang Cheng and Michele Tufano and José Cambronero and Renyao Wei and Sherry Shi and Grant Uy and Pat Rondon and Franjo Ivančić},
 eprint = {2601.19066},
 primaryclass = {cs.SE},
 title = {Dynamic Cogeneration of Bug Reproduction Test in Agentic Program Repair},
 url = {https://arxiv.org/abs/2601.19066},
 year = {2026}
}

@misc{cynthia2026bugfixesempiricalinvestigation,
 abstract = {The increasing adoption of AI coding agents has increased the number of agent-generated pull requests (PRs) merged with little or no human intervention. Although such PRs promise productivity gains, their post-merge code quality remains underexplored, as prior work has largely relied on benchmarks and controlled tasks rather than large-scale post-merge analyses. To address this gap, we analyze 1,210 merged agent-generated bug-fix PRs from Python repositories in the AIDev dataset. Using SonarQube, we perform a differential analysis between base and merged commits to identify code quality issues newly introduced by PR changes. We examine issue frequency, density, severity, and rule-level prevalence across five agents. Our results show that apparent differences in raw issue counts across agents largely disappear after normalizing by code churn, indicating that higher issue counts are primarily driven by larger PRs. Across all agents, code smells dominate, particularly at critical and major severities, while bugs are less frequent but often severe. Overall, our findings show that merge success does not reliably reflect post-merge code quality, highlighting the need for systematic quality checks for agent-generated bug-fix PRs.},
 archiveprefix = {arXiv},
 author = {Shamse Tasnim Cynthia and Al Muttakin and Banani Roy},
 doi = {https://doi.org/10.1145/3793302.3793615},
 eprint = {2601.20109},
 primaryclass = {cs.SE},
 title = {Beyond Bug Fixes: An Empirical Investigation of Post-Merge Code Quality Issues in Agent-Generated Pull Requests},
 url = {https://arxiv.org/abs/2601.20109},
 year = {2026}
}

@misc{liu2025repodebugrepositorylevelmultitaskmultilanguage,
 abstract = {Large Language Models (LLMs) have exhibited significant proficiency in code debugging, especially in automatic program repair, which may substantially reduce the time consumption of developers and enhance their efficiency. Significant advancements in debugging datasets have been made to promote the development of code debugging. However, these datasets primarily focus on assessing the LLM's function-level code repair capabilities, neglecting the more complex and realistic repository-level scenarios, which leads to an incomplete understanding of the LLM's challenges in repository-level debugging. While several repository-level datasets have been proposed, they often suffer from limitations such as limited diversity of tasks, languages, and error types. To mitigate this challenge, this paper introduces RepoDebug, a multi-task and multi-language repository-level code debugging dataset with 22 subtypes of errors that supports 8 commonly used programming languages and 3 debugging tasks. Furthermore, we conduct evaluation experiments on 10 LLMs, where Claude 3.5 Sonnect, the best-performing model, still cannot perform well in repository-level debugging.},
 archiveprefix = {arXiv},
 author = {Jingjing Liu and Zeming Liu and Zihao Cheng and Mengliang He and Xiaoming Shi and Yuhang Guo and Xiangrong Zhu and Yuanfang Guo and Yunhong Wang and Haifeng Wang},
 eprint = {2509.04078},
 primaryclass = {cs.SE},
 title = {RepoDebug: Repository-Level Multi-Task and Multi-Language Debugging Evaluation of Large Language Models},
 url = {https://arxiv.org/abs/2509.04078},
 year = {2025}
}

@misc{lv2025vitadtimingviolationawaredebugging,
 abstract = {In modern Very Large Scale Integrated (VLSI) circuit design flow, the Register-Transfer Level (RTL) stage presents a critical opportunity for timing optimization. Addressing timing violations at this early stage is essential, as modern systems demand higher speeds, where even minor timing violations can lead to functional failures or system crashes. However, traditional timing optimization heavily relies on manual expertise, requiring engineers to iteratively analyze timing reports and debug. To automate this process, this paper proposes ViTAD, a method that efficiently analyzes the root causes of timing violations and dynamically generates targeted repair strategies. Specifically, we first parse Verilog code and timing reports to construct a Signal Timing Dependency Graph (STDG). Based on the STDG, we perform violation path analysis and use large language models (LLMs) to infer the root causes of violations. Finally, by analyzing the causes of violations, we selectively retrieve relevant debugging knowledge from a domain-specific knowledge base to generate customized repair solutions. To evaluate the effectiveness of our method, we construct a timing violation dataset based on real-world open-source projects. This dataset contains 54 cases of violations. Experimental results show that our method achieves a 73.68% success rate in repairing timing violations, while the baseline using only LLM is 54.38%. Our method improves the success rate by 19.30%.},
 archiveprefix = {arXiv},
 author = {Wenhao Lv and Yingjie Xia and Xiyuan Chen and Li Kuang},
 eprint = {2508.13257},
 primaryclass = {cs.AR},
 title = {ViTAD: Timing Violation-Aware Debugging of RTL Code using Large Language Models},
 url = {https://arxiv.org/abs/2508.13257},
 year = {2025}
}

@misc{ashraf2025serverlessarchitecturerealtimestock,
 abstract = {The advent of powerful, accessible Large Language Models (LLMs) like Google's Gemini presents new opportunities for democratizing financial data analysis. This paper documents the design, implementation, and iterative debugging of a novel, serverless system for real-time stock analysis. The system leverages the Gemini API for qualitative assessment, automates data ingestion and processing via GitHub Actions, and presents the findings through a decoupled, static frontend. We detail the architectural evolution of the system, from initial concepts to a robust, event-driven pipeline, highlighting the practical challenges encountered during deployment. A significant portion of this paper is dedicated to a case study on the debugging process, covering common software errors, platform-specific permission issues, and rare, environment-level platform bugs. The final architecture operates at a near-zero cost, demonstrating a viable model for individuals to build sophisticated AI-powered financial tools. The operational application is publicly accessible, and the complete source code is available for review. We conclude by discussing the role of LLMs in financial analysis, the importance of robust debugging methodologies, and the emerging paradigm of human-AI collaboration in software development.},
 archiveprefix = {arXiv},
 author = {Taniv Ashraf},
 eprint = {2507.09583},
 primaryclass = {cs.SE},
 title = {A Serverless Architecture for Real-Time Stock Analysis using Large Language Models: An Iterative Development and Debugging Case Study},
 url = {https://arxiv.org/abs/2507.09583},
 year = {2025}
}

@misc{ji2025leveraginglargelanguagemodel,
 abstract = {With the increasing complexity and rapid expansion of the scale of AI systems in cloud platforms, the log data generated during system operation is massive, unstructured, and semantically ambiguous, which brings great challenges to fault location and system self-repair. In order to solve this problem, this paper proposes an intelligent log processing and automatic debugging framework based on Large Language Model (LLM), named Intelligent Debugger (LLM-ID). This method is extended on the basis of the existing pre-trained Transformer model, and integrates a multi-stage semantic inference mechanism to realize the context understanding of system logs and the automatic reconstruction of fault chains. Firstly, the system log is dynamically structured, and the unsupervised clustering and embedding mechanism is used to extract the event template and semantic schema. Subsequently, the fine-tuned LLM combined with the multi-round attention mechanism to perform contextual reasoning on the log sequence to generate potential fault assumptions and root cause paths. Furthermore, this paper introduces a reinforcement learning-based policy-guided recovery planner, which is driven by the remediation strategy generated by LLM to support dynamic decision-making and adaptive debugging in the cloud environment. Compared with the existing rule engine or traditional log analysis system, the proposed model has stronger semantic understanding ability, continuous learning ability and heterogeneous environment adaptability. Experiments on the cloud platform log dataset show that LLM-ID improves the fault location accuracy by 16.2%, which is significantly better than the current mainstream methods},
 archiveprefix = {arXiv},
 author = {Cheng Ji and Huaiying Luo},
 eprint = {2506.17900},
 primaryclass = {cs.AI},
 title = {Leveraging Large Language Model for Intelligent Log Processing and Autonomous Debugging in Cloud AI Platforms},
 url = {https://arxiv.org/abs/2506.17900},
 year = {2025}
}

@misc{straubinger2025mutationtestingiterativelarge,
 abstract = {Large Language Models (LLMs) can generate plausible test code. Intuitively they generate this by imitating tests seen in their training data, rather than reasoning about execution semantics. However, such reasoning is important when applying mutation testing, where individual tests need to demonstrate differences in program behavior between a program and specific artificial defects (mutants). In this paper, we evaluate whether Scientific Debugging, which has been shown to help LLMs when debugging, can also help them to generate tests for mutants. In the resulting approach, LLMs form hypotheses about how to kill specific mutants, and then iteratively generate and refine tests until they succeed, all with detailed explanations for each step. We compare this method to three baselines: (1) directly asking the LLM to generate tests, (2) repeatedly querying the LLM when tests fail, and (3) search-based test generation with Pynguin. Our experiments evaluate these methods based on several factors, including mutation score, code coverage, success rate, and the ability to identify equivalent mutants. The results demonstrate that LLMs, although requiring higher computation cost, consistently outperform Pynguin in generating tests with better fault detection and coverage. Importantly, we observe that the iterative refinement of test cases is important for achieving high-quality test suites.},
 archiveprefix = {arXiv},
 author = {Philipp Straubinger and Marvin Kreis and Stephan Lukasczyk and Gordon Fraser},
 eprint = {2503.08182},
 primaryclass = {cs.SE},
 title = {Mutation Testing via Iterative Large Language Model-Driven Scientific Debugging},
 url = {https://arxiv.org/abs/2503.08182},
 year = {2025}
}

@article{Grishina_2025,
 abstract = {Program synthesis with Large Language Models (LLMs) suffers from a "near-miss syndrome": the generated code closely resembles a correct solution but fails unit tests due to minor errors. We address this with a multi-agent framework called Synthesize, Execute, Instruct, Debug, and Repair (SEIDR). Effectively applying SEIDR to instruction-tuned LLMs requires determining (a) optimal prompts for LLMs, (b) what ranking algorithm selects the best programs in debugging rounds, and (c) balancing the repair of unsuccessful programs with the generation of new ones. We empirically explore these trade-offs by comparing replace-focused, repair-focused, and hybrid debug strategies. We also evaluate lexicase and tournament selection to rank candidates in each generation. On Program Synthesis Benchmark 2 (PSB2), our framework outperforms both conventional use of OpenAI Codex without a repair phase and traditional genetic programming approaches. SEIDR outperforms the use of an LLM alone, solving 18 problems in C++ and 20 in Python on PSB2 at least once across experiments. To assess generalizability, we employ GPT-3.5 and Llama 3 on the PSB2 and HumanEval-X benchmarks. Although SEIDR with these models does not surpass current state-of-the-art methods on the Python benchmarks, the results on HumanEval-C++ are promising. SEIDR with Llama 3-8B achieves an average pass@100 of 84.2%. Across all SEIDR runs, 163 of 164 problems are solved at least once with GPT-3.5 in HumanEval-C++, and 162 of 164 with the smaller Llama 3-8B. We conclude that SEIDR effectively overcomes the near-miss syndrome in program synthesis with LLMs.},
 author = {Grishina, Anastasiia and Liventsev, Vadim and Härmä, Aki and Moonen, Leon},
 doi = {10.1145/3719351},
 issn = {2688-3007},
 journal = {ACM Transactions on Evolutionary Learning and Optimization},
 month = {March},
 number = {1},
 pages = {1–37},
 publisher = {Association for Computing Machinery (ACM)},
 title = {Fully Autonomous Programming Using Iterative Multi-Agent Debugging with Large Language Models},
 url = {http://dx.doi.org/10.1145/3719351},
 volume = {5},
 year = {2025}
}

@misc{adnan2025largelanguagemodelguided,
 abstract = {Automated code generation is gaining significant importance in intelligent computer programming and system deployment. However, current approaches often face challenges in computational efficiency and lack robust mechanisms for code parsing and error correction. In this work, we propose a novel framework, PyCapsule, with a simple yet effective two-agent pipeline and efficient self-debugging modules for Python code generation. PyCapsule features sophisticated prompt inference, iterative error handling, and case testing, ensuring high generation stability, safety, and correctness. Empirically, PyCapsule achieves up to 5.7% improvement of success rate on HumanEval, 10.3% on HumanEval-ET, and 24.4% on BigCodeBench compared to the state-of-art methods. We also observe a decrease in normalized success rate given more self-debugging attempts, potentially affected by limited and noisy error feedback in retention. PyCapsule demonstrates broader impacts on advancing lightweight and efficient code generation for artificial intelligence systems.},
 archiveprefix = {arXiv},
 author = {Muntasir Adnan and Zhiwei Xu and Carlos C. N. Kuhn},
 eprint = {2502.02928},
 primaryclass = {cs.SE},
 title = {Large Language Model Guided Self-Debugging Code Generation},
 url = {https://arxiv.org/abs/2502.02928},
 year = {2025}
}

@misc{brancas2024combininglogiclargelanguage,
 abstract = {Logic programs are a powerful approach for solving NP-Hard problems. However, due to their declarative nature, debugging logic programs poses significant challenges. Unlike procedural paradigms, which allow for step-by-step inspection of program state, logic programs require reasoning about logical statements for fault localization. This complexity is amplified in learning environments due to students' inexperience.
We introduce FormHe, a novel tool that combines logic-based techniques and Large Language Models to identify and correct issues in Answer Set Programming submissions. FormHe consists of two components: a fault localization module and a program repair module. First, the fault localizer identifies a set of faulty program statements requiring modification. Subsequently, FormHe employs program mutation techniques and Large Language Models to repair the flawed ASP program. These repairs can then serve as guidance for students to correct their programs.
Our experiments with real buggy programs submitted by students show that FormHe accurately detects faults in 94% of cases and successfully repairs 58% of incorrect submissions.},
 archiveprefix = {arXiv},
 author = {Ricardo Brancas and Vasco Manquinho and Ruben Martins},
 eprint = {2410.20962},
 primaryclass = {cs.SE},
 title = {Combining Logic with Large Language Models for Automatic Debugging and Repair of ASP Programs},
 url = {https://arxiv.org/abs/2410.20962},
 year = {2024}
}

@inproceedings{Majdoub_2024,
 abstract = {Large language models have shown good potential in supporting software development tasks. This is why more and more developers turn to LLMs (e.g. ChatGPT) to support them in fixing their buggy code. While this can save time and effort, many companies prohibit it due to strict code sharing policies. To address this, companies can run open-source LLMs locally. But until now there is not much research evaluating the performance of open-source large language models in debugging. This work is a preliminary evaluation of the capabilities of open-source LLMs in fixing buggy code. The evaluation covers five open-source large language models and uses the benchmark DebugBench which includes more than 4000 buggy code instances written in Python, Java and C++. Open-source LLMs achieved scores ranging from 43.9% to 66.6% with DeepSeek-Coder achieving the best score for all three programming languages.},
 author = {Majdoub, Yacine and Ben Charrada, Eya},
 booktitle = {Proceedings of the 18th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement},
 collection = {ESEM ’24},
 doi = {10.1145/3674805.3690758},
 month = {October},
 pages = {510–516},
 publisher = {ACM},
 series = {ESEM ’24},
 title = {Debugging with Open-Source Large Language Models: An Evaluation},
 url = {http://dx.doi.org/10.1145/3674805.3690758},
 year = {2024}
}

@misc{song2024effectivelargelanguagemodel,
 abstract = {Large Language Models (LLMs) show promise in code generation tasks. However, their code-writing abilities are often limited in scope: while they can successfully implement simple functions, they struggle with more complex tasks. A fundamental difference with how an LLM writes code, compared to a human programmer, is that it cannot consistently spot and fix bugs. Debugging is a crucial skill for programmers and it enables iterative code refinement towards a correct implementation. In this work, we propose a novel algorithm to enable LLMs to debug their code via self-reflection and search where a model attempts to identify its previous mistakes. Our key contributions are 1) a best-first tree search algorithm with self-reflections (BESTER) that achieves state-of-the-art Pass@1 in three code generation benchmarks. BESTER maintains its superiority when we measure pass rates taking into account additional inference costs incurred by tree search. 2) A novel interpretability study on what self-reflections attend to in buggy programs and how they impact bug fixes, which provides a deeper understanding of the debugging process. 3) An extensive study on when self-reflections are effective in finding bugs.},
 archiveprefix = {arXiv},
 author = {Jialin Song and Jonathan Raiman and Bryan Catanzaro},
 eprint = {2407.19055},
 primaryclass = {cs.SE},
 title = {Effective Large Language Model Debugging with Best-first Tree Search},
 url = {https://arxiv.org/abs/2407.19055},
 year = {2024}
}

@article{Levin_2025,
 abstract = {Debugging is a critical but challenging task for programmers. This paper proposes ChatDBG, an AI-powered debugging assistant. ChatDBG integrates large language models (LLMs) to significantly enhance the capabilities and user-friendliness of conventional debuggers. ChatDBG lets programmers engage in a collaborative dialogue with the debugger, allowing them to pose complex questions about program state, perform root cause analysis for crashes or assertion failures, and explore open-ended queries like "why is x null?". To handle these queries, ChatDBG grants the LLM autonomy to "take the wheel": it can act as an independent agent capable of querying and controlling the debugger to navigate through stacks and inspect program state. It then reports its findings and yields back control to the programmer. By leveraging the real-world knowledge embedded in LLMs, ChatDBG can diagnose issues identifiable only through the use of domain-specific reasoning. Our ChatDBG prototype integrates with standard debuggers including LLDB and GDB for native code and Pdb for Python. Our evaluation across a diverse set of code, including C/C++ code with known bugs and a suite of Python code including standalone scripts and Jupyter notebooks, demonstrates that ChatDBG can successfully analyze root causes, explain bugs, and generate accurate fixes for a wide range of real-world errors. For the Python programs, a single query led to an actionable bug fix 67% of the time; one additional follow-up query increased the success rate to 85%. ChatDBG has seen rapid uptake; it has already been downloaded more than 75,000 times.},
 author = {Levin, Kyla H. and van Kempen, Nicolas and Berger, Emery D. and Freund, Stephen N.},
 doi = {10.1145/3729355},
 issn = {2994-970X},
 journal = {Proceedings of the ACM on Software Engineering},
 month = {June},
 number = {FSE},
 pages = {1892–1913},
 publisher = {Association for Computing Machinery (ACM)},
 title = {ChatDBG: Augmenting Debugging with Large Language Models},
 url = {http://dx.doi.org/10.1145/3729355},
 volume = {2},
 year = {2025}
}

@misc{yao2024hdldebuggerstreamlininghdldebugging,
 abstract = {In the domain of chip design, Hardware Description Languages (HDLs) play a pivotal role. However, due to the complex syntax of HDLs and the limited availability of online resources, debugging HDL codes remains a difficult and time-intensive task, even for seasoned engineers. Consequently, there is a pressing need to develop automated HDL code debugging models, which can alleviate the burden on hardware engineers. Despite the strong capabilities of Large Language Models (LLMs) in generating, completing, and debugging software code, their utilization in the specialized field of HDL debugging has been limited and, to date, has not yielded satisfactory results. In this paper, we propose an LLM-assisted HDL debugging framework, namely HDLdebugger, which consists of HDL debugging data generation via a reverse engineering approach, a search engine for retrieval-augmented generation, and a retrieval-augmented LLM fine-tuning approach. Through the integration of these components, HDLdebugger can automate and streamline HDL debugging for chip design. Our comprehensive experiments, conducted on an HDL code dataset sourced from Huawei, reveal that HDLdebugger outperforms 13 cutting-edge LLM baselines, displaying exceptional effectiveness in HDL code debugging.},
 archiveprefix = {arXiv},
 author = {Xufeng Yao and Haoyang Li and Tsz Ho Chan and Wenyi Xiao and Mingxuan Yuan and Yu Huang and Lei Chen and Bei Yu},
 eprint = {2403.11671},
 primaryclass = {cs.AR},
 title = {HDLdebugger: Streamlining HDL debugging with Large Language Models},
 url = {https://arxiv.org/abs/2403.11671},
 year = {2024}
}

@misc{zhong2024debuglikehumanlarge,
 abstract = {Large language models (LLMs) are leading significant progress in code generation. Beyond one-pass code generation, recent works further integrate unit tests and program verifiers into LLMs to iteratively refine the generated programs. However, these works consider the generated programs as an indivisible entity, which falls short for LLMs in debugging the programs, especially when the programs contain complex logic flows and data operations. In contrast, when human developers debug programs, they typically set breakpoints and selectively examine runtime execution information. The execution flow and the intermediate variables play a crucial role in the debugging process, yet they are underutilized in the existing literature on code generation. In this study, we introduce Large Language Model Debugger (LDB), a novel debugging framework that enables LLMs to refine their generated programs with the runtime execution information. Specifically, LDB segments the programs into basic blocks and tracks the values of intermediate variables after each block throughout the runtime execution. This allows LLMs to concentrate on simpler code units within the overall execution flow, verify their correctness against the task description block by block, and efficiently pinpoint any potential errors. Experiments demonstrate that LDB consistently enhances the baseline performance by up to 9.8% across the HumanEval, MBPP, and TransCoder benchmarks, archiving new state-of-the-art performance in code debugging for various LLM selections.},
 archiveprefix = {arXiv},
 author = {Li Zhong and Zilong Wang and Jingbo Shang},
 eprint = {2402.16906},
 primaryclass = {cs.SE},
 title = {Debug like a Human: A Large Language Model Debugger via Verifying Runtime Execution Step-by-step},
 url = {https://arxiv.org/abs/2402.16906},
 year = {2024}
}

@inproceedings{Fu_2023,
 abstract = {This paper presents LLM4SecHW, a novel framework for hardware debugging that leverages domain specific Large Language Model (LLM). Despite the success of LLMs in automating various software development tasks, their application in the hardware security domain has been limited due to the constraints of commercial LLMs and the scarcity of domain specific data. To address these challenges, we propose a unique approach to compile a dataset of open source hardware design defects and their remediation steps, utilizing version control data. This dataset provides a substantial foundation for training machine learning models for hardware. LLM4SecHW employs fine tuning of medium sized LLMs based on this dataset, enabling the identification and rectification of bugs in hardware designs. This pioneering approach offers a reference workflow for the application of fine tuning domain specific LLMs in other research areas. We evaluate the performance of our proposed system on various open source hardware designs, demonstrating its efficacy in accurately identifying and correcting defects. Our work brings a new perspective on automating the quality control process in hardware design.},
 author = {Fu, Weimin and Yang, Kaichen and Dutta, Raj Gautam and Guo, Xiaolong and Qu, Gang},
 booktitle = {2023 Asian Hardware Oriented Security and Trust Symposium (AsianHOST)},
 doi = {10.1109/asianhost59942.2023.10409307},
 month = {December},
 pages = {1–6},
 publisher = {IEEE},
 title = {LLM4SecHW: Leveraging Domain-Specific Large Language Model for Hardware Debugging},
 url = {http://dx.doi.org/10.1109/AsianHOST59942.2023.10409307},
 year = {2023}
}

@misc{hu2024leveragingprintdebuggingimprove,
 abstract = {Large language models (LLMs) have made significant progress in code generation tasks, but their performance in tackling programming problems with complex data structures and algorithms remains suboptimal. To address this issue, we propose an in-context learning approach that guides LLMs to debug by using a "print debugging" method, which involves inserting print statements to trace and analysing logs for fixing the bug. We collect a Leetcode problem dataset and evaluate our method using the Leetcode online judging system. Experiments with GPT-4 demonstrate the effectiveness of our approach, outperforming rubber duck debugging in easy and medium-level Leetcode problems by 1.5% and 17.9%.},
 archiveprefix = {arXiv},
 author = {Xueyu Hu and Kun Kuang and Jiankai Sun and Hongxia Yang and Fei Wu},
 eprint = {2401.05319},
 primaryclass = {cs.CL},
 title = {Leveraging Print Debugging to Improve Code Generation in Large Language Models},
 url = {https://arxiv.org/abs/2401.05319},
 year = {2024}
}

@misc{tian2024debugbenchevaluatingdebuggingcapability,
 abstract = {Large Language Models (LLMs) have demonstrated exceptional coding capability. However, as another critical component of programming proficiency, the debugging capability of LLMs remains relatively unexplored. Previous evaluations of LLMs' debugging ability are significantly limited by the risk of data leakage, the scale of the dataset, and the variety of tested bugs. To overcome these deficiencies, we introduce `DebugBench', an LLM debugging benchmark consisting of 4,253 instances. It covers four major bug categories and 18 minor types in C++, Java, and Python. To construct DebugBench, we collect code snippets from the LeetCode community, implant bugs into source data with GPT-4, and assure rigorous quality checks. We evaluate two commercial and four open-source models in a zero-shot scenario. We find that (1) while closed-source models exhibit inferior debugging performance compared to humans, open-source models relatively lower pass rate scores; (2) the complexity of debugging notably fluctuates depending on the bug category; (3) incorporating runtime feedback has a clear impact on debugging performance which is not always helpful. As an extension, we also compare LLM debugging and code generation, revealing a strong correlation between them for closed-source models. These findings will benefit the development of LLMs in debugging.},
 archiveprefix = {arXiv},
 author = {Runchu Tian and Yining Ye and Yujia Qin and Xin Cong and Yankai Lin and Yinxu Pan and Yesai Wu and Haotian Hui and Weichuan Liu and Zhiyuan Liu and Maosong Sun},
 eprint = {2401.04621},
 primaryclass = {cs.SE},
 title = {DebugBench: Evaluating Debugging Capability of Large Language Models},
 url = {https://arxiv.org/abs/2401.04621},
 year = {2024}
}

@misc{englhardt2023exploringcharacterizinglargelanguage,
 abstract = {Large language models (LLMs) have shown remarkable abilities to generate code, however their ability to develop software for embedded systems, which requires cross-domain knowledge of hardware and software has not been studied. In this paper we develop an extensible, open source hardware-in-the-loop framework to systematically evaluate leading LLMs (GPT-3.5, GPT-4, PaLM 2) to assess their capabilities and limitations for embedded system development. We observe through our study that even when these tools fail to produce working code, they consistently generate helpful reasoning about embedded design tasks. We leverage this finding to study how human programmers interact with these tools, and develop an human-AI based software engineering workflow for building embedded systems.
Our evaluation platform for verifying LLM generated programs uses sensor actuator pairs for physical evaluation. We compare all three models with N=450 experiments and find surprisingly that GPT-4 especially shows an exceptional level of cross-domain understanding and reasoning, in some cases generating fully correct programs from a single prompt. In N=50 trials, GPT-4 produces functional I2C interfaces 66% of the time. GPT-4 also produces register-level drivers, code for LoRa communication, and context-specific power optimizations for an nRF52 program resulting in over 740x current reduction to 12.2uA. We also characterize the models' limitations to develop a generalizable human-AI workflow for using LLMs in embedded system development. We evaluate our workflow with 15 users including novice and expert programmers. We find that our workflow improves productivity for all users and increases the success rate for building a LoRa environmental sensor from 25% to 100%, including for users with zero hardware or C/C++ experience.},
 archiveprefix = {arXiv},
 author = {Zachary Englhardt and Richard Li and Dilini Nissanka and Zhihan Zhang and Girish Narayanswamy and Joseph Breda and Xin Liu and Shwetak Patel and Vikram Iyer},
 eprint = {2307.03817},
 primaryclass = {cs.SE},
 title = {Exploring and Characterizing Large Language Models For Embedded System Development and Debugging},
 url = {https://arxiv.org/abs/2307.03817},
 year = {2023}
}

@misc{chen2023teachinglargelanguagemodels,
 abstract = {Large language models (LLMs) have achieved impressive performance on code generation. However, for complex programming tasks, generating the correct solution in one go becomes challenging, thus some prior works have designed program repair approaches to improve code generation performance. In this work, we propose Self-Debugging, which teaches a large language model to debug its predicted program via few-shot demonstrations. In particular, we demonstrate that Self-Debugging can teach the large language model to perform rubber duck debugging; i.e., without any human feedback on the code correctness or error messages, the model is able to identify its mistakes by investigating the execution results and explaining the generated code in natural language. Self-Debugging achieves the state-of-the-art performance on several code generation benchmarks, including the Spider dataset for text-to-SQL generation, TransCoder for C++-to-Python translation, and MBPP for text-to-Python generation. On the Spider benchmark where there are no unit tests to verify the correctness of predictions, Self-Debugging with code explanation consistently improves the baseline by 2-3%, and improves the prediction accuracy on problems of the hardest level by 9%. On TransCoder and MBPP where unit tests are available, Self-Debugging improves the baseline accuracy by up to 12%. Meanwhile, by leveraging feedback messages and reusing failed predictions, Self-Debugging notably improves sample efficiency, and can match or outperform baseline models that generate more than 10x candidate programs.},
 archiveprefix = {arXiv},
 author = {Xinyun Chen and Maxwell Lin and Nathanael Schärli and Denny Zhou},
 eprint = {2304.05128},
 primaryclass = {cs.CL},
 title = {Teaching Large Language Models to Self-Debug},
 url = {https://arxiv.org/abs/2304.05128},
 year = {2023}
}

@misc{kang2023explainableautomateddebugginglarge,
 abstract = {Automated debugging techniques have the potential to reduce developer effort in debugging, and have matured enough to be adopted by industry. However, one critical issue with existing techniques is that, while developers want rationales for the provided automatic debugging results, existing techniques are ill-suited to provide them, as their deduction process differs significantly from that of human developers. Inspired by the way developers interact with code when debugging, we propose Automated Scientific Debugging (AutoSD), a technique that given buggy code and a bug-revealing test, prompts large language models to automatically generate hypotheses, uses debuggers to actively interact with buggy code, and thus automatically reach conclusions prior to patch generation. By aligning the reasoning of automated debugging more closely with that of human developers, we aim to produce intelligible explanations of how a specific patch has been generated, with the hope that the explanation will lead to more efficient and accurate developer decisions. Our empirical analysis on three program repair benchmarks shows that AutoSD performs competitively with other program repair baselines, and that it can indicate when it is confident in its results. Furthermore, we perform a human study with 20 participants, including six professional developers, to evaluate the utility of explanations from AutoSD. Participants with access to explanations could judge patch correctness in roughly the same time as those without, but their accuracy improved for five out of six real-world bugs studied: 70% of participants answered that they wanted explanations when using repair tools, while 55% answered that they were satisfied with the Scientific Debugging presentation.},
 archiveprefix = {arXiv},
 author = {Sungmin Kang and Bei Chen and Shin Yoo and Jian-Guang Lou},
 eprint = {2304.02195},
 primaryclass = {cs.SE},
 title = {Explainable Automated Debugging via Large Language Model-driven Scientific Debugging},
 url = {https://arxiv.org/abs/2304.02195},
 year = {2023}
}

@misc{ye2026texttosqlllmsreallydebug,
 abstract = {SQL is central to enterprise data engineering, yet generating fully correct SQL code in a single attempt remains difficult, even for experienced developers and advanced text-to-SQL LLMs, often requiring multiple debugging iterations. We introduce OurBench, the first benchmark for enterprise-level SQL reasoning and debugging. Our benchmark is built on two key innovations: (1) an automated construction workflow that uses reverse engineering to systematically inject realistic bugs into large-scale SQL code, enabling scalable and diverse benchmark generation; and (2) an execution-free evaluation framework tailored to enterprise settings, providing fast, accurate, and resource-efficient assessment.
OurBench comprises 469 OurBenchSyn queries featuring syntax errors with explicit error messages, and 516 OurBenchSem queries targeting semantic errors in which the code fails to meet user intent. The queries are highly complex, averaging over 140 lines and featuring deep and wide abstract syntax trees.
Evaluation of nearly 30 LLMs reveals a substantial performance gap: the best-performing model, Claude-4-Sonnet, achieves only 36.46 percent accuracy on OurBenchSyn and 32.17 percent on OurBenchSem, while most models score below 20 percent. We further explore four solution strategies, identify key challenges, and outline promising directions for enterprise SQL debugging with LLMs.},
 archiveprefix = {arXiv},
 author = {Jing Ye and Yiwen Duan and Yonghong Yu and Victor Ma and Yang Gao and Xing Chen},
 eprint = {2601.18119},
 primaryclass = {cs.AI},
 title = {Beyond Text-to-SQL: Can LLMs Really Debug Enterprise ETL SQL?},
 url = {https://arxiv.org/abs/2601.18119},
 year = {2026}
}

@misc{nandal2026laudellmassistedunittest,
 abstract = {Unit tests are critical in the hardware design lifecycle to ensure that component design modules are functionally correct and conform to the specification before they are integrated at the system level. Thus developing unit tests targeting various design features requires deep understanding of the design functionality and creativity. When one or more unit tests expose a design failure, the debugging engineer needs to diagnose, localize, and debug the failure to ensure design correctness, which is often a painstaking and intense process. In this work, we introduce LAUDE, a unified unit-test generation and debugging framework for hardware designs that cross-pollinates the semantic understanding of the design source code with the Chain-of-Thought (CoT) reasoning capabilities of foundational Large-Language Models (LLMs). LAUDE integrates prompt engineering and design execution information to enhance its unit test generation accuracy and code debuggability. We apply LAUDE with closed- and open-source LLMs to a large corpus of buggy hardware design codes derived from the VerilogEval dataset, where generated unit tests detected bugs in up to 100% and 93% of combinational and sequential designs and debugged up to 93% and 84% of combinational and sequential designs, respectively.},
 archiveprefix = {arXiv},
 author = {Deeksha Nandal and Riccardo Revalor and Soham Dan and Debjit Pal},
 eprint = {2601.08856},
 primaryclass = {cs.SE},
 title = {LAUDE: LLM-Assisted Unit Test Generation and Debugging of Hardware DEsigns},
 url = {https://arxiv.org/abs/2601.08856},
 year = {2026}
}

@misc{ma2026doverinterventiondrivenautodebugging,
 abstract = {Large language model (LLM)-based multi-agent systems are challenging to debug because failures often arise from long, branching interaction traces. The prevailing practice is to leverage LLMs for log-based failure localization, attributing errors to a specific agent and step. However, this paradigm has two key limitations: (i) log-only debugging lacks validation, producing untested hypotheses, and (ii) single-step or single-agent attribution is often ill-posed, as we find that multiple distinct interventions can independently repair the failed task. To address the first limitation, we introduce DoVer, an intervention-driven debugging framework, which augments hypothesis generation with active verification through targeted interventions (e.g., editing messages, altering plans). For the second limitation, rather than evaluating on attribution accuracy, we focus on measuring whether the system resolves the failure or makes quantifiable progress toward task success, reflecting a more outcome-oriented view of debugging. Within the Magnetic-One agent framework, on the datasets derived from GAIA and AssistantBench, DoVer flips 18-28% of failed trials into successes, achieves up to 16% milestone progress, and validates or refutes 30-60% of failure hypotheses. DoVer also performs effectively on a different dataset (GSMPlus) and agent framework (AG2), where it recovers 49% of failed trials. These results highlight intervention as a practical mechanism for improving reliability in agentic systems and open opportunities for more robust, scalable debugging methods for LLM-based multi-agent systems. Project website and code will be available at this https URL.},
 archiveprefix = {arXiv},
 author = {Ming Ma and Jue Zhang and Fangkai Yang and Yu Kang and Qingwei Lin and Saravan Rajmohan and Dongmei Zhang},
 eprint = {2512.06749},
 primaryclass = {cs.AI},
 title = {DoVer: Intervention-Driven Auto Debugging for LLM Multi-Agent Systems},
 url = {https://arxiv.org/abs/2512.06749},
 year = {2026}
}

@misc{bai2025learningdebugllmorganizedknowledge,
 abstract = {Debugging is the dominant cost in modern hardware verification, where assertion failures are among the most frequent and expensive to resolve. While Large Language Models (LLMs) show promise, they often fail to capture the precise, reusable expertise that engineers apply, leading to inaccurate responses. We propose GROVE, a hierarchical knowledge management framework that learns and organizes reusable debugging expertise into an LLM-organized knowledge tree for solving assertion failures. GROVE distills debugging knowledge from prior cases and organizes it into a vertical tree of configurable depth, with each node encoding a concise knowledge item and explicit applicability conditions. During training, GROVE uses a parallel, gradient-free loop where an LLM proposes tree modifications as structured JSON edits by learning from the cases. At test time, a budget-aware iterative zoom is performed to navigate the tree, retrieving a small set of applicable knowledge items that guide a base LLM's hypothesis generation and fix proposals. Evaluated on a suite of assertion-failure cases, GROVE delivers consistent gains in pass@1 and pass@5, demonstrating the value of structured knowledge evolution.},
 archiveprefix = {arXiv},
 author = {Yunsheng Bai and Haoxing Ren},
 eprint = {2511.17833},
 primaryclass = {cs.AI},
 title = {Learning to Debug: LLM-Organized Knowledge Trees for Solving RTL Assertion Failures},
 url = {https://arxiv.org/abs/2511.17833},
 year = {2025}
}

@misc{hsieh2025dualprocessscaffoldreasoningenhancing,
 abstract = {Recent LLMs have demonstrated sophisticated problem-solving capabilities on various benchmarks through advanced reasoning algorithms. However, the key research question of identifying reasoning steps that balance complexity and computational efficiency remains unsolved. Recent research has increasingly drawn upon psychological theories to explore strategies for optimizing cognitive pathways. The LLM's final outputs and intermediate steps are regarded as System 1 and System 2, respectively. However, an in-depth exploration of the System 2 reasoning is still lacking. Therefore, we propose a novel psychologically backed Scaffold Reasoning framework for code debugging, which encompasses the Scaffold Stream, Analytic Stream, and Integration Stream. The construction of reference code within the Scaffold Stream is integrated with the buggy code analysis results produced by the Analytic Stream through the Integration Stream. Our framework achieves an 88.91% pass rate and an average inference time of 5.36 seconds per-problem on DebugBench, outperforming other reasoning approaches across various LLMs in both reasoning accuracy and efficiency. Further analyses elucidate the advantages and limitations of various cognitive pathways across varying problem difficulties and bug types. Our findings also corroborate the alignment of the proposed Scaffold Reasoning framework with human cognitive processes.},
 archiveprefix = {arXiv},
 author = {Po-Chung Hsieh and Chin-Po Chen and Jeng-Lin Li and Ming-Ching Chang},
 eprint = {2511.08052},
 primaryclass = {cs.AI},
 title = {Dual-Process Scaffold Reasoning for Enhancing LLM Code Debugging},
 url = {https://arxiv.org/abs/2511.08052},
 year = {2025}
}

@misc{bai2025fvdebugllmdrivendebuggingassistant,
 abstract = {Debugging formal verification (FV) failures represents one of the most time-consuming bottlenecks in modern hardware design workflows. When properties fail, engineers must manually trace through complex counter-examples spanning multiple cycles, analyze waveforms, and cross-reference design specifications to identify root causes - a process that can consume hours or days per bug. Existing solutions are largely limited to manual waveform viewers or simple automated tools that cannot reason about the complex interplay between design intent and implementation logic. We present FVDebug, an intelligent system that automates root-cause analysis by combining multiple data sources - waveforms, RTL code, design specifications - to transform failure traces into actionable insights. Our approach features a novel pipeline: (1) Causal Graph Synthesis that structures failure traces into directed acyclic graphs, (2) Graph Scanner using batched Large Language Model (LLM) analysis with for-and-against prompting to identify suspicious nodes, and (3) Insight Rover leveraging agentic narrative exploration to generate high-level causal explanations. FVDebug further provides concrete RTL fixes through its Fix Generator. Evaluated on open benchmarks, FVDebug attains high hypothesis quality and strong Pass@k fix rates. We further report results on two proprietary, production-scale FV counterexamples. These results demonstrate FVDebug's applicability from academic benchmarks to industrial designs.},
 archiveprefix = {arXiv},
 author = {Yunsheng Bai and Ghaith Bany Hamad and Chia-Tung Ho and Syed Suhaib and Haoxing Ren},
 eprint = {2510.15906},
 primaryclass = {cs.AR},
 title = {FVDebug: An LLM-Driven Debugging Assistant for Automated Root Cause Analysis of Formal Verification Failures},
 url = {https://arxiv.org/abs/2510.15906},
 year = {2025}
}

@misc{fu2025debugtallmbasedagentsimplifying,
 abstract = {In programming education, Debugging and Teaching (DT) task is a common scenario where students receive assistance in correcting their erroneous code. The task involves multiple inputs, including erroneous code, error messages, reference solutions, and the question description, with the goal of generating modification suggestions to the erroneous code. However, two key challenges hinder the effectiveness of existing approaches. Firstly, the complexity and heterogeneity of inputs inherent in DT tasks significantly elevate the reasoning challenges faced by LLMs. Second, existing approaches often fail to fully leverage the availability of standard code in DT tasks, forcing models to rely solely on complex multi-step reasoning, which limits the potential of LLMs in addressing DT tasks effectively. To address these challenges, we propose DebugTA, a novel LLM-based debugging and teaching agent with specialized tools for standard code retrieval, variable substitution to align reference code, and an external compiler for real-time code analysis. Guided by explicit pedagogical and debugging principles, DebugTA acts as an agent that decomposes a complex task into sequential LLM interactions, each utilizing distinct tools for specific subtasks, thereby simplifying the logical reasoning at each step and reducing overall reasoning complexity. Furthermore, DebugTA utilizes tool calls to align the standard code with the erroneous code as much as possible, allowing the LLM to focus on logic errors within the erroneous code and improving the accuracy of the generated suggestions. To rigorously assess the quality of modification suggestions, we introduce a student simulator-teacher interaction paradigm. Experimental results on three real-world code datasets demonstrate that DebugTA consistently improves teaching effectiveness while significantly reducing computational costs.},
 archiveprefix = {arXiv},
 author = {Lingyue Fu and Haowei Yuan and Datong Chen and Xinyi Dai and Qingyao Li and Weinan Zhang and Weiwen Liu and Yong Yu},
 eprint = {2510.11076},
 primaryclass = {cs.SE},
 title = {DebugTA: An LLM-Based Agent for Simplifying Debugging and Teaching in Programming Education},
 url = {https://arxiv.org/abs/2510.11076},
 year = {2025}
}

@misc{ozerova2025tgprtreeguidedpolicyrefinement,
 abstract = {Iterative refinement has been a promising paradigm to enable large language models (LLMs) to resolve difficult reasoning and problem-solving tasks. One of the key challenges, however, is how to effectively search through the enormous search space of possible refinements. Existing methods typically fall back on predefined heuristics, which are troubled by the exploration-exploitation dilemma and cannot adapt based on past refinement outcomes. We introduce Tree-Guided Policy Refinement (TGPR), a novel framework that combines GRPO with a Thompson-Sampling-based tree search. TGPR explores both failed and successful refinement paths actively, with denser training trajectories and more adaptive policies. On HumanEval, MBPP, and APPS benchmarks, our method achieves up to +4.2 percentage points absolute improvement in pass@1 (on MBPP) and up to +12.51 percentage points absolute improvement in pass@10 (on APPS) compared to a competitive GRPO baseline. Apart from debugging code, TGPR focuses on a principled approach to combining learned policies with structured search methods, offering a general framework for enhancing iterative refinement and stateful reasoning in LLMs.},
 archiveprefix = {arXiv},
 author = {Daria Ozerova and Ekaterina Trofimova},
 eprint = {2510.06878},
 primaryclass = {cs.AI},
 title = {TGPR: Tree-Guided Policy Refinement for Robust Self-Debugging of LLMs},
 url = {https://arxiv.org/abs/2510.06878},
 year = {2025}
}

@misc{pei2025llmexecutionestimatorrecovering,
 abstract = {Determining the dynamic data dependency of a step that reads a variable v
is challenging. It typically requires either exhaustive instrumentation, which becomes prohibitively expensive when v
is defined within library calls, or repeated executions, which are impractical for non-deterministic programs. In this work, we propose RecovSlicing for computing dynamic data dependency in a single run, with only partial instrumentation. We explore the intuition that LLM can potentially infer program dynamics based on a partially recorded trace and relevant code as its context. Given (1) a partially recorded trace of a program P
and (2) the slicing criteria consisting of a query step s
and a query variable v
read by s
, RecovSlicing computes the runtime definition of v
on the trace by estimating the miss-recorded execution of P
. In this work, we allow the user to specify implicit query variable. Technically, built upon non-deterministic LLM, we address the challenges of (1) precise recovery of runtime variable value and structure from the recorded execution and (2) aligning the memory address of recovered variables and the recorded variables for definition analysis. We evaluate RecovSlicing on 8300 data dependencies across three slicing benchmarks, comparing it with Slicer4J, ND-Slicer, LLM Slicer, and re-execution Slicer. RecovSlicing achieves significantly higher accuracy (80.3%, 91.1%, 98.3%) and recall (up to 98.3%) than the best baseline (accuracy: 39.0%, 82.0%, 59.9%; recall: 53.4%, 79.1%, 87.1%). Integrated into a dual-slicing regression bug localizer, it identifies 16% more regressions.},
 archiveprefix = {arXiv},
 author = {Yunrui Pei and Hongshu Wang and Wenjie Zhang and Yun Lin and Weiyu Kong and Jin song Dong},
 eprint = {2508.18721},
 primaryclass = {cs.SE},
 title = {LLM as an Execution Estimator: Recovering Missing Dependency for Practical Time-travelling Debugging},
 url = {https://arxiv.org/abs/2508.18721},
 year = {2025}
}

@misc{schiese2025posthocllmsupporteddebuggingdistributed,
 abstract = {In this paper, we address the problem of manual debugging, which nowadays remains resource-intensive and in some parts archaic. This problem is especially evident in increasingly complex and distributed software systems. Therefore, our objective of this work is to introduce an approach that can possibly be applied to any system, at both the macro- and micro-level, to ease this debugging process. This approach utilizes a system's process data, in conjunction with generative AI, to generate natural-language explanations. These explanations are generated from the actual process data, interface information, and documentation to guide the developers more efficiently to understand the behavior and possible errors of a process and its sub-processes. Here, we present a demonstrator that employs this approach on a component-based Java system. However, our approach is language-agnostic. Ideally, the generated explanations will provide a good understanding of the process, even if developers are not familiar with all the details of the considered system. Our demonstrator is provided as an open-source web application that is freely accessible to all users.},
 archiveprefix = {arXiv},
 author = {Dennis Schiese and Andreas Both},
 eprint = {2508.14540},
 primaryclass = {cs.SE},
 title = {Post-hoc LLM-Supported Debugging of Distributed Processes},
 url = {https://arxiv.org/abs/2508.14540},
 year = {2025}
}

@misc{tang2025mahlmultiagentllmguidedhierarchical,
 abstract = {As program workloads (e.g., AI) increase in size and algorithmic complexity, the primary challenge lies in their high dimensionality, encompassing computing cores, array sizes, and memory hierarchies. To overcome these obstacles, innovative approaches are required. Agile chip design has already benefited from machine learning integration at various stages, including logic synthesis, placement, and routing. With Large Language Models (LLMs) recently demonstrating impressive proficiency in Hardware Description Language (HDL) generation, it is promising to extend their abilities to 2.5D integration, an advanced technique that saves area overhead and development costs. However, LLM-driven chiplet design faces challenges such as flatten design, high validation cost and imprecise parameter optimization, which limit its chiplet design capability. To address this, we propose MAHL, a hierarchical LLM-based chiplet design generation framework that features six agents which collaboratively enable AI algorithm-hardware mapping, including hierarchical description generation, retrieval-augmented code generation, diverseflow-based validation, and multi-granularity design space exploration. These components together enhance the efficient generation of chiplet design with optimized Power, Performance and Area (PPA). Experiments show that MAHL not only significantly improves the generation accuracy of simple RTL design, but also increases the generation accuracy of real-world chiplet design, evaluated by Pass@5, from 0 to 0.72 compared to conventional LLMs under the best-case scenario. Compared to state-of-the-art CLARIE (expert-based), MAHL achieves comparable or even superior PPA results under certain optimization objectives.},
 archiveprefix = {arXiv},
 author = {Jinwei Tang and Jiayin Qin and Nuo Xu and Pragnya Sudershan Nalla and Yu Cao and Yang and Zhao and Caiwen Ding},
 eprint = {2508.14053},
 primaryclass = {cs.AR},
 title = {MAHL: Multi-Agent LLM-Guided Hierarchical Chiplet Design with Adaptive Debugging},
 url = {https://arxiv.org/abs/2508.14053},
 year = {2025}
}

@misc{kleijwegt2025toolsupportingdebuggingunderstanding,
 abstract = {Normative requirements specify social, legal, ethical, empathetic, and cultural (SLEEC) norms that must be observed by a system. To support the identification of SLEEC requirements, numerous standards and regulations have been developed. These requirements are typically defined by stakeholders in the non-technical system with diverse expertise (e.g., ethicists, lawyers, social scientists). Hence, ensuring their consistency and managing the requirement elicitation process are complex and error-prone tasks. Recent research has addressed this challenge using domain-specific languages to specify normative requirements as rules, whose consistency can then be analyzed with formal methods. Nevertheless, these approaches often present the results from formal verification tools in a way that is inaccessible to non-technical users. This hinders understanding and makes the iterative process of eliciting and validating these requirements inefficient in terms of both time and effort. To address this problem, we introduce SLEEC-LLM, a tool that uses large language models (LLMs) to provide natural-language interpretations for model-checking counterexamples corresponding to SLEEC rule inconsistencies. SLEEC-LLM improves the efficiency and explainability of normative requirements elicitation and consistency analysis. To demonstrate its effectiveness, we summarise its use in two real-world case studies involving non-technical stakeholders.},
 archiveprefix = {arXiv},
 author = {Alex Kleijwegt and Sinem Getir Yaman and Radu Calinescu},
 eprint = {2507.05504},
 primaryclass = {cs.SE},
 title = {Tool for Supporting Debugging and Understanding of Normative Requirements Using LLMs},
 url = {https://arxiv.org/abs/2507.05504},
 year = {2025}
}

@article{Adnan_2025,
 abstract = {The effectiveness of AI debugging follows a predictable exponential decay pattern; most models lose 60-80% of their debugging capability within just 2-3 attempts, despite iterative debugging being a critical capability for practical code generation systems. We introduce the Debugging Decay Index (DDI), a mathematical framework that quantifies when debugging becomes ineffective and predicts intervention points. Our strategic fresh start approach shifts from exploitation to exploration at strategic points in the debugging process, demonstrating that well-timed interventions can rescue the effectiveness of debugging. DDI reveals a fundamental limitation in current AI debugging and provides the first quantitative framework for optimising iterative code generation strategies.},
 author = {Adnan, Muntasir and Kuhn, Carlos C. N.},
 doi = {10.1038/s41598-025-27846-5},
 issn = {2045-2322},
 journal = {Scientific Reports},
 month = {December},
 number = {1},
 publisher = {Springer Science and Business Media LLC},
 title = {Measuring and mitigating debugging effectiveness decay in code language models},
 url = {http://dx.doi.org/10.1038/s41598-025-27846-5},
 volume = {15},
 year = {2025}
}

@misc{bo2025whosleaderanalyzingnovice,
 abstract = {While LLMs are often touted as tools for democratizing specialized knowledge to beginners, their actual effectiveness for improving task performance and learning is still an open question. It is known that novices engage with LLMs differently from experts, with prior studies reporting meta-cognitive pitfalls that affect novices' ability to verify outputs and prompt effectively. We focus on a task domain, machine learning (ML), which embodies both high complexity and low verifiability to understand the impact of LLM assistance on novices. Provided a buggy ML script and open access to ChatGPT, we conduct a formative study with eight novice ML engineers to understand their reliance on, interactions with, and perceptions of the LLM. We find that user actions can be roughly categorized into leading the LLM and led-by the LLM, and further investigate how they affect reliance outcomes like over- and under-reliance. These results have implications on novices' cognitive engagement in LLM-assisted tasks and potential negative effects on downstream learning. Lastly, we pose potential augmentations to the novice-LLM interaction paradigm to promote cognitive engagement.},
 archiveprefix = {arXiv},
 author = {Jessica Y. Bo and Majeed Kazemitabaar and Emma Zhuang and Ashton Anderson},
 eprint = {2505.08063},
 primaryclass = {cs.HC},
 title = {Who's the Leader? Analyzing Novice Workflows in LLM-Assisted Debugging of Machine Learning Code},
 url = {https://arxiv.org/abs/2505.08063},
 year = {2025}
}

@misc{ashrafi2025enhancingllmcodegeneration,
 abstract = {The use of large language models (LLMs) for automated code generation has emerged as a significant focus within AI research. As these pretrained models continue to evolve, their ability to understand and generate complex code structures has opened new possibilities for automating intricate programming tasks for the sake of accurate code generation. Although contemporary foundational models demonstrate promoting results, researchers continue to explore optimal post-training strategies to enhance code quality. These include supervised fine-tuning, retrieval-augmented generation (RAG), debugging, and many others. In this paper, we combine two widely used approaches namely multi-agent collaboration and runtime execution information-based debugging, for improving code generation functionality, reliability, and practical applicability. We perform an empirical study in order to extend the evaluation of the individual strategies as well as the proposed composition of the activities of both strategies. Our study use 19 LLMs to examines the performance of individual and the proposed strategies, offering comprehensive insights into how different programming activities compositions and training paradigms influence code generation effectiveness. In particular, we implement a chained system that combines both strategies to assess their combined impact on functional accuracy, code reliability, and generation latency using two benchmark datasets commonly used for code generation. Our findings provide valuable insights for organizations seeking robust AI-driven coding solutions by guiding them in selecting models that can better adapt to complex post-training strategies, ultimately fostering the adoption of more effective and reliable code generation technologies.},
 archiveprefix = {arXiv},
 author = {Nazmus Ashrafi and Salah Bouktif and Mohammed Mediani},
 eprint = {2505.02133},
 primaryclass = {cs.SE},
 title = {Enhancing LLM Code Generation: A Systematic Evaluation of Multi-Agent Collaboration and Runtime Debugging for Improved Accuracy, Reliability, and Latency},
 url = {https://arxiv.org/abs/2505.02133},
 year = {2025}
}

@misc{wang2025veridebugunifiedllmverilog,
 abstract = {Large Language Models (LLMs) have demonstrated remarkable potential in debugging for various programming languages. However, the application of LLMs to Verilog debugging remains insufficiently explored. Here, we present VeriDebug, an approach that integrates contrastive representation and guided correction capabilities for automated Verilog debugging. Unlike existing methods, VeriDebug employs an embedding-based technique to accurately retrieve internal information, followed by bug-fixing. VeriDebug unifies Verilog bug detection and correction through a shared parameter space. By simultaneously learning bug patterns and fixes, it streamlines debugging via contrastive embedding and guided correction. Empirical results show the efficacy of VeriDebug in enhancing Verilog debugging. Our VeriDebugLoc, Type model achieves 64.7 accuracy in bug fixing (Acc1), a significant improvement from the existing open-source SOTAs 11.3. This performance not only outperforms open-source alternatives but also exceeds larger closed-source models like GPT-3.5-turbo (36.6), offering a more accurate alternative to conventional debugging methods.},
 archiveprefix = {arXiv},
 author = {Ning Wang and Bingkun Yao and Jie Zhou and Yuchen Hu and Xi Wang and Nan Guan and Zhe Jiang},
 eprint = {2504.19099},
 primaryclass = {cs.SE},
 title = {VeriDebug: A Unified LLM for Verilog Debugging via Contrastive Embedding and Guided Correction},
 url = {https://arxiv.org/abs/2504.19099},
 year = {2025}
}

@misc{salmon2025debuggingerrormessagesllm,
 abstract = {Making errors is part of the programming process -- even for the most seasoned professionals. Novices in particular are bound to make many errors while learning. It is well known that traditional (compiler/interpreter) programming error messages have been less than helpful for many novices and can have effects such as being frustrating, containing confusing jargon, and being downright misleading. Recent work has found that large language models (LLMs) can generate excellent error explanations, but that the effectiveness of these error messages heavily depends on whether the LLM has been provided with context -- typically the original source code where the problem occurred. Knowing that programming error messages can be misleading and/or contain that serves little-to-no use (particularly for novices) we explore the reverse: what happens when GPT-3.5 is prompted for error explanations on just the erroneous source code itself -- original compiler/interpreter produced error message excluded. We utilized various strategies to make more effective error explanations, including one-shot prompting and fine-tuning. We report the baseline results of how effective the error explanations are at providing feedback, as well as how various prompting strategies might improve the explanations' effectiveness. Our results can help educators by understanding how LLMs respond to such prompts that novices are bound to make, and hopefully lead to more effective use of Generative AI in the classroom.},
 archiveprefix = {arXiv},
 author = {Audrey Salmon and Katie Hammer and Eddie Antonio Santos and Brett A. Becker},
 eprint = {2501.05706},
 primaryclass = {cs.SE},
 title = {Debugging Without Error Messages: How LLM Prompting Strategy Affects Programming Error Explanation Effectiveness},
 url = {https://arxiv.org/abs/2501.05706},
 year = {2025}
}

@misc{yang2025coastenhancingcodedebugging,
 abstract = {Code debugging is a vital stage of software development, essential for ensuring the reliability and performance of Large Language Models (LLMs) in the code generation task. Human debugging typically follows a multi-stage process, which includes Bug Localization, Bug Identification, Code Repair, and Code Recognition. However, existing code debugging benchmarks predominantly focus on the Code Repair stage, which offers only a limited perspective on evaluating the debugging capabilities of LLMs. In this paper, we introduce DEBUGEVAL, a comprehensive benchmark for evaluating the debugging abilities of LLMs by emulating the multi-stage human debugging process. Through evaluating on DEBUGEVAL, we observe that 7B-scale models consistently underperform compared to their larger counterparts, highlighting their limitations in comprehending code semantics. In this case, we propose the COmmunicative Agent-based data SynThesis (COAST) framework, which employs a multi-agent system to generate high-quality training data for supervised fine-tuning (SFT). Experimental results demonstrate that COAST-generated data outperform human-curated and GPT-4-generated data, enabling 7B-scale LLMs to achieve debugging performance comparable to GPT-3.5. All data and codes are available at this https URL.},
 archiveprefix = {arXiv},
 author = {Weiqing Yang and Hanbin Wang and Zhenghao Liu and Xinze Li and Yukun Yan and Shuo Wang and Yu Gu and Minghe Yu and Zhiyuan Liu and Ge Yu},
 eprint = {2408.05006},
 primaryclass = {cs.SE},
 title = {COAST: Enhancing the Code Debugging Ability of LLMs through Communicative Agent Based Data Synthesis},
 url = {https://arxiv.org/abs/2408.05006},
 year = {2025}
}

@misc{kargupta2024instructassistllmbasedmultiturn,
 abstract = {Socratic questioning is an effective teaching strategy, encouraging critical thinking and problem-solving. The conversational capabilities of large language models (LLMs) show great potential for providing scalable, real-time student guidance. However, current LLMs often give away solutions directly, making them ineffective instructors. We tackle this issue in the code debugging domain with TreeInstruct, an Instructor agent guided by a novel state space-based planning algorithm. TreeInstruct asks probing questions to help students independently identify and resolve errors. It estimates a student's conceptual and syntactical knowledge to dynamically construct a question tree based on their responses and current knowledge state, effectively addressing both independent and dependent mistakes concurrently in a multi-turn interaction setting. In addition to using an existing single-bug debugging benchmark, we construct a more challenging multi-bug dataset of 150 coding problems, incorrect solutions, and bug fixes -- all carefully constructed and annotated by experts. Extensive evaluation shows TreeInstruct's state-of-the-art performance on both datasets, proving it to be a more effective instructor than baselines. Furthermore, a real-world case study with five students of varying skill levels further demonstrates TreeInstruct's ability to guide students to debug their code efficiently with minimal turns and highly Socratic questioning.},
 archiveprefix = {arXiv},
 author = {Priyanka Kargupta and Ishika Agarwal and Dilek Hakkani-Tur and Jiawei Han},
 eprint = {2406.11709},
 primaryclass = {cs.CL},
 title = {Instruct, Not Assist: LLM-based Multi-Turn Planning and Hierarchical Questioning for Socratic Code Debugging},
 url = {https://arxiv.org/abs/2406.11709},
 year = {2024}
}

@misc{jiang2025ledextrainingllmsbetter,
 abstract = {In the domain of code generation, self-debugging is crucial. It allows LLMs to refine their generated code based on execution feedback. This is particularly important because generating correct solutions in one attempt proves challenging for complex tasks. Prior works on self-debugging mostly focus on prompting methods by providing LLMs with few-shot examples, which work poorly on small open-sourced LLMs. In this work, we propose LeDex, a training framework that significantly improves the self-debugging capability of LLMs. Intuitively, we observe that a chain of explanations on the wrong code followed by code refinement helps LLMs better analyze the wrong code and do refinement. We thus propose an automated pipeline to collect a high-quality dataset for code explanation and refinement by generating a number of explanations and refinement trajectories from the LLM itself or a larger teacher model and filtering via execution verification. We perform supervised fine-tuning (SFT) and further reinforcement learning (RL) on both success and failure trajectories with a novel reward design considering code explanation and refinement quality. SFT improves the pass@1 by up to 15.92% and pass@10 by 9.30% over four benchmarks. RL training brings additional up to 3.54% improvement on pass@1 and 2.55% improvement on pass@10. The trained LLMs show iterative refinement ability and can keep refining code continuously. Lastly, our human evaluation shows that the LLMs trained with our framework generate more useful code explanations and help developers better understand bugs in source code.},
 archiveprefix = {arXiv},
 author = {Nan Jiang and Xiaopeng Li and Shiqi Wang and Qiang Zhou and Soneya Binta Hossain and Baishakhi Ray and Varun Kumar and Xiaofei Ma and Anoop Deoras},
 eprint = {2405.18649},
 primaryclass = {cs.CL},
 title = {LeDex: Training LLMs to Better Self-Debug and Explain Code},
 url = {https://arxiv.org/abs/2405.18649},
 year = {2025}
}

@misc{xu2024meicrethinkingrtldebug,
 abstract = {The deployment of Large Language Models (LLMs) for code debugging (e.g., C and Python) is widespread, benefiting from their ability to understand and interpret intricate concepts. However, in the semiconductor industry, utilising LLMs to debug Register Transfer Level (RTL) code is still insufficient, largely due to the underrepresentation of RTL-specific data in training sets. This work introduces a novel framework, Make Each Iteration Count (MEIC), which contrasts with traditional one-shot LLM-based debugging methods that heavily rely on prompt engineering, model tuning, and model training. MEIC utilises LLMs in an iterative process to overcome the limitation of LLMs in RTL code debugging, which is suitable for identifying and correcting both syntax and function errors, while effectively managing the uncertainties inherent in LLM operations. To evaluate our framework, we provide an open-source dataset comprising 178 common RTL programming errors. The experimental results demonstrate that the proposed debugging framework achieves fix rate of 93% for syntax errors and 78% for function errors, with up to 48x speedup in debugging processes when compared with experienced engineers. The Repo. of dataset and code: this https URL.},
 archiveprefix = {arXiv},
 author = {Ke Xu and Jialin Sun and Yuchen Hu and Xinwei Fang and Weiwei Shan and Xi Wang and Zhe Jiang},
 eprint = {2405.06840},
 primaryclass = {cs.AR},
 title = {MEIC: Re-thinking RTL Debug Automation using LLMs},
 url = {https://arxiv.org/abs/2405.06840},
 year = {2024}
}

@misc{saben2024enablingblvdevelopersllmdriven,
 abstract = {BLVRUN is a command line shell script designed to offer developers within the BLV community a succinct and insightful overview of traceback errors. Its primary function involves parsing errors and utilizing a refined large language model to generate informative error summaries. In terms of performance, our model rivals that of well-known models like ChatGPT or AI-chatbot plug-ins tailored for specific Integrated Development Environments (IDEs). Importantly, BLV users can seamlessly integrate this tool into their existing development workflows, eliminating the need for any modifications or adaptations to facilitate debugging tasks.},
 archiveprefix = {arXiv},
 author = {Clark Saben and Prashant Chandrasekar},
 eprint = {2401.16654},
 primaryclass = {cs.HC},
 title = {Enabling BLV Developers with LLM-driven Code Debugging},
 url = {https://arxiv.org/abs/2401.16654},
 year = {2024}
}

@misc{lee2023githubrecentbugsdataset,
 abstract = {Large Language Models (LLMs) have demonstrated strong natural language processing and code synthesis capabilities, which has led to their rapid adoption in software engineering applications. However, details about LLM training data are often not made public, which has caused concern as to whether existing bug benchmarks are included. In lieu of the training data for the popular GPT models, we examine the training data of the open-source LLM StarCoder, and find it likely that data from the widely used Defects4J benchmark was included, raising the possibility of its inclusion in GPT training data as well. This makes it difficult to tell how well LLM-based results on Defects4J would generalize, as for any results it would be unclear whether a technique's performance is due to LLM generalization or memorization. To remedy this issue and facilitate continued research on LLM-based SE, we present the GitHub Recent Bugs (GHRB) dataset, which includes 76 real-world Java bugs that were gathered after the OpenAI data cut-off point.},
 archiveprefix = {arXiv},
 author = {Jae Yong Lee and Sungmin Kang and Juyeon Yoon and Shin Yoo},
 eprint = {2310.13229},
 primaryclass = {cs.SE},
 title = {The GitHub Recent Bugs Dataset for Evaluating LLM-based Debugging Applications},
 url = {https://arxiv.org/abs/2310.13229},
 year = {2023}
}

@inbook{Ma_2024,
 abstract = {Large Language Models (LLMs) now excel at generative skills and can create content at impeccable speeds. However, they are imperfect and still make various mistakes. In a Computer Science education context, as these models are widely recognized as "AI pair programmers," it becomes increasingly important to train students on evaluating and debugging the LLM-generated code. In this work, we introduce HypoCompass, a novel system to facilitate deliberate practice on debugging, where human novices play the role of Teaching Assistants and help LLM-powered teachable agents debug code. We enable effective task delegation between students and LLMs in this learning-by-teaching environment: students focus on hypothesizing the cause of code errors, while adjacent skills like code completion are offloaded to LLM-agents. Our evaluations demonstrate that HypoCompass generates high-quality training materials (e.g., bugs and fixes), outperforming human counterparts fourfold in efficiency, and significantly improves student performance on debugging by 12% in the pre-to-post test.},
 author = {Ma, Qianou and Shen, Hua and Koedinger, Kenneth and Wu, Sherry Tongshuang},
 booktitle = {Artificial Intelligence in Education},
 doi = {10.1007/978-3-031-64302-6_19},
 isbn = {9783031643026},
 issn = {1611-3349},
 pages = {265–279},
 publisher = {Springer Nature Switzerland},
 title = {How to Teach Programming in the AI Era? Using LLMs as a Teachable Agent for Debugging},
 url = {http://dx.doi.org/10.1007/978-3-031-64302-6_19},
 year = {2024}
}

@misc{temyingyong2025roadreflectiveoptimizationautomated,
 abstract = {Automatic Prompt Optimization (APO) has emerged as a critical technique for enhancing Large Language Model (LLM) performance, yet current state-of-the-art methods typically rely on large, labeled gold-standard development sets to compute fitness scores for evolutionary or Reinforcement Learning (RL) approaches. In real-world software engineering, however, such curated datasets are rarely available during the initial cold start of agent development, where engineers instead face messy production logs and evolving failure modes. We present ROAD (Reflective Optimization via Automated Debugging), a novel framework that bypasses the need for refined datasets by treating optimization as a dynamic debugging investigation rather than a stochastic search. Unlike traditional mutation strategies, ROAD utilizes a specialized multi-agent architecture, comprising an Analyzer for root-cause analysis, an Optimizer for pattern aggregation, and a Coach for strategy integration, to convert unstructured failure logs into robust, structured Decision Tree Protocols. We evaluated ROAD across both a standardized academic benchmark and a live production Knowledge Management engine. Experimental results demonstrate that ROAD is highly sample-efficient, achieving a 5.6 percent increase in success rate (73.6 percent to 79.2 percent) and a 3.8 percent increase in search accuracy within just three automated iterations. Furthermore, on complex reasoning tasks in the retail domain, ROAD improved agent performance by approximately 19 percent relative to the baseline. These findings suggest that mimicking the human engineering loop of failure analysis and patching offers a viable, data-efficient alternative to resource-intensive RL training for deploying reliable LLM agents.},
 archiveprefix = {arXiv},
 author = {Natchaya Temyingyong and Daman Jain and Neeraj Kumarsahu and Prabhat Kumar and Rachata Phondi and Wachiravit Modecrua and Krittanon Kaewtawee and Krittin Pachtrachai and Touchapon Kraisingkorn},
 eprint = {2512.24040},
 primaryclass = {cs.AI},
 title = {ROAD: Reflective Optimization via Automated Debugging for Zero-Shot Agent Alignment},
 url = {https://arxiv.org/abs/2512.24040},
 year = {2025}
}

@misc{ma2026doverinterventiondrivenautodebugging,
 abstract = {Large language model (LLM)-based multi-agent systems are challenging to debug because failures often arise from long, branching interaction traces. The prevailing practice is to leverage LLMs for log-based failure localization, attributing errors to a specific agent and step. However, this paradigm has two key limitations: (i) log-only debugging lacks validation, producing untested hypotheses, and (ii) single-step or single-agent attribution is often ill-posed, as we find that multiple distinct interventions can independently repair the failed task. To address the first limitation, we introduce DoVer, an intervention-driven debugging framework, which augments hypothesis generation with active verification through targeted interventions (e.g., editing messages, altering plans). For the second limitation, rather than evaluating on attribution accuracy, we focus on measuring whether the system resolves the failure or makes quantifiable progress toward task success, reflecting a more outcome-oriented view of debugging. Within the Magnetic-One agent framework, on the datasets derived from GAIA and AssistantBench, DoVer flips 18-28% of failed trials into successes, achieves up to 16% milestone progress, and validates or refutes 30-60% of failure hypotheses. DoVer also performs effectively on a different dataset (GSMPlus) and agent framework (AG2), where it recovers 49% of failed trials. These results highlight intervention as a practical mechanism for improving reliability in agentic systems and open opportunities for more robust, scalable debugging methods for LLM-based multi-agent systems. Project website and code will be available at this https URL.},
 archiveprefix = {arXiv},
 author = {Ming Ma and Jue Zhang and Fangkai Yang and Yu Kang and Qingwei Lin and Saravan Rajmohan and Dongmei Zhang},
 eprint = {2512.06749},
 primaryclass = {cs.AI},
 title = {DoVer: Intervention-Driven Auto Debugging for LLM Multi-Agent Systems},
 url = {https://arxiv.org/abs/2512.06749},
 year = {2026}
}

@misc{fu2025debugtallmbasedagentsimplifying,
 abstract = {In programming education, Debugging and Teaching (DT) task is a common scenario where students receive assistance in correcting their erroneous code. The task involves multiple inputs, including erroneous code, error messages, reference solutions, and the question description, with the goal of generating modification suggestions to the erroneous code. However, two key challenges hinder the effectiveness of existing approaches. Firstly, the complexity and heterogeneity of inputs inherent in DT tasks significantly elevate the reasoning challenges faced by LLMs. Second, existing approaches often fail to fully leverage the availability of standard code in DT tasks, forcing models to rely solely on complex multi-step reasoning, which limits the potential of LLMs in addressing DT tasks effectively. To address these challenges, we propose DebugTA, a novel LLM-based debugging and teaching agent with specialized tools for standard code retrieval, variable substitution to align reference code, and an external compiler for real-time code analysis. Guided by explicit pedagogical and debugging principles, DebugTA acts as an agent that decomposes a complex task into sequential LLM interactions, each utilizing distinct tools for specific subtasks, thereby simplifying the logical reasoning at each step and reducing overall reasoning complexity. Furthermore, DebugTA utilizes tool calls to align the standard code with the erroneous code as much as possible, allowing the LLM to focus on logic errors within the erroneous code and improving the accuracy of the generated suggestions. To rigorously assess the quality of modification suggestions, we introduce a student simulator-teacher interaction paradigm. Experimental results on three real-world code datasets demonstrate that DebugTA consistently improves teaching effectiveness while significantly reducing computational costs.},
 archiveprefix = {arXiv},
 author = {Lingyue Fu and Haowei Yuan and Datong Chen and Xinyi Dai and Qingyao Li and Weinan Zhang and Weiwen Liu and Yong Yu},
 eprint = {2510.11076},
 primaryclass = {cs.SE},
 title = {DebugTA: An LLM-Based Agent for Simplifying Debugging and Teaching in Programming Education},
 url = {https://arxiv.org/abs/2510.11076},
 year = {2025}
}

@misc{tang2025mahlmultiagentllmguidedhierarchical,
 abstract = {As program workloads (e.g., AI) increase in size and algorithmic complexity, the primary challenge lies in their high dimensionality, encompassing computing cores, array sizes, and memory hierarchies. To overcome these obstacles, innovative approaches are required. Agile chip design has already benefited from machine learning integration at various stages, including logic synthesis, placement, and routing. With Large Language Models (LLMs) recently demonstrating impressive proficiency in Hardware Description Language (HDL) generation, it is promising to extend their abilities to 2.5D integration, an advanced technique that saves area overhead and development costs. However, LLM-driven chiplet design faces challenges such as flatten design, high validation cost and imprecise parameter optimization, which limit its chiplet design capability. To address this, we propose MAHL, a hierarchical LLM-based chiplet design generation framework that features six agents which collaboratively enable AI algorithm-hardware mapping, including hierarchical description generation, retrieval-augmented code generation, diverseflow-based validation, and multi-granularity design space exploration. These components together enhance the efficient generation of chiplet design with optimized Power, Performance and Area (PPA). Experiments show that MAHL not only significantly improves the generation accuracy of simple RTL design, but also increases the generation accuracy of real-world chiplet design, evaluated by Pass@5, from 0 to 0.72 compared to conventional LLMs under the best-case scenario. Compared to state-of-the-art CLARIE (expert-based), MAHL achieves comparable or even superior PPA results under certain optimization objectives.},
 archiveprefix = {arXiv},
 author = {Jinwei Tang and Jiayin Qin and Nuo Xu and Pragnya Sudershan Nalla and Yu Cao and Yang and Zhao and Caiwen Ding},
 eprint = {2508.14053},
 primaryclass = {cs.AR},
 title = {MAHL: Multi-Agent LLM-Guided Hierarchical Chiplet Design with Adaptive Debugging},
 url = {https://arxiv.org/abs/2508.14053},
 year = {2025}
}

@misc{ashrafi2025enhancingllmcodegeneration,
 abstract = {The use of large language models (LLMs) for automated code generation has emerged as a significant focus within AI research. As these pretrained models continue to evolve, their ability to understand and generate complex code structures has opened new possibilities for automating intricate programming tasks for the sake of accurate code generation. Although contemporary foundational models demonstrate promoting results, researchers continue to explore optimal post-training strategies to enhance code quality. These include supervised fine-tuning, retrieval-augmented generation (RAG), debugging, and many others. In this paper, we combine two widely used approaches namely multi-agent collaboration and runtime execution information-based debugging, for improving code generation functionality, reliability, and practical applicability. We perform an empirical study in order to extend the evaluation of the individual strategies as well as the proposed composition of the activities of both strategies. Our study use 19 LLMs to examines the performance of individual and the proposed strategies, offering comprehensive insights into how different programming activities compositions and training paradigms influence code generation effectiveness. In particular, we implement a chained system that combines both strategies to assess their combined impact on functional accuracy, code reliability, and generation latency using two benchmark datasets commonly used for code generation. Our findings provide valuable insights for organizations seeking robust AI-driven coding solutions by guiding them in selecting models that can better adapt to complex post-training strategies, ultimately fostering the adoption of more effective and reliable code generation technologies.},
 archiveprefix = {arXiv},
 author = {Nazmus Ashrafi and Salah Bouktif and Mohammed Mediani},
 eprint = {2505.02133},
 primaryclass = {cs.SE},
 title = {Enhancing LLM Code Generation: A Systematic Evaluation of Multi-Agent Collaboration and Runtime Debugging for Improved Accuracy, Reliability, and Latency},
 url = {https://arxiv.org/abs/2505.02133},
 year = {2025}
}

@misc{majdoub2025adaptivesoftwareagentsdebugging,
 abstract = {Using multiple agents was found to improve the debugging capabilities of Large Language Models. However, increasing the number of LLM-agents has several drawbacks such as increasing the running costs and rising the risk for the agents to lose focus. In this work, we propose an adaptive agentic design, where the number of agents and their roles are determined dynamically based on the characteristics of the task to be achieved. In this design, the agents roles are not predefined, but are generated after analyzing the problem to be solved. Our initial evaluation shows that, with the adaptive design, the number of agents that are generated depends on the complexity of the buggy code. In fact, for simple code with mere syntax issues, the problem was usually fixed using one agent only. However, for more complex problems, we noticed the creation of a higher number of agents. Regarding the effectiveness of the fix, we noticed an average improvement of 11% compared to the one-shot prompting. Given these promising results, we outline future research directions to improve our design for adaptive software agents that can autonomously plan and conduct their software goals.},
 archiveprefix = {arXiv},
 author = {Yacine Majdoub and Eya Ben Charrada and Haifa Touati},
 eprint = {2504.18316},
 primaryclass = {cs.SE},
 title = {Towards Adaptive Software Agents for Debugging},
 url = {https://arxiv.org/abs/2504.18316},
 year = {2025}
}

@misc{nainani2025timinganalysisagentautonomous,
 abstract = {Timing analysis is an essential and demanding verification method for Very Large Scale Integrated (VLSI) circuit design and optimization. In addition, it also serves as the cornerstone of the final sign-off, determining whether the chip is ready to be sent to the semiconductor foundry for fabrication. Recently, as the technology advance relentlessly, smaller metal pitches and the increasing number of devices have led to greater challenges and longer turn-around-time for experienced human designers to debug timing issues from the Multi-Corner Multi-Mode (MCMM) timing reports. As a result, an efficient and intelligent methodology is highly necessary and essential for debugging timing issues and reduce the turnaround times. Recently, Large Language Models (LLMs) have shown great promise across various tasks in language understanding and interactive decision-making, incorporating reasoning and actions. In this work, we propose a timing analysis agent, that is empowered by multi-LLMs task solving, and incorporates a novel hierarchical planning and solving flow to automate the analysis of timing reports from commercial tool. In addition, we build a Timing Debug Relation Graph (TDRG) that connects the reports with the relationships of debug traces from experienced timing engineers. The timing analysis agent employs the novel Agentic Retrieval Augmented Generation (RAG) approach, that includes agent and coding to retrieve data accurately, on the developed TDRG. In our studies, the proposed timing analysis agent achieves an average 98% pass-rate on a single-report benchmark and a 90% pass-rate for multi-report benchmark from industrial designs, demonstrating its effectiveness and adaptability.},
 archiveprefix = {arXiv},
 author = {Jatin Nainani and Chia-Tung Ho and Anirudh Dhurka and Haoxing Ren},
 eprint = {2504.11502},
 primaryclass = {cs.SE},
 title = {Timing Analysis Agent: Autonomous Multi-Corner Multi-Mode (MCMM) Timing Debugging with Timing Debug Relation Graph},
 url = {https://arxiv.org/abs/2504.11502},
 year = {2025}
}

@article{Grishina_2025,
 abstract = {Program synthesis with Large Language Models (LLMs) suffers from a "near-miss syndrome": the generated code closely resembles a correct solution but fails unit tests due to minor errors. We address this with a multi-agent framework called Synthesize, Execute, Instruct, Debug, and Repair (SEIDR). Effectively applying SEIDR to instruction-tuned LLMs requires determining (a) optimal prompts for LLMs, (b) what ranking algorithm selects the best programs in debugging rounds, and (c) balancing the repair of unsuccessful programs with the generation of new ones. We empirically explore these trade-offs by comparing replace-focused, repair-focused, and hybrid debug strategies. We also evaluate lexicase and tournament selection to rank candidates in each generation. On Program Synthesis Benchmark 2 (PSB2), our framework outperforms both conventional use of OpenAI Codex without a repair phase and traditional genetic programming approaches. SEIDR outperforms the use of an LLM alone, solving 18 problems in C++ and 20 in Python on PSB2 at least once across experiments. To assess generalizability, we employ GPT-3.5 and Llama 3 on the PSB2 and HumanEval-X benchmarks. Although SEIDR with these models does not surpass current state-of-the-art methods on the Python benchmarks, the results on HumanEval-C++ are promising. SEIDR with Llama 3-8B achieves an average pass@100 of 84.2%. Across all SEIDR runs, 163 of 164 problems are solved at least once with GPT-3.5 in HumanEval-C++, and 162 of 164 with the smaller Llama 3-8B. We conclude that SEIDR effectively overcomes the near-miss syndrome in program synthesis with LLMs.},
 author = {Grishina, Anastasiia and Liventsev, Vadim and Härmä, Aki and Moonen, Leon},
 doi = {10.1145/3719351},
 issn = {2688-3007},
 journal = {ACM Transactions on Evolutionary Learning and Optimization},
 month = {March},
 number = {1},
 pages = {1–37},
 publisher = {Association for Computing Machinery (ACM)},
 title = {Fully Autonomous Programming Using Iterative Multi-Agent Debugging with Large Language Models},
 url = {http://dx.doi.org/10.1145/3719351},
 volume = {5},
 year = {2025}
}

@inproceedings{Epperson_2025,
 abstract = {Fully autonomous teams of LLM-powered AI agents are emerging that collaborate to perform complex tasks for users. What challenges do developers face when trying to build and debug these AI agent teams? In formative interviews with five AI agent developers, we identify core challenges: difficulty reviewing long agent conversations to localize errors, lack of support in current tools for interactive debugging, and the need for tool support to iterate on agent configuration. Based on these needs, we developed an interactive multi-agent debugging tool, AGDebugger, with a UI for browsing and sending messages, the ability to edit and reset prior agent messages, and an overview visualization for navigating complex message histories. In a two-part user study with 14 participants, we identify common user strategies for steering agents and highlight the importance of interactive message resets for debugging. Our studies deepen understanding of interfaces for debugging increasingly important agentic workflows.},
 author = {Epperson, Will and Bansal, Gagan and Dibia, Victor C and Fourney, Adam and Gerrits, Jack and Zhu, Erkang (Eric) and Amershi, Saleema},
 booktitle = {Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems},
 collection = {CHI ’25},
 doi = {10.1145/3706598.3713581},
 month = {April},
 pages = {1–15},
 publisher = {ACM},
 series = {CHI ’25},
 title = {Interactive Debugging and Steering of Multi-Agent AI Systems},
 url = {http://dx.doi.org/10.1145/3706598.3713581},
 year = {2025}
}

@misc{islam2025codesimmultiagentcodegeneration,
 abstract = {Large Language Models (LLMs) have made significant strides in code generation and problem solving. Current approaches employ external tool-based iterative debuggers that use compiler or other tool-based runtime feedback to refine coarse programs generated by various methods. However, the effectiveness of these approaches heavily relies on the quality of the initial code generation, which remains an open challenge. In this paper, we introduce CodeSim, a novel multi-agent code generation framework that comprehensively addresses the stages of program synthesis-planning, coding, and debugging-through a human-like perception approach. As human verifies their understanding of any algorithms through visual simulation, CodeSim uniquely features a method of plan verification and internal debugging through the step-by-step simulation of input/output. Extensive experiments across seven challenging competitive problem-solving and program synthesis benchmarks demonstrate CodeSim's remarkable code generation capabilities. Our framework achieves new state-of-the-art (pass@1) results-(HumanEval 95.1%, MBPP 90.7%, APPS 22%, and CodeContests 29.1%). Furthermore, our method shows potential for even greater enhancement when cascaded with external debuggers. To facilitate further research and development in this area, we have open-sourced our framework in this link (this https URL).},
 archiveprefix = {arXiv},
 author = {Md. Ashraful Islam and Mohammed Eunus Ali and Md Rizwan Parvez},
 eprint = {2502.05664},
 primaryclass = {cs.CL},
 title = {CODESIM: Multi-Agent Code Generation and Problem Solving through Simulation-Driven Planning and Debugging},
 url = {https://arxiv.org/abs/2502.05664},
 year = {2025}
}

@misc{grotov2024debugsmarterharderai,
 abstract = {Computational notebooks became indispensable tools for research-related development, offering unprecedented interactivity and flexibility in the development process. However, these benefits come at the cost of reproducibility and an increased potential for bugs. With the rise of code-fluent Large Language Models empowered with agentic techniques, smart bug-fixing tools with a high level of autonomy have emerged. However, those tools are tuned for classical script programming and still struggle with non-linear computational notebooks. In this paper, we present an AI agent designed specifically for error resolution in a computational notebook. We have developed an agentic system capable of exploring a notebook environment by interacting with it -- similar to how a user would -- and integrated the system into the JetBrains service for collaborative data science called Datalore. We evaluate our approach against the pre-existing single-action solution by comparing costs and conducting a user study. Users rate the error resolution capabilities of the agentic system higher but experience difficulties with UI. We share the results of the study and consider them valuable for further improving user-agent collaboration.},
 archiveprefix = {arXiv},
 author = {Konstantin Grotov and Artem Borzilov and Maksim Krivobok and Timofey Bryksin and Yaroslav Zharov},
 eprint = {2410.14393},
 primaryclass = {cs.LG},
 title = {Debug Smarter, Not Harder: AI Agents for Error Resolution in Computational Notebooks},
 url = {https://arxiv.org/abs/2410.14393},
 year = {2024}
}

@misc{dibia2024autogenstudionocodedeveloper,
 abstract = {Multi-agent systems, where multiple agents (generative AI models + tools) collaborate, are emerging as an effective pattern for solving long-running, complex tasks in numerous domains. However, specifying their parameters (such as models, tools, and orchestration mechanisms etc,.) and debugging them remains challenging for most developers. To address this challenge, we present AUTOGEN STUDIO, a no-code developer tool for rapidly prototyping, debugging, and evaluating multi-agent workflows built upon the AUTOGEN framework. AUTOGEN STUDIO offers a web interface and a Python API for representing LLM-enabled agents using a declarative (JSON-based) specification. It provides an intuitive drag-and-drop UI for agent workflow specification, interactive evaluation and debugging of workflows, and a gallery of reusable agent components. We highlight four design principles for no-code multi-agent developer tools and contribute an open-source implementation at this https URL},
 archiveprefix = {arXiv},
 author = {Victor Dibia and Jingya Chen and Gagan Bansal and Suff Syed and Adam Fourney and Erkang Zhu and Chi Wang and Saleema Amershi},
 eprint = {2408.15247},
 primaryclass = {cs.SE},
 title = {AutoGen Studio: A No-Code Developer Tool for Building and Debugging Multi-Agent Systems},
 url = {https://arxiv.org/abs/2408.15247},
 year = {2024}
}

@misc{yang2025coastenhancingcodedebugging,
 abstract = {Code debugging is a vital stage of software development, essential for ensuring the reliability and performance of Large Language Models (LLMs) in the code generation task. Human debugging typically follows a multi-stage process, which includes Bug Localization, Bug Identification, Code Repair, and Code Recognition. However, existing code debugging benchmarks predominantly focus on the Code Repair stage, which offers only a limited perspective on evaluating the debugging capabilities of LLMs. In this paper, we introduce DEBUGEVAL, a comprehensive benchmark for evaluating the debugging abilities of LLMs by emulating the multi-stage human debugging process. Through evaluating on DEBUGEVAL, we observe that 7B-scale models consistently underperform compared to their larger counterparts, highlighting their limitations in comprehending code semantics. In this case, we propose the COmmunicative Agent-based data SynThesis (COAST) framework, which employs a multi-agent system to generate high-quality training data for supervised fine-tuning (SFT). Experimental results demonstrate that COAST-generated data outperform human-curated and GPT-4-generated data, enabling 7B-scale LLMs to achieve debugging performance comparable to GPT-3.5. All data and codes are available at this https URL.},
 archiveprefix = {arXiv},
 author = {Weiqing Yang and Hanbin Wang and Zhenghao Liu and Xinze Li and Yukun Yan and Shuo Wang and Yu Gu and Minghe Yu and Zhiyuan Liu and Ge Yu},
 eprint = {2408.05006},
 primaryclass = {cs.SE},
 title = {COAST: Enhancing the Code Debugging Ability of LLMs through Communicative Agent Based Data Synthesis},
 url = {https://arxiv.org/abs/2408.05006},
 year = {2025}
}

@misc{lee2025unidebuggerhierarchicalmultiagentframework,
 abstract = {Software debugging is a time-consuming endeavor involving a series of steps, such as fault localization and patch generation, each requiring thorough analysis and a deep understanding of the underlying logic. While large language models (LLMs) demonstrate promising potential in coding tasks, their performance in debugging remains limited. Current LLM-based methods often focus on isolated steps and struggle with complex bugs. In this paper, we propose the first end-to-end framework, FixAgent, for unified debugging through multi-agent synergy. It mimics the entire cognitive processes of developers, with each agent specialized as a particular component of this process rather than mirroring the actions of an independent expert as in previous multi-agent systems. Agents are coordinated through a three-level design, following a cognitive model of debugging, allowing adaptive handling of bugs with varying complexities. Experiments on extensive benchmarks demonstrate that FixAgent significantly outperforms state-of-the-art repair methods, fixing 1.25\times
to 2.56\times
bugs on the repo-level benchmark, Defects4J. This performance is achieved without requiring ground-truth root-cause code statements, unlike the baselines. Our source code is available on this https URL.},
 archiveprefix = {arXiv},
 author = {Cheryl Lee and Chunqiu Steven Xia and Longji Yang and Jen-tse Huang and Zhouruixin Zhu and Lingming Zhang and Michael R. Lyu},
 eprint = {2404.17153},
 primaryclass = {cs.SE},
 title = {UniDebugger: Hierarchical Multi-Agent Framework for Unified Software Debugging},
 url = {https://arxiv.org/abs/2404.17153},
 year = {2025}
}

@inbook{Ma_2024,
 abstract = {Large Language Models (LLMs) now excel at generative skills and can create content at impeccable speeds. However, they are imperfect and still make various mistakes. In a Computer Science education context, as these models are widely recognized as "AI pair programmers," it becomes increasingly important to train students on evaluating and debugging the LLM-generated code. In this work, we introduce HypoCompass, a novel system to facilitate deliberate practice on debugging, where human novices play the role of Teaching Assistants and help LLM-powered teachable agents debug code. We enable effective task delegation between students and LLMs in this learning-by-teaching environment: students focus on hypothesizing the cause of code errors, while adjacent skills like code completion are offloaded to LLM-agents. Our evaluations demonstrate that HypoCompass generates high-quality training materials (e.g., bugs and fixes), outperforming human counterparts fourfold in efficiency, and significantly improves student performance on debugging by 12% in the pre-to-post test.},
 author = {Ma, Qianou and Shen, Hua and Koedinger, Kenneth and Wu, Sherry Tongshuang},
 booktitle = {Artificial Intelligence in Education},
 doi = {10.1007/978-3-031-64302-6_19},
 isbn = {9783031643026},
 issn = {1611-3349},
 pages = {265–279},
 publisher = {Springer Nature Switzerland},
 title = {How to Teach Programming in the AI Era? Using LLMs as a Teachable Agent for Debugging},
 url = {http://dx.doi.org/10.1007/978-3-031-64302-6_19},
 year = {2024}
}

@misc{liang2026montecarlotreesearch,
 abstract = {Automated program repair with large language models remains challenging at the repository level due to long-horizon reasoning requirements and the limitations of autoregressive decoding. We present CodePilot, a hybrid framework that integrates Monte Carlo Tree Search (MCTS) with large language models to enable execution-guided program repair for real-world GitHub issues. CodePilot performs hierarchical fault localization from repository to file and function level, explores diverse patch trajectories using MCTS, and leverages execution feedback as a reward signal to guide search and refinement. The framework further incorporates confidence-calibrated generation to selectively refine low-confidence outputs. Experiments on SWE-bench Lite demonstrate that CodePilot achieves a 24.67% issue resolution rate using open-weight models, outperforming comparable baselines. These results suggest that combining symbolic search with neural language models is an effective strategy for scalable, execution-aware software engineering automation.},
 archiveprefix = {arXiv},
 author = {Yixuan Liang},
 eprint = {2602.00129},
 primaryclass = {cs.LG},
 title = {Monte Carlo Tree Search for Execution-Guided Program Repair with Large Language Models},
 url = {https://arxiv.org/abs/2602.00129},
 year = {2026}
}

@misc{sepidband2026rgflreasoningguidedfault,
 abstract = {Fault Localization (FL) is a critical step in Automated Program Repair (APR), and its importance has increased with the rise of Large Language Model (LLM)-based repair agents. In realistic project-level repair scenarios, software repositories often span millions of tokens, far exceeding current LLM context limits. Consequently, models must first identify a small, relevant subset of code, making accurate FL essential for effective repair. We present a novel project-level FL approach that improves both file- and element-level localization. Our method introduces a hierarchical reasoning module that (i) generates structured, bug-specific explanations for candidate files and elements, and (ii) leverages these explanations in a two-stage ranking scheme combining LLM-based and embedding-based signals. We further propose a counterfactual upper-bound analysis to quantify the contribution of each localization stage to repair success. We evaluate our approach on Python and Java projects from SWE-bench Verified, Lite, and Java. Compared to state-of-the-art baselines, including Agentless and OpenHands, our method consistently improves localization accuracy. On SWE-bench Verified, file-level Hit@1 improves from 71.4% to 85%, and MRR from 81.8% to 88.8%. At the element level, Exact Match under top-3 files increases from 36% to 69%. Integrating our localization into Agentless yields a 12.8% end-to-end repair success improvement.},
 archiveprefix = {arXiv},
 author = {Melika Sepidband and Hamed Taherkhani and Hung Viet Pham and Hadi Hemmati},
 eprint = {2601.18044},
 primaryclass = {cs.SE},
 title = {RGFL: Reasoning Guided Fault Localization for Automated Program Repair Using Large Language Models},
 url = {https://arxiv.org/abs/2601.18044},
 year = {2026}
}

@misc{yamagishi2025automatedprogramrepairbased,
 abstract = {Many cloud services provide REST API accessible to client applications. However, developers often identify specification violations only during testing, as error messages typically lack the detail necessary for effective diagnosis. Consequently, debugging requires trial and error. This study proposes dcFix, a method for detecting and automatically repairing REST API misuses in client programs. In particular, dcFix identifies non-conforming code fragments, integrates them with the relevant API specifications into prompts, and leverages a Large Language Model (LLM) to produce the corrected code. Our evaluation demonstrates that dcFix accurately detects misuse and outperforms the baseline approach, in which prompts to the LLM omit any indication of code fragments non conforming to REST API specifications.},
 archiveprefix = {arXiv},
 author = {Katsuki Yamagishi and Norihiro Yoshida and Erina Makihara and Katsuro Inoue},
 eprint = {2510.25148},
 primaryclass = {cs.SE},
 title = {Automated Program Repair Based on REST API Specifications Using Large Language Models},
 url = {https://arxiv.org/abs/2510.25148},
 year = {2025}
}

@misc{farzandway2025automatedrepaircprograms,
 abstract = {This study explores the potential of Large Language Models (LLMs) in automating the repair of C programs. We present a framework that integrates spectrum-based fault localization (SBFL), runtime feedback, and Chain-of-Thought-structured prompting into an autonomous repair loop. Unlike prior approaches, our method explicitly combines statistical program analysis with LLM reasoning. The iterative repair cycle leverages a structured Chain-of-Thought (CoT) prompting approach, where the model reasons over failing tests, suspicious code regions, and prior patch outcomes, before generating new candidate patches. The model iteratively changes the code, evaluates the results, and incorporates reasoning from previous attempts into subsequent modifications, reducing repeated errors and clarifying why some bugs remain unresolved. Our evaluation spans 3,902 bugs from the Codeflaws benchmark, where our approach achieves 44.93% repair accuracy, representing a 3.61% absolute improvement over strong state-of-the-art APR baselines such as GPT-4 with CoT. This outcome highlights a practical pathway toward integrating statistical program analysis with generative AI in automated debugging.},
 archiveprefix = {arXiv},
 author = {Mahdi Farzandway and Fatemeh Ghassemi},
 eprint = {2509.01947},
 primaryclass = {cs.SE},
 title = {Automated Repair of C Programs Using Large Language Models},
 url = {https://arxiv.org/abs/2509.01947},
 year = {2025}
}

@misc{macháček2025impactfinetuninglargelanguage,
 abstract = {Automated Program Repair (APR) uses various tools and techniques to help developers achieve functional and error-free code faster. In recent years, Large Language Models (LLMs) have gained popularity as components in APR tool chains because of their performance and flexibility. However, training such models requires a significant amount of resources. Fine-tuning techniques have been developed to adapt pre-trained LLMs to specific tasks, such as APR, and enhance their performance at far lower computational costs than training from scratch. In this study, we empirically investigate the impact of various fine-tuning techniques on the performance of LLMs used for APR. Our experiments provide insights into the performance of a selection of state-of-the-art LLMs pre-trained on code. The evaluation is done on three popular APR benchmarks (i.e., QuixBugs, Defects4J and HumanEval-Java) and considers six different LLMs with varying parameter sizes (resp. CodeGen, CodeT5, StarCoder, DeepSeekCoder, Bloom, and CodeLlama-2). We consider three training regimens: no fine-tuning, full fine-tuning, and parameter-efficient fine-tuning (PEFT) using LoRA and IA3. We observe that full fine-tuning techniques decrease the benchmarking performance of various models due to different data distributions and overfitting. By using parameter-efficient fine-tuning methods, we restrict models in the amount of trainable parameters and achieve better results.
Keywords: large language models, automated program repair, parameter-efficient fine-tuning, AI4Code, AI4SE, ML4SE.},
 archiveprefix = {arXiv},
 author = {Roman Macháček and Anastasiia Grishina and Max Hort and Leon Moonen},
 eprint = {2507.19909},
 primaryclass = {cs.SE},
 title = {The Impact of Fine-tuning Large Language Models on Automated Program Repair},
 url = {https://arxiv.org/abs/2507.19909},
 year = {2025}
}

@misc{guo2025acceleratingautomaticprogramrepair,
 abstract = {Automated Program Repair (APR) is essential for ensuring software reliability and quality while enhancing efficiency and reducing developers' workload. Although rule-based and learning-based APR methods have demonstrated their effectiveness, their performance was constrained by the defect type of repair, the quality of training data, and the size of model parameters. Recently, Large Language Models (LLMs) combined with Retrieval-Augmented-Generation (RAG) have been increasingly adopted in APR tasks. However, current code LLMs and RAG designs neither fully address code repair tasks nor consider code-specific features. To overcome these limitations, we propose SelRepair, a novel APR approach with integration of a fine-tuned LLM with a newly-designed dual RAG module. This approach uses a bug-fix pair dataset for fine-tuning and incorporates semantic and syntactic/structural similarity information through an RAG selection gate. This design ensures relevant information is retrieved efficiently, thereby reducing token length and inference time. Evaluations on Java datasets show SelRepair outperforms other APR methods, achieving 26.29% and 17.64% in terms of exact match (EM) on different datasets while reducing inference time by at least 6.42% with controlled input lengths.},
 archiveprefix = {arXiv},
 author = {Hanyang Guo and Xiaoheng Xie and Hong-Ning Dai and Peng Di and Yu Zhang and Bishenghui Tao and Zibin Zheng},
 eprint = {2507.10103},
 primaryclass = {cs.SE},
 title = {Accelerating Automatic Program Repair with Dual Retrieval-Augmented Fine-Tuning and Patch Generation on Large Language Models},
 url = {https://arxiv.org/abs/2507.10103},
 year = {2025}
}

@misc{zhang2025repairingredientsneedimproving,
 abstract = {Automated Program Repair (APR) techniques aim to automatically fix buggy programs. Among these, Large Language Model-based (LLM-based) approaches have shown great promise. Recent advances demonstrate that directly leveraging LLMs can achieve leading results. However, these techniques remain suboptimal in generating contextually relevant and accurate patches, as they often overlook repair ingredients crucial for practical program repair. In this paper, we propose ReinFix, a novel framework that enables LLMs to autonomously search for repair ingredients throughout both the reasoning and solution phases of bug fixing. In the reasoning phase, ReinFix integrates static analysis tools to retrieve internal ingredients, such as variable definitions, to assist the LLM in root cause analysis when it encounters difficulty understanding the context. During the solution phase, when the LLM lacks experience in fixing specific bugs, ReinFix searches for external ingredients from historical bug fixes with similar bug patterns, leveraging both the buggy code and its root cause to guide the LLM in identifying appropriate repair actions, thereby increasing the likelihood of generating correct patches. Evaluations on two popular benchmarks (Defects4J V1.2 and V2.0) demonstrate the effectiveness of our approach over SOTA baselines. Notably, ReinFix fixes 146 bugs, which is 32 more than the baselines on Defects4J V1.2. On Defects4J V2.0, ReinFix fixes 38 more bugs than the SOTA. Importantly, when evaluating on the recent benchmarks that are free of data leakage risk, ReinFix also maintains the best performance.},
 archiveprefix = {arXiv},
 author = {Jiayi Zhang and Kai Huang and Jian Zhang and Yang Liu and Chunyang Chen},
 eprint = {2506.23100},
 primaryclass = {cs.SE},
 title = {Repair Ingredients Are All You Need: Improving Large Language Model-Based Program Repair via Repair Ingredients Search},
 url = {https://arxiv.org/abs/2506.23100},
 year = {2025}
}

@misc{liu2025t3multileveltreebasedautomatic,
 abstract = {Automatic Program Repair (APR) is a core technology in software development and maintenance, with aims to enable automated defect repair with minimal human intervention. In recent years, the substantial advancements in Large Language Models (LLMs) and the Chain-of-Thought (CoT) techniques have significantly enhanced the reasoning capabilities of these models. However, due to the complex logic and multi-step reasoning ability needed, the application of CoT techniques in the APR domain remains insufficient. This study systematically evaluates the performance of several common CoT techniques in APR tasks and proposes an innovative framework T^3
, which integrates the powerful reasoning capabilities of LLMs with tree search, effectively improving the precision of generating candidate repair solutions. Furthermore, T^3
provides valuable guidance for optimizing sample selection and repair strategies in APR tasks, establishing a robust framework for achieving efficient automated debugging.},
 archiveprefix = {arXiv},
 author = {Quanming Liu and Xupeng Bu and Zhichao Yan and Ru Li},
 eprint = {2506.21211},
 primaryclass = {cs.SE},
 title = {$T^3$: Multi-level Tree-based Automatic Program Repair with Large Language Models},
 url = {https://arxiv.org/abs/2506.21211},
 year = {2025}
}

@misc{sun2025empiricalevaluationlargelanguage,
 abstract = {The increasing prevalence of software bugs has made automated program repair (APR) a key research focus. Large language models (LLMs) offer new opportunities for APR, but existing studies mostly rely on smaller, earlier-generation models and Java benchmarks. The repair capabilities of modern, large-scale LLMs across diverse languages and scenarios remain underexplored. To address this, we conduct a comprehensive empirical study of four open-source LLMs, CodeLlama, LLaMA, StarCoder, and DeepSeek-Coder, spanning 7B to 33B parameters, diverse architectures, and purposes. We evaluate them across two bug scenarios (enterprise-grades and algorithmic), three languages (Java, C/C++, Python), and four prompting strategies, analyzing over 600K generated patches on six benchmarks. Key findings include: (1) model specialization (e.g., CodeLlama) can outperform larger general-purpose models (e.g., LLaMA); (2) repair performance does not scale linearly with model size; (3) correct patches often appear early in generation; and (4) prompts significantly affect results. These insights offer practical guidance for designing effective and efficient LLM-based APR systems.},
 archiveprefix = {arXiv},
 author = {Jiajun Sun and Fengjie Li and Xinzhu Qi and Hongyu Zhang and Jiajun Jiang},
 eprint = {2506.13186},
 primaryclass = {cs.SE},
 title = {Empirical Evaluation of Large Language Models in Automated Program Repair},
 url = {https://arxiv.org/abs/2506.13186},
 year = {2025}
}

@misc{campos2025empiricalevaluationgeneralizableautomated,
 abstract = {Automated Program Repair (APR) proposes bug fixes to aid developers in maintaining software. The state of the art in this domain focuses on using LLMs, leveraging their strong capabilities to comprehend specifications in natural language and to generate program code. Recent works have shown that LLMs can be used to generate repairs. However, despite the APR community's research achievements and several industry deployments in the last decade, APR still lacks the capabilities to generalize broadly. In this work, we present an intensive empirical evaluation of LLMs for generating patches. We evaluate a diverse set of 13 recent models, including open ones (e.g., Llama 3.3, Qwen 2.5 Coder, and DeepSeek R1 (dist.)) and closed ones (e.g., o3-mini, GPT-4o, Claude 3.7 Sonnet, Gemini 2.0 Flash). In particular, we explore language-agnostic repairs by utilizing benchmarks for Java (e.g., Defects4J), JavaScript (e.g., BugsJS), Python (e.g., BugsInPy), and PHP (e.g., BugsPHP). Besides the generalization between different languages and levels of patch complexity, we also investigate the effects of fault localization (FL) as a preprocessing step and compare the progress for open vs closed models. Our evaluation represents a snapshot of the current repair capabilities of the latest LLMs. Key results include: (1) Different LLMs tend to perform best for different languages, which makes it hard to develop cross-platform repair techniques with single LLMs. (2) The combinations of models add value with respect to uniquely fixed bugs, so a committee of expert models should be considered. (3) Under realistic assumptions of imperfect FL, we observe significant drops in accuracy from the usual practice of using perfect FL. Our findings and insights will help both researchers and practitioners develop reliable and generalizable APR techniques and evaluate them in realistic and fair environments.},
 archiveprefix = {arXiv},
 author = {Viola Campos and Ridwan Shariffdeen and Adrian Ulges and Yannic Noller},
 eprint = {2506.03283},
 primaryclass = {cs.SE},
 title = {Empirical Evaluation of Generalizable Automated Program Repair with Large Language Models},
 url = {https://arxiv.org/abs/2506.03283},
 year = {2025}
}

@misc{brancas2024combininglogiclargelanguage,
 abstract = {Logic programs are a powerful approach for solving NP-Hard problems. However, due to their declarative nature, debugging logic programs poses significant challenges. Unlike procedural paradigms, which allow for step-by-step inspection of program state, logic programs require reasoning about logical statements for fault localization. This complexity is amplified in learning environments due to students' inexperience.
We introduce FormHe, a novel tool that combines logic-based techniques and Large Language Models to identify and correct issues in Answer Set Programming submissions. FormHe consists of two components: a fault localization module and a program repair module. First, the fault localizer identifies a set of faulty program statements requiring modification. Subsequently, FormHe employs program mutation techniques and Large Language Models to repair the flawed ASP program. These repairs can then serve as guidance for students to correct their programs.
Our experiments with real buggy programs submitted by students show that FormHe accurately detects faults in 94% of cases and successfully repairs 58% of incorrect submissions.},
 archiveprefix = {arXiv},
 author = {Ricardo Brancas and Vasco Manquinho and Ruben Martins},
 eprint = {2410.20962},
 primaryclass = {cs.SE},
 title = {Combining Logic with Large Language Models for Automatic Debugging and Repair of ASP Programs},
 url = {https://arxiv.org/abs/2410.20962},
 year = {2024}
}

@misc{lecong2025memoryefficientlargelanguagemodels,
 abstract = {In this paper, we first show that increases in beam size, even for small-sized LLMs (1B-7B params), require extensive GPU usage, leading to up to 80% of recurring crashes due to memory overloads in LLM-based APR. Seemingly simple solutions to reduce memory consumption are (1) to quantize LLM models, i.e., converting the weights of an LLM from high-precision values to lower-precision ones, and (2) to make beam search sequential, i.e., forwarding each beam through the model sequentially and then concatenating them back into a single output. However, we show that these approaches still do not work via both theoretical analysis and experiments.
To address this, we introduce FLAMES, a novel LLM-based APR technique that employs semantic-guided patch generation to enhance repair effectiveness and memory efficiency. Unlike conventional methods that rely on beam search, FLAMES utilizes greedy decoding to enhance memory efficiency while steering the search towards more potentially good repair candidates via a semantic-guided best-first search algorithm. At each decoding step, FLAMES uses semantic feedback from test validation, such as the number of passing and failing test cases, to select the most promising token to explore further. Our empirical evaluation on Defects4J shows thatFLAMES substantially reduces memory consumption by up to 83% compared to LLM-based APR without compromising time efficiency. Moreover, FLAMES correctly fixes 133 bugs on Defects4J, fixing 10 bugs more than the best baseline. Additionally, these improvements also generalize to the HumanEval-Java and TransformedD4J datasets, where FLAMES generates 12% and 36.5% more correct patches, respectively, than the best baseline.},
 archiveprefix = {arXiv},
 author = {Thanh Le-Cong and Bach Le and Toby Murray},
 eprint = {2410.16655},
 primaryclass = {cs.SE},
 title = {Memory-Efficient Large Language Models for Program Repair with Semantic-Guided Patch Generation},
 url = {https://arxiv.org/abs/2410.16655},
 year = {2025}
}

@misc{li2024exploringparameterefficientfinetuninglarge,
 abstract = {Automated Program Repair (APR) aims to fix bugs by generating patches. And existing work has demonstrated that "pre-training and fine-tuning" paradigm enables Large Language Models (LLMs) improve fixing capabilities on APR. However, existing work mainly focuses on Full-Model Fine-Tuning (FMFT) for APR and limited research has been conducted on the execution-based evaluation of Parameter-Efficient Fine-Tuning (PEFT) for APR. Comparing to FMFT, PEFT can reduce computing resource consumption without compromising performance and has been widely adopted to other software engineering tasks.
To fill this gap, we enhance the existing APR dataset by employing prompt engineering to create an instruction dataset, APR-INSTRUCTION, at first. Secondly, we fine-tune four pre-trained LLMs using four different PEFT methods with APR-INSTRUCTION. The best fine-tuned model fixes 58% more bugs than the state-of-the-art LLM-based APR techniques. The results also show that (IA)^3
improves the creativity of LLMs more effectively through fine-tuning and achieves the highest fixing capability compared to the other three PEFT methods. Thirdly, we explore the optimal configuration of PEFT hyperparameters, and assess the impact of instruction dataset size, showing that a larger number of parameters and a larger training dataset do not necessarily result in better performance for PEFT. Lastly, we analyze peak memory usage and trainable parameters to show the efficiency of PEFT.
This work provides a comprehensive exploration of PEFT on APR and suggests potentially promising directions for extension to other software engineering downstream tasks. APR-INSTRUCTION, PEFT weights, and the fine-tuning code are publicly available as open-source resources.},
 archiveprefix = {arXiv},
 author = {Guochang Li and Chen Zhi and Jialiang Chen and Junxiao Han and Shuiguang Deng},
 eprint = {2406.05639},
 primaryclass = {cs.SE},
 title = {Exploring Parameter-Efficient Fine-Tuning of Large Language Model on Automated Program Repair},
 url = {https://arxiv.org/abs/2406.05639},
 year = {2024}
}

@misc{li2024hybridautomatedprogramrepair,
 abstract = {Automated Program Repair (APR) has garnered significant attention due to its potential to streamline the bug repair process for human developers. Recently, LLM-based APR methods have shown promise in repairing real-world bugs. However, existing APR methods often utilize patches generated by LLMs without further optimization, resulting in reduced effectiveness due to the lack of program-specific knowledge. Furthermore, the evaluations of these APR methods have typically been conducted under the assumption of perfect fault localization, which may not accurately reflect their real-world effectiveness. To address these limitations, this paper introduces an innovative APR approach called GIANTREPAIR. Our approach leverages the insight that LLM-generated patches, although not necessarily correct, offer valuable guidance for the patch generation process. Based on this insight, GIANTREPAIR first constructs patch skeletons from LLM-generated patches to confine the patch space, and then generates high-quality patches tailored to specific programs through context-aware patch generation by instantiating the skeletons. To evaluate the performance of our approach, we conduct two large-scale experiments. The results demonstrate that GIANTREPAIR not only effectively repairs more bugs (an average of 27.78% on Defects4J v1.2 and 23.40% on Defects4J v2.0) than using LLM-generated patches directly, but also outperforms state-of-the-art APR methods by repairing at least 42 and 7 more bugs under perfect and automated fault localization scenarios, respectively.},
 archiveprefix = {arXiv},
 author = {Fengjie Li and Jiajun Jiang and Jiajun Sun and Hongyu Zhang},
 eprint = {2406.00992},
 primaryclass = {cs.SE},
 title = {Hybrid Automated Program Repair by Combining Large Language Models and Program Analysis},
 url = {https://arxiv.org/abs/2406.00992},
 year = {2024}
}

@misc{zhang2025systematicliteraturereviewlarge,
 abstract = {Automated Program Repair (APR) attempts to patch software bugs and reduce manual debugging efforts. Very recently, with the advances in Large Language Models (LLMs), an increasing number of APR techniques have been proposed, facilitating software development and maintenance and demonstrating remarkable performance. However, due to ongoing explorations in the LLM-based APR field, it is challenging for researchers to understand the current achievements, challenges, and potential opportunities. This work provides the first systematic literature review to summarize the applications of LLMs in APR between 2020 and 2025. We analyze 189 relevant papers from LLMs, APR and their integration perspectives. First, we categorize existing popular LLMs that are applied to support APR and outline four types of utilization strategies for their deployment. Besides, we detail some specific repair scenarios that benefit from LLMs, e.g., semantic bugs and security vulnerabilities. Furthermore, we discuss several critical aspects of integrating LLMs into APR research, e.g., input forms and open science. Finally, we highlight a set of challenges remaining to be investigated and the potential guidelines for future research. Overall, our paper provides a systematic overview of the research landscape to the APR community, helping researchers gain a comprehensive understanding of achievements and promote future research.},
 archiveprefix = {arXiv},
 author = {Quanjun Zhang and Chunrong Fang and Yang Xie and YuXiang Ma and Weisong Sun and Yun Yang and Zhenyu Chen},
 eprint = {2405.01466},
 primaryclass = {cs.SE},
 title = {A Systematic Literature Review on Large Language Models for Automated Program Repair},
 url = {https://arxiv.org/abs/2405.01466},
 year = {2025}
}

@misc{yang2024revisitingunnaturalnessautomatedprogram,
 abstract = {Language models have improved by orders of magnitude with the recent emergence of Transformer-based Large Language Models (LLMs). LLMs have demonstrated their ability to generate natural code that is highly similar to code written by professional developers. One intermediate value an LLM can emit is entropy, which measures the naturalness of a token of code. We hypothesize that entropy can be used to improve the performance of Automated Program Repair (APR) tasks. While much progress has been made in Automated Program Repair (APR), fault localization techniques suffer from a lack of diversity in ranking scores, patch generation tools tend to be inefficient as all tests need to run before determining if a patch is likely to be correct, and patch ranking often suffers from the test-suite over-fitting problem. However, using an LLM directly for APR introduces concerns for training data leakage. In this work, we introduce a novel way of using the entropy of LLMs in combination with prior APR tools to improve all stages of APR. We show that entropy is highly complementary with prior fault localization tools. Our proposed re-ranking method achieves a 50% Top-5 score improvement over SBFL. We propose a patch-naturalness measurement, entropy-delta, to improve the efficiency of template-based repair techniques by ranking plausible patches before undergoing testing. When using entropy-delta for patch ranking and classification, our proposed method can rank correct patches more effectively than state-of-the-art machine learning tools with an 49% improvement in Top-1. Our work suggests that LLMs can be an effective addition to compliment prior APR tasks while minimizing both the test-suite overfitting problem and the LLM data leakage problem.},
 archiveprefix = {arXiv},
 author = {Aidan Z. H. Yang and Sophia Kolak and Vincent J. Hellendoorn and Ruben Martins and Claire Le Goues},
 eprint = {2404.15236},
 primaryclass = {cs.SE},
 title = {Revisiting Unnaturalness for Automated Program Repair in the Era of Large Language Models},
 url = {https://arxiv.org/abs/2404.15236},
 year = {2024}
}

@misc{chen2024largelanguagemodelsconfront,
 abstract = {In recent years, large language models (LLMs) have demonstrated substantial potential in addressing automatic program repair (APR) tasks. However, the current evaluation of these models for APR tasks focuses solely on the limited context of the single function or file where the bug is located, overlooking the valuable information in the repository-level context. This paper investigates the performance of popular LLMs in handling repository-level repair tasks. We introduce RepoBugs, a new benchmark comprising 124 typical repository-level bugs from open-source repositories. Preliminary experiments using GPT3.5 based on the function where the error is located, reveal that the repair rate on RepoBugs is only 22.58%, significantly diverging from the performance of GPT3.5 on function-level bugs in related studies. This underscores the importance of providing repository-level context when addressing bugs at this level. However, the repository-level context offered by the preliminary method often proves redundant and imprecise and easily exceeds the prompt length limit of LLMs. To solve the problem, we propose a simple and universal repository-level context extraction method (RLCE) designed to provide more precise context for repository-level code repair tasks. Evaluations of three mainstream LLMs show that RLCE significantly enhances the ability to repair repository-level bugs. The improvement reaches a maximum of 160% compared to the preliminary method. Additionally, we conduct a comprehensive analysis of the effectiveness and limitations of RLCE, along with the capacity of LLMs to address repository-level bugs, offering valuable insights for future research.},
 archiveprefix = {arXiv},
 author = {Yuxiao Chen and Jingzheng Wu and Xiang Ling and Changjiang Li and Zhiqing Rui and Tianyue Luo and Yanjun Wu},
 eprint = {2403.00448},
 primaryclass = {cs.SE},
 title = {When Large Language Models Confront Repository-Level Automatic Program Repair: How Well They Done?},
 url = {https://arxiv.org/abs/2403.00448},
 year = {2024}
}

@article{Vallecillos_Ruiz_2025,
 abstract = {Research shows that errors in natural language can be corrected by translating texts to another language and back using language models. We explore to what extent this latent correction capability extends to Automated Program Repair (APR) by investigating Round-Trip Translation (RTT): translating code from one programming language into another programming or natural language and back, using Large Language Models (LLMs). We hypothesize that RTT restores patterns most commonly seen in the LLM's training corpora through regression toward the mean, replacing infrequent bugs with more frequent, natural, bug-free code. To test this hypothesis, we employ nine LLMs and four common APR benchmarks in Java, and perform a detailed quantitative and qualitative analysis of RTT-generated patches. We find that RTT through English generates plausible patches for 100 of 164 bugs with GPT-4 on the HumanEval-Java benchmark, and 97 are found to be correct in our manual assessment. Moreover, RTT uniquely generates plausible patches for 46 bugs that were missed by LLMs specifically fine-tuned for APR. While this demonstrates the viability of RTT for APR, we also observe limitations, such as a lower overall bug fix rate than the state-of-the-art and diluting the original coding style. We analyze the impact of these limitations and discuss the potential of using RTT as a complementary component in APR frameworks. A replication package is available for download from this https URL.
Keywords: automated program repair, large language model, machine translation},
 author = {Vallecillos Ruiz, Fernando and Grishina, Anastasiia and Hort, Max and Moonen, Leon},
 doi = {10.1145/3771922},
 issn = {1557-7392},
 journal = {ACM Transactions on Software Engineering and Methodology},
 month = {October},
 publisher = {Association for Computing Machinery (ACM)},
 title = {Assessing the Latent Automated Program Repair Capabilities of Large Language Models using Round-Trip Translation},
 url = {http://dx.doi.org/10.1145/3771922},
 year = {2025}
}

@misc{zhang2024criticalreviewlargelanguage,
 abstract = {Large Language Models (LLMs) have been gaining increasing attention and demonstrated promising performance across a variety of Software Engineering (SE) tasks, such as Automated Program Repair (APR), code summarization, and code completion. For example, ChatGPT, the latest black-box LLM, has been investigated by numerous recent research studies and has shown impressive performance in various tasks. However, there exists a potential risk of data leakage since these LLMs are usually close-sourced with unknown specific training details, e.g., pre-training datasets.
In this paper, we seek to review the bug-fixing capabilities of ChatGPT on a clean APR benchmark with different research objectives. We first introduce {\benchmark}, a new benchmark with buggy and the corresponding fixed programs from competitive programming problems starting from 2023, after the training cutoff point of ChatGPT. The results on {\benchmark} show that ChatGPT is able to fix 109 out of 151 buggy programs using the basic prompt within 35 independent rounds, outperforming state-of-the-art LLMs CodeT5 and PLBART by 27.5\% and 62.4\% prediction accuracy. We also investigate the impact of three types of prompts, i.e., problem description, error feedback, and bug localization, leading to additional 34 fixed bugs. Besides, we provide additional discussion from the interactive nature of ChatGPT to illustrate the capacity of a dialog-based repair workflow with 9 additional fixed bugs. Inspired by the findings, we further pinpoint various challenges and opportunities for advanced SE study equipped with such LLMs (e.g.,~ChatGPT) in the near future. More importantly, our work calls for more research on the reevaluation of the achievements obtained by existing black-box LLMs across various SE tasks, not limited to ChatGPT on APR.},
 archiveprefix = {arXiv},
 author = {Quanjun Zhang and Tongke Zhang and Juan Zhai and Chunrong Fang and Bowen Yu and Weisong Sun and Zhenyu Chen},
 eprint = {2310.08879},
 primaryclass = {cs.SE},
 title = {A Critical Review of Large Language Model on Software Engineering: An Example from ChatGPT and Automated Program Repair},
 url = {https://arxiv.org/abs/2310.08879},
 year = {2024}
}

@inproceedings{Wei_2023,
 abstract = {During Automated Program Repair (APR), it can be challenging to synthesize correct patches for real-world systems in general-purpose programming languages. Recent Large Language Models (LLMs) have been shown to be helpful "copilots" in assisting developers with various coding tasks, and have also been directly applied for patch synthesis. However, most LLMs treat programs as sequences of tokens, meaning that they are ignorant of the underlying semantics constraints of the target programming language. This results in plenty of statically invalid generated patches, impeding the practicality of the technique. Therefore, we propose Repilot, a general code generation framework to further copilot the AI "copilots" (i.e., LLMs) by synthesizing more valid patches during the repair process. Our key insight is that many LLMs produce outputs autoregressively (i.e., token by token), resembling human writing programs, which can be significantly boosted and guided through a Completion Engine. Repilot synergistically synthesizes a candidate patch through the interaction between an LLM and a Completion Engine, which 1) prunes away infeasible tokens suggested by the LLM and 2) proactively completes the token based on the suggestions provided by the Completion Engine. Our evaluation on a subset of the widely-used Defects4j 1.2 and 2.0 datasets shows that Repilot outperforms state-of-the-art techniques by fixing 27% and 47% more bugs, respectively. Moreover, Repilot produces more valid and correct patches than the base LLM with the same budget. While we focus on leveraging Repilot for APR in this work, the overall approach is also generalizable to other code generation tasks.},
 author = {Wei, Yuxiang and Xia, Chunqiu Steven and Zhang, Lingming},
 booktitle = {Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
 collection = {ESEC/FSE ’23},
 doi = {10.1145/3611643.3616271},
 month = {November},
 pages = {172–184},
 publisher = {ACM},
 series = {ESEC/FSE ’23},
 title = {Copiloting the Copilots: Fusing Large Language Models with Completion Engines for Automated Program Repair},
 url = {http://dx.doi.org/10.1145/3611643.3616271},
 year = {2023}
}

@inproceedings{Xia_2023,
 abstract = {Automated Program Repair (APR) aims to help developers automatically patch software bugs. However, current state-of-the-art traditional and learning-based APR techniques face the problem of limited patch variety, failing to fix complicated bugs. This is mainly due to the reliance on bug-fixing datasets to craft fix templates or directly predict potential patches. Large Pre-Trained Language Models (PLMs), trained using billions of text/code tokens, can potentially help avoid this issue. Very recently, researchers have directly leveraged PLMs for APR without relying on any bug-fixing datasets. Meanwhile, such existing work either failed to include state-of-the-art PLMs or was not evaluated on realistic datasets.
In this work, we perform the first extensive study on directly applying PLMs for APR. We select 9 recent state-of-the-art PLMs, including both generative and infilling models, ranging from 125M to 20B in size. We designed 3 different repair settings to evaluate the different ways we can use PLMs to generate patches. We apply the PLMs under these repair settings on 5 datasets across 3 different languages and compare different PLMs in the number of bugs fixed, generation speed and compilation rate. Our study demonstrates that directly applying state-of-the-art PLMs can already substantially outperform all existing APR techniques on all our datasets. Among the studied PLMs, the scaling effect exists for APR where larger models tend to achieve better performance. Also, we show for the first time that suffix code after the buggy line (adopted in infilling-style APR) is important in not only generating more fixes but more patches with higher compilation rate. Besides patch generation, the PLMs consider correct patches to be more natural than other ones, and can even be leveraged for effective patch ranking or patch correctness checking.},
 author = {Xia, Chunqiu Steven and Wei, Yuxiang and Zhang, Lingming},
 booktitle = {2023 IEEE/ACM 45th International Conference on Software Engineering (ICSE)},
 doi = {10.1109/icse48619.2023.00129},
 month = {May},
 pages = {1482–1494},
 publisher = {IEEE},
 title = {Automated Program Repair in the Era of Large Pre-trained Language Models},
 url = {http://dx.doi.org/10.1109/ICSE48619.2023.00129},
 year = {2023}
}

@misc{fan2023automatedrepairprogramslarge,
 abstract = {Large language models such as Codex, have shown the capability to produce code for many programming tasks. However, the success rate of existing models is low, especially for complex programming tasks. One of the reasons is that language models lack awareness of program semantics, resulting in incorrect programs, or even programs which do not compile. In this paper, we systematically study whether automated program repair (APR) techniques can fix the incorrect solutions produced by language models in LeetCode contests. The goal is to study whether APR techniques can enhance reliability in the code produced by large language models. Our study revealed that: (1) automatically generated code shares common programming mistakes with human-crafted solutions, indicating APR techniques may have potential to fix auto-generated code; (2) given bug location information provided by a statistical fault localization approach, the newly released Codex edit mode, which supports editing code, is similar to or better than existing Java repair tools TBar and Recoder in fixing incorrect solutions. By analyzing the experimental results generated by these tools, we provide several suggestions: (1) enhancing APR tools to surpass limitations in patch space (e.g., introducing more flexible fault localization) is desirable; (2) as large language models can derive more fix patterns by training on more data, future APR tools could shift focus from adding more fix patterns to synthesis/semantics based approaches, (3) combination of language models with APR to curate patch ingredients, is worth studying.},
 archiveprefix = {arXiv},
 author = {Zhiyu Fan and Xiang Gao and Martin Mirchev and Abhik Roychoudhury and Shin Hwei Tan},
 eprint = {2205.10583},
 primaryclass = {cs.SE},
 title = {Automated Repair of Programs from Large Language Models},
 url = {https://arxiv.org/abs/2205.10583},
 year = {2023}
}

@misc{yoshida2026leveragingmutationanalysisllmbased,
 abstract = {In recent years, Automated Program Repair (APR) techniques specifically designed for quantum programs have been proposed. However, existing approaches often suffer from low repair success rates or poor understandability of the generated patches. In this study, we construct a framework in which a large language model (LLM) generates code repairs along with a natural language explanation of the applied repairs. To investigate how the contextual information included in prompts influences APR performance for quantum programs, we design four prompt configurations with different combinations of static information, dynamic information, and mutation analysis results. Mutation analysis evaluates how small changes to specific parts of a program affect its execution results and provides more detailed dynamic information than simple execution outputs such as stack traces. Our experimental results show that mutation analysis can provide valuable contextual information for LLM-based APR of quantum programs, improving repair success rates (achieving 94.4% in our experiment) and in some cases also improving the quality of generated explanations. Our findings point toward new directions for developing APR techniques for quantum programs that enhance both reliability and explainability.},
 archiveprefix = {arXiv},
 author = {Chihiro Yoshida and Yuta Ishimoto and Olivier Nourry and Masanari Kondo and Makoto Matsushita and Yasutaka Kamei and Yoshiki Higo},
 eprint = {2601.12273},
 primaryclass = {cs.SE},
 title = {Leveraging Mutation Analysis for LLM-based Repair of Quantum Programs},
 url = {https://arxiv.org/abs/2601.12273},
 year = {2026}
}

@misc{twist2025summarymediatedrepairllmsuse,
 abstract = {Large Language Models (LLMs) often produce code with subtle implementation-level bugs despite strong benchmark performance. These errors are hard for LLMs to spot and can have large behavioural effects; yet when asked to summarise code, LLMs can frequently surface high-level intent and sometimes overlook this low-level noise. Motivated by this, we propose summary-mediated repair, a prompt-only pipeline for program repair that leverages natural-language code summarisation as an explicit intermediate step, extending previous work that has already shown code summarisation to be a useful intermediary for downstream tasks. We evaluate our method across eight production-grade LLMs on two function level benchmarks (HumanEvalPack and MBPP), comparing several summary styles against a direct repair baseline. Error-aware diagnostic summaries consistently yield the largest gains - repairing up to 65% of unseen errors, on average of 5% more than the baseline - though overall improvements are modest and LLM-dependent. Our results position summaries as a cheap, human-interpretable diagnostic artefact that can be integrated into program-repair pipelines rather than a stand-alone fix-all.},
 archiveprefix = {arXiv},
 author = {Lukas Twist},
 eprint = {2511.18782},
 primaryclass = {cs.SE},
 title = {Summary-Mediated Repair: Can LLMs use code summarisation as a tool for program repair?},
 url = {https://arxiv.org/abs/2511.18782},
 year = {2025}
}

@misc{shehada2025rethinkingkernelprogramrepair,
 abstract = {Large Language Models (LLMs) have revolutionized automated program repair (APR) but current benchmarks like SWE-Bench predominantly focus on userspace applications and overlook the complexities of kernel-space debugging and repair. The Linux kernel poses unique challenges due to its monolithic structure, concurrency, and low-level hardware interactions. Prior efforts such as KGym and CrashFixer have highlighted the difficulty of APR in this domain, reporting low success rates or relying on costly and complex pipelines and pricey cloud infrastructure. In this work, we introduce RGym, a lightweight, platform-agnostic APR evaluation framework for the Linux kernel designed to operate on local commodity hardware. Built on RGym, we propose a simple yet effective APR pipeline leveraging specialized localization techniques (e.g., call stacks and blamed commits) to overcome the unrealistic usage of oracles in KGym. We test on a filtered and verified dataset of 143 bugs. Our method achieves up to a 43.36% pass rate with GPT-5 Thinking while maintaining a cost of under $0.20 per bug. We further conduct an ablation study to analyze contributions from our proposed localization strategy, prompt structure, and model choice, and demonstrate that feedback-based retries can significantly enhance success rates.},
 archiveprefix = {arXiv},
 author = {Kareem Shehada and Yifan Wu and Wyatt D. Feng and Adithya Iyer and Gryphon Kumfert and Yangruibo Ding and Zhiyun Qian},
 eprint = {2511.15757},
 primaryclass = {cs.SE},
 title = {Rethinking Kernel Program Repair: Benchmarking and Enhancing LLMs with RGym},
 url = {https://arxiv.org/abs/2511.15757},
 year = {2025}
}

@misc{cambronero2026abstainvalidatedualllmpolicy,
 abstract = {Agentic Automated Program Repair (APR) is increasingly tackling complex, repository-level bugs in industry, but ultimately these patches still need to be reviewed by a human before committing them to ensure they address the bug. Showing patches unlikely to be accepted can lead to substantial noise, wasting valuable developer time and eroding trust in automated code changes. We introduce two complementary LLM-based policies to reduce such noise: bug abstention and patch validation policies. Bug abstention excludes bugs that the agentic APR system is unlikely to fix. Patch validation rejects patches that are unlikely to be a good fix for the given bug. We evaluate both policies on three sets of bugs from Google's codebase, and their candidate patches generated by an internal agentic APR system. On a set of 174 human-reported bugs, removing bugs and patches rejected by our policies can raise success rates by up to 13 percentage points and 15 percentage points, respectively, and by up to 39 percentage points in combination. On null pointer exceptions and sanitizer-reported bugs with machine-generated bug reports, patch validation also improves average single-sample success rates. This two-policy approach provides a practical path to the reliable, industrial-scale deployment of agentic APR systems.},
 archiveprefix = {arXiv},
 author = {José Cambronero and Michele Tufano and Sherry Shi and Renyao Wei and Grant Uy and Runxiang Cheng and Chin-Jung Liu and Shiying Pan and Satish Chandra and Pat Rondon},
 doi = {https://doi.org/10.1145/3786583.3786858},
 eprint = {2510.03217},
 primaryclass = {cs.SE},
 title = {Abstain and Validate: A Dual-LLM Policy for Reducing Noise in Agentic Program Repair},
 url = {https://arxiv.org/abs/2510.03217},
 year = {2026}
}

@misc{luiz2025warpwebaugmentedrealtime,
 abstract = {Compilation errors represent a significant bottleneck in software development productivity. This paper introduces WARP (Web-Augmented Real-time Program Repairer), a novel system that leverages Large Language Models (LLMs) and dynamic web-augmented synthesis for real-time resolution of these errors. WARP actively monitors developer terminals, intelligently detects compilation errors, and synergistically combines the understanding of a fine-tuned Code-LLM with relevant solutions, explanations, and code snippets retrieved from up-to-date web sources like developer forums and official documentation. Experimental results on our curated benchmark, CGP (featuring C/C++, Python, and Go errors), demonstrate WARP achieves a superior fix rate (72.5 % Compiles correctly) and higher semantic correctness compared to baseline LLM-only approaches and traditional IDE quick-fixes. Key technical challenges in achieving high-accuracy synthesis from noisy web data.},
 archiveprefix = {arXiv},
 author = {Anderson de Lima Luiz},
 eprint = {2509.25192},
 primaryclass = {cs.SE},
 title = {WARP -- Web-Augmented Real-time Program Repairer: A Real-Time Compilation Error Resolution using LLMs and Web-Augmented Synthesis},
 url = {https://arxiv.org/abs/2509.25192},
 year = {2025}
}

@misc{yang2025inputreductionenhancedllmbased,
 abstract = {Large Language Models (LLMs) have shown great potential in Automated Program Repair (APR). Test inputs, being crucial for reasoning the root cause of failures, are always included in the prompt for LLM-based APR. Unfortunately, LLMs struggle to retain key information in long prompts. When the test inputs are extensive in the prompt, this may trigger the "lost-in-the-middle" issue, compromising repair performance. ReduceFix prompts an LLM to generate a reducer that minimizes failure-inducing test inputs without human effort, and then feeds the reduced failure-inducing inputs to guide patch generation.
For targeted evaluation, we constructed LFTBench, the first long-input APR benchmark with 200 real bugs from 20 programming tasks, each paired with a failure-inducing input whose median size is 1 MB. On this benchmark, ReduceFix shrinks inputs by 89.1% on average and improves overall pass@10 by up to 53.8% relative to a prompt that includes the original test, and by 17.6% compared with omitting the test entirely. Adding the same reduction step to ChatRepair and CREF increases their fix rate by 21.3% and 2.6%, respectively, without other changes. Our gains hold against a ddmin-only reducing template baseline and transfer to repository-level OSS-Fuzz cases. Ablation studies further highlight the impact of input length and compressed failure information on repair success. These results underscore that automatically reducing failing inputs is a practical and powerful complement to LLM-based APR, significantly improving its scalability and effectiveness.},
 archiveprefix = {arXiv},
 author = {Boyang Yang and Luyao Ren and Xin Yin and Jiadong Ren and Haoye Tian and Shunfu Jin},
 eprint = {2507.15251},
 primaryclass = {cs.SE},
 title = {Input Reduction Enhanced LLM-based Program Repair},
 url = {https://arxiv.org/abs/2507.15251},
 year = {2025}
}

@misc{wu2025specificationguidedrepairarithmeticerrors,
 abstract = {Debugging and repairing faults when programs fail to formally verify can be complex and time-consuming. Automated Program Repair (APR) can ease this burden by automatically identifying and fixing faults. However, traditional APR techniques often rely on test suites for validation, but these may not capture all possible scenarios. In contrast, formal specifications provide strong correctness criteria, enabling more effective automated repair.
In this paper, we present an APR tool for Dafny, a verification-aware programming language that uses formal specifications - including pre-conditions, post-conditions, and invariants - as oracles for fault localization and repair. Assuming the correctness of the specifications and focusing on arithmetic bugs, we localize faults through a series of steps, which include using Hoare logic to determine the state of each statement within the program, and applying Large Language Models (LLMs) to synthesize candidate fixes. The models considered are GPT-4o mini, Llama 3, Mistral 7B, and Llemma 7B.
We evaluate our approach using DafnyBench, a benchmark of real-world Dafny programs. Our tool achieves 89.6% fault localization coverage and GPT-4o mini yields the highest repair success rate of 74.18%. These results highlight the potential of combining formal reasoning with LLM-based program synthesis for automated program repair.},
 archiveprefix = {arXiv},
 author = {Valentina Wu and Alexandra Mendes and Alexandre Abreu},
 eprint = {2507.03659},
 primaryclass = {cs.SE},
 title = {Specification-Guided Repair of Arithmetic Errors in Dafny Programs using LLMs},
 url = {https://arxiv.org/abs/2507.03659},
 year = {2025}
}

@misc{sajadi2025safeaigeneratedpatcheslargescale,
 abstract = {Large language models (LLMs) and their agentic frameworks are increasingly adopted to perform development tasks such as automated program repair (APR). While prior work has identified security risks in LLM-generated code, most have focused on synthetic, simplified, or isolated tasks that lack the complexity of real-world program repair. In this study, we present the first large-scale security analysis of LLM-generated patches using 20,000+ GitHub issues. We evaluate patches proposed by developers, a standalone LLM (Llama 3.3 Instruct-70B), and three top-performing agentic frameworks (OpenHands, AutoCodeRover, HoneyComb). Finally, we analyze a wide range of code, issue, and project-level factors to understand the conditions under which generating insecure patches is more likely. Our findings reveal that Llama introduces many new vulnerabilities, exhibiting unique patterns not found in developers' code. Agentic workflows also generate a number of vulnerabilities, particularly when given more autonomy. We find that vulnerabilities in LLM-generated patches are associated with distinctive code characteristics and are commonly observed in issues missing specific types of information. These results suggest that contextual factors play a critical role in the security of the generated patches and point toward the need for proactive risk assessment methods that account for both issue and code-level information.},
 archiveprefix = {arXiv},
 author = {Amirali Sajadi and Kostadin Damevski and Preetha Chatterjee},
 eprint = {2507.02976},
 primaryclass = {cs.CR},
 title = {How Safe Are AI-Generated Patches? A Large-scale Study on Security Risks in LLM and Agentic Automated Program Repair on SWE-bench},
 url = {https://arxiv.org/abs/2507.02976},
 year = {2025}
}

@misc{ehsani2025hierarchicalknowledgeinjectionimproving,
 abstract = {Prompting LLMs with bug-related context (e.g., error messages, stack traces) improves automated program repair, but many bugs still remain unresolved. In real-world projects, developers often rely on broader repository and project-level context beyond the local code to resolve such bugs. In this paper, we investigate how automatically extracting and providing such knowledge can improve LLM-based program repair. We propose a layered knowledge injection framework that incrementally augments LLMs with structured context. It starts with the Bug Knowledge Layer, which includes information such as the buggy function and failing tests; expands to the Repository Knowledge Layer, which adds structural dependencies, related files, and commit history; and finally injects the Project Knowledge Layer, which incorporates relevant details from documentation and previously fixed bugs. We evaluate this framework on a dataset of 314 bugs from BugsInPy using two LLMs (Llama 3.3 and GPT-4o-mini), and analyze fix rates across six bug types. By progressively injecting knowledge across layers, our approach achieves a fix rate of 79% (250/314) using Llama 3.3, a significant improvement of 23% over previous work. All bug types show improvement with the addition of repository-level context, while only a subset benefit further from project-level knowledge, highlighting that different bug types require different levels of contextual information for effective repair. We also analyze the remaining unresolved bugs and find that more complex and structurally isolated bugs, such as Program Anomaly and GUI bugs, remain difficult even after injecting all available information. Our results show that layered context injection improves program repair and suggest the need for interactive and adaptive APR systems.},
 archiveprefix = {arXiv},
 author = {Ramtin Ehsani and Esteban Parra and Sonia Haiduc and Preetha Chatterjee},
 eprint = {2506.24015},
 primaryclass = {cs.SE},
 title = {Hierarchical Knowledge Injection for Improving LLM-based Program Repair},
 url = {https://arxiv.org/abs/2506.24015},
 year = {2025}
}

@misc{yang2025surveyllmbasedautomatedprogram,
 abstract = {Large language models (LLMs) are reshaping automated program repair. We present a unified taxonomy that groups 62 recent LLM-based repair systems into four paradigms defined by parameter adaptation and control authority over the repair loop, and overlays two cross-cutting layers for retrieval and analysis augmentation. Prior surveys have either focused on classical software repair techniques, on LLMs in software engineering more broadly, or on subsets of LLM-based software repair, such as fine-tuning strategies or vulnerability repair. We complement these works by treating fine-tuning, prompting, procedural pipelines, and agentic frameworks as first-class paradigms and systematically mapping representative systems to each of these paradigms. We also consolidate evaluation practice on common benchmarks by recording benchmark scope, pass@k, and fault-localization assumptions to support a more meaningful comparison of reported success rates. We clarify trade-offs among paradigms in task alignment, deployment cost, controllability, and ability to repair multi-hunk or cross-file bugs. We discuss challenges in current LLM-based software repair and outline research directions. Our artifacts, including the representation papers and scripted survey pipeline, are publicly available at this https URL.},
 archiveprefix = {arXiv},
 author = {Boyang Yang and Zijian Cai and Fengling Liu and Bach Le and Lingming Zhang and Tegawendé F. Bissyandé and Yang Liu and Haoye Tian},
 eprint = {2506.23749},
 primaryclass = {cs.SE},
 title = {A Survey of LLM-based Automated Program Repair: Taxonomies, Design Paradigms, and Applications},
 url = {https://arxiv.org/abs/2506.23749},
 year = {2025}
}

@misc{mu2025experepairdualmemoryenhancedllmbased,
 abstract = {Automatically repairing software issues remains a fundamental challenge at the intersection of software engineering and AI. Although recent advancements in Large Language Models (LLMs) have demonstrated potential for repository-level repair tasks, current methodologies exhibit two notable limitations: (1) they often address issues in isolation, neglecting to incorporate insights from previously resolved issues, and (2) they rely on static and rigid prompting strategies, which constrain their ability to generalize across diverse and evolving issue scenarios. Inspired by the dual memory systems of human cognition, where episodic and semantic memories work synergistically to support human reasoning and decision-making, we propose ExpeRepair, a novel LLM-based approach that continuously learns from historical repair experiences through dual-channel knowledge accumulation. ExpeRepair organizes historical repair experiences into two complementary memories: an episodic memory that stores concrete repair demonstrations, and a semantic memory that encodes abstract reflective insights. At inference time, ExpeRepair activates both memory systems by retrieving relevant demonstrations from episodic memory and recalling high-level repair insights from semantic memory. It further enhances adaptability through dynamic prompt composition, synergistically integrating both memory types to replace static prompts with context-aware, experience-driven prompts. Experiments on the SWE-bench Lite benchmark demonstrate that ExpeRepair achieves a pass@1 score of 49.3% with Claude 3.7 Sonnet, outperforming all state-of-the-art open-source methods.},
 archiveprefix = {arXiv},
 author = {Fangwen Mu and Junjie Wang and Lin Shi and Song Wang and Shoubin Li and Qing Wang},
 eprint = {2506.10484},
 primaryclass = {cs.SE},
 title = {EXPEREPAIR: Dual-Memory Enhanced LLM-based Repository-Level Program Repair},
 url = {https://arxiv.org/abs/2506.10484},
 year = {2025}
}

@misc{tang2025boostingopensourcellmsprogram,
 abstract = {Several closed-source LLMs have consistently outperformed open-source alternatives in program repair tasks, primarily due to their superior reasoning capabilities and extensive pre-training. This paper introduces Repairity, a novel three-stage methodology that significantly narrows this performance gap through reasoning extraction and reinforcement learning. Our approach: (1) systematically filters high-quality reasoning traces from closed-source models using correctness verification, (2) transfers this reasoning knowledge to open-source models via supervised fine-tuning, and (3) develops reinforcement learning with LLM-based feedback to further optimize performance. Empirical evaluation across multiple program repair benchmarks demonstrates that Repairity improves the performance of Qwen2.5-Coder-32B-Instruct, a base open source LLM, by 8.68\% on average, reducing the capability gap with Claude-Sonnet3.7, a state-of-the-art closed-source model, from 10.05% to 1.35%. Ablation studies confirm that both reasoning extraction and LLM-guided reinforcement learning contribute significantly to these improvements. Our methodology generalizes effectively to additional code-related tasks, enabling organizations to leverage high-quality program repair capabilities while maintaining the customizability, transparency, and deployment flexibility inherent to open-source models.},
 archiveprefix = {arXiv},
 author = {Xunzhu Tang and Jacques Klein and Tegawendé F. Bissyandé},
 eprint = {2506.03921},
 primaryclass = {cs.SE},
 title = {Boosting Open-Source LLMs for Program Repair via Reasoning Transfer and LLM-Guided Reinforcement Learning},
 url = {https://arxiv.org/abs/2506.03921},
 year = {2025}
}

@misc{haque2025effectivelyleveragingexecutiontraces,
 abstract = {Large Language Models (LLMs) show promising performance on various programming tasks, including Automatic Program Repair (APR). However, most approaches to LLM-based APR are limited to the static analysis of the programs, while disregarding their runtime behavior. Inspired by knowledge-augmented NLP, in this work, we aim to remedy this potential blind spot by augmenting standard APR prompts with program execution traces. We evaluate our approach using the GPT family of models on three popular APR datasets. Our findings suggest that simply incorporating execution traces into the prompt provides a limited performance improvement over trace-free baselines, in only 2 out of 6 tested dataset / model configurations. We further find that the effectiveness of execution traces for APR diminishes as their complexity increases. We explore several strategies for leveraging traces in prompts and demonstrate that LLM-optimized prompts help outperform trace-free prompts more consistently. Additionally, we show trace-based prompting to be superior to finetuning a smaller LLM on a small-scale dataset; and conduct probing studies reinforcing the notion that execution traces can complement the reasoning abilities of the LLMs.},
 archiveprefix = {arXiv},
 author = {Mirazul Haque and Petr Babkin and Farima Farmahinifarahani and Manuela Veloso},
 eprint = {2505.04441},
 primaryclass = {cs.LG},
 title = {Towards Effectively Leveraging Execution Traces for Program Repair with Code LLMs},
 url = {https://arxiv.org/abs/2505.04441},
 year = {2025}
}

@misc{li2025evaluatinggeneralizabilityllmsautomated,
 abstract = {LLM-based automated program repair methods have attracted significant attention for their state-of-the-art performance. However, they were primarily evaluated on a few well known datasets like Defects4J, raising questions about their effectiveness on new datasets. In this study, we evaluate 11 top-performing LLMs on DEFECTS4J-TRANS, a new dataset derived from transforming Defects4J while maintaining the original semantics. Results from experiments on both Defects4J and DEFECTS4J-TRANS show that all studied LLMs have limited generalizability in APR tasks, with the average number of correct and plausible patches decreasing by 49.48% and 42.90%, respectively, on DEFECTS4J-TRANS. Further investigation into incorporating additional repair-relevant information in repair prompts reveals that, although this information significantly enhances the LLMs' capabilities (increasing the number of correct and plausible patches by up to 136.67% and 121.82%, respectively), performance still falls short of their original results. This indicates that prompt engineering alone is insufficient to substantially enhance LLMs' repair capabilities. Based on our study, we also offer several recommendations for future research.},
 archiveprefix = {arXiv},
 author = {Fengjie Li and Jiajun Jiang and Jiajun Sun and Hongyu Zhang},
 eprint = {2503.09217},
 primaryclass = {cs.SE},
 title = {Evaluating the Generalizability of LLMs in Automated Program Repair},
 url = {https://arxiv.org/abs/2503.09217},
 year = {2025}
}

@misc{luo2024finetuningllmsmeetsdata,
 abstract = {Software systems have been evolving rapidly and inevitably introducing bugs at an increasing rate, leading to significant losses in resources consumed by software maintenance. Recently, large language models (LLMs) have demonstrated remarkable potential in enhancing software development and maintenance practices, particularly in automated program repair (APR) with improved accuracy and efficiency of bug fixing. However, LLM-based APR heavily relies on high-quality code repositories. A larger portion of existing code repositories are for private use and proprietary assets from various industries, reflecting more diversity and nuances in the data since real-world industries often have more extensive software development practices, which cannot be covered by merely public datasets. Therefore, utilizing private datasets shows significant potential in enhancing software development and maintenance. However, obtaining such data from various industries is hindered by data privacy concerns, as companies are reluctant to share their codebases. To address the gap, we investigate the use of federated learning as a privacy-preserving approach that enables private entities to fine-tune LLMs on proprietary and decentralized data, facilitating the collaboration between clients to fully utilize their data to help enhance software development and maintenance. Our evaluation reveals that federated fine-tuning can effectively enhance program repair capabilities. Notably, the impact of heterogeneous code on LLM fine-tuning is negligible, indicating that real-world industries can benefit from collaborative development regardless of diverse data distributions. Furthermore, each type of federated algorithm exhibits unique strengths across different LLMs, suggesting that fine-tuning for program repair can be enhanced by tailoring the optimization process to specific characteristics of different LLMs.},
 archiveprefix = {arXiv},
 author = {Wenqiang Luo and Jacky Wai Keung and Boyang Yang and He Ye and Claire Le Goues and Tegawende F. Bissyande and Haoye Tian and Bach Le},
 eprint = {2412.01072},
 primaryclass = {cs.SE},
 title = {When Fine-Tuning LLMs Meets Data Privacy: An Empirical Study of Federated Learning in LLM-Based Program Repair},
 url = {https://arxiv.org/abs/2412.01072},
 year = {2024}
}

@misc{xue2025exploringliftingrobustnessllmpowered,
 abstract = {In recent years, Large language model-powered Automated Program Repair (LAPR) techniques have achieved state-of-the-art bug-fixing performance and have been pervasively applied and studied in both industry and academia. Nonetheless, LLMs were proved to be highly sensitive to input prompts, with slight differences in the expressions of semantically equivalent programs potentially causing repair failures. Therefore, it is crucial to conduct robustness testing on LAPR techniques before their practical deployment. However, related research is scarce. To this end, we propose MT-LAPR, a Metamorphic Testing framework exclusively for LAPR techniques, which summarizes nine widely-recognized Metamorphic Relations (MRs) by developers across three perturbation levels: token, statement, and block. Afterward, our proposed MRs are applied to buggy codes to generate test cases, which are semantically equivalent yet to affect the inference of LAPR. Experiments are carried out on two extensively examined bug-fixing datasets, i.e., Defect4J and QuixBugs, and four bug-fixing abled LLMs released recently, demonstrating that 34.4% - 48.5% of the test cases expose the instability of LAPR techniques on average, showing the effectiveness of MT-LAPR and uncovering a positive correlation between code readability and the robustness of LAPR techniques. Inspired by the above findings, this paper uses the test cases generated by MT-LAPR as samples to train a CodeT5-based code editing model aiming at improving code readability and then embeds it into the LAPR workflow as a data preprocessing step. Extensive experiments demonstrate that this approach significantly enhances the robustness of LAPR by 49.32% at most.},
 archiveprefix = {arXiv},
 author = {Pengyu Xue and Linhao Wu and Zhen Yang and Zhongxing Yu and Zhi Jin and Ge Li and Yan Xiao and Shuo Liu and Xinyi Li and Hongyi Lin and Jingwen Wu},
 eprint = {2410.07516},
 primaryclass = {cs.SE},
 title = {Exploring and Lifting the Robustness of LLM-powered Automated Program Repair with Metamorphic Testing},
 url = {https://arxiv.org/abs/2410.07516},
 year = {2025}
}

@misc{dehghan2025mergerepairexploratorystudymerging,
 abstract = {Large Language Models (LLMs) have shown high capabilities in several software development-related tasks such as program repair, documentation, code refactoring, debugging, and testing. However, training these models requires massive amount of data and significant computational resources. Adapters are specialized, small modules designed for parameter efficient fine-tuning of LLMs for specific tasks, domains, or applications without requiring extensive retraining of the entire model. These adapters offer a more efficient way to customize LLMs for particular needs, leveraging the pre-existing capabilities of the large model. Model (and adapter) merging have emerged as a technique to develop one model capable of multiple tasks, with minimal or no training required. Although model and adapter merging has shown promising performance in domains such as natural language processing and computer vision, its applicability to software engineering tasks remains underexplored. In this paper, we investigate the effectiveness of merged adapters within the context of software engineering, with a particular focus on the Automated Program Repair (APR) task, through our approach, MergeRepair. In particular, we merge multiple task-specific adapters using three different merging methods, including weight-averaging, ties, and dare-ties, and evaluate the performance of the merged adapter on the APR task. We introduce a continual merging approach, a novel method in which we sequentially merge the task-specific adapters where the order and weight of the merged adapters play a significant role. We further compare the performance of our approach with a baseline method consisting of equal-weight merging applied on parameters of different adapters, where all adapters are of equal importance.},
 archiveprefix = {arXiv},
 author = {Meghdad Dehghan and Jie JW Wu and Fatemeh H. Fard and Ali Ouni},
 eprint = {2408.09568},
 primaryclass = {cs.SE},
 title = {MergeRepair: An Exploratory Study on Merging Task-Specific Adapters in Code LLMs for Automated Program Repair},
 url = {https://arxiv.org/abs/2408.09568},
 year = {2025}
}

@misc{yang2024crefllmbasedconversationalsoftware,
 abstract = {Program repair techniques offer cost-saving benefits for debugging within software development and programming education scenarios. With the proven effectiveness of Large Language Models (LLMs) in code-related tasks, researchers have explored their potential for program repair. However, it is crucial to recognize that existing repair benchmarks may have influenced LLM training data, potentially causing data leakage. To evaluate LLMs' realistic repair capabilities, (1) we introduce an extensive, non-crawled benchmark, referred to as TutorCode, comprising 1,239 C++ defect codes and associated information such as tutor guidance, solution description, failing test cases, and the corrected code. Our work assesses the repair performance of 12 LLMs on TutorCode, measuring repair correctness (TOP-5 and AVG-5) and patch precision (RPSR). (2) We then provide a comprehensive investigation into which types of extra information can help LLMs improve their performance in repairing defects. Among these types, tutor guidance was found to be the most effective information in enhancing LLM repair capabilities. To fully harness LLMs' conversational capabilities and the benefits of augmented information, (3) we introduce a novel conversational semi-automatic repair framework CREF assisting human tutor. It demonstrates a remarkable AVG-5 improvement of 17.2%-24.6% compared to the baseline, achieving an impressive AVG-5 of 76.6% when utilizing GPT-4. These results highlight the potential for enhancing LLMs' repair capabilities through interactions with tutors and historical conversations involving incorrect responses. The successful application of CREF in a real-world educational setting demonstrates its effectiveness in reducing tutors' workload and improving students' learning experience, while also showcasing its promise for facilitating other software engineering tasks, such as code review.},
 archiveprefix = {arXiv},
 author = {Boyang Yang and Haoye Tian and Weiguo Pian and Haoran Yu and Haitao Wang and Jacques Klein and Tegawendé F. Bissyandé and Shunfu Jin},
 eprint = {2406.13972},
 primaryclass = {cs.SE},
 title = {CREF: An LLM-based Conversational Software Repair Framework for Programming Tutors},
 url = {https://arxiv.org/abs/2406.13972},
 year = {2024}
}

@misc{xu2025aligningobjectivellmbasedprogram,
 abstract = {Large language models (LLMs) have achieved decent results on automated program repair (APR). However, the next token prediction training objective of decoder-only LLMs (e.g., GPT-4) is misaligned with the masked span prediction objective of current infilling-style methods, which impedes LLMs from fully leveraging pre-trained knowledge for program repair. In addition, while some LLMs can locate and repair bugs in certain functions using the related artifacts (e.g., test cases), existing methods still depend on statement-level fault localization methods to provide a list of buggy hunks for repair. This restriction hinders LLMs from exploring potential patches beyond the given locations.
In this paper, we investigate a new approach to adapt LLMs to program repair. Our core insight is that LLM's APR capability can be greatly improved by simply aligning the output to their training objective and allowing them to refine the whole program without first identifying faulty statements. Based on this insight, we designed D4C, a straightforward prompting framework for APR. D4C can repair 180 bugs correctly in Defects4J, with each patch being sampled only 10 times. This surpasses the SOTA APR methods with perfect fault localization by 10% and reduces the patch sampling number by 90%. Our findings reveal that (1) objective alignment is crucial for fully exploiting LLM's pre-trained capability, and (2) replacing the traditional localize-buggy-hunks-then-repair workflow with direct debugging is more effective for LLM-based APR methods. Thus, we believe this paper introduces a new mindset for harnessing LLMs in APR.},
 archiveprefix = {arXiv},
 author = {Junjielong Xu and Ying Fu and Shin Hwei Tan and Pinjia He},
 eprint = {2404.08877},
 primaryclass = {cs.SE},
 title = {Aligning the Objective of LLM-based Program Repair},
 url = {https://arxiv.org/abs/2404.08877},
 year = {2025}
}

@misc{parasaram2024factselectionproblemllmbased,
 abstract = {Recent research has shown that incorporating bug-related facts, such as stack traces and GitHub issues, into prompts enhances the bug-fixing capabilities of large language models (LLMs). Considering the ever-increasing context window of these models, a critical question arises: what and how many facts should be included in prompts to maximise the chance of correctly fixing bugs? To answer this question, we conducted a large-scale study, employing over 19K prompts featuring various combinations of seven diverse facts to rectify 314 bugs from open-source Python projects within the BugsInPy benchmark. Our findings revealed that each fact, ranging from simple syntactic details like code context to semantic information previously unexplored in the context of LLMs such as angelic values, is beneficial. Specifically, each fact aids in fixing some bugs that would remain unresolved or only be fixed with a low success rate without it. Importantly, we discovered that the effectiveness of program repair prompts is non-monotonic over the number of used facts; using too many facts leads to subpar outcomes. These insights led us to define the fact selection problem: determining the optimal set of facts for inclusion in a prompt to maximise LLM's performance on a given task instance. We found that there is no one-size-fits-all set of facts for bug repair. Therefore, we developed a basic statistical model, named Maniple, which selects facts specific to a given bug to include in the prompt. This model significantly surpasses the performance of the best generic fact set. To underscore the significance of the fact selection problem, we benchmarked Maniple against the state-of-the-art zero-shot, non-conversational LLM-based bug repair methods. On our testing dataset of 157 bugs, Maniple repairs 88 bugs, 17% above the best configuration.},
 archiveprefix = {arXiv},
 author = {Nikhil Parasaram and Huijie Yan and Boyu Yang and Zineb Flahy and Abriele Qudsi and Damian Ziaber and Earl Barr and Sergey Mechtaev},
 eprint = {2404.05520},
 primaryclass = {cs.SE},
 title = {The Fact Selection Problem in LLM-Based Program Repair},
 url = {https://arxiv.org/abs/2404.05520},
 year = {2024}
}

@misc{bouzenia2024repairagentautonomousllmbasedagent,
 abstract = {Automated program repair has emerged as a powerful technique to mitigate the impact of software bugs on system reliability and user experience. This paper introduces RepairAgent, the first work to address the program repair challenge through an autonomous agent based on a large language model (LLM). Unlike existing deep learning-based approaches, which prompt a model with a fixed prompt or in a fixed feedback loop, our work treats the LLM as an agent capable of autonomously planning and executing actions to fix bugs by invoking suitable tools. RepairAgent freely interleaves gathering information about the bug, gathering repair ingredients, and validating fixes, while deciding which tools to invoke based on the gathered information and feedback from previous fix attempts. Key contributions that enable RepairAgent include a set of tools that are useful for program repair, a dynamically updated prompt format that allows the LLM to interact with these tools, and a finite state machine that guides the agent in invoking the tools. Our evaluation on the popular Defects4J dataset demonstrates RepairAgent's effectiveness in autonomously repairing 164 bugs, including 39 bugs not fixed by prior techniques. Interacting with the LLM imposes an average cost of 270,000 tokens per bug, which, under the current pricing of OpenAI's GPT-3.5 model, translates to 14 cents of USD per bug. To the best of our knowledge, this work is the first to present an autonomous, LLM-based agent for program repair, paving the way for future agent-based techniques in software engineering.},
 archiveprefix = {arXiv},
 author = {Islem Bouzenia and Premkumar Devanbu and Michael Pradel},
 eprint = {2403.17134},
 primaryclass = {cs.SE},
 title = {RepairAgent: An Autonomous, LLM-Based Agent for Program Repair},
 url = {https://arxiv.org/abs/2403.17134},
 year = {2024}
}

@misc{hidvégi2024cigarcostefficientprogramrepair,
 abstract = {Large language models (LLM) have proven to be effective at automated program repair (APR). However, using LLMs can be costly, with companies invoicing users by the number of tokens. In this paper, we propose CigaR, the first LLM-based APR tool that focuses on minimizing the repair cost. CigaR works in two major steps: generating a first plausible patch and multiplying plausible patches. CigaR optimizes the prompts and the prompt setting to maximize the information given to LLMs using the smallest possible number of tokens. Our experiments on 429 bugs from the widely used Defects4J and HumanEval-Java datasets shows that CigaR reduces the token cost by 73%. On average, CigaR spends 127k tokens per bug while the baseline uses 467k tokens per bug. On the subset of bugs that are fixed by both, CigaR spends 20k per bug while the baseline uses 608k tokens, a cost saving of 96%. Our extensive experiments show that CigaR is a cost-effective LLM-based program repair tool that uses a low number of tokens to automatically generate patches.},
 archiveprefix = {arXiv},
 author = {Dávid Hidvégi and Khashayar Etemadi and Sofia Bobadilla and Martin Monperrus},
 eprint = {2402.06598},
 primaryclass = {cs.SE},
 title = {CigaR: Cost-efficient Program Repair with LLMs},
 url = {https://arxiv.org/abs/2402.06598},
 year = {2024}
}

@misc{wu2023condefectsnewdatasetaddress,
 abstract = {With the growing interest on Large Language Models (LLMs) for fault localization and program repair, ensuring the integrity and generalizability of the LLM-based methods becomes paramount. The code in existing widely-adopted benchmarks for these tasks was written before the the bloom of LLMs and may be included in the training data of existing popular LLMs, thereby suffering from the threat of data leakage, leading to misleadingly optimistic performance metrics. To address this issue, we introduce "ConDefects", a novel dataset of real faults meticulously curated to eliminate such overlap. ConDefects contains 1,254 Java faulty programs and 1,625 Python faulty programs. All these programs are sourced from the online competition platform AtCoder and were produced between October 2021 and September 2023. We pair each fault with fault locations and the corresponding repaired code versions, making it tailored for in fault localization and program repair related research. We also provide interfaces for selecting subsets based on different time windows and coding task difficulties. While inspired by LLM-based tasks, ConDefects can be adopted for benchmarking ALL types of fault localization and program repair methods. The dataset is publicly available, and a demo video can be found at this https URL.},
 archiveprefix = {arXiv},
 author = {Yonghao Wu and Zheng Li and Jie M. Zhang and Yong Liu},
 eprint = {2310.16253},
 primaryclass = {cs.SE},
 title = {ConDefects: A New Dataset to Address the Data Leakage Concern for LLM-based Fault Localization and Program Repair},
 url = {https://arxiv.org/abs/2310.16253},
 year = {2023}
}

@misc{jin2023inferfixendtoendprogramrepair,
 abstract = {Software development life cycle is profoundly influenced by bugs: their introduction, identification, and eventual resolution account for a significant portion of software cost. This has motivated software engineering researchers and practitioners to propose different approaches for automating the identification and repair of software defects. Large language models have been adapted to the program repair task through few-shot demonstration learning and instruction prompting, treating this as an infilling task. However, these models have only focused on learning general bug-fixing patterns for uncategorized bugs mined from public repositories. In this paper, we propose InferFix: a transformer-based program repair framework paired with a state-of-the-art static analyzer to fix critical security and performance bugs. InferFix combines a Retriever -- transformer encoder model pretrained via contrastive learning objective, which aims at searching for semantically equivalent bugs and corresponding fixes; and a Generator -- a large language model (Codex Cushman) finetuned on supervised bug-fix data with prompts augmented via bug type annotations and semantically similar fixes retrieved from an external non-parametric memory. To train and evaluate our approach, we curated InferredBugs, a novel, metadata-rich dataset of bugs extracted by executing the Infer static analyzer on the change histories of thousands of Java and C# repositories. Our evaluation demonstrates that InferFix outperforms strong LLM baselines, with a top-1 accuracy of 65.6% for generating fixes in C# and 76.8% in Java. We discuss the deployment of InferFix alongside Infer at Microsoft which offers an end-to-end solution for detection, classification, and localization of bugs, as well as fixing and validation of candidate patches, integrated in the continuous integration pipeline to automate the software development workflow.},
 archiveprefix = {arXiv},
 author = {Matthew Jin and Syed Shahriar and Michele Tufano and Xin Shi and Shuai Lu and Neel Sundaresan and Alexey Svyatkovskiy},
 eprint = {2303.07263},
 primaryclass = {cs.SE},
 title = {InferFix: End-to-End Program Repair with LLMs},
 url = {https://arxiv.org/abs/2303.07263},
 year = {2023}
}

@misc{joshi2022repairnearlygenerationmultilingual,
 abstract = {Most programmers make mistakes when writing code. Some of these mistakes are small and require few edits to the original program -- a class of errors recently termed last mile mistakes. These errors break the flow for experienced developers and can stump novice programmers. Existing automated repair techniques targeting this class of errors are language-specific and do not easily carry over to new languages. Transferring symbolic approaches requires substantial engineering and neural approaches require data and retraining. We introduce RING, a multilingual repair engine powered by a large language model trained on code (LLMC) such as Codex. Such a multilingual engine enables a flipped model for programming assistance, one where the programmer writes code and the AI assistance suggests fixes, compared to traditional code suggestion technology. Taking inspiration from the way programmers manually fix bugs, we show that a prompt-based strategy that conceptualizes repair as localization, transformation, and candidate ranking, can successfully repair programs in multiple languages with minimal effort. We present the first results for such a multilingual repair engine by evaluating on 6 different languages and comparing performance to language-specific repair engines. We show that RING can outperform language-specific repair engines for three of these languages.},
 archiveprefix = {arXiv},
 author = {Harshit Joshi and José Cambronero and Sumit Gulwani and Vu Le and Ivan Radicek and Gust Verbruggen},
 eprint = {2208.11640},
 primaryclass = {cs.SE},
 title = {Repair Is Nearly Generation: Multilingual Program Repair with LLMs},
 url = {https://arxiv.org/abs/2208.11640},
 year = {2022}
}

@misc{akbarpour2025collaborativeagentsautomatedprogram,
 abstract = {Automated Program Repair (APR) has advanced rapidly with Large Language Models (LLMs), but most existing methods remain computationally expensive, and focused on a small set of languages. Ruby, despite its widespread use in web development and the persistent challenges faced by its developers, has received little attention in APR research. In this paper, we introduce RAMP, a novel lightweight framework that formulates program repair as a feedback-driven, iterative process for Ruby. RAMP employs a team of collaborative agents that generate targeted tests, reflect on errors, and refine candidate fixes until a correct solution is found. Unlike prior approaches, RAMP is designed to avoid reliance on large multilingual repair databases or costly fine-tuning, instead operating directly on Ruby through lightweight prompting and test-driven feedback. Evaluation on the XCodeEval benchmark shows that RAMP achieves a pass@1 of 67% on Ruby, outper-forming prior approaches. RAMP converges quickly within five iterations, and ablation studies confirm that test generation and self-reflection are key drivers of its performance. Further analysis shows that RAMP is particularly effective at repairing wrong answers, compilation errors, and runtime errors. Our approach provides new insights into multi-agent repair strategies, and establishes a foundation for extending LLM-based debugging tools to under-studied languages.},
 archiveprefix = {arXiv},
 author = {Nikta Akbarpour and Mahdieh Sadat Benis and Fatemeh Hendijani Fard and Ali Ouni and Mohamed Aymen Saied},
 eprint = {2511.03925},
 primaryclass = {cs.SE},
 title = {Collaborative Agents for Automated Program Repair in Ruby},
 url = {https://arxiv.org/abs/2511.03925},
 year = {2025}
}

@misc{shi2025hafixagenthistoryawareautomatedprogram,
 abstract = {Automated program repair (APR) has recently shifted toward large language models and agent-based systems, yet most systems rely on local snapshot context, overlooking repository history. Prior work shows that repository history helps repair single-line bugs, since the last commit touching the buggy line is often the bug-introducing one. In this paper, we investigate whether repository history can also improve agentic APR systems at scale, especially for complex multi-hunk bugs. We present HAFixAgent, a History-Aware Bug-Fixing Agent that injects blame-derived repository heuristics into its repair loop. A preliminary study of all 854 real-world bugs from Defects4J motivates our design, showing that bug-relevant history is both widely available and highly concentrated. Empirical comparison of HAFixAgent with two state-of-the-art baselines shows: (1) Effectiveness: HAFixAgent significantly improves over the agent-based baseline (by 212.3%) and the multi-hunk baseline (by 29.9%). (2) Efficiency: history does not significantly increase agent steps and keeps token costs comparable, with notably lower median costs for complex multi-file-multi-hunk bugs. (3) Practicality: combining different historical heuristics repairs more bugs, offering a clear cost-benefit trade-off. HAFixAgent offers a practical recipe for history-aware agentic APR: ground the agent in version control history, prioritize diff-based historical context, and integrate complementary heuristics when needed.},
 archiveprefix = {arXiv},
 author = {Yu Shi and Hao Li and Bram Adams and Ahmed E. Hassan},
 eprint = {2511.01047},
 primaryclass = {cs.SE},
 title = {HAFixAgent: History-Aware Automated Program Repair Agent},
 url = {https://arxiv.org/abs/2511.01047},
 year = {2025}
}

@misc{pabba2025refineenhancingprogramrepair,
 abstract = {Large Language Models (LLMs) have recently shown strong potential in automatic program repair (APR), especially in repository-level settings where the goal is to generate patches based on natural language issue descriptions, large codebases, and regression tests. However, despite their promise, current LLM-based APR techniques often struggle to produce correct fixes due to limited understanding of code context and over-reliance on incomplete test suites. As a result, they frequently generate Draft Patches-partially correct patches that either incompletely address the bug or overfit to the test cases. In this work, we propose a novel patch refinement framework, Refine, that systematically transforms Draft Patches into correct ones. Refine addresses three key challenges: disambiguating vague issue and code context, diversifying patch candidates through test-time scaling, and aggregating partial fixes via an LLM-powered code review process. We implement Refine as a general refinement module that can be integrated into both open-agent-based and workflow-based APR systems. Our evaluation on the SWE-Bench Lite benchmark shows that Refine achieves state-of-the-art results among workflow-based approaches and approaches the best-known performance across all APR categories. Specifically, Refine boosts AutoCodeRover's performance by 14.67%, achieving a score of 51.67% and surpassing all prior baselines. On SWE-Bench Verified, Refine improves the resolution rate by 12.2%, and when integrated across multiple APR systems, it yields an average improvement of 14%-demonstrating its broad effectiveness and generalizability. These results highlight the effectiveness of refinement as a missing component in current APR pipelines and the potential of agentic collaboration in closing the gap between near-correct and correct patches. We also open source our code.},
 archiveprefix = {arXiv},
 author = {Anvith Pabba and Simin Chen and Alex Mathai and Anindya Chakraborty and Baishakhi Ray},
 eprint = {2510.03588},
 primaryclass = {cs.SE},
 title = {REFINE: Enhancing Program Repair Agents through Context-Aware Patch Refinement},
 url = {https://arxiv.org/abs/2510.03588},
 year = {2025}
}

@misc{cambronero2026abstainvalidatedualllmpolicy,
 abstract = {Agentic Automated Program Repair (APR) is increasingly tackling complex, repository-level bugs in industry, but ultimately these patches still need to be reviewed by a human before committing them to ensure they address the bug. Showing patches unlikely to be accepted can lead to substantial noise, wasting valuable developer time and eroding trust in automated code changes. We introduce two complementary LLM-based policies to reduce such noise: bug abstention and patch validation policies. Bug abstention excludes bugs that the agentic APR system is unlikely to fix. Patch validation rejects patches that are unlikely to be a good fix for the given bug. We evaluate both policies on three sets of bugs from Google's codebase, and their candidate patches generated by an internal agentic APR system. On a set of 174 human-reported bugs, removing bugs and patches rejected by our policies can raise success rates by up to 13 percentage points and 15 percentage points, respectively, and by up to 39 percentage points in combination. On null pointer exceptions and sanitizer-reported bugs with machine-generated bug reports, patch validation also improves average single-sample success rates. This two-policy approach provides a practical path to the reliable, industrial-scale deployment of agentic APR systems.},
 archiveprefix = {arXiv},
 author = {José Cambronero and Michele Tufano and Sherry Shi and Renyao Wei and Grant Uy and Runxiang Cheng and Chin-Jung Liu and Shiying Pan and Satish Chandra and Pat Rondon},
 doi = {https://doi.org/10.1145/3786583.3786858},
 eprint = {2510.03217},
 primaryclass = {cs.SE},
 title = {Abstain and Validate: A Dual-LLM Policy for Reducing Noise in Agentic Program Repair},
 url = {https://arxiv.org/abs/2510.03217},
 year = {2026}
}

@misc{maddila2025agenticprogramrepairtest,
 abstract = {Aim: With the advent of LLMs, sophisticated agentic program repair has become viable at large organizations with large codebases. In this work, we develop an Engineering Agent that fixes the source code based on test failures at scale across diverse software offerings internally.
Method: Using Llama as the base, we employ the ReAct harness to develop an agent. We start with a test failure that was triaged by a rule-based test failure bot. We then set up an agentic harness and allow the agent to reason and run a set of 15 actions from reading a file to generating a patch. We provide feedback to the agent through static analysis and test failures so it can refine its solution. We leverage an LLM-as-a-Judge to ensure that the patch conforms to the standards followed by a human review to land fixes.
Benchmark Findings: We curated offline benchmarks for our patch generator, the Engineering Agent loop, and the LLM-as-a-Judge. In offline evaluations we found that a specialized 70B model is highly competitive with the much larger but vanilla Llama-405B. In an ablation study, we found that the ReAct harness (neural model) benefited from the symbolic information from static analysis tools and test execution traces. A model that strikes a balance between the solve rate and error rate vs the cost and latency has a benchmark solve rate of 42.3% using an average 11.8 feedback iterations.
Production Findings: In a three month period, 80% of the generated fixes were reviewed, of which 31.5% were landed (25.5% of the total number of generated fixes).
Feedback from Engineers: We used open coding to extract qualitative themes from engineers' feedback. We saw positive feedback in the form of quick approvals, gratitude, and surprise. We also found mixed feedback when the Engineering Agent's solution was partially correct and it served as a good starting point.},
 archiveprefix = {arXiv},
 author = {Chandra Maddila and Adam Tait and Claire Chang and Daniel Cheng and Nauman Ahmad and Vijayaraghavan Murali and Marshall Roch and Arnaud Avondet and Aaron Meltzer and Victor Montalvao and Michael Hopko and Chris Waterson and Parth Thakkar and Renuka Fernandez and Kristian Kristensen and Sivan Barzily and Sherry Chen and Rui Abreu and Nachiappan Nagappan and Payam Shodjai and Killian Murphy and James Everingham and Aparna Ramani and Peter C. Rigby},
 eprint = {2507.18755},
 primaryclass = {cs.SE},
 title = {Agentic Program Repair from Test Failures at Scale: A Neuro-symbolic approach with static analysis and test execution feedback},
 url = {https://arxiv.org/abs/2507.18755},
 year = {2025}
}

@misc{sajadi2025safeaigeneratedpatcheslargescale,
 abstract = {Large language models (LLMs) and their agentic frameworks are increasingly adopted to perform development tasks such as automated program repair (APR). While prior work has identified security risks in LLM-generated code, most have focused on synthetic, simplified, or isolated tasks that lack the complexity of real-world program repair. In this study, we present the first large-scale security analysis of LLM-generated patches using 20,000+ GitHub issues. We evaluate patches proposed by developers, a standalone LLM (Llama 3.3 Instruct-70B), and three top-performing agentic frameworks (OpenHands, AutoCodeRover, HoneyComb). Finally, we analyze a wide range of code, issue, and project-level factors to understand the conditions under which generating insecure patches is more likely. Our findings reveal that Llama introduces many new vulnerabilities, exhibiting unique patterns not found in developers' code. Agentic workflows also generate a number of vulnerabilities, particularly when given more autonomy. We find that vulnerabilities in LLM-generated patches are associated with distinctive code characteristics and are commonly observed in issues missing specific types of information. These results suggest that contextual factors play a critical role in the security of the generated patches and point toward the need for proactive risk assessment methods that account for both issue and code-level information.},
 archiveprefix = {arXiv},
 author = {Amirali Sajadi and Kostadin Damevski and Preetha Chatterjee},
 eprint = {2507.02976},
 primaryclass = {cs.CR},
 title = {How Safe Are AI-Generated Patches? A Large-scale Study on Security Risks in LLM and Agentic Automated Program Repair on SWE-bench},
 url = {https://arxiv.org/abs/2507.02976},
 year = {2025}
}

@misc{pabba2025semagentsemanticsawareprogram,
 abstract = {Large Language Models (LLMs) have shown impressive capabilities in downstream software engineering tasks such as Automated Program Repair (APR). In particular, there has been a lot of research on repository-level issue-resolution benchmarks such as SWE-Bench. Although there has been significant progress on this topic, we notice that in the process of solving such issues, existing agentic systems tend to hyper-localize on immediately suspicious lines of code and fix them in isolation, without a deeper understanding of the issue semantics, code semantics, or execution semantics. Consequently, many existing systems generate patches that overfit to the user issue, even when a more general fix is preferable. To address this limitation, we introduce SemAgent, a novel workflow-based procedure that leverages issue, code, and execution semantics to generate patches that are complete - identifying and fixing all lines relevant to the issue. We achieve this through a novel pipeline that (a) leverages execution semantics to retrieve relevant context, (b) comprehends issue-semantics via generalized abstraction, (c) isolates code-semantics within the context of this abstraction, and (d) leverages this understanding in a two-stage architecture: a repair stage that proposes fine-grained fixes, followed by a reviewer stage that filters relevant fixes based on the inferred issue-semantics. Our evaluations show that our methodology achieves a solve rate of 44.66% on the SWEBench-Lite benchmark beating all other workflow-based approaches, and an absolute improvement of 7.66% compared to our baseline, which lacks such deep semantic understanding. We note that our approach performs particularly well on issues requiring multi-line reasoning (and editing) and edge-case handling, suggesting that incorporating issue and code semantics into APR pipelines can lead to robust and semantically consistent repairs.},
 archiveprefix = {arXiv},
 author = {Anvith Pabba and Alex Mathai and Anindya Chakraborty and Baishakhi Ray},
 eprint = {2506.16650},
 primaryclass = {cs.SE},
 title = {SemAgent: A Semantics Aware Program Repair Agent},
 url = {https://arxiv.org/abs/2506.16650},
 year = {2025}
}

@misc{rondon2025evaluatingagentbasedprogramrepair,
 abstract = {Agent-based program repair offers to automatically resolve complex bugs end-to-end by combining the planning, tool use, and code generation abilities of modern LLMs. Recent work has explored the use of agent-based repair approaches on the popular open-source SWE-Bench, a collection of bugs from highly-rated GitHub Python projects. In addition, various agentic approaches such as SWE-Agent have been proposed to solve bugs in this benchmark. This paper explores the viability of using an agentic approach to address bugs in an enterprise context. To investigate this, we curate an evaluation set of 178 bugs drawn from Google's issue tracking system. This dataset spans both human-reported (78) and machine-reported bugs (100).
To establish a repair performance baseline on this benchmark, we implement Passerine, an agent similar in spirit to SWE-Agent that can work within Google's development environment. We show that with 20 trajectory samples and Gemini 1.5 Pro, Passerine can produce a patch that passes bug tests (i.e., plausible) for 73% of machine-reported and 25.6% of human-reported bugs in our evaluation set. After manual examination, we found that 43% of machine-reported bugs and 17.9% of human-reported bugs have at least one patch that is semantically equivalent to the ground-truth patch.
These results establish a baseline on an industrially relevant benchmark, which as we show, contains bugs drawn from a different distribution -- in terms of language diversity, size, and spread of changes, etc. -- compared to those in the popular SWE-Bench dataset.},
 archiveprefix = {arXiv},
 author = {Pat Rondon and Renyao Wei and José Cambronero and Jürgen Cito and Aaron Sun and Siddhant Sanyam and Michele Tufano and Satish Chandra},
 eprint = {2501.07531},
 primaryclass = {cs.SE},
 title = {Evaluating Agent-based Program Repair at Google},
 url = {https://arxiv.org/abs/2501.07531},
 year = {2025}
}

@misc{bouzenia2024repairagentautonomousllmbasedagent,
 abstract = {Automated program repair has emerged as a powerful technique to mitigate the impact of software bugs on system reliability and user experience. This paper introduces RepairAgent, the first work to address the program repair challenge through an autonomous agent based on a large language model (LLM). Unlike existing deep learning-based approaches, which prompt a model with a fixed prompt or in a fixed feedback loop, our work treats the LLM as an agent capable of autonomously planning and executing actions to fix bugs by invoking suitable tools. RepairAgent freely interleaves gathering information about the bug, gathering repair ingredients, and validating fixes, while deciding which tools to invoke based on the gathered information and feedback from previous fix attempts. Key contributions that enable RepairAgent include a set of tools that are useful for program repair, a dynamically updated prompt format that allows the LLM to interact with these tools, and a finite state machine that guides the agent in invoking the tools. Our evaluation on the popular Defects4J dataset demonstrates RepairAgent's effectiveness in autonomously repairing 164 bugs, including 39 bugs not fixed by prior techniques. Interacting with the LLM imposes an average cost of 270,000 tokens per bug, which, under the current pricing of OpenAI's GPT-3.5 model, translates to 14 cents of USD per bug. To the best of our knowledge, this work is the first to present an autonomous, LLM-based agent for program repair, paving the way for future agent-based techniques in software engineering.},
 archiveprefix = {arXiv},
 author = {Islem Bouzenia and Premkumar Devanbu and Michael Pradel},
 eprint = {2403.17134},
 primaryclass = {cs.SE},
 title = {RepairAgent: An Autonomous, LLM-Based Agent for Program Repair},
 url = {https://arxiv.org/abs/2403.17134},
 year = {2024}
}

@misc{prenner2021automaticprogramrepairopenais,
 abstract = {OpenAI's Codex, a GPT-3 like model trained on a large code corpus, has made headlines in and outside of academia. Given a short user-provided description, it is capable of synthesizing code snippets that are syntactically and semantically valid in most cases. In this work, we want to investigate whether Codex is able to localize and fix bugs, a task of central interest in the field of automated program repair. Our initial evaluation uses the multi-language QuixBugs benchmark (40 bugs in both Python and Java). We find that, despite not being trained for APR, Codex is surprisingly effective, and competitive with recent state of the art techniques. Our results also show that Codex is slightly more successful at repairing Python than Java.},
 archiveprefix = {arXiv},
 author = {Julian Aron Prenner and Romain Robbes},
 eprint = {2111.03922},
 primaryclass = {cs.SE},
 title = {Automatic Program Repair with OpenAI's Codex: Evaluating QuixBugs},
 url = {https://arxiv.org/abs/2111.03922},
 year = {2021}
}


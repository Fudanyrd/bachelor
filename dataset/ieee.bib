@INPROCEEDINGS{10653250,
  author={Lajkó, Márk and Csuvik, Viktor and Gyimothy, Tibor and Vidács, László},
  booktitle={2024 IEEE/ACM International Workshop on Automated Program Repair (APR)}, 
  title={Automated Program Repair with the GPT Family, Including GPT-2, GPT-3 and CodeX}, 
  year={2024},
  volume={},
  number={},
  pages={34-41},
  url={https://ieeexplore.ieee.org/document/10653250/},
  abstract={Automated Program Repair (APR) is a promising approach for addressing software defects and improving software reliability. There are various approaches to APR, including using Machine Learning (ML) techniques such as neural networks and evolutionary algorithms, as well as more traditional methods such as static analysis and symbolic execution. In recent years, there has been growing interest in using ML techniques for APR, including the use of large language models such as GPT-2 and GPT-3. These models have the ability to generate human-like text and code, making them well-suited for tasks such as generating repair patches for defective programs. In this paper, we explore the use of the GPT family (including GPT-2, GPT-J-6B, GPT-3 and Codex) for APR of JavaScript programs and evaluate their performance in terms of the number and quality of repair patches generated. Our results show that these state-of-the-art language models are able to generate repair patches that successfully fix the defects in the JavaScript programs, with Codex performing slightly better overall. To be precise, in our self-assembled dataset, Codex was able to generate 108 repair patches that are exactly the same as the developer fix for the first try. If we consider multiple patch generations, up to 201 buggy programs are being repaired automatically from the 1559 evaluation dataset (12.89%).},
  keywords={Source coding;Large language models;Computer bugs;Neural networks;Static analysis;Machine learning;Maintenance engineering;Automated Program Repair;Transformers;GPT-3;Codex;JavaScript},
  doi={10.1145/3643788.3648021},
  ISSN={},
  month={April},}
@INPROCEEDINGS{11029650,
  author={Kong, Jiaolong and Cheng, Mingfei and Xie, Xiaofei},
  booktitle={2025 IEEE/ACM International Workshop on Automated Program Repair (APR)}, 
  title={Memorization in LLM-Based Program Repair}, 
  year={2025},
  volume={},
  number={},
  url={https://ieeexplore.ieee.org/document/11029650/},
  pages={40-42},
  abstract={Automated Program Repair (APR) is a powerful technique for mitigating the impact of software bugs in software development. The recent remarkable success of Large Language Models (LLMs) has set new state-of-the-art performance in APR. However, the extensive use of large training corpora raises concerns about whether these impressive capabilities genuinely generalize to unseen tasks or primarily rely on memorizing vast amounts of pretraining data. To address this issue, this paper introduces a memorization-inducing prompting strategy, MemInducer, to investigate the extent of memorization in LLMs for APR. Specifically, MemInducer is designed to prompt LLMs to recall responses from their training corpus. Subsequently, we assess memorization by measuring the similarity between the responses generated by the LLM and the corresponding ground truth. Experimental results reveal that memorization is indeed present in existing APR benchmarks, with over 78% of corrected bugs producing results that are entirely identical to the ground truth.},
  keywords={Training;Codes;Large language models;Conferences;Computer bugs;Maintenance engineering;Benchmark testing;Software;Software development management;Code Memorization;Automated Program Repair;Large Language Model},
  doi={10.1109/APR66717.2025.00011},
  ISSN={},
  month={April},
}
@INPROCEEDINGS{10764872,
  author={Li, Guochang and Zhi, Chen and Chen, Jialiang and Han, Junxiao and Deng, Shuiguang},
  booktitle={2024 39th IEEE/ACM International Conference on Automated Software Engineering (ASE)}, 
  title={Exploring Parameter-Efficient Fine-Tuning of Large Language Model on Automated Program Repair}, 
  year={2024},
  volume={},
  number={},
  pages={719-731},
  url={https://ieeexplore.ieee.org/document/10764872/},
  abstract={Automated Program Repair (APR) aims to fix bugs by generating patches. And existing work has demonstrated that "pre-training and fine-tuning" paradigm enables Large Language Models (LLMs) improve fixing capabilities on APR. However, existing work mainly focuses on Full-Model Fine-Tuning (FMFT) for APR and limited research has been conducted on the execution-based evaluation of Parameter-Efficient Fine-Tuning (PEFT) for APR. Comparing to FMFT, PEFT can reduce computing resource consumption without compromising performance and has been widely adopted to other software engineering tasks.To fill this gap, we enhance the existing APR dataset by employing prompt engineering to create an instruction dataset, APR-Instruction, at first. Secondly, we fine-tune four pre-trained LLMs using four different PEFT methods with APR-Instruction. The best fine-tuned model fixes 58% more bugs than the state-of-the-art LLM-based APR techniques. The results also show that (IA)3 improves the creativity of LLMs more effectively through fine-tuning and achieves the highest fixing capability compared to the other three PEFT methods. Thirdly, we explore the optimal configuration of PEFT hyperparameters, and assess the impact of instruction dataset size, showing that a larger number of parameters and a larger training dataset do not necessarily result in better performance for PEFT. Lastly, we analyze peak memory usage and trainable parameters to show the efficiency of PEFT.This work provides a comprehensive exploration of PEFT on APR and suggests potentially promising directions for extension to other software engineering downstream tasks. APR-Instruction, PEFT weights, and the fine-tuning code are publicly available as open-source resources.},
  keywords={Training;Codes;Large language models;Computer bugs;Memory management;Training data;Maintenance engineering;Prompt engineering;Creativity;Software engineering;Automated Program Repair;Parameter-Effective Fine-Tuning;Large Language Model;Execution-based Evaluation},
  doi={},
  ISSN={2643-1572},
  month={Oct},
}
@INPROCEEDINGS{9809071,
  author={Lajkó, Márk and Csuvik, Viktor and Vidács, László},
  booktitle={2022 IEEE/ACM International Workshop on Automated Program Repair (APR)}, 
  title={Towards JavaScript program repair with Generative Pre-trained Transformer (GPT-2)}, 
  year={2022},
  volume={},
  number={},
  pages={61-68},
  url={https://ieeexplore.ieee.org/document/9809071/},
  abstract={The goal of Automated Program Repair (APR) is to find a fix to software bugs, without human intervention. The so-called Gener-ate and Validate (G&V) approach deemed to be the most popular method in the last few years, where the APR tool creates a patch and it is validated against an oracle. Recent years for Natural Language Processing (NLP) were of great interest, with new pre-trained models shattering records on tasks ranging from sentiment analysis to question answering. Usually these deep learning models inspire the APR community as well. These approaches usually require a large dataset on which the model can be trained (or fine-tuned) and evaluated. The criterion to accept a patch depends on the underlying dataset, but usually the generated patch should be exactly the same as the one created by a human developer. As NLP models are more and more capable to form sentences, and the sentences will form coherent paragraphs, the APR tools are also better and better at generating syntactically and semantically correct source code. As the Generative Pre-trained Transformer (GPT) model is now avail-able to everyone thanks to the NLP and AI research community, it can be fine-tuned to specific tasks (not necessarily on natural language). In this work we use the GPT-2 model to generate source code, to the best of our knowledge, the GPT-2 model was not used for Automated Program Repair so far. The model is fine-tuned for a specific task: it has been taught to fix JavaScript bugs automatically. To do so, we trained the model on 16863JS code snippets, where it could learn the nature of the observed programming language. In our experiments we observed that the GPT-2 model was able to learn how to write syntactically correct source code almost on every attempt, although it failed to learn good bug-fixes in some cases. Nonetheless it was able to generate the correct fixes in most of the cases, resulting in an overall accuracy up to 17.25%.},
  keywords={Training;Sentiment analysis;Codes;Computer bugs;Maintenance engineering;Transformers;Software;Automated Program Repair;Machine learning;JavaScript;Code Refinement;GPT},
  doi={10.1145/3524459.3527350},
  ISSN={},
  month={May},}
@INPROCEEDINGS{11029914,
  author={Bouzenia, Islem and Devanbu, Premkumar and Pradel, Michael},
  booktitle={2025 IEEE/ACM 47th International Conference on Software Engineering (ICSE)}, 
  title={RepairAgent: An Autonomous, LLM-Based Agent for Program Repair}, 
  year={2025},
  volume={},
  number={},
  pages={2188-2200},
  url={https://ieeexplore.ieee.org/document/11029914/},
  abstract={Automated program repair has emerged as a powerful technique to mitigate the impact of software bugs on system reliability and user experience. This paper introduces Repair Agent, the first work to address the program repair challenge through an autonomous agent based on a large language model (LLM). Unlike existing deep learning-based approaches, which prompt a model with a fixed prompt or in a fixed feedback loop, our work treats the LLM as an agent capable of autonomously planning and executing actions to fix bugs by invoking suitable tools. Repair Agent freely interleaves gathering information about the bug, gathering repair ingredients, and validating fixes, while deciding which tools to invoke based on the gathered information and feedback from previous fix attempts. Key contributions that enable Repair Agent include a set of tools that are useful for program repair, a dynamically updated prompt format that allows the LLM to interact with these tools, and a finite state machine that guides the agent in invoking the tools. Our evaluation on the popular Defects4J dataset demonstrates Repair Agent's effectiveness in autonomously repairing 164 bugs, including 39 bugs not fixed by prior techniques. Interacting with the LLM imposes an average cost of 270k tokens per bug, which, under the current pricing of OpenAI's GPT-3.5 model, translates to 14 cents per bug. To the best of our knowledge, this work is the first to present an autonomous, LLM-based agent for program repair, paving the way for future agent-based techniques in software engineering.},
  keywords={Translation;Large language models;Computer bugs;Pricing;Maintenance engineering;Autonomous agents;User experience;Software;Reliability;Software engineering;ai for se;program repair;llm agents},
  doi={10.1109/ICSE55347.2025.00157},
  ISSN={1558-1225},
  month={April},}
@INPROCEEDINGS{11029841,
  author={Parasaram, Nikhil and Yan, Huijie and Yang, Boyu and Flahy, Zineb and Qudsi, Abriele and Ziaber, Damian and Barr, Earl T. and Mechtaev, Sergey},
  booktitle={2025 IEEE/ACM 47th International Conference on Software Engineering (ICSE)}, 
  title={The Fact Selection Problem in LLM-Based Program Repair}, 
  year={2025},
  volume={},
  number={},
  pages={2574-2586},
  url={https://ieeexplore.ieee.org/document/11029841/},
  abstract={Recent research has shown that incorporating bug-related facts, such as stack traces and GitHub issues, into prompts enhances the bug-fixing capabilities of large language models (LLMs). Considering the ever-increasing context window of these models, a critical question arises: what and how many facts should be included in prompts to maximise the chance of correctly fixing bugs? To answer this question, we conducted a large-scale study, employing over 19K prompts featuring various combinations of seven diverse facts to rectify 314 bugs from open -source Python projects within the BugsInPy benchmark. Our findings revealed that each fact, ranging from simple syntactic details like code context to semantic information previously unexplored in the context of LLMs such as angelic values, is beneficial. Specifically, each fact aids in fixing some bugs that would remain unresolved or only be fixed with a low success rate without it. Importantly, we discovered that the effectiveness of program repair prompts is non-monotonic over the number of used facts; using too many facts leads to subpar outcomes. These insights led us to define the fact selection problem: determining the optimal set of facts for inclusion in a prompt to maximise LLM's performance on a given task instance. We found that there is no one-size-fits-all set of facts for bug repair. Therefore, we developed a basic statistical model, named Maniple, which selects facts specific to a given bug to include in the prompt. This model significantly surpasses the performance of the best generic fact set. To underscore the significance of the fact selection problem, we benchmarked Maniple against the state-of-the-art zero-shot, non-conversational LLM-based bug repair methods. On our testing dataset of 157 bugs, Maniple repairs 88 bugs, 17% above the best configuration.},
  keywords={Systematics;Large language models;Computer bugs;Semantics;Maintenance engineering;Benchmark testing;Syntactics;Software engineering;Software development management;Python;automated program repair;large language models;prompt engineering},
  doi={10.1109/ICSE55347.2025.00162},
  ISSN={1558-1225},
  month={April},}
@INPROCEEDINGS{11029731,
  author={Xu, Junjielong and Fu, Ying and Tan, Shin Hwei and He, Pinjia},
  booktitle={2025 IEEE/ACM 47th International Conference on Software Engineering (ICSE)}, 
  title={Aligning the Objective of LLM-Based Program Repair}, 
  year={2025},
  volume={},
  number={},
  pages={2548-2560},
  url={https://ieeexplore.ieee.org/document/11029731/},
  abstract={Large language models (LLMs) have achieved decent results on automated program repair (APR). However, the next token prediction training objective of decoder-only LLMs (e.g., GPT-4) is misaligned with the masked span prediction objective of current infilling-style methods, which impedes LLMs from fully leveraging pre-trained knowledge for program repair. In addition, while some LLMs can locate and repair bugs in certain functions using the related artifacts (e.g., test cases), existing methods still depend on statement-level fault localization methods to provide a list of buggy hunks for repair. This restriction hinders LLMs from exploring potential patches beyond the given locations. In this paper, we investigate a new approach to adapt LLMs to program repair. Our core insight is that LLM's APR capability can be greatly improved by simply aligning the output to their training objective and allowing them to refine the whole program without first identifying faulty statements. Based on this insight, we designed D4C, a straightforward prompting framework for APR. D4C can repair 180 bugs correctly in Defects4J, with each patch being sampled only 10 times. This surpasses the SOTA APR methods with perfect fault localization by 10 % and reduces the patch sampling number by 90 %. Our findings reveal that (1) objective alignment is crucial for fully exploiting LLM's pre-trained capability, and (2) replacing the traditional localize-buggy-hunks-then-repair workflow with direct debugging is more effective for LLM-based APR methods. Thus, we believe this paper introduces a new mindset for harnessing LLMs in APR.},
  keywords={Training;Location awareness;Fault diagnosis;Large language models;Computer bugs;Debugging;Maintenance engineering;Object recognition;Software engineering;Automated Program Repair;Large Language Model;Objective Alignment},
  doi={10.1109/ICSE55347.2025.00169},
  ISSN={1558-1225},
  month={April},}
@INPROCEEDINGS{11252591,
  author={Ebrahim, Yasser},
  booktitle={2025 IEEE/ACIS 29th International Conference on Software Engineering, Artificial Intelligence, Networking and Parallel/Distributed Computing (SNPD)}, 
  title={Making the Case for LLM-Generated Automated Program Repair Benchmarks}, 
  year={2025},
  volume={},
  number={},
  pages={842-847},
  abstract={Automated Program Repair (APR) has made significant strides in recent years, particularly with the integration of large language models (LLMs) and deep learning techniques. Yet despite this progress, one fundamental issue continues to hinder advancement: how we evaluate these systems. Many of today’s APR benchmarks suffer from serious limitations—including small dataset sizes, synthetic or unrealistic bug scenarios, overfitting risks, ambiguous evaluation criteria, and a narrow focus on certain programming languages.In this paper, we take a critical look at these challenges by identifying eight core limitations in widely used benchmarks. We then explore how LLM-generated benchmarks can help overcome these obstacles. Finally, we address some potential concerns about LLM-generated benchmarks and propose a quality assurance and validation framework.By combining the strengths of LLMs with thoughtful benchmark design, this work lays the foundation for more robust, diverse, and meaningful evaluation frameworks—paving the way for future breakthroughs in APR research.},
  keywords={Systematics;Large language models;Diversity reception;Computer bugs;Benchmark testing;Maintenance engineering;Software;Standards;Software engineering;Overfitting;Automated Program Repair;Benchmarking;Large Language Models;Software Engineering},
  doi={10.1109/SNPD65828.2025.11252591},
  url={https://ieeexplore.ieee.org/document/11252591/},
  ISSN={2693-8421},
  month={June},}
@INPROCEEDINGS{11334717,
  author={Ehsani, Ramtin and Parra, Esteban and Haiduc, Sonia and Chatterjee, Preetha},
  booktitle={2025 40th IEEE/ACM International Conference on Automated Software Engineering (ASE)}, 
  title={Hierarchical Knowledge Injection for Improving LLM-based Program Repair}, 
  year={2025},
  volume={},
  number={},
  pages={1440-1452},
  abstract={Prompting LLMs with bug-related context (e.g., error messages, stack traces) improves automated program repair, but many bugs still remain unresolved. In real-world projects, developers often rely on broader repository and project-level context beyond the local code to resolve such bugs. In this paper, we investigate how automatically extracting and providing such knowledge can improve LLM-based program repair. We propose a layered knowledge injection framework that incrementally augments LLMs with structured context. It starts with the Bug Knowledge Layer, which includes information such as the buggy function and failing tests; expands to the Repository Knowledge Layer, which adds structural dependencies, related files, and commit history; and finally injects the Project Knowledge Layer, which incorporates relevant details from documentation and previously fixed bugs. We evaluate this framework on a dataset of 314 bugs from BugsInPy using two LLMs (Llama 3.3 and GPT-4o-mini), and analyze fix rates across six bug types. By progressively injecting knowledge across layers, our approach achieves a fix rate of 79% (250/314) using Llama 3.3, a significant improvement of 23% over previous work. All bug types show improvement with the addition of repository-level context, while only a subset benefit further from project-level knowledge, highlighting that different bug types require different levels of contextual information for effective repair. We also analyze the remaining unresolved bugs and find that more complex and structurally isolated bugs, such as Program Anomaly and GUI bugs, remain difficult even after injecting all available information. Our results show that layered context injection improves program repair and suggest the need for interactive and adaptive APR systems.},
  keywords={Knowledge engineering;Codes;Adaptive systems;Large language models;Computer bugs;Documentation;Maintenance engineering;History;Software engineering;Graphical user interfaces;automated program repair;large language models;knowledge injection;in-context learning},
  doi={10.1109/ASE63991.2025.00122},
  url={https://ieeexplore.ieee.org/document/11334717/},
  ISSN={2643-1572},
  month={Nov},
}

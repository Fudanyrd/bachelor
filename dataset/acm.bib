@inproceedings{10.1145/3735950.3735954,
author = {Wang, Huanting and Jacob, Dejice and Kelly, David and Elkhatib, Yehia and Singer, Jeremy and Wang, Zheng},
title = {SecureMind: A Framework for Benchmarking Large Language Models in Memory Bug Detection and Repair},
year = {2025},
isbn = {9798400716102},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3735950.3735954},
doi = {10.1145/3735950.3735954},
abstract = {Large language models (LLMs) hold great promise for automating software vulnerability detection and repair, but ensuring their correctness remains a challenge. While recent work has developed benchmarks for evaluating LLMs in bug detection and repair, existing studies rely on hand-crafted datasets that quickly become outdated. Moreover, systematic evaluation of advanced reasoning-based LLMs using chain-of-thought prompting for software security is lacking.   We introduce SecureMind, an open-source framework for evaluating LLMs in vulnerability detection and repair, focusing on memory-related vulnerabilities. SecureMind provides a user-friendly Python interface for defining test plans, which automates data retrieval, preparation, and benchmarking across a wide range of metrics.   Using SecureMind, we assess 10 representative LLMs, including 7 state-of-the-art reasoning models, on 16K test samples spanning 8 Common Weakness Enumeration (CWE) types related to memory safety violations. Our findings highlight the strengths and limitations of current LLMs in handling memory-related vulnerabilities.},
booktitle = {Proceedings of the 2025 ACM SIGPLAN International Symposium on Memory Management},
pages = {27–40},
numpages = {14},
keywords = {Bug repair, Large language models, Software bug detection},
location = {Seoul, Republic of Korea},
series = {ISMM '25}
}

@inproceedings{10.1109/ICSE-Companion66252.2025.00082,
author = {Wu, Zhaonan and Zhao, Yanjie and Wei, Chen and Wan, Zirui and Liu, Yue and Wang, Haoyu},
title = {CommitShield: Tracking Vulnerability Introduction and Fix in Version Control Systems},
year = {2025},
isbn = {9798331536831},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-Companion66252.2025.00082},
doi = {10.1109/ICSE-Companion66252.2025.00082},
abstract = {Version control systems are commonly used to manage open-source software, in which each commit may introduce new vulnerabilities or fix existing ones. Researchers have developed various tools for detecting vulnerabilities in code commits, but their performance is limited by factors such as neglecting descriptive data and challenges in accurately identifying vulnerability introductions. To overcome these limitations, we propose CommitShield, which combines the code analysis capabilities of static analysis tools with the natural language and code understanding capabilities of large language models (LLMs) to enhance the accuracy of vulnerability introduction and fix detection by generating precise descriptions and obtaining rich patch contexts. We evaluate CommitShield using the newly constructed vulnerability fix dataset, CommitVulFix, and a cleaned vulnerability introduction dataset. Experimental results indicate that CommitShield improves recall by 74\%-77\% over state-of-the-art methods in the vulnerability fix detection task, and its F1-score improves by 15\%-27\% in the vulnerability introduction detection task.},
booktitle = {Proceedings of the IEEE/ACM 47th International Conference on Software Engineering: Companion Proceedings},
pages = {279–290},
numpages = {12},
location = {Ottawa, Ontario, Canada},
series = {ICSE '25}
}

@article{10.1016/j.engappai.2024.109291,
author = {de-Fitero-Dominguez, David and Garcia-Lopez, Eva and Garcia-Cabot, Antonio and Martinez-Herraiz, Jose-Javier},
title = {Enhanced automated code vulnerability repair using large language models},
year = {2024},
issue_date = {Dec 2024},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {138},
number = {PA},
issn = {0952-1976},
url = {https://doi.org/10.1016/j.engappai.2024.109291},
doi = {10.1016/j.engappai.2024.109291},
journal = {Eng. Appl. Artif. Intell.},
month = dec,
numpages = {13},
keywords = {Automated code repair, Deep learning, Large language models, Vulnerability repair, Mistral, Code llama}
}

@inproceedings{10.1109/ICSE55347.2025.00162,
author = {Parasaram, Nikhil and Yan, Huijie and Yang, Boyu and Flahy, Zineb and Qudsi, Abriele and Ziaber, Damian and Barr, Earl T. and Mechtaev, Sergey},
title = {The Fact Selection Problem in LLM-Based Program Repair},
year = {2025},
isbn = {9798331505691},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE55347.2025.00162},
doi = {10.1109/ICSE55347.2025.00162},
abstract = {Recent research has shown that incorporating bug-related facts, such as stack traces and GitHub issues, into prompts enhances the bug-fixing capabilities of large language models (LLMs). Considering the ever-increasing context window of these models, a critical question arises: what and how many facts should be included in prompts to maximise the chance of correctly fixing bugs? To answer this question, we conducted a large-scale study, employing over 19K prompts featuring various combinations of seven diverse facts to rectify 314 bugs from open-source Python projects within the BugsInPy benchmark. Our findings revealed that each fact, ranging from simple syntactic details like code context to semantic information previously unexplored in the context of LLMs such as angelic values, is beneficial. Specifically, each fact aids in fixing some bugs that would remain unresolved or only be fixed with a low success rate without it. Importantly, we discovered that the effectiveness of program repair prompts is non-monotonic over the number of used facts; using too many facts leads to subpar outcomes. These insights led us to define the fact selection problem: determining the optimal set of facts for inclusion in a prompt to maximise LLM's performance on a given task instance. We found that there is no one-size-fits-all set of facts for bug repair. Therefore, we developed a basic statistical model, named Maniple, which selects facts specific to a given bug to include in the prompt. This model significantly surpasses the performance of the best generic fact set. To underscore the significance of the fact selection problem, we benchmarked MANIPLE against the state-of-the-art zero-shot, non-conversational LLM-based bug repair methods. On our testing dataset of 157 bugs, Maniple repairs 88 bugs, 17\% above the best configuration.},
booktitle = {Proceedings of the IEEE/ACM 47th International Conference on Software Engineering},
pages = {2574–2586},
numpages = {13},
keywords = {automated program repair, large language models, prompt engineering},
location = {Ottawa, Ontario, Canada},
series = {ICSE '25}
}

@inproceedings{10.1145/3524459.3527350,
author = {Lajk\'{o}, M\'{a}rk and Csuvik, Viktor and Vid\'{a}cs, L\'{a}szl\'{o}},
title = {Towards JavaScript program repair with generative pre-trained transformer (GPT-2)},
year = {2022},
isbn = {9781450392853},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3524459.3527350},
doi = {10.1145/3524459.3527350},
abstract = {The goal of Automated Program Repair (APR) is to find a fix to software bugs, without human intervention. The so-called Generate and Validate (G&amp;V) approach deemed to be the most popular method in the last few years, where the APR tool creates a patch and it is validated against an oracle. Recent years for Natural Language Processing (NLP) were of great interest, with new pre-trained models shattering records on tasks ranging from sentiment analysis to question answering. Usually these deep learning models inspire the APR community as well. These approaches usually require a large dataset on which the model can be trained (or fine-tuned) and evaluated. The criterion to accept a patch depends on the underlying dataset, but usually the generated patch should be exactly the same as the one created by a human developer. As NLP models are more and more capable to form sentences, and the sentences will form coherent paragraphs, the APR tools are also better and better at generating syntactically and semantically correct source code. As the Generative Pre-trained Transformer (GPT) model is now available to everyone thanks to the NLP and AI research community, it can be fine-tuned to specific tasks (not necessarily on natural language). In this work we use the GPT-2 model to generate source code, to the best of our knowledge, the GPT-2 model was not used for Automated Program Repair so far. The model is fine-tuned for a specific task: it has been taught to fix JavaScript bugs automatically. To do so, we trained the model on 16863 JS code snippets, where it could learn the nature of the observed programming language. In our experiments we observed that the GPT-2 model was able to learn how to write syntactically correct source code almost on every attempt, although it failed to learn good bug-fixes in some cases. Nonetheless it was able to generate the correct fixes in most of the cases, resulting in an overall accuracy up to 17.25\%.},
booktitle = {Proceedings of the Third International Workshop on Automated Program Repair},
pages = {61–68},
numpages = {8},
keywords = {GPT, JavaScript, automated program repair, code refinement, machine learning},
location = {Pittsburgh, Pennsylvania},
series = {APR '22}
}

@inproceedings{10.1007/978-3-032-00630-1_5,
author = {Khan, Zanis Ali and Garg, Aayush and Tang, Qiang},
title = {A Multi-dataset Evaluation of&nbsp;Models for&nbsp;Automated Vulnerability Repair},
year = {2025},
isbn = {978-3-032-00629-5},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-032-00630-1_5},
doi = {10.1007/978-3-032-00630-1_5},
abstract = {Software vulnerabilities pose significant security threats, requiring effective mitigation. While Automated Program Repair (APR) has advanced in fixing general bugs, vulnerability patching—a security-critical aspect of APR—remains underexplored. This study investigates pre-trained language models, CodeBERT and CodeT5, for automated vulnerability patching across six datasets and four languages. We evaluate their accuracy and generalization to unknown vulnerabilities. Results show that while both models face challenges with fragmented or sparse context, CodeBERT performs comparatively better in such scenarios, whereas CodeT5 excels in capturing complex vulnerability patterns. CodeT5 also demonstrates superior scalability. Furthermore, we test fine-tuned models on both in-distribution (trained) and out-of-distribution (unseen) datasets. While fine-tuning improves in-distribution performance, models struggle to generalize to unseen data, highlighting challenges in robust vulnerability detection. This study benchmarks model performance, identifies limitations in generalization, and provides actionable insights to advance automated vulnerability patching for real-world security applications.},
booktitle = {Availability, Reliability and Security: ARES 2025 International Workshops, Ghent, Belgium, August 11–14, 2025, Proceedings, Part I},
pages = {73–87},
numpages = {15},
keywords = {code patching, vulnerability patching, large language models, automated program repair},
location = {Ghent, Belgium}
}

@inproceedings{10.1145/3638530.3664174,
author = {Bin Murtaza, Sardar and Mccoy, Aidan and Ren, Zhiyuan and Murphy, Aidan and Banzhaf, Wolfgang},
title = {LLM Fault Localisation within Evolutionary Computation Based Automated Program Repair},
year = {2024},
isbn = {9798400704956},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3638530.3664174},
doi = {10.1145/3638530.3664174},
abstract = {Repairing bugs can be a daunting task for even a human experienced in debugging, so naturally, attempting to automatically repair programs with a computer system is quite challenging. The existing methods of automated program repair leave a lot of room for improvement. Fault localization, which aims to find lines of code that are potentially buggy, minimises the search space of an automated program repair system. Recent work has shown improvement in these fault localization methods, with the use of Large Language Models. Here, we propose a system where a LLM-based fault localization tool, which we call SemiAutoFL, is used within a fully automatic program repair program, ARJA-e. We show that utilising LLM-based fault localization with ARJA-e can significantly improve its performance on real world bugs. ARJA-e with SemiAutoFL can repair 10 bugs that ARJA-e was previously unable to so do. This finding adds to our understanding of how to improve fault localization and automated program repair, highlighting the potential for more efficient and accurate fault localisation methods being applied to automated program repair.},
booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference Companion},
pages = {1824–1829},
numpages = {6},
keywords = {genetic improvement, fault localisation, large language models},
location = {Melbourne, VIC, Australia},
series = {GECCO '24 Companion}
}

@inproceedings{10.1145/3691620.3695555,
author = {Zhang, Jian and Wang, Chong and Li, Anran and Wang, Wenhan and Li, Tianlin and Liu, Yang},
title = {VulAdvisor: Natural Language Suggestion Generation for Software Vulnerability Repair},
year = {2024},
isbn = {9798400712487},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3691620.3695555},
doi = {10.1145/3691620.3695555},
abstract = {Software vulnerabilities pose serious threats to the security of modern software systems. Deep Learning-based Automated Vulnerability Repair (AVR) has gained attention as a potential solution to accelerate the remediation of vulnerabilities. However, recent studies indicate that existing AVR approaches often only generate patches, which may not align with developers' current repair practices or expectations. In this paper, we introduce VulAdvisor, an automated approach that generates natural language suggestions to guide developers or AVR tools in repairing vulnerabilities. VulAdvisor comprises two main components: oracle extraction and suggestion learning. To address the challenge of limited historical data, we propose an oracle extraction method facilitating ChatGPT to construct a comprehensive and high-quality dataset. For suggestion learning, we take the supervised fine-tuning CodeT5 model as the basis, integrating local context into Multi-Head Attention and introducing a repair action loss, to improve the relevance and meaningfulness of the generated suggestions. Extensive experiments on a large-scale dataset from real-world C/C++ projects demonstrate the effectiveness of VulAdvisor, surpassing several alternatives in terms of both lexical and semantic metrics. Moreover, we show that the generated suggestions enhance the patch generation capabilities of existing AVR tools. Human evaluations further validate the quality and utility of VulAdvisor's suggestions, confirming their potential to improve software vulnerability repair practices.},
booktitle = {Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering},
pages = {1932–1944},
numpages = {13},
keywords = {vulnerability repair, large language models, suggestion generation, program repair},
location = {Sacramento, CA, USA},
series = {ASE '24}
}

@inproceedings{10.1145/3716368.3735300,
author = {Jha, Manvi and Wan, Jiaxin and Zhang, Huan and Chen, Deming},
title = {PREFACE - A Reinforcement Learning Framework for Code Verification via LLM Prompt Repair},
year = {2025},
isbn = {9798400714962},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3716368.3735300},
doi = {10.1145/3716368.3735300},
abstract = {Large Language Models (LLMs) have emerged as powerful tools for code generation. Yet, they often struggle to produce code that is both syntactically and semantically correct, particularly when correctness must be formally verified. Addressing this gap, we present a novel, model-agnostic framework that couples LLMs with a lightweight reinforcement learning (RL) agent to enable scalable, robust, and formally verifiable code generation without the need for costly model fine-tuning. Centered on the generation of Dafny code, a formally verifiable programming language, our system initiates with LLM-generated code, which is rigorously evaluated within an integrated verification environment. Upon failure, we feed the erroneous code and error metadata to an RL agent trained to explore the prompt-code space (i.e., the set of possible prompt edits paired with their resulting generated code variants). This agent strategically selects corrective prompts to minimize verification iterations, effectively steering the LLM toward correct outputs using its latent capabilities. Once verified, Dafny code can be systematically translated to C for high-level synthesis (HLS), ensuring correctness-by-construction in downstream hardware design workflows. On a 100‑task benchmark, PREFACE’s error‑guided prompt refinement raises verification success by up to 21\% — 14\% for ChatGPT‑4o, 17\% for ChatGPT‑o1‑mini, 10\% for Qwen2.5‑Coder‑14B, 4\% for Qwen2.5‑7B, and 21\% for Gemini‑2‑Flash—demonstrating substantial gains across diverse LLMs.},
booktitle = {Proceedings of the Great Lakes Symposium on VLSI 2025},
pages = {547–553},
numpages = {7},
keywords = {Dafny, Formal verification, Large Language Models (LLMs), Reinforcement learning, Prompt optimization, Automated reasoning},
location = {
},
series = {GLSVLSI '25}
}

@inproceedings{10.1145/3639478.3640023,
author = {Saavedra, Nuno and Silva, Andr\'{e} and Monperrus, Martin},
title = {GitBug-Actions: Building Reproducible Bug-Fix Benchmarks with GitHub Actions},
year = {2024},
isbn = {9798400705021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639478.3640023},
doi = {10.1145/3639478.3640023},
abstract = {Bug-fix benchmarks are fundamental in advancing various sub-fields of software engineering such as automatic program repair (APR) and fault localization (FL). A good benchmark must include recent examples that accurately reflect technologies and development practices of today. To be executable in the long term, a benchmark must feature test suites that do not degrade overtime due to, for example, dependencies that are no longer available. Existing benchmarks fail in meeting both criteria. For instance, Defects4J, one of the foremost Java benchmarks, last received an update in 2020. Moreover, full-reproducibility has been neglected by the majority of existing benchmarks. In this paper, we present GitBug-Actions: a novel tool for building bug-fix benchmarks with modern and fully-reproducible bug-fixes. GitBug-Actions relies on the most popular CI platform, GitHub Actions, to detect bug-fixes and smartly locally execute the CI pipeline in a controlled and reproducible environment. To the best of our knowledge, we are the first to rely on GitHub Actions to collect bug-fixes. To demonstrate our toolchain, we deploy GitBug-Actions to build a proof-of-concept Go bug-fix benchmark containing executable, fully-reproducible bug-fixes from different repositories. A video demonstrating GitBug-Actions is available at: https://youtu.be/aBWwa1sJYBs.},
booktitle = {Proceedings of the 2024 IEEE/ACM 46th International Conference on Software Engineering: Companion Proceedings},
pages = {1–5},
numpages = {5},
keywords = {software bugs, bug benchmark, bug database, reproducibility, software testing, program analysis, github actions},
location = {Lisbon, Portugal},
series = {ICSE-Companion '24}
}

@article{10.1145/3660773,
author = {Hossain, Soneya Binta and Jiang, Nan and Zhou, Qiang and Li, Xiaopeng and Chiang, Wen-Hao and Lyu, Yingjun and Nguyen, Hoan and Tripp, Omer},
title = {A Deep Dive into Large Language Models for Automated Bug Localization and Repair},
year = {2024},
issue_date = {July 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {FSE},
url = {https://doi.org/10.1145/3660773},
doi = {10.1145/3660773},
abstract = {Large language models (LLMs) have shown impressive effectiveness in various software engineering tasks, including automated program repair (APR). In this study, we take a deep dive into automated bug localization and repair utilizing LLMs. In contrast to many deep learning-based APR methods that assume known bug locations, rely on line-level localization tools, or address bug prediction and fixing in one step, our approach uniquely employs LLMs to predict bug location at the token level and subsequently utilizes them for bug fixing. This methodological separation of bug localization and fixing using different LLMs enables effective integration of diverse contextual information and improved incorporation of inductive biases. We introduce Toggle: Token-Granulated Bug Localization and Repair, a comprehensive program repair framework that integrates a bug localization model, an adjustment model to address tokenizer inconsistencies, and a bug-fixing model. Toggle takes a buggy function as input and generates a complete corrected function. We investigate various styles of prompting to the bug fixing model to identify the most effective prompts that better utilize the inductive bias and significantly outperform others. Toggle achieves the new state-of-the-art performance on the CodeXGLUE code refinement benchmark, and exhibits better and comparable performance on several other widely-used APR datasets, including Defects4J. In the Defects4J benchmark, our approach consistently ranks above other methods, achieving superior results in the Top-10, Top-30, Top-50, and Top-100 metrics. Besides examining Toggle’s generalizability to unseen data, evaluating the effectiveness of various prompts, we also investigate the impact of additional contextual information such as buggy lines and code comments on bug localization, and explore the importance of the adjustment model. Our extensive experiments offer valuable insights and answers to critical research questions.},
journal = {Proc. ACM Softw. Eng.},
month = jul,
articleno = {66},
numpages = {23},
keywords = {Software and its engineering → Software testing and debugging, Automated Bug Localization and Fix, Large Language Models}
}

@inproceedings{10.1145/3626252.3630875,
author = {Ishizue, Ryosuke and Sakamoto, Kazunori and Washizaki, Hironori and Fukazawa, Yoshiaki},
title = {Improved Program Repair Methods using Refactoring with GPT Models},
year = {2024},
isbn = {9798400704239},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3626252.3630875},
doi = {10.1145/3626252.3630875},
abstract = {Teachers often utilize automatic program repair methods to provide feedback on submitted student code using model answer code. A state-of-the-art tool is Refactory, which achieves a high repair success rate and small patch size (less code repair) by refactoring code to expand the variety of correct code samples that can be referenced. However, Refactory has two major limitations. First, it cannot fix code with syntax errors. Second, it has difficulty fixing code when there are few correct submissions. Herein we propose a new method that combines Refactory and OpenAI's GPT models to address these issues and conduct a performance measurement experiment. The experiment uses a dataset consisting of 5 programming assignment problems and almost 1,800 real-life incorrect Python program submissions from 361 students for an introductory programming course at a large public university. The proposed method improves the repair success rate by 1-21\% when the set of correct code samples is sufficient and the patch size is smaller than Refactory alone in 16-45\% of the cases. When there was no set of correct code samples at all (only the model answer code was used as a reference for repair), method improves the repair success rate by 1-43\% and the patch size is smaller than Refactory alone in 42-68\% of the cases.},
booktitle = {Proceedings of the 55th ACM Technical Symposium on Computer Science Education V. 1},
pages = {569–575},
numpages = {7},
keywords = {generative ai, program repair, programming assignment},
location = {Portland, OR, USA},
series = {SIGCSE 2024}
}

@article{10.1145/3729390,
author = {Kong, Jiaolong and Xie, Xiaofei and Liu, Shangqing},
title = {Demystifying Memorization in LLM-Based Program Repair via a General Hypothesis Testing Framework},
year = {2025},
issue_date = {July 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {FSE},
url = {https://doi.org/10.1145/3729390},
doi = {10.1145/3729390},
abstract = {Large Language Models (LLMs) have achieved remarkable success in various applications, particularly in code-related tasks such as code generation and program repair, setting new performance benchmarks. However, the extensive use of large training corpora raises concerns about whether these achievements stem from genuine understanding or mere memorization of training data—a question often overlooked in current research. This paper aims to study the memorization issue within LLM-based program repair by investigating whether the correct patches generated by LLMs are the result of memorization. The key challenge lies in the absence of ground truth for confirming memorization, leading to various ad-hoc methods designed for its detection. To address this challenge, we first propose a general framework that formalizes memorization detection as a general hypothesis testing problem, where existing approaches can be unified by defining a low-probability event under the null hypothesis that the data is not memorized. The occurrence of such an event leads to the rejection of the null hypothesis, indicating potential memorization. Based on this framework, we design two specific methods (i.e., low-probability events) to detect potential memorization: 1) basic ground-truth matching, and 2) reassessment after substantial code mutation. We investigate the memorization issue in LLM-based program repair using two datasets: Defects4J, a widely used benchmark that is likely included in the training data, and GitBug-Java, a new dataset that is unlikely to be part of the training data. Our findings reveal that a significant portion of correct patches exactly match the ground truths in Defects4J (e.g., 78.83\% and 87.42\% on GPT-3.5 and CodeLlama-7b, respectively). Moreover, even after significant modifications to the buggy code, where the original repairs should not be generated, a considerable percentage of bugs (e.g., 81.82\% on GPT-3.5 and 88.24\% on CodeLlama-7b) continue to be fixed exactly as in the original bug fixes, indicating a high likelihood of memorization. Furthermore, we evaluate existing memorization detection methods and demonstrate their ineffectiveness in this context (e.g., most AUROCs are below 0.5). The theoretical analysis under our hypothesis testing framework shows that their defined events may not meet the requirements for being low-probability. The study highlights the critical need for more robust and rigorous evaluations in LLM-based software engineering research, ensuring a clear distinction between true problem-solving capabilities and mere memorization.},
journal = {Proc. ACM Softw. Eng.},
month = jun,
articleno = {FSE120},
numpages = {23},
keywords = {Code Memorization, Program Repair}
}

@inproceedings{10.1145/3623476.3623522,
author = {Ribeiro, Francisco and de Macedo, Jos\'{e} Nuno Castro and Tsushima, Kanae and Abreu, Rui and Saraiva, Jo\~{a}o},
title = {GPT-3-Powered Type Error Debugging: Investigating the Use of Large Language Models for Code Repair},
year = {2023},
isbn = {9798400703966},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3623476.3623522},
doi = {10.1145/3623476.3623522},
abstract = {Type systems are responsible for assigning types to terms in programs. That way, they enforce the actions that can be taken and can, consequently, detect type errors during compilation. However, while they are able to flag the existence of an error, they often fail to pinpoint its cause or provide a helpful error message. Thus, without adequate support, debugging this kind of errors can take a considerable amount of effort. Recently, neural network models have been developed that are able to understand programming languages and perform several downstream tasks. We argue that type error debugging can be enhanced by taking advantage of this deeper understanding of the language’s structure. In this paper, we present a technique that leverages GPT-3’s capabilities to automatically fix type errors in OCaml programs. We perform multiple source code analysis tasks to produce useful prompts that are then provided to GPT-3 to generate potential patches. Our publicly available tool, Mentat, supports multiple modes and was validated on an existing public dataset with thousands of OCaml programs. We automatically validate successful repairs by using Quickcheck to verify which generated patches produce the same output as the user-intended fixed version, achieving a 39\% repair rate. In a comparative study, Mentat outperformed two other techniques in automatically fixing ill-typed OCaml programs.},
booktitle = {Proceedings of the 16th ACM SIGPLAN International Conference on Software Language Engineering},
pages = {111–124},
numpages = {14},
keywords = {Automated Program Repair, Code Generation, Fault Localization, GPT-3},
location = {Cascais, Portugal},
series = {SLE 2023}
}

@inbook{10.1145/3696630.3728616,
author = {Fu, Xing},
title = {Test Script Repair of Deep Learning Library Testing},
year = {2025},
isbn = {9798400712760},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3696630.3728616},
abstract = {Deep learning (DL) libraries such as TensorFlow and PyTorch are widely utilized to develop machine learning models. However, when testing DL libraries, evolving library versions and flaws of testing approaches may result in invalid models within test scripts, which influence testing efficiency severely. Repairing these models can be difficult to achieve manually because of the complexity of DL models. In this paper, we propose a novel approach that utilizes a specialized prompt-based strategy with Large Language Model (LLM) to repair invalid DL models. We manually provide structured error information and model configurations to LLM, allowing it to generate code to fix invalid models. Our work shows that most invalid models can be repaired successfully with our strategy. Moreover, our approach can help to detect flaws in the DL library testing approaches and issues caused by version updates, which enhances the robustness and transferability of DL models.},
booktitle = {Proceedings of the 33rd ACM International Conference on the Foundations of Software Engineering},
pages = {1016–1018},
numpages = {3}
}

@inbook{10.5555/3766078.3766305,
author = {Kim, Youngjoon and Shin, Sunguk and Kim, Hyoungshick and Yoon, Jiwon},
title = {Logs in, patches out: automated vulnerability repair via tree-of-thought LLM analysis},
year = {2025},
isbn = {978-1-939133-52-6},
publisher = {USENIX Association},
address = {USA},
abstract = {Research on automated vulnerability repair often requires extensive program analysis and expert input, making it challenging to deploy in practice. We propose SAN2PATCH, a system that generates patches using only sanitizer logs and source code, eliminating the need for costly program analysis or manual intervention. SAN2PATCH employs multi-stage reasoning with Large Language Models (LLMs) to decompose the patching process into four distinct tasks: vulnerability comprehension, fault localization, fix strategy formulation, and patch generation. Through tree-structured prompting and rigorous validation, SAN2PATCH can generate diverse, functionally-correct patches. Evaluations on the VulnLoc dataset show that SAN2PATCH successfully patches 79.5\% of vulnerabilities, surpassing state-of-the-art tools like ExtractFix (43\%) and VulnFix (51\%) by significant margins. On our newly curated SAN2VULN dataset of 27 new vulnerabilities from various open-source projects, SAN2PATCH achieves a 63\% success rate, demonstrating its effectiveness on modern security flaws. Notably, SAN2PATCH excels at patching complex memory-related vulnerabilities, successfully fixing 81.8\% of buffer overflows while preserving program functionality. This high performance, combined with minimal deployment requirements and elimination of manual steps, makes SAN2PATCH a practical solution for real-world vulnerability remediation.},
booktitle = {Proceedings of the 34th USENIX Conference on Security Symposium},
articleno = {227},
numpages = {19}
}

@article{10.1145/3728939,
author = {Ye, He and Yang, Aidan Z.H. and Hu, Chang and Wang, Yanlin and Zhang, Tao and Le Goues, Claire},
title = {AdverIntent-Agent: Adversarial Reasoning for Repair Based on Inferred Program Intent},
year = {2025},
issue_date = {July 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {ISSTA},
url = {https://doi.org/10.1145/3728939},
doi = {10.1145/3728939},
abstract = {Automated program repair (APR) has shown promising results, particularly with the use of neural networks. Currently, most APR tools focus on code transformations specified by test suites, rather than reasoning about the program’s intent and the high-level bug specification. Without a proper understanding of program intent, these tools tend to generate patches that overfit incomplete test suites and fail to reflect the developer’s intentions. However, reasoning about program intent is challenging. In our work, we propose an approach called AdverIntent-Agent, based on critique and adversarial reasoning. Our approach is novel to shift the focus from generating multiple APR patches to inferring multiple potential program intents. Ideally, we aim to infer intents that are, to some extent, adversarial to each other, maximizing the probability that at least one aligns closely with the developer’s original intent. AdverIntent-Agent is a multi-agent approach consisting of three agents: a reasoning agent, a test agent, and a repair agent. First, the reasoning agent generates adversarial program intents along with the corresponding faulty statements. Next, the test agent produces adversarial test cases that align with each inferred intent, constructing oracles that use the same inputs but have different expected outputs. Finally, the repair agent uses dynamic and precise LLM prompts to generate patches that satisfy both the inferred program intent and the generated tests. AdverIntent-Agent was evaluated on two benchmarks: Defects4J 2.0 and HumanEval-Java. AdverIntentAgent correctly repaired 77 and 105 bugs in both benchmarks, respectively. Our work helps reduce the effort required to review patches by enabling developers to assess program intent in natural language, rather than reviewing code patches.},
journal = {Proc. ACM Softw. Eng.},
month = jun,
articleno = {ISSTA062},
numpages = {23},
keywords = {Large Language Models, Program Repair}
}

@inproceedings{10.1145/3746252.3761035,
author = {Zhao, Yicong and Chen, Shisong and Zhang, Jiacheng and Li, Zhixu},
title = {ReCode: Improving LLM-based Code Repair with Fine-Grained Retrieval-Augmented Generation},
year = {2025},
isbn = {9798400720406},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3746252.3761035},
doi = {10.1145/3746252.3761035},
abstract = {Recent advances in large language models (LLMs) have demonstrated impressive capabilities in code-related tasks such as code generation and automated program repair. Despite their promising performance, most existing approaches for code repair suffer from high training costs or computationally expensive inference. Retrieval-augmented generation (RAG), with its efficient in-context learning paradigm, offers a more scalable alternative. However, conventional retrieval strategies, which are often based on holistic code-text embeddings, fail to capture the structural intricacies of code, resulting in suboptimal retrieval quality. To address the above limitations, we propose ReCode, a fine-grained retrieval-augmented in-context learning framework designed for accurate and efficient code repair. Specifically, ReCode introduces two key innovations: (1) an algorithm-aware retrieval strategy that narrows the search space using preliminary algorithm type predictions; and (2) a modular dual-encoder architecture that separately processes code and textual inputs, enabling fine-grained semantic matching between input and retrieved contexts. Furthermore, we propose RACodeBench, a new benchmark constructed from real-world user-submitted buggy code, which addresses the limitations of synthetic benchmarks and supports realistic evaluation. Experimental results on RACodeBench and competitive programming datasets demonstrate that ReCode achieves higher repair accuracy with significantly reduced inference cost, highlighting its practical value for real-world code repair scenarios.},
booktitle = {Proceedings of the 34th ACM International Conference on Information and Knowledge Management},
pages = {4368–4378},
numpages = {11},
keywords = {benchmark, code repair, in-context learning, retrieval augmented},
location = {Seoul, Republic of Korea},
series = {CIKM '25}
}

@inbook{10.1109/ICSE55347.2025.00126,
author = {Steenhoek, Benjamin and Sivaraman, Kalpathy and Gonzalez, Renata Saldivar and Mohylevskyy, Yevhen and Moghaddam, Roshanak Zilouchian and Le, Wei},
title = {Closing the Gap: A User Study on the Real-World Usefulness of AI-Powered Vulnerability Detection \&amp; Repair in the IDE},
year = {2025},
isbn = {9798331505691},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE55347.2025.00126},
abstract = {Security vulnerabilities impose significant costs on users and organizations. Detecting and addressing these vulnerabilities early is crucial to avoid exploits and reduce development costs. Recent studies have shown that deep learning models can effectively detect security vulnerabilities. Yet, little research explores how to adapt these models from benchmark tests to practical applications, and whether they can be useful in practice.This paper presents the first empirical study of a vulnerability detection and fix tool with professional software developers on real projects that they own. We implemented DeepVulGuard, an IDE-integrated tool based on state-of-the-art detection and fix models, and show that it has promising performance on benchmarks of historic vulnerability data. DeepVulGuard scans code for vulnerabilities (including identifying the vulnerability type and vulnerable region of code), suggests fixes, provides natural-language explanations for alerts and fixes, leveraging chat interfaces. We recruited 17 professional software developers at Microsoft, observed their usage of the tool on their code, and conducted interviews to assess the tool's usefulness, speed, trust, relevance, and workflow integration. We also gathered detailed qualitative feedback on users' perceptions and their desired features. Study participants scanned a total of 24 projects, 6.9k files, and over 1.7 million lines of source code, and generated 170 alerts and 50 fix suggestions. We find that although state-of-the-art AI-powered detection and fix tools show promise, they are not yet practical for real-world use due to a high rate of false positives and non-applicable fixes. User feedback reveals several actionable pain points, ranging from incomplete context to lack of customization for the user's codebase. Additionally, we explore how AI features, including confidence scores, explanations, and chat interaction, can apply to vulnerability detection and fixing. Based on these insights, we offer practical recommendations for evaluating and deploying AI detection and fix models. Our code and data are available at this link: https://doi.org/10.6084/m9.figshare.26367139.},
booktitle = {Proceedings of the IEEE/ACM 47th International Conference on Software Engineering},
pages = {2650–2662},
numpages = {13}
}

@article{10.1007/s10664-025-10716-z,
author = {Zhao, Qianhui and Zhang, Li and Liu, Fang and Liu, Yang and Yan, Zhen and Chen, Zhenghao and Zhou, Yufei and Jiang, Jing and Li, Ge and Sun, Zian and Li, Zhongqi and Ma, Yuchi},
title = {Peer-aided repairer: empowering large language models to repair advanced student assignments},
year = {2025},
issue_date = {Dec 2025},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {31},
number = {2},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-025-10716-z},
doi = {10.1007/s10664-025-10716-z},
abstract = {Automated generation of feedback on programming assignments holds significant benefits for programming education, especially when it comes to advanced assignments. Automated Program Repair techniques, especially Large Language Model-based approaches, have gained notable recognition for their potential in fixing introductory assignments. However, the programs used for evaluation are relatively simple. It remains unclear how existing approaches perform in repairing programs from higher-level programming courses. To address these limitations, we curate a new advanced student assignment dataset named Defects4DS from a higher-level programming course. Subsequently, we identify the challenges related to fixing bugs in advanced assignments. Based on the analysis, we develop a framework called PaR that is powered by the Large Language Models. PaR works in three phases: Peer Solution Selection, Multi-Source Prompt Generation, and Program Repair. Peer Solution Selection identifies the closely related peer programs based on lexical, semantic, and syntactic criteria. Then Multi-Source Prompt Generation adeptly combines multiple sources of information to create a comprehensive and informative prompt for the last Program Repair stage. Evaluation reveals that PaR achieves state-of-the-art performance on Defects4DS compared to baseline approaches, with the impressive improvement of 16.13\% in repair rate. And experimental results on several introductory programming assignment datasets further demonstrate the effectiveness of PaR, achieving state-of-the-art results on ITSP and IntroClass datasets.},
journal = {Empirical Softw. Engg.},
month = dec,
numpages = {49},
keywords = {Programming education, Program repair, Large language models, Software engineering}
}

@inproceedings{10.1145/3610542.3626140,
author = {Yamauchi, Yuta and Ino, Keiko and Zempo, Keiichi},
title = {Auditory VR Generative System for Non-Experts to Reproduce Human Memories Through Natural Language Interactions},
year = {2023},
isbn = {9798400703133},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3610542.3626140},
doi = {10.1145/3610542.3626140},
abstract = {We propose an automatic auditory VR generative system based on natural language input and attempt to apply it to VR exposure therapy, a promising treatment for Post-Traumatic Stress Disorder (PTSD). The system consisted of the user interface, developed based on the Large Language Model (LLM), the auditory event dataset, which has metadata of “subject” and “verb” and spatial audio generator.},
booktitle = {SIGGRAPH Asia 2023 Posters},
articleno = {6},
numpages = {2},
location = {Sydney, NSW, Australia},
series = {SA '23}
}

@article{10.1145/3660804,
author = {Yang, Haoran and Nong, Yu and Zhang, Tao and Luo, Xiapu and Cai, Haipeng},
title = {Learning to Detect and Localize Multilingual Bugs},
year = {2024},
issue_date = {July 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {FSE},
url = {https://doi.org/10.1145/3660804},
doi = {10.1145/3660804},
abstract = {Increasing studies have shown bugs in multi-language software as a critical loophole in modern software quality assurance, especially those induced by language interactions (i.e., multilingual bugs). Yet existing tool support for bug detection/localization remains largely limited to single-language software, despite the long-standing prevalence of multi-language systems in various real-world software domains. Extant static/dynamic analysis and deep learning (DL) based approaches all face major challenges in addressing multilingual bugs. In this paper, we present xLoc, a DL-based technique/tool for detecting and localizing multilingual bugs. Motivated by results of our bug-characteristics study on top locations of multilingual bugs, xLoc first learns the general knowledge relevant to differentiating various multilingual control-flow structures. This is achieved by pre-training a Transformer model with customized position encoding against novel objectives. Then, xLoc learns task-specific knowledge for the task of multilingual bug detection/localization, through another new position encoding scheme (based on cross-language API vicinity) that allows for the model to attend particularly to control-flow constructs that bear most multilingual bugs during fine-tuning. We have implemented xLoc for Python-C software and curated a dataset of 3,770 buggy and 15,884 non-buggy Python-C samples, which enabled our extensive evaluation of xLoc against two state-of-the-art baselines: fine-tuned CodeT5 and zero-shot ChatGPT. Our results show that xLoc achieved 94.98\% F1 and 87.24\% @Top-1 accuracy, which are significantly (up to 162.88\% and 511.75\%) higher than the baselines. Ablation studies further confirmed significant contributions of each of the novel design elements in xLoc. With respective bug-location characteristics and labeled bug datasets for fine-tuning, our design may be applied to other language combinations beyond Python-C.},
journal = {Proc. ACM Softw. Eng.},
month = jul,
articleno = {97},
numpages = {24},
keywords = {Multi-language software, Multilingual Bugs, Bug Detection, Fault Localization}
}

@inproceedings{10.1145/3716815.3729010,
author = {Luo, Yining and Li, Baobao and Singhal, Anoop and Tseng, Peiyu and Zhang, Lan and Zou, Qingtian and Sun, Xiaoyan and Liu, Peng},
title = {Exploring Prompt Patterns for Effective Vulnerability Repair in Real-World Code by Large Language Models},
year = {2025},
isbn = {9798400715013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3716815.3729010},
doi = {10.1145/3716815.3729010},
abstract = {Large Language Models (LLMs) have shown promise in automating code vulnerability repair, but their effectiveness in handling real-world code remains limited. This paper investigates the capability of LLMs,  in repairing vulnerabilities and proposes a systematic approach to enhance their performance through specialized prompt engineering. Through extensive evaluation of 5,826 code samples, we found that while LLMs successfully repair vulnerabilities in simple cases, they struggle with complex real-world code that involves intricate dependencies, contextual requirements, and multi-file interactions. To address these limitations, we first incorporated Control Flow Graphs (CFGs) as supplementary prompts, achieving a 14.4\% success rate in fixing previously unresolvable vulnerabilities. Through analysis of repair failures, we identified three primary challenge categories and developed corresponding prompt patterns incorporating techniques such as granular contextual information provision and progressive code simplification. Evaluation on real-world projects demonstrated that our approach significantly improved LLMs' repair capabilities, achieving over 85\% success rates across all identified challenge categories. Our findings suggest that while LLMs have inherent limitations in handling complex vulnerabilities independently, they can become effective tools for automated vulnerability repair when guided by carefully crafted prompts.},
booktitle = {Proceedings of the 10th ACM International Workshop on Security and Privacy Analytics},
pages = {23–33},
numpages = {11},
keywords = {large language models, program repair, deep learning},
location = {Pittsburgh, PA, USA},
series = {IWSPA '25}
}

@inproceedings{10.1609/aaai.v37i4.25642,
author = {Joshi, Harshit and Sanchez, Jos\'{e} Cambronero and Gulwani, Sumit and Le, Vu and Radi\v{c}ek, Ivan and Verbruggen, Gust},
title = {Repair is nearly generation: multilingual program repair with LLMs},
year = {2023},
isbn = {978-1-57735-880-0},
publisher = {AAAI Press},
url = {https://doi.org/10.1609/aaai.v37i4.25642},
doi = {10.1609/aaai.v37i4.25642},
abstract = {Most programmers make mistakes when writing code. Some of these mistakes are small and require few edits to the original program - a class of errors recently termed last mile mistakes. These errors break the flow for experienced developers and can stump novice programmers. Existing automated repair techniques targeting this class of errors are language-specific and do not easily carry over to new languages. Transferring symbolic approaches requires substantial engineering and neural approaches require data and retraining. We introduce RING, a multilingual repair engine powered by a large language model trained on code (LLMC) such as Codex. Such a multilingual engine enables a flipped model for programming assistance, one where the programmer writes code and the AI assistance suggests fixes, compared to traditional code suggestion technology. Taking inspiration from the way programmers manually fix bugs, we show that a prompt-based strategy that conceptualizes repair as localization, transformation, and candidate ranking, can successfully repair programs in multiple languages with minimal effort. We present the first results for such a multilingual repair engine by evaluating on 6 different languages and comparing performance to language-specific repair engines. We show that RING can outperform language- specific repair engines for three of these languages.},
booktitle = {Proceedings of the Thirty-Seventh AAAI Conference on Artificial Intelligence and Thirty-Fifth Conference on Innovative Applications of Artificial Intelligence and Thirteenth Symposium on Educational Advances in Artificial Intelligence},
articleno = {573},
numpages = {10},
series = {AAAI'23/IAAI'23/EAAI'23}
}

@inproceedings{10.1109/ICSE-NIER66352.2025.00024,
author = {Li, Fengjie and Jiang, Jiajun and Sun, Jiajun and Zhang, Hongyu},
title = {Evaluating the Generalizability of LLMs in Automated Program Repair},
year = {2025},
isbn = {9798331537111},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-NIER66352.2025.00024},
doi = {10.1109/ICSE-NIER66352.2025.00024},
abstract = {LLM-based automated program repair methods have attracted significant attention for their state-of-the-art performance. However, they were primarily evaluated on a few well-known datasets like Defects4J, raising questions about their effectiveness on new datasets. In this study, we evaluate 11 top-performing LLMs on Defects4J-Trans, a new dataset derived from transforming Defects4J while maintaining the original semantics. Results from experiments on both Defects4J and Defects4J-Trans show that all studied LLMs have limited generalizability in APR tasks, with the average number of correct and plausible patches decreasing by 49.48\% and 42.90\%, respectively, on Defects4J-Trans. Further investigation into incorporating additional repair-relevant information in repair prompts reveals that, although this information significantly enhances the LLMs' capabilities (increasing the number of correct and plausible patches by up to 136.67\% and 121.82\%, respectively), performance still falls short of their original results. This indicates that prompt engineering alone is insufficient to substantially enhance LLMs' repair capabilities. Based on our study, we also offer several recommendations for future research.},
booktitle = {Proceedings of the IEEE/ACM 47th International Conference on Software Engineering: New Ideas and Emerging Results},
pages = {91–95},
numpages = {5},
keywords = {program repair, LLM, generalizability of LLM},
location = {Ottawa, Ontario, Canada},
series = {ICSE-NIER '25}
}

@inproceedings{10.1145/3540250.3549098,
author = {Fu, Michael and Tantithamthavorn, Chakkrit and Le, Trung and Nguyen, Van and Phung, Dinh},
title = {VulRepair: a T5-based automated software vulnerability repair},
year = {2022},
isbn = {9781450394130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3540250.3549098},
doi = {10.1145/3540250.3549098},
abstract = {As software vulnerabilities grow in volume and complexity, researchers proposed various Artificial Intelligence (AI)-based approaches to help under-resourced security analysts to find, detect, and localize vulnerabilities. However, security analysts still have to spend a huge amount of effort to manually fix or repair such vulnerable functions. Recent work proposed an NMT-based Automated Vulnerability Repair, but it is still far from perfect due to various limitations. In this paper, we propose VulRepair, a T5-based automated software vulnerability repair approach that leverages the pre-training and BPE components to address various technical limitations of prior work. Through an extensive experiment with over 8,482 vulnerability fixes from 1,754 real-world software projects, we find that our VulRepair achieves a Perfect Prediction of 44\%, which is 13\%-21\% more accurate than competitive baseline approaches. These results lead us to conclude that our VulRepair is considerably more accurate than two baseline approaches, highlighting the substantial advancement of NMT-based Automated Vulnerability Repairs. Our additional investigation also shows that our VulRepair can accurately repair as many as 745 out of 1,706 real-world well-known vulnerabilities (e.g., Use After Free, Improper Input Validation, OS Command Injection), demonstrating the practicality and significance of our VulRepair for generating vulnerability repairs, helping under-resourced security analysts on fixing vulnerabilities.},
booktitle = {Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {935–947},
numpages = {13},
keywords = {Software Vulnerability Repair},
location = {Singapore, Singapore},
series = {ESEC/FSE 2022}
}

@inbook{10.5555/3766078.3766307,
author = {Li, Ying and Shezan, Faysal Hossain and Wei, Bomin and Wang, Gang and Tian, Yuan},
title = {SoK: towards effective automated vulnerability repair},
year = {2025},
isbn = {978-1-939133-52-6},
publisher = {USENIX Association},
address = {USA},
abstract = {The increasing prevalence of software vulnerabilities necessitates automated vulnerability repair (AVR) techniques. This Systematization of Knowledge (SoK) provides a comprehensive overview of the AVR landscape, encompassing both synthetic and real-world vulnerabilities. Through a systematic literature review and quantitative benchmarking across diverse datasets, methods, and strategies, we establish a taxonomy of existing AVR methodologies, categorizing them into template- guided, search-based, constraint-based, and learning-driven approaches. We evaluate the strengths and limitations of these approaches, highlighting common challenges and practical implications. Our comprehensive analysis of existing AVR methods reveals a diverse landscape with no single "best" approach. Learning-based methods excel in specific scenarios but lack complete program understanding, and both learning and non-learning methods face challenges with complex vulnerabilities. Additionally, we identify emerging trends and propose future research directions to advance the field of AVR. This SoK serves as a valuable resource for researchers and practitioners, offering a structured understanding of the current state-of-the-art and guiding future research and development in this critical domain.},
booktitle = {Proceedings of the 34th USENIX Conference on Security Symposium},
articleno = {229},
numpages = {22}
}

@article{10.1007/s10586-024-04490-8,
author = {Omari, Safwan and Basnet, Kshitiz and Wardat, Mohammad},
title = {Investigating large language models capabilities for automatic code repair in Python},
year = {2024},
issue_date = {Nov 2024},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {27},
number = {8},
issn = {1386-7857},
url = {https://doi.org/10.1007/s10586-024-04490-8},
doi = {10.1007/s10586-024-04490-8},
abstract = {Developers often encounter challenges with their introductory programming tasks as part of the development process. Unfortunately, rectifying these mistakes manually can be time-consuming and demanding. Automated program repair (APR) techniques offer a potential solution by synthesizing fixes for such errors. Previous research has investigated the utilization of both symbolic and neural techniques within the APR domain. However, these approaches typically demand significant engineering efforts or extensive datasets and training. In this paper, we explore the potential of using a large language model trained on code, specifically, we assess ChatGPT’s capability to detect and repair bugs in simple Python programs. The experimental evaluation encompasses two benchmarks: QuixBugs and Textbook. Each benchmark consists of simple Python functions that implement well-known algorithms and each function contains a single bug. To gauge repair performance in various settings, several benchmark variations were introduced including addition of plain English documentation and code obfuscation. Based on thorough experiments, we found that ChatGPT was able to correctly detect and fix about 50\% of the methods, when code is documented. Repair performance drops to 25\% when code is obfuscated, and 15\% when documentation is removed and code is obfuscated. Furthermore, when compared to existing APR systems, ChatGPT considerably outperformed them.},
journal = {Cluster Computing},
month = may,
pages = {10717–10731},
numpages = {15},
keywords = {Automatic program repair, Large language models, Python, Bug detection}
}

@inproceedings{10.1145/3643651.3659892,
author = {Zhang, Lan and Zou, Qingtian and Singhal, Anoop and Sun, Xiaoyan and Liu, Peng},
title = {Evaluating Large Language Models for Real-World Vulnerability Repair in C/C++ Code},
year = {2024},
isbn = {9798400705564},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643651.3659892},
doi = {10.1145/3643651.3659892},
abstract = {The advent of Large Language Models (LLMs) has enabled advancement in automated code generation, translation, and summarization. Despite their promise, evaluating the use of LLMs in repairing real-world code vulnerabilities remains underexplored. In this study, we address this gap by evaluating the capability of advanced LLMs, such as ChatGPT-4 and Claude, in fixing memory corruption vulnerabilities in real-world C/C++ code. We meticulously curated 223 real-world C/C++ code snippets encompassing a spectrum of memory corruption vulnerabilities, ranging from straightforward memory leaks to intricate buffer errors. Our findings demonstrate the proficiency of LLMs in rectifying simple memor errors like leaks, where fixes are confined to localized code segments. However, their effectiveness diminishes when addressing complicated vulnerabilities necessitating reasoning about cross-cutting concerns and deeper program semantics. Furthermore, we explore techniques for augmenting LLM performance by incorporating additional knowledge. Our results shed light on both the strengths and limitations of LLMs in automated program repair on genuine code, underscoring the need for advancements in reasoning abilities for handling complex code repair tasks.},
booktitle = {Proceedings of the 10th ACM International Workshop on Security and Privacy Analytics},
pages = {49–58},
numpages = {10},
keywords = {deep learning, large language models, program repair},
location = {Porto, Portugal},
series = {IWSPA '24}
}

@inproceedings{10.1007/978-3-031-75434-0_10,
author = {Tahat, Amer and Hardin, David and Petz, Adam and Alexander, Perry},
title = {Proof Repair Utilizing Large Language Models: A Case Study on&nbsp;the&nbsp;Copland Remote Attestation Proofbase},
year = {2025},
isbn = {978-3-031-75433-3},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-75434-0_10},
doi = {10.1007/978-3-031-75434-0_10},
abstract = {Large Language Model (LLM) Artificial Intelligence (AI) systems have generated significant enthusiasm in the computer science research community for their potential in various computer language processing tasks, such as source code generation and source-to-source translation. We are particularly interested in using LLMs for automated theorem proving, specifically for proof repair. To this end, we introduce CoqDog Copilot, which leverages the neuro-symbolic interplay between generative AI and the Coq theorem prover to form a productive “generate-and-test” loop, incrementally improving proofs based on failure information and human hints until valid proofs are achieved. Our research introduces innovative solutions to critical challenges in developing CoqDog Copilot, including addressing context limitations, enhancing the soundness of recommendation systems, defining effective metrics for measuring proof repair progress, and designing a statistically robust evaluation system for conversational quality assessment. We present a comprehensive evaluation of CoqDog Copilot’s performance in proof repair across multiple samples from the Copland Coq proofbase, which consists of a total of 21,000 lines of Coq code. We have attained in excess of 60\% accuracy for proof generation using GPT-4 in one ‘shot’, with approximately 30\% more lemmas proved given one additional user prompt (yielding 90\% correctness overall). With three ‘shots’, the overall proof correctness rate increases to 97\%. We can generate Coq proofs with up to 50 proof steps using this technique. Our LLM-generated proofbase currently consists of over 1,400 lines of Copland Coq source.},
booktitle = {Bridging the Gap Between AI and Reality: Second International Conference, AISoLA 2024, Crete, Greece, October 30 – November 3, 2024, Proceedings},
pages = {145–166},
numpages = {22},
location = {Crete, Greece}
}

@inproceedings{10.1145/3714393.3726486,
author = {Braconaro, Elisa and Losiouk, Eleonora},
title = {A Dataset for Evaluating LLMs Vulnerability Repair Performance in Android Applications: Data/Toolset paper},
year = {2025},
isbn = {9798400714764},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3714393.3726486},
doi = {10.1145/3714393.3726486},
abstract = {Automated Program Repair (APR) is a well-established research area that enhances software reliability and security by automatically fixing bugs, reducing manual effort, and accelerating debugging. Despite progress in publishing benchmarks to evaluate APR tools, datasets specifically targeting Android are lacking.To address this gap, we introduce a dataset of 272 real-world violations of Google's Android Security Best Practices, identified by statically analyzing 113 real-world Android apps. In addition to the faulty code, we manually crafted repairs based on Google's guidelines, covering 176 Java-based and 96 XML-based violations from Android Java classes and Manifest files, respectively. Additionally, we leveraged our novel dataset to evaluate Large Language Models (LLMs) as they are the latest promising APR tools. In particular, we evaluated GPT-4o, Gemini 1.5 Flash and Gemini in Android Studio and we found that GPT-4o outperforms Google's models, demonstrating higher accuracy and robustness across a range of violations types. Hence, with this dataset, we aim to provide valuable insights for advancing APR research and improving tools for Android security.},
booktitle = {Proceedings of the Fifteenth ACM Conference on Data and Application Security and Privacy},
pages = {353–358},
numpages = {6},
keywords = {android vulnerabilities, automated program repair, large language models},
location = {Pittsburgh, PA, USA},
series = {CODASPY '25}
}

@inproceedings{10.1145/3756681.3756966,
author = {Vallecillos Ruiz, Fernando and Hort, Max and Moonen, Leon},
title = {The Art of Repair: Optimizing Iterative Program Repair with Instruction-Tuned Models},
year = {2025},
isbn = {9798400713859},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3756681.3756966},
doi = {10.1145/3756681.3756966},
abstract = {Automatic program repair (APR) aims at reducing the manual efforts required to identify and fix errors in source code. Before the rise of Large Language Model (LLM)-based agents, a common strategy was simply to increase the number of generated patches, sometimes to the thousands, which usually yielded better repair results on benchmarks. More recently, self-iterative capabilities enabled LLMs to refine patches over multiple rounds guided by feedback. However, literature often focuses on many iterations and disregards different numbers of outputs.We investigate an APR pipeline that balances these two approaches, the generation of multiple outputs and multiple rounds of iteration, while imposing a limit of 10 total patches per bug. We apply three SOTA instruction-tuned LLMs – DeepSeekCoder-Instruct, Codellama-Instruct, Llama3.1-Instruct – to the APR task. We further fine-tune each model on an APR dataset with three sizes (1K, 30K, 65K) and two techniques (Full Fine-Tuning and LoRA), allowing us to assess their repair capabilities on two APR benchmarks: HumanEval-Java and Defects4J.Our results show that by using only a fraction (&lt;1\%) of the fine-tuning dataset, we can achieve improvements of up to 78\% in the number of plausible patches generated, challenging prior studies that reported limited gains using Full Fine-Tuning. However, we find that exceeding certain thresholds leads to diminishing outcomes, likely due to overfitting. Moreover, we show that base models greatly benefit from creating patches in an iterative fashion rather than generating them all at once. In addition, the benefit of iterative strategies becomes more pronounced in complex benchmarks. Even fine-tuned models, while benefiting less from iterations, still gain advantages, particularly on complex benchmarks. The research underscores the need for balanced APR strategies that combine multi-output generation and iterative refinement.},
booktitle = {Proceedings of the 29th International Conference on Evaluation and Assessment in Software Engineering},
pages = {500–511},
numpages = {12},
keywords = {Automated Program Repair, Software Testing, Software Maintenance, Large Language Models},
location = {
},
series = {EASE '25}
}

@inproceedings{10.1145/3589335.3651463,
author = {Le, Tan Khang and Alimadadi, Saba and Ko, Steven Y.},
title = {A Study of Vulnerability Repair in JavaScript Programs with Large Language Models},
year = {2024},
isbn = {9798400701726},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3589335.3651463},
doi = {10.1145/3589335.3651463},
abstract = {In recent years, JavaScript has become the most widely used programming language, especially in web development. However, writing secure JavaScript code is not trivial, and programmers often make mistakes that lead to security vulnerabilities in web applications. Large Language Models (LLMs) have demonstrated substantial advancements across multiple domains, and their evolving capabilities indicate their potential for automatic code generation based on a required specification, including automatic bug fixing. In this study, we explore the accuracy of LLMs, namely ChatGPT and Bard, in finding and fixing security vulnerabilities in JavaScript programs. We also investigate the impact of context in a prompt on directing LLMs to produce a correct patch of vulnerable JavaScript code. Our experiments on real-world software vulnerabilities show that while LLMs are promising in automatic program repair of JavaScript code, achieving a correct bug fix often requires an appropriate amount of context in the prompt.},
booktitle = {Companion Proceedings of the ACM Web Conference 2024},
pages = {666–669},
numpages = {4},
keywords = {automatic program repair, cwe, javascript, large language models, prompt engineering},
location = {Singapore, Singapore},
series = {WWW '24}
}

@inproceedings{10.1145/3639478.3643114,
author = {Martinez, Matias and Mart\'{\i}nez-Fern\'{a}ndez, Silverio and Franch, Xavier},
title = {Energy Consumption of Automated Program Repair},
year = {2024},
isbn = {9798400705021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639478.3643114},
doi = {10.1145/3639478.3643114},
abstract = {In the last decade, following current societal needs, software sustainability has emerged as research field [2]. In this paper, we particularly focus on environmental sustainability, defined as "how software product development, maintenance, and use affect energy consumption and the consumption of other natural resources. [...] This dimension is also known as Green Software" [2].},
booktitle = {Proceedings of the 2024 IEEE/ACM 46th International Conference on Software Engineering: Companion Proceedings},
pages = {358–359},
numpages = {2},
location = {Lisbon, Portugal},
series = {ICSE-Companion '24}
}

@inproceedings{10.1007/978-3-031-93982-2_3,
author = {Glawe, Felix and Vervier, Luisa and Brauner, Philipp and Ziefle, Martina},
title = {From Failure to&nbsp;Trust Repair: Expert Insights on&nbsp;Factors Influencing Trust in&nbsp;Collaborative Robots},
year = {2025},
isbn = {978-3-031-93981-5},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-93982-2_3},
doi = {10.1007/978-3-031-93982-2_3},
abstract = {The use of collaborative robots (cobots), designed to work alongside human operators in shared spaces, is predicted to increase in the coming years. However, successful deployment requires human trust, otherwise the collaboration may lack efficiency and human acceptance. As failures are an inevitable part of human-robot collaboration (HRC), it is important to examine potential failures, their likely negative impact on human trust, and how trust can be rebuilt. Interviews were conducted with 12 robot experts to understand their perspectives on factors affecting human trust, potential failures, and trust repair mechanisms in HRC. Qualitative content analysis was used and the results showed that failures can be classified according to their cause, symptoms and costs. This classification should be context and goal specific to enable optimal failure handling. Furthermore, the potential role of trust in third parties was uncovered and the importance of reliability and predictability for human trust in the cobot was confirmed. Experts mention both cobots and humans as being able to use trust repair mechanisms mainly following a pattern of failure detection, cause communication, failure correction, and communication of future steps.},
booktitle = {Human-Computer Interaction: Thematic Area, HCI 2025, Held as Part of the 27th HCI International Conference, HCII 2025, Gothenburg, Sweden, June 22–27, 2025, Proceedings, Part VII},
pages = {36–52},
numpages = {17},
keywords = {Human-Robot Interaction, Cobot, Trust, Trust Repair},
location = {Gothenburg, Sweden}
}

@article{10.1016/j.future.2024.107671,
author = {Hou, Jing and Han, Jiaxuan and Huang, Cheng and Wang, Nannan and Li, Lerong},
title = {         LineJLocRepair: A line-level method for Automated Vulnerability Repair based on joint training},
year = {2025},
issue_date = {May 2025},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {166},
number = {C},
issn = {0167-739X},
url = {https://doi.org/10.1016/j.future.2024.107671},
doi = {10.1016/j.future.2024.107671},
journal = {Future Gener. Comput. Syst.},
month = may,
numpages = {14},
keywords = {Vulnerability localization, Vulnerability repair, Code LLM}
}

@article{10.1007/s11219-025-09728-1,
author = {Liu, Zixin and Du, Xiaozhi and Liu, Hairui},
title = {ReAPR: Automatic program repair via retrieval-augmented large language models: ReAPR: Automatic program repair via retrieval-augmented...},
year = {2025},
issue_date = {Jul 2025},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {33},
number = {3},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-025-09728-1},
doi = {10.1007/s11219-025-09728-1},
abstract = {Automatic Program Repair (APR) aims to automatically fix software defects, significantly reducing the efforts of manual debugging. Recent studies have demonstrated impressive results in utilizing Large Language Models (LLMs) for software bug fixing. Current LLM-based approaches depend solely on the pre-trained knowledge of LLMs, overlooking the prior knowledge contained in historical bug repair records, which increases the likelihood of hallucinations. To address this challenge, this paper proposes ReAPR, a retrieval-augmented framework for APR. We first curate a high-quality retrieval database by carefully compiling and filtering the existing datasets for APR. Subsequently, ReAPR leverages a retriever to fetch bug-fix pairs similar to the target bug from a retrieval database, providing contextual hints to guide the LLMs in the repair process. We then investigate two techniques to retrieve bug-fix pairs associated with the function to be fixed: BM25 and Dense Passage Retrieval (DPR). After retrieving the relevant bug-fix pair, we construct a prompt and integrate the retrieved pair into it. Besides, we also compare the proposed RAG-based approach with the parameter-efficient fine-tuning (PEFT) approaches on repair performance. To validate the effectiveness of ReAPR, we conduct extensive experiments based on the widely-used benchmark dataset Defects4j 2.0 as well as the latest benchmark GitBug-Java. The results show that ReAPR, based on the CodeLlama(7B) backbone, successfully fixes 68 and 59 bugs in the DPR and BM25 settings, respectively, in Defects4j 2.0, outperforming the best baseline approach by 18 and 9 bugs under the same repair settings.},
journal = {Software Quality Journal},
month = jul,
numpages = {31},
keywords = {Automated Program Repair, Retrieval-Augmented Generation, Large Language Models, Prompt Learning}
}

@inbook{10.5555/3766078.3766304,
author = {Yu, Zheng and Guo, Ziyi and Wu, Yuhang and Yu, Jiahao and Xu, Meng and Mu, Dongliang and Chen, Yan and Xing, Xinyu},
title = {PATCHAGENT: a practical program repair agent mimicking human expertise},
year = {2025},
isbn = {978-1-939133-52-6},
publisher = {USENIX Association},
address = {USA},
abstract = {Automated program repair (APR) techniques, which aim to triage and fix software bugs autonomously, have emerged as powerful tools against vulnerable code. Recent advancements in large language models (LLMs) have further shown promising results when applied to APR, especially on patch generation. However, without effective fault localization and patch validation, APR tools specialized in patching alone cannot handle a more practical and end-to-end setting—given a concrete input that triggers a vulnerability, how to patch the program without breaking existing tests?In this paper, we introduce PATCHAGENT, a novel LLM-based APR tool that seamlessly integrates fault localization, patch generation, and validation within a single autonomous agent. PATCHAGENT employs a language server, a patch verifier, and interaction optimization techniques to mimic human-like reasoning during vulnerability repair. Evaluated on a dataset of 178 real-world vulnerabilities, PATCHAGENT successfully repairs over 90\% of the cases, outperforming state-of-the-art APR tools where applicable. Our ablation study further offer insights into the how various interaction optimizations contribute to PATCHAGENT'S effectiveness.},
booktitle = {Proceedings of the 34th USENIX Conference on Security Symposium},
articleno = {226},
numpages = {20}
}

@article{10.1145/3688834,
author = {Zhong, Wenkang and Li, Chuanyi and Liu, Kui and Ge, Jidong and Luo, Bin and Bissyand\'{e}, Tegawend\'{e} F. and Ng, Vincent},
title = {Benchmarking and Categorizing the Performance of Neural Program Repair Systems for Java},
year = {2025},
issue_date = {January 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {34},
number = {1},
issn = {1049-331X},
url = {https://doi.org/10.1145/3688834},
doi = {10.1145/3688834},
abstract = {Recent years have seen a rise in Neural Program Repair (NPR) systems in the software engineering community, which adopt advanced deep learning techniques to automatically fix bugs. Having a comprehensive understanding of existing systems can facilitate new improvements in this area and provide practical instructions for users. However, we observe two potential weaknesses in the current evaluation of NPR systems: ① published systems are trained with varying data, and ② NPR systems are roughly evaluated through the number of totally fixed bugs. Questions such as what types of bugs are repairable for current systems cannot be answered yet. Consequently, researchers cannot make target improvements in this area and users have no idea of the real affair of existing systems. In this article, we perform a systematic evaluation of the existing nine state-of-the-art NPR systems. To perform a fair and detailed comparison, we (1) build a new benchmark and framework that supports training and validating the nine systems with unified data and (2) evaluate re-trained systems with detailed performance analysis, especially on the effectiveness and the efficiency. We believe our benchmark tool and evaluation results could offer practitioners the real affairs of current NPR systems and the implications of further facilitating the improvements of NPR.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = dec,
articleno = {11},
numpages = {35},
keywords = {datasets, program repair, benchmark, empirical study}
}

@inproceedings{10.1145/3650212.3652140,
author = {Ouyang, Yicheng and Yang, Jun and Zhang, Lingming},
title = {Benchmarking Automated Program Repair: An Extensive Study on Both Real-World and Artificial Bugs},
year = {2024},
isbn = {9798400706127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3650212.3652140},
doi = {10.1145/3650212.3652140},
abstract = {As bugs are inevitable and prevalent in real-world programs, many Automated Program Repair (APR) techniques have been proposed to generate patches for them. However, due to the lack of a standard for evaluating APR techniques, prior works tend to use different settings and benchmarks in evaluation, threatening the trustworthiness of the evaluation results. Additionally, they typically only adopt plausibility and genuineness as evaluation metrics, which may potentially mask some underlying issues in APR techniques. To overcome these issues, in this paper, we conduct an extensive and multi-dimensional evaluation of nine learning-based and three traditional state-of-the-art APR techniques under the same environment and settings. We employ the widely studied Defects4J V2.0.0 benchmark and a newly constructed large-scale mutation-based benchmark named MuBench, derived from Defects4J and including 1,700 artificial bugs generated by various mutators, to uncover potential limitations in these APR techniques. We also apply multi-dimensional metrics, including compilability/plausibility/genuineness metrics, as well as SYE (SYntactic Equivalence) and TCE (Trivial Compiler Equivalence) metrics, to thoroughly analyze the 1,814,652 generated patches. This paper presents noteworthy findings from the extensive evaluation: Firstly, Large Language Model (LLM) based APR demonstrates less susceptibility to overfitting on the Defects4J V1.2.0 dataset and fixes the most number of bugs. Secondly, the study suggests a promising future for combining traditional and learning-based APR techniques, as they exhibit complementary advantages in fixing different types of bugs. Additionally, this work highlights the necessity for further enhancing patch compilability of learning-based APR techniques, despite the presence of various existing strategies attempting to improve it. The study also reveals other guidelines for enhancing APR techniques, including the need for handling unresolvable symbol compilability issues and reducing duplicate/no-op patch generation. Finally, our study uncovers seven implementation issues in the studied techniques, with five of them confirmed and fixed by the corresponding authors.},
booktitle = {Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {440–452},
numpages = {13},
keywords = {Empirical assessment, Mutation testing, Program repair},
location = {Vienna, Austria},
series = {ISSTA 2024}
}

@article{10.1145/3733599,
author = {Luo, Wenqiang and Keung, Jacky and Yang, Boyang and Ye, He and Le Goues, Claire and Bissyand\'{e}, Tegawend\'{e} F. and Tian, Haoye and Le, Xuan Bach D.},
title = {When Fine-Tuning LLMs Meets Data Privacy: An Empirical Study of Federated Learning in LLM-Based Program Repair},
year = {2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1049-331X},
url = {https://doi.org/10.1145/3733599},
doi = {10.1145/3733599},
abstract = {Software systems have been evolving rapidly and inevitably introducing bugs at an increasing rate, leading to significant maintenance costs. While large language models (LLMs) have demonstrated remarkable potential in enhancing software development and maintenance practices, particularly in automated program repair (APR), they rely heavily on high-quality code repositories. Most code repositories are proprietary assets that capture the diversity and nuances of real-world industry software practices, which public datasets cannot fully represent. However, obtaining such data from various industries is hindered by data privacy concerns, as companies are reluctant to share their proprietary codebases. There has also been no in-depth investigation of collaborative software development by learning from private and decentralized data while preserving data privacy for program repair.To address the gap, we investigate federated learning as a privacy-preserving method for fine-tuning LLMs on proprietary and decentralized data to boost collaborative software development and maintenance. We use the private industrial dataset TutorCode for fine-tuning and the EvalRepair-Java benchmark for evaluation, and assess whether federated fine-tuning enhances program repair. We then further explore how code heterogeneity (i.e., variations in coding style, complexity, and embedding) and different federated learning algorithms affect bug fixing to provide practical implications for real-world software development collaboration. Our evaluation reveals that federated fine-tuning can significantly enhance program repair, achieving increases of up to 16.67\% for Top@10 and 18.44\% for Pass@10, even comparable to the bug-fixing capabilities of centralized learning. Moreover, the negligible impact of code heterogeneity implies that industries can effectively collaborate despite diverse data distributions. Different federated algorithms also demonstrate unique strengths across LLMs, suggesting that tailoring the optimization process to specific LLM characteristics can further improve program repair.},
note = {Just Accepted},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = may,
keywords = {Program Repair, Federated Learning, Large Language Models}
}

@inproceedings{10.1145/3691620.3695537,
author = {Zhao, Jiuang and Yang, Donghao and Zhang, Li and Lian, Xiaoli and Yang, Zitian and Liu, Fang},
title = {Enhancing Automated Program Repair with Solution Design},
year = {2024},
isbn = {9798400712487},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3691620.3695537},
doi = {10.1145/3691620.3695537},
abstract = {Automatic Program Repair (APR) endeavors to autonomously rectify issues within specific projects, which generally encompasses three categories of tasks: bug resolution, new feature development, and feature enhancement. Despite extensive research proposing various methodologies, their efficacy in addressing real issues remains unsatisfactory. It's worth noting that, typically, engineers have design rationales (DR) on solution--- planed solutions and a set of underlying reasons---before they start patching code. In open-source projects, these DRs are frequently captured in issue logs through project management tools like Jira. This raises a compelling question: How can we leverage DR scattered across the issue logs to efficiently enhance APR?To investigate this premise, we introduce DRCodePilot, an approach designed to augment GPT-4-Turbo's APR capabilities by incorporating DR into the prompt instruction. Furthermore, given GPT-4's constraints in fully grasping the broader project context and occasional shortcomings in generating precise identifiers, we have devised a feedback-based self-reflective framework, in which we prompt GPT-4 to reconsider and refine its outputs by referencing a provided patch and suggested identifiers. We have established a benchmark comprising 938 issue-patch pairs sourced from two open-source repositories hosted on GitHub and Jira. Our experimental results are impressive: DRCodePilot achieves a full-match ratio that is a remarkable 4.7x higher than when GPT-4 is utilized directly. Additionally, the CodeBLEU scores also exhibit promising enhancements. Moreover, our findings reveal that the standalone application of DR can yield promising increase in the full-match ratio across CodeLlama, GPT-3.5, and GPT-4 within our benchmark suite. We believe that our DRCodePilot initiative heralds a novel human-in-the-loop avenue for advancing the field of APR.},
booktitle = {Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering},
pages = {1706–1718},
numpages = {13},
keywords = {design rationale, issue logs, developer discussion, automated program repair},
location = {Sacramento, CA, USA},
series = {ASE '24}
}

@inproceedings{10.1145/3696630.3728556,
author = {Lin, Zhihao and Zhou, Mingyi and Ma, Wei and Chen, Chi and Yang, Yun and Wang, Jun and Hu, Chunming and Li, Li},
title = {HapRepair: Learn to Repair OpenHarmony Apps},
year = {2025},
isbn = {9798400712760},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3696630.3728556},
doi = {10.1145/3696630.3728556},
abstract = {Software defect detection and repair are essential software engineering tasks that mitigate potential risks in the early development stages. Large Language Models (LLMs) have demonstrated significant capabilities in software defect detection and repair. However, it is hard for LLMs to handle the new programming languages such as ArkTS (which is predominantly used in the OpenHarmony platform) due to training data shortage. Additionally, LLM-based multi-defect repair suffers from the limitation of the context window of LLMs. These issues significantly affect the performance of LLM-based defect repair in new programming languages. To address the above challenges, we propose HapRepair, a defect repair framework that integrates static analysis tools with retrieval-augmented generation (RAG) to improve the effectiveness of the defect repair. Specifically, we integrate CodeLinter into our iterative defect repair framework for defect detection, which is the basis of defect repair, and utilize RAG together with ArkAnalyzer to improve the quality of our repair solutions. To overcome the context window limitation of LLMs, we propose the Surrounding Context Extractor and the Context Combination Tool. Experiment results show that HapRepair effectively repairs defects in OpenHarmony Apps, demonstrating high reliability and efficiency in addressing code issues, achieving a defect repair rate of about 99\% on the test set, compared to only about 37\% when directly using LLMs for defect repair based on the defect information. Our approach demonstrates a robust solution for defect repair on new programming languages that have limited code corpus.},
booktitle = {Proceedings of the 33rd ACM International Conference on the Foundations of Software Engineering},
pages = {319–330},
numpages = {12},
location = {Clarion Hotel Trondheim, Trondheim, Norway},
series = {FSE Companion '25}
}

@inproceedings{10.1145/3597503.3639222,
author = {Zhou, Xin and Kim, Kisub and Xu, Bowen and Han, Donggyun and Lo, David},
title = {Out of Sight, Out of Mind: Better Automatic Vulnerability Repair by Broadening Input Ranges and Sources},
year = {2024},
isbn = {9798400702174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597503.3639222},
doi = {10.1145/3597503.3639222},
abstract = {The advances of deep learning (DL) have paved the way for automatic software vulnerability repair approaches, which effectively learn the mapping from the vulnerable code to the fixed code. Nevertheless, existing DL-based vulnerability repair methods face notable limitations: 1) they struggle to handle lengthy vulnerable code, 2) they treat code as natural language texts, neglecting its inherent structure, and 3) they do not tap into the valuable expert knowledge present in the expert system. To address this, we propose VulMaster, a Transformer-based neural network model that excels at generating vulnerability repairs by comprehensively understanding the entire vulnerable code, irrespective of its length. This model also integrates diverse information, encompassing vulnerable code structures and expert knowledge from the CWE system. We evaluated VulMaster on a real-world C/C++ vulnerability repair dataset comprising 1,754 projects with 5,800 vulnerable functions. The experimental results demonstrated that VulMaster exhibits substantial improvements compared to the learning-based state-of-the-art vulnerability repair approach. Specifically, VulMaster improves the EM, BLEU, and CodeBLEU scores from 10.2\% to 20.0\%, 21.3\% to 29.3\%, and 32.5\% to 40.9\%, respectively.},
booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
articleno = {88},
numpages = {13},
location = {Lisbon, Portugal},
series = {ICSE '24}
}

@article{10.1145/3715751,
author = {Tan, Siwei and Lu, Liqiang and Xiang, Debin and Chu, Tianyao and Lang, Congliang and Chen, Jintao and Hu, Xing and Yin, Jianwei},
title = {HornBro: Homotopy-Like Method for Automated Quantum Program Repair},
year = {2025},
issue_date = {July 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {FSE},
url = {https://doi.org/10.1145/3715751},
doi = {10.1145/3715751},
abstract = {Quantum programs provide exponential speedups compared to classical programs in certain areas, but they also inevitably encounter logical faults. Automatically repairing quantum programs is much more challenging than repairing classical programs due to the non-replicability of data, the vast search space of program inputs, and the new programming paradigm. Existing works based on semantic-based or learning-based program repair techniques are fundamentally limited in repairing efficiency and effectiveness. In this work, we propose HornBro, an efficient framework for automated quantum program repair. The key insight of HornBro lies in the homotopy-like method, which iteratively switches between the classical and quantum parts. This approach allows the repair tasks to be efficiently offloaded to the most suitable platforms, enabling a progressive convergence toward the correct program. We start by designing an implication assertion pragma to enable rigorous specifications of quantum program behavior, which helps to generate a quantum test suite automatically. This suite leverages the orthonormal bases of quantum programs to accommodate different encoding schemes. Given a fixed number of test cases, it allows the maximum input coverage of potential counter-example candidates. Then, we develop a Clifford approximation method with an SMT-based search to transform the patch localization program into a symbolic reasoning problem. Finally, we offload the computationally intensive repair of gate parameters to quantum hardware by leveraging the differentiability of quantum gates. Experiments suggest that HornBro increases the repair success rate by more than 62.5\% compared to the existing repair techniques, supporting more types of quantum bugs. It also achieves 35.7\texttimes{} speedup in the repair and 99.9\% gate reduction of the patch.},
journal = {Proc. ACM Softw. Eng.},
month = jun,
articleno = {FSE034},
numpages = {23},
keywords = {Automated Program Repair, Program Verification, Quantum Computing}
}

@inproceedings{10.1007/978-3-032-10444-1_16,
author = {Wu, Valentina and Mendes, Alexandra and Abreu, Alexandre},
title = {Specification-Guided Repair of&nbsp;Arithmetic Errors in&nbsp;Dafny Programs Using LLMs},
year = {2025},
isbn = {978-3-032-10443-4},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-032-10444-1_16},
doi = {10.1007/978-3-032-10444-1_16},
abstract = {Debugging and repairing faults when programs fail to formally verify can be complex and time-consuming. Automated Program Repair (APR) can ease this burden by automatically identifying and fixing faults. However, traditional APR techniques often rely on test suites for validation, but these may not capture all possible scenarios. In contrast, formal specifications provide strong correctness criteria, enabling more effective automated repair.In this paper, we present an APR tool for Dafny, a verification-aware programming language that uses formal specifications&nbsp;—&nbsp;including pre-conditions, post-conditions, and invariants&nbsp;—&nbsp;as oracles for fault localization and repair. Assuming the correctness of the specifications and focusing on arithmetic bugs, we localize faults through a series of steps, which include using Hoare logic to determine the state of each statement within the program, and applying Large Language Models (LLMs) to synthesize candidate fixes. The models considered are GPT-4o mini, Llama 3, Mistral 7B, and Llemma 7B.We evaluate our approach using DafnyBench, a benchmark of real-world Dafny programs. Our tool achieves 89.7\% fault localization success rate and GPT-4o mini yields the highest repair success rate of 74.18\%. These results highlight the potential of combining formal reasoning with LLM-based program synthesis for automated program repair.},
booktitle = {Software Engineering and Formal Methods: 23rd International Conference, SEFM 2025, Toledo, Spain, November 10–14, 2025, Proceedings},
pages = {261–278},
numpages = {18},
keywords = {Automated Program Repair, Fault Localization, Large Language Models (LLMs), Dafny},
location = {Toledo, Spain}
}

@article{10.1145/3744900,
author = {Martinez, Matias and Mart\'{\i}nez-Fern\'{a}ndez, Silverio and Franch, Xavier},
title = {The Sustainability Face of Automated Program Repair Tools},
year = {2025},
issue_date = {November 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {34},
number = {8},
issn = {1049-331X},
url = {https://doi.org/10.1145/3744900},
doi = {10.1145/3744900},
abstract = {Automated program repair (APR) aims to automatize the process of repairing software bugs in order to reduce the cost of maintaining software programs. While APR accuracy has significantly improved in recent years, its energy impact remains unstudied. The field of green software research aims to measure the energy consumption required to develop, maintain, and use software products. Our main goal is to define the foundation for measuring the energy consumption of the APR activity. We state that an environmentally sustainable (or green) APR tool achieves a good balance between the ability to correctly repair bugs and the amount of energy consumed during such process. We measure the energy consumption of 10 traditional APR tools for Java and 11 fine-tuned large-language models (LLM) trying to repair real bugs from Defects4J. The results of this study show the existing tradeoff between energy consumption and repairability. In particular, APR tools such as TBar and RepairLlama repair more bugs than other approaches at the expense of a higher energy consumption. Other tools, such as SimFix and the LLM CodeT5-large, provide a good tradeoff between energy consumption and repairability. We also present guidelines consisting of a set of recommendations for developing greener APR.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = oct,
articleno = {237},
numpages = {37},
keywords = {Automated Program Repair, Software Sustainability, Energy Consumption of Software Tools, Green Computing}
}

@inbook{10.1109/ICSE55347.2025.00022,
author = {Deligiannis, Pantazis and Lal, Akash and Mehrotra, Nikita and Poddar, Rishi and Rastogi, Aseem},
title = {RustAssistant: Using LLMs to Fix Compilation Errors in Rust Code},
year = {2025},
isbn = {9798331505691},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE55347.2025.00022},
abstract = {The Rust programming language, with its safety guarantees, has established itself as a viable choice for low-level systems programming language over the traditional, unsafe alternatives like C/C++. These guarantees come from a strong ownership-based type system, as well as primitive support for features like closures, pattern matching, etc., that make the code more concise and amenable to reasoning. These unique Rust features also pose a steep learning curve for programmers.This paper presents a tool called RustAssistant that leverages the emergent capabilities of Large Language Models (LLMs) to automatically suggest fixes for Rust compilation errors. RustAssistant uses a careful combination of prompting techniques as well as iteration between an LLM and the Rust compiler to deliver high accuracy of fixes. RUSTASSISTANT is able to achieve an impressive peak accuracy of roughly 74\% on real-world compilation errors in popular open-source Rust repositories. We also contribute a dataset of Rust compilation errors to enable further research.},
booktitle = {Proceedings of the IEEE/ACM 47th International Conference on Software Engineering},
pages = {3097–3109},
numpages = {13}
}

@inproceedings{10.5555/3692070.3692186,
author = {Bao, Han and Hataya, Ryuichiro and Karakida, Ryo},
title = {Self-attention networks localize when QK-eigenspectrum concentrates},
year = {2024},
publisher = {JMLR.org},
abstract = {The self-attention mechanism prevails in modern machine learning. It has an interesting functionality of adaptively selecting tokens from an input sequence by modulating the degree of attention localization, which many researchers speculate is the basis of the powerful model performance but complicates the underlying mechanism of the learning dynamics. In recent years, mainly two arguments have connected attention localization to the model performances. One is the rank collapse, where the embedded tokens by a self-attention block become very similar across different tokens, leading to a less expressive network. The other is the entropy collapse, where the attention probability approaches non-uniform and entails low entropy, making the learning dynamics more likely to be trapped in plateaus. These two failure modes may apparently contradict each other because the rank and entropy collapses are relevant to uniform and non-uniform attention, respectively. To this end, we characterize the notion of attention localization by the eigenspectrum of query-key parameter matrices and reveal that a small eigenspectrum variance leads attention to be localized. Interestingly, the small eigenspectrum variance can prevent both rank and entropy collapses, leading to better model expressivity and trainability.},
booktitle = {Proceedings of the 41st International Conference on Machine Learning},
articleno = {116},
numpages = {20},
location = {Vienna, Austria},
series = {ICML'24}
}

@inproceedings{10.1145/3712255.3726692,
author = {Bhaumik, Debosmita and Togelius, Julian and Yannakakis, Georgios N. and Khalifa, Ahmed},
title = {Evolutionary Level Repair},
year = {2025},
isbn = {9798400714641},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3712255.3726692},
doi = {10.1145/3712255.3726692},
abstract = {We address the problem of game level repair, which consists of taking a designed but non-functional game level and making it functional. This might consist of ensuring the completeness of the level, reachability of objects, or other performance characteristics. The repair problem may also be constrained in that it can only make a small number of changes to the level. We investigate search-based solutions to the level repair problem, particularly using evolutionary and quality-diversity algorithms, with good results. This level repair method is applied to levels generated using a machine learning-based procedural content generation (PCGML) method that generates stylistically appropriate but frequently broken levels. This combination of PCGML for generation and search-based methods for repair shows great promise as a hybrid procedural content generation (PCG) method.},
booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference Companion},
pages = {771–774},
numpages = {4},
keywords = {procedural level generation, generative AI, evolutionary algorithms, quality diversity search, lode runner},
location = {NH Malaga Hotel, Malaga, Spain},
series = {GECCO '25 Companion}
}

@inproceedings{10.1145/3712255.3726692,
author = {Bhaumik, Debosmita and Togelius, Julian and Yannakakis, Georgios N. and Khalifa, Ahmed},
title = {Evolutionary Level Repair},
year = {2025},
isbn = {9798400714641},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3712255.3726692},
doi = {10.1145/3712255.3726692},
abstract = {We address the problem of game level repair, which consists of taking a designed but non-functional game level and making it functional. This might consist of ensuring the completeness of the level, reachability of objects, or other performance characteristics. The repair problem may also be constrained in that it can only make a small number of changes to the level. We investigate search-based solutions to the level repair problem, particularly using evolutionary and quality-diversity algorithms, with good results. This level repair method is applied to levels generated using a machine learning-based procedural content generation (PCGML) method that generates stylistically appropriate but frequently broken levels. This combination of PCGML for generation and search-based methods for repair shows great promise as a hybrid procedural content generation (PCG) method.},
booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference Companion},
pages = {771–774},
numpages = {4},
keywords = {procedural level generation, generative AI, evolutionary algorithms, quality diversity search, lode runner},
location = {NH Malaga Hotel, Malaga, Spain},
series = {GECCO '25 Companion}
}

@inproceedings{10.1109/ICSE55347.2025.00041,
author = {Zhang, Kunpeng and Wang, Shuai and Han, Jitao and Zhu, Xiaogang and Li, Xian and Wang, Shaohua and Wen, Sheng},
title = {Your Fix Is My Exploit: Enabling Comprehensive DL Library API Fuzzing with Large Language Models},
year = {2025},
isbn = {9798331505691},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE55347.2025.00041},
doi = {10.1109/ICSE55347.2025.00041},
abstract = {Deep learning (DL) libraries are widely used to form the basis of various AI applications in computer vision, natural language processing, and software engineering domains. Despite their popularity, DL libraries are known to have vulnerabilities, such as buffer overflows, use-after-free, and integer overflows, that can be exploited to compromise the security or effectiveness of the underlying libraries. While traditional fuzzing techniques have been used to find bugs in software, they are not well-suited for DL libraries. In general, the complexity of DL libraries and the diversity of their APIs make it challenging to test them thoroughly. To date, mainstream DL libraries like TensorFlow and PyTorch have featured over 1,000 APIs, and the number of APIs is still growing. Fuzzing all these APIs is a daunting task, especially when considering the complexity of the input data and the diversity of the API usage patterns.Recent advances in large language models (LLMs) have illustrated the high potential of LLMs in understanding and synthesizing human-like code. Despite their high potential, we find that emerging LLM-based fuzzers are less optimal for DL library API fuzzing, given their lack of in-depth knowledge on API input edge cases and inefficiency in generating test inputs. In this paper, we propose DFuzz, a LLM-driven DL library fuzzing approach. We have two key insights: (1) With high reasoning ability, LLMs can replace human experts to reason edge cases (likely error-triggering inputs) from checks in an API's code, and transfer the extracted knowledge to test other (new or rarely-tested) APIs. (2) With high generation ability, LLMs can synthesize initial test programs with high accuracy that automates API testing. DFUZZ provides LLMs with a novel "white-box view" of DL library APIs, and therefore, can leverage LLMs' reasoning and generation abilities to achieve comprehensive fuzzing. Our experimental results on popular DL libraries demonstrate that DFUZZ is able to cover more APIs than SOTA (LLM-based) fuzzers on TensorFlow and PyTorch, respectively. Moreover, DFUZZ successfully detected 37 bugs, with 8 already fixed and 19 replicated by the developer but still under investigation.},
booktitle = {Proceedings of the IEEE/ACM 47th International Conference on Software Engineering},
pages = {3110–3122},
numpages = {13},
location = {Ottawa, Ontario, Canada},
series = {ICSE '25}
}

@inproceedings{10.1145/3611643.3613892,
author = {Jin, Matthew and Shahriar, Syed and Tufano, Michele and Shi, Xin and Lu, Shuai and Sundaresan, Neel and Svyatkovskiy, Alexey},
title = {InferFix: End-to-End Program Repair with LLMs},
year = {2023},
isbn = {9798400703270},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3611643.3613892},
doi = {10.1145/3611643.3613892},
abstract = {Software development life cycle is profoundly influenced by bugs; their introduction, identification, and eventual resolution account for a significant portion of software development cost. This has motivated software engineering researchers and practitioners to propose different approaches for automating the identification and repair of software defects. Large Language Models (LLMs) have been adapted to the program repair task through few-shot demonstration learning and instruction prompting, treating this as an infilling task. However, these models have only focused on learning general bug-fixing patterns for uncategorized bugs mined from public repositories. In this paper, we propose : a transformer-based program repair framework paired with a state-of-the-art static analyzer to fix critical security and performance bugs.  combines a Retriever – transformer encoder model pretrained via contrastive learning objective, which aims at searching for semantically equivalent bugs and corresponding fixes; and a Generator – an LLM (12 billion parameter Codex Cushman model) finetuned on supervised bug-fix data with prompts augmented via adding bug type annotations and semantically similar fixes retrieved from an external non-parametric memory. To train and evaluate our approach, we curated , a novel, metadata-rich dataset of bugs extracted by executing the Infer static analyzer on the change histories of thousands of Java and C# repositories. Our evaluation demonstrates that  outperforms strong LLM baselines, with a top-1 accuracy of 65.6\% for generating fixes in C# and 76.8\% in Java. We discuss the deployment of alongside Infer at Microsoft which offers an end-to-end solution for detection, classification, and localization of bugs, as well as fixing and validation of candidate patches, integrated in the continuous integration (CI) pipeline to automate the software development workflow.},
booktitle = {Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {1646–1656},
numpages = {11},
keywords = {Program repair, finetuning, prompt augmentation, static analyses},
location = {San Francisco, CA, USA},
series = {ESEC/FSE 2023}
}

@inbook{10.1109/ICSE55347.2025.00238,
author = {Shabani, Taha and Nashid, Noor and Alian, Parsa and Mesbah, Ali},
title = {Dockerfile Flakiness: Characterization and Repair},
year = {2025},
isbn = {9798331505691},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE55347.2025.00238},
abstract = {Dockerfile flakiness—unpredictable temporal build failures caused by external dependencies and evolving environments—undermines deployment reliability and increases debugging overhead. Unlike traditional Dockerfile issues, flakiness occurs without modifications to the Dockerfile itself, complicating its resolution. In this work, we present the first comprehensive study of Dockerfile flakiness, featuring a nine-month analysis of 8,132 Dockerized projects, revealing that around 10\% exhibit flaky behavior. We propose a taxonomy categorizing common flakiness causes, including dependency errors and server connectivity issues. Existing tools fail to effectively address these challenges due to their reliance on pre-defined rules and limited generalizability. To overcome these limitations, we introduce FlakiDock, a novel repair framework combining static and dynamic analysis, similarity retrieval, and an iterative feedback loop powered by Large Language Models (LLMs). Our evaluation demonstrates that FlakiDock achieves a repair accuracy of 73.55\%, significantly surpassing state-of-the-art tools and baselines.},
booktitle = {Proceedings of the IEEE/ACM 47th International Conference on Software Engineering},
pages = {1793–1805},
numpages = {13}
}

@article{10.1145/3715757,
author = {Wu, Yiwen and Zhang, Yang and Wang, Tao and Ding, Bo and Wang, Huaimin},
title = {Towards Understanding Docker Build Faults in Practice: Symptoms, Root Causes, and Fix Patterns},
year = {2025},
issue_date = {July 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {FSE},
url = {https://doi.org/10.1145/3715757},
doi = {10.1145/3715757},
abstract = {Docker building is a critical component of containerization in modern software development, automating the process of packaging and converting sources into container images. It is not uncommon to find that Docker build faults (DBFs) occur frequently across Docker-based projects, inducing non-negligible costs in development activities. DBF resolution is a challenging problem and previous studies have demonstrated that developers spend non-trivial time in resolving encountered build faults. However, the characteristics of DBFs is still under-investigated, hindering practical solutions to build management in the Docker community. In this paper, to bridge this gap, we present a comprehensive study for understanding the real-world DBFs in practice. We collect and analyze a DBF dataset of 255 issues and 219 posts from GitHub, Stack Overflow, and Docker Forum. We investigate and construct characteristic taxonomies for the DBFs, including 15 symptoms, 23 root causes, and 35 fix patterns. Moreover, we study the fault distributions of symptoms and root causes, in terms of the different build types, i.e., Dockerfile builds and Docker-compose builds. Based on the results, we provide actionable implications and develop a knowledge-based application, which can potentially facilitate research and assist developers in improving the Docker build management.},
journal = {Proc. ACM Softw. Eng.},
month = jun,
articleno = {FSE040},
numpages = {23},
keywords = {Build fault, Docker build, Empirical study}
}

@inproceedings{10.1145/3533767.3534396,
author = {Zhang, Jialu and Mytkowicz, Todd and Kaufman, Mike and Piskac, Ruzica and Lahiri, Shuvendu K.},
title = {Using pre-trained language models to resolve textual and semantic merge conflicts (experience paper)},
year = {2022},
isbn = {9781450393799},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3533767.3534396},
doi = {10.1145/3533767.3534396},
abstract = {Program merging is standard practice when developers integrate their individual changes to a common code base. When the merge algorithm fails, this is called a merge conflict. The conflict either manifests as a textual merge conflict where the merge fails to produce code, or as a semantic merge conflict where the merged code results in compiler errors or broken tests. Resolving these conflicts for large code projects is expensive because it requires developers to manually identify the sources of conflicts and correct them.   In this paper, we explore the feasibility of automatically repairing merge conflicts (both textual and semantic) using k-shot learning with pre-trained large neural language models (LM) such as GPT-3. One of the challenges in leveraging such language models is fitting the examples and the queries within a small prompt (2048 tokens). We evaluate LMs and k-shot learning for both textual and semantic merge conflicts for Microsoft Edge. Our results are mixed: on one-hand, LMs provide the state-of-the-art (SOTA) performance on semantic merge conflict resolution for Edge compared to earlier symbolic approaches; on the other hand, LMs do not yet obviate the benefits of special purpose domain-specific languages (DSL) for restricted patterns for program synthesis.},
booktitle = {Proceedings of the 31st ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {77–88},
numpages = {12},
keywords = {GPT-3, Resolving merge conflicts, k-shot learning, language model},
location = {Virtual, South Korea},
series = {ISSTA 2022}
}

@article{10.1007/s10515-025-00549-x,
author = {Wang, Bo and Deng, Ming and Chen, Mingda and Lin, Youfang and Zhou, Jianyi and Zhang, Jie M.},
title = {Assessing the effectiveness of recent closed-source large language models in fault localization and automated program repair},
year = {2025},
issue_date = {Dec 2025},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {33},
number = {1},
issn = {0928-8910},
url = {https://doi.org/10.1007/s10515-025-00549-x},
doi = {10.1007/s10515-025-00549-x},
abstract = {Large Language Models (LLMs) have made significant advancements in code-related tasks. In the field of automated debugging, fault localization (FL) and automated program repair (APR) are two prevalent topics attracting significant research effort. Recently, in the field of FL and APR, many novel LLM-based approaches have emerged. However, most existing LLM-based studies primarily focus on the GPT models from OpenAI or open-source LLMs. With the rapid development of LLMs, various internet giants have introduced new closed-source models. In addition, due to policy restrictions, some regions can only access the commercial LLMs provided by specified companies. Despite the LLMs of OpenAI, the effectiveness of the other closed-source LLMs in FL and APR remains unknown. To better understand the effectiveness of contemporary closed-source models, we conduct a large-scale empirical study on their performance with respect to FL and APR. Specifically, our study involves 4 recent commercial closed-source LLMs (i.e., GPT-4o-Mini, Ernie-3.5, Qwen-turbo, and Doubao-pro) and 1 open-source LLM (i.e., DeepSeek-V3-chat). Note that only the GPT models have region restrictions among all LLMs we studied. We designed a total of 12 distinct prompt templates, 6 each for FL and APR, incorporating various formats and information sources. We conducted experiments to evaluate the effectiveness of FL and APR on 1036 real Java bugs from two datasets, Defects4J 2.0 and ConDefects. The key findings of the experiments indicate that (1) different LLMs tend to succeed on different sets of bugs in both FL and APR, with relatively little overlap among successful cases, implying the models possess distinct strengths in handling specific kinds of bugs, (2) the effectiveness of prompt templates varies across different models, and (3) the effectiveness of FL and APR capabilities of the studied models is significantly correlated with the bug type. We summarized all 14 findings obtained into 3 implications, which could help researchers further improve the performance of LLMs on FL and APR.},
journal = {Automated Software Engg.},
month = oct,
numpages = {42},
keywords = {Large language models, Software debugging, Fault localization, Automated program repair, Empirical study}
}

@inproceedings{10.1145/3672608.3707774,
author = {Lijzenga, Oebele and Hemati Moghadam, Iman and Zaytsev, Vadim},
title = {Leveraging Search-Based and Pre-Trained Code Language Models for Automated Program Repair},
year = {2025},
isbn = {9798400706295},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3672608.3707774},
doi = {10.1145/3672608.3707774},
abstract = {Background. Automated Program Repair (APR) techniques often face challenges in navigating vast search space of possible patches and often rely on redundancy-based assumptions, which can restrict the diversity of generated patches. Recently, Code Language Models (CLMs) have emerged as a method for dynamically generating patch ingredients, potentially enhancing patch quality.Aim. This study aims to enhance APR by integrating search-based methods with CLMs to improve both the quality of generated patch ingredients and the efficiency of the search process.Method. We propose ARJACLM, a novel APR technique that uses a genetic algorithm for search space navigation and dynamically generates patch ingredients with the CodeLLaMA-13B model, combining redundancy-based and CLM-derived patch ingredients.Results. Testing on 176 bugs across 9 Java projects from Defect4J shows that CLM-generated patch ingredients significantly boost ARJACLM's performance, though at the cost of increased computation time. ARJACLM outperforms ARJA and GenProg, and CLM-generated patch ingredients are of higher quality than their redundancy-based counterparts. Additionally, ARJACLM performs best when redundancy-based patch ingredients are ignored.},
booktitle = {Proceedings of the 40th ACM/SIGAPP Symposium on Applied Computing},
pages = {1627–1636},
numpages = {10},
keywords = {program repair, search-based algorithm, code language model},
location = {Catania International Airport, Catania, Italy},
series = {SAC '25}
}

@inproceedings{10.1145/3643788.3648014,
author = {Diaz-De-Arcaya, Josu and L\'{o}pez-De-Armentia, Juan and Z\'{a}rate, Gorka and Torre-Bastida, Ana I.},
title = {Towards the self-healing of Infrastructure as Code projects using constrained LLM technologies},
year = {2024},
isbn = {9798400705779},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643788.3648014},
doi = {10.1145/3643788.3648014},
abstract = {The generalization of the use of cloud computing and edge computing solutions in industry requires innovative techniques to keep up with the complexity of these scenarios. In particular, the large heterogeneity of the infrastructural devices and the myriad of services offered by the various private and cloud providers represent a challenge. Infrastructure as Code (IaC) technologies have been adopted to reduce the complexity of these scenarios, but even IaC technologies have their drawbacks, as the errors resulting from their use often combine the complexities of the underlying layers and require a high level of expertise. In this regard, the recent upsurge of Large Language Models represents an opportunity as they are able to tackle different problems. In this article, we aspire to shed light on the automated patching of IaC projects with the help of LLMs. We evaluate the suitability of this hypothesis by using a well-known LLM that is able to solve all the scenarios we envisioned and assess the possibility of doing the same with smaller, offline LLMs, which could lead to the use of these technologies in resource-constrained environments, such as edge computing.},
booktitle = {Proceedings of the 5th ACM/IEEE International Workshop on Automated Program Repair},
pages = {22–25},
numpages = {4},
keywords = {infrastructure as code, IaC, large language models, LLMs, self-healing, automated patching},
location = {Lisbon, Portugal},
series = {APR '24}
}

@article{10.1007/s10115-025-02383-9,
author = {Dikici, Sena and Bilgin, Turgay Tugay},
title = {Advancements in automated program repair: a comprehensive review},
year = {2025},
issue_date = {Jun 2025},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {67},
number = {6},
issn = {0219-1377},
url = {https://doi.org/10.1007/s10115-025-02383-9},
doi = {10.1007/s10115-025-02383-9},
abstract = {This review paper presents a comprehensive examination of automated program repair (APR) and its significant contribution to the field of modern software engineering. It elucidates how APR methodologies markedly mitigate manual debugging needs by automating the detection and resolution of software glitches. The study encompasses an in-depth exploration of three primary categories of APR techniques: template-based, machine learning, and deep learning approaches, drawing from an exhaustive evaluation of 41 APR tools. Each category showcases distinct strategies for managing diverse software errors, underscoring the breadth and effectiveness of current APR methodologies. Template-based APR solutions utilize pre-established patterns to efficiently tackle common coding issues, while machine learning-driven approaches dynamically devise repair strategies from historical bug-fix datasets. Deep learning methods extend error rectification boundaries by delving into the semantic context of code, yielding more precise adjustments. The ongoing advancement of APR technologies necessitates researchers to address critical challenges, including the integration of semantic-syntactic analyses, mitigation of data scarcity, optimization of cross-platform tools, development of context-aware approaches, enhancement of fault localization and patch validation processes, and establishment of standardized performance evaluation metrics. This comprehensive analysis underscores the pivotal role of APR in enhancing software efficiency and reliability, representing significant progress in software development and maintenance practices.},
journal = {Knowl. Inf. Syst.},
month = mar,
pages = {4737–4783},
numpages = {47},
keywords = {Automated program repair, Software bugs, Machine learning, Deep learning, Code patterns}
}

@article{10.1145/3705310,
author = {Zhang, Yuntong and Costea, Andreea and Shariffdeen, Ridwan and McCall, Davin and Roychoudhury, Abhik},
title = {EffFix: Efficient and Effective Repair of Pointer Manipulating Programs},
year = {2025},
issue_date = {March 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {34},
number = {3},
issn = {1049-331X},
url = {https://doi.org/10.1145/3705310},
doi = {10.1145/3705310},
abstract = {This work introduces EffFix, a tool that applies a novel static analysis-driven automated program repair (APR) technique for fixing memory errors. APR tools typically rely on a given test-suite to guide the repair process. Apart from the need to provide test oracles, this reliance is also one of the main contributors to the over-fitting problem. Static analysis based APR techniques bypass these issues only to introduce new ones, such as soundness, scalability, and generalizability. This work demonstrates how we can overcome these challenges and achieve sound memory bug repair at scale by leveraging static analysis (specifically incorrectness separation logic (ISL)) to guide repair. This is the first repair approach to use ISL. Our key insight is that the abstract domain used by static analysis to detect the bugs also contains key information to derive correct patches. Our proposed approach learns what a desirable patch is by inspecting how close a patch is to fixing the bug based on the feedback from ISL based static analysis (specifically the Pulse analyzer), and turning this information into a distribution of probabilities over context free grammars. This approach to repair is generic in that its learning strategy allows for finding patches without relying on the commonly used patch templates. Furthermore, to achieve efficient program repair, instead of focusing on heuristics for reducing the search space of patches, we make repair scalable by creating classes of equivalent patches according to the effect they have on the symbolic heap. We then conduct candidate patch validation only once per patch equivalence class. This allows EffFix to efficiently discover quality repairs even in the presence of a large pool of patch candidates. Experimental evaluation of fixing real world memory errors in medium to large scale subjects like OpenSSL, Linux Kernel, swoole, shows the efficiency and effectiveness of EffFix— in terms of automatically producing repairs from large search spaces. In particular, EffFix has a fix ratio of 66\% for memory leak bugs and 83\% for Null Pointer Dereferences for the considered dataset.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = feb,
articleno = {69},
numpages = {27},
keywords = {Automated Program Repair, Incorrectness Separation Logic, Probabilistic Context Free Grammars}
}

@inproceedings{10.1145/3719027.3765170,
author = {Liu, Zeyu and Wang, Yunhao and Fisch, Ben},
title = {IND-CPA-D of Relaxed Functional Bootstrapping: A New Attack, A General Fix, and A Stronger Model},
year = {2025},
isbn = {9798400715259},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3719027.3765170},
doi = {10.1145/3719027.3765170},
abstract = {Fully homomorphic encryption (FHE) is a powerful and widely used primitive in lots of real-world applications. Recently, Li and Micciancio [Eurocrypt'21] introduced IND-CPA-D security, which strengthens the standard IND-CPA security by allowing the attacker to access a decryption oracle for honestly generated ciphertexts. Recently, Jung et al. [CCS'24] and Checri et al. [Crypto'24] have shown that even exact FHE schemes like FHEW/TFHE/BGV/BFV may still not be IND-CPA-D secure, by exploiting the bootstrapping failure. However, such attacks can be mitigated by setting negligible bootstrapping failure probability. On the other hand, Liu and Wang [Asiacrypt'24] proposed relaxed functional bootstrapping, which has orders of magnitude performance improvement and furthermore allows a free function evaluation during bootstrapping. These efficiency advantages make it a competitive choice in many applications. In this work, we show that the underlying secret key could be recovered within 10 minutes against all existing relaxed functional bootstrapping constructions, and even within 1 minute for some of them. Moreover, our attack works even with a negligible bootstrapping failure probability. Additionally, we propose a general fix that mitigates all the existing modulus-switching-error-based attacks in the IND-CPA-D model. This is achieved by constructing a new modulus switching procedure with essentially no overhead. Lastly, we show that IND-CPA-D may not be sufficient even for passive adversary model. Thus, we extend this model to IND-CPA-D with randomness (IND-CPA-DR).},
booktitle = {Proceedings of the 2025 ACM SIGSAC Conference on Computer and Communications Security},
pages = {2907–2921},
numpages = {15},
keywords = {fully homomorphic encryption, ind-cpa-d security},
location = {Taipei, Taiwan},
series = {CCS '25}
}

@inproceedings{10.1609/aaai.v39i1.31988,
author = {Dai, Zhenlong and Chen, Bingrui and Zhao, Zhuoluo and Tang, Xiu and Wu, Sai and Yao, Chang and Gao, Zhipeng and Chen, Jingyuan},
title = {Less is more: adaptive program repair with bug localization and preference learning},
year = {2025},
isbn = {978-1-57735-897-8},
publisher = {AAAI Press},
url = {https://doi.org/10.1609/aaai.v39i1.31988},
doi = {10.1609/aaai.v39i1.31988},
abstract = {Automated Program Repair (APR) is a task to automatically generate patches for the buggy code. However, most research focuses on generating correct patches while ignoring the consistency between the fixed code and the original buggy code. How to conduct adaptive bug fixing and generate patches with minimal modifications have seldom been investigated. To bridge this gap, we first introduce a novel task, namely AdaPR (Adaptive Program Repair). We then propose a two-stage approach AdaPatcher (Adaptive Patch Generator) to enhance program repair while maintaining the consistency. In the first stage, we utilize a Bug Locator with self-debug learning to accurately pinpoint bug locations. In the second stage, we train a Program Modifier to ensure consistency between the post-modified fixed code and the pre-modified buggy code. The Program Modifier is enhanced with a location-aware repair learning strategy to generate patches based on identified buggy lines, a hybrid training strategy for selective reference and an adaptive preference learning to prioritize fewer changes. The experimental results show that our approach outperforms a set of baselines by a large margin, validating the effectiveness of our two-stage framework for the newly proposed AdaPR task. Code — https://github.com/zhenlongDai/AdaPatcher},
booktitle = {Proceedings of the Thirty-Ninth AAAI Conference on Artificial Intelligence and Thirty-Seventh Conference on Innovative Applications of Artificial Intelligence and Fifteenth Symposium on Educational Advances in Artificial Intelligence},
articleno = {15},
numpages = {9},
series = {AAAI'25/IAAI'25/EAAI'25}
}

@proceedings{10.1145/3643788,
title = {APR '24: Proceedings of the 5th ACM/IEEE International Workshop on Automated Program Repair},
year = {2024},
isbn = {9798400705779},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Welcome to the fifth International Workshop on Automated Program Repair (APR 2024), hosted by International Conference on Software Engineering (ICSE) 2024. Since its inception in 2020, APR has become a central event of the program repair community, reflecting a growing interest in the field among the software engineering, programming language, machine learning and formal methods communities.APR 2024 continues the tradition of fostering interaction among researchers in program repair. As always, we are particularly focused on narrowing the divide between academic research and real-world industry applications.},
location = {Lisbon, Portugal}
}

@article{10.1016/j.eswa.2024.124877,
author = {Zheng, Zelong and Wang, Ruihan and Tao, Zijian and Li, Hui and Chen, Chen and Li, Tingting and Guo, Shikai},
title = {Automated patch correctness predicting to fix software defect},
year = {2024},
issue_date = {Dec 2024},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {256},
number = {C},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2024.124877},
doi = {10.1016/j.eswa.2024.124877},
journal = {Expert Syst. Appl.},
month = dec,
numpages = {16},
keywords = {Automated program repair, Pre-trained model, Patch correctness prediction}
}

@article{10.1145/3705306,
author = {Reiss, Steven P. and Wei, Xuan and Yuan, Jiahao and Xin, Qi},
title = {ROSE: An IDE-Based Interactive Repair Framework for Debugging},
year = {2025},
issue_date = {May 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {34},
number = {4},
issn = {1049-331X},
url = {https://doi.org/10.1145/3705306},
doi = {10.1145/3705306},
abstract = {Debugging is costly. Automated program repair (APR) holds the promise of reducing its cost by automatically fixing errors. However, current techniques are not easily applicable in a realistic debugging scenario because they assume a high-quality test suite and frequent program re-execution, have low repair efficiency, and only handle a limited set of errors. To improve the practicality of APR for debugging, we propose ROSE, an interactive repair framework that is able to suggest quick and effective repairs of semantic errors while debugging in an Integrated Development Environment (IDE). ROSE allows an easy integration of existing APR patch generators and can do program repair without assuming the existence of a test suite and without requiring program re-execution. It works in conjunction with an IDE debugger and assumes a debugger stopping point where a problem symptom is observed. ROSE asks the developer to quickly describe the symptom. Then it uses the stopping point, the identified symptom, and the current environment to identify potentially faulty lines, uses a variety of APR techniques to suggest repairs at those lines, and validates those repairs without re-executing the program. Finally, it presents the results so the developer can examine, select, and make the appropriate repair. ROSE uses novel approaches to achieve effective fault localization and patch validation without a test suite or program re-execution. For fault localization, ROSE builds on a fast abstract interpretation-based flow analysis to compute a static backward slice approximating the real dynamic slice while taking into account the symptom and the current execution. For patch validation without re-running the program, ROSE generates simulated traces based on a live-programming system for both the original and repaired executions and compares the traces with respect to the problem symptoms to infer patch correctness. We implemented a prototype of ROSE that works in an Eclipse-based IDE and evaluated its potency and utility with an effectiveness study and a user study. We found that ROSE’s fault localization and validation are highly effective and a ROSE-based tool using existing APR patch generators generated correct repair suggestions for many errors in only seconds. Moreover, the user study demonstrated that ROSE was helpful for debugging and developers liked to use it.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = apr,
articleno = {112},
numpages = {39},
keywords = {Debugging, Interactive Repair Framework, Automated Program Repair, Integrated Development Environment}
}

@article{10.1145/3716167,
author = {Le-Cong, Thanh and Nguyen, Dat and Le, Bach and Murray, Toby},
title = {Towards Reliable Evaluation of Neural Program Repair with Natural Robustness Testing},
year = {2025},
issue_date = {September 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {34},
number = {7},
issn = {1049-331X},
url = {https://doi.org/10.1145/3716167},
doi = {10.1145/3716167},
abstract = {Automated program repair (APR) has recently gained ground, with numerous research efforts being conducted in the area that have been adopted in the industry. One notable class of APR is neural program repair (NPR), which typically employs deep learning techniques that are trained on vast amounts of historical data to fix bugs that have not been seen in the past. To study the true effectiveness of NPR on existing limited datasets, recent work augments the evaluation data by employing semantics-preserving transformations to convert original buggy programs to semantically equivalent ones. Experiments show that NPR techniques are not robust; e.g., NPR cannot repair semantically equivalent counterparts of 20\%–35\% of bugs that they can repair in the original dataset. However, we found that many of these transformations are unnatural, that are unlikely to occur in real-world scenarios, leading to misleading conclusions about NPR effectiveness and misguide the improvement on unrobust behaviors, which have minimal real-world impact.In this article, we propose shifting the focus of robustness evaluation for NPR techniques towards naturally occurring data transformations. To accomplish this, we first examine the naturalness of semantic-preserving transformations through a two-stage human study. This study includes: (i) interviews with senior software developers to establish concrete criteria for evaluating the naturalness of these transformations and (ii) a survey involving 10 developers to assess the naturalness of 1,178 transformations, i.e., pairs of original and transformed programs, applied to 225 real-world bugs. Our findings show that only 60\% of these transformations are considered natural, while 20\% are considered unnatural, with strong agreement among the annotators. Moreover, the unnaturalness of these transformations significantly impacts both their applicability to benchmarks and the conclusions drawn from robustness testing.Next, we conduct natural robustness tests on NPR techniques to assess their true effectiveness against real-world data variations. Our experimental results reveal a substantial number of prediction changes in NPR techniques, leading to significant reductions in both plausible and correct patch rates when comparing performance on the original and transformed datasets. Furthermore, we observe notable differences in performance improvements between NPR techniques, suggesting potential biases in the evaluation of NPR introduced by limited datasets. Finally, we explore automating the assessment of transformation naturalness by developing a new naturalness metric, namely RNC, using large language models. This metric effectively evaluates naturalness with an AUC of 0.7, offering a promising direction for automating the naturalness assessment of code transformations.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = aug,
articleno = {213},
numpages = {44},
keywords = {Automated Program Repair, Natural Robustness, Code Naturalness, Code Transformations}
}

@inproceedings{10.5555/3737916.3741662,
author = {Tang, Hao and Hu, Keya and Zhou, Jin Peng and Zhong, Sicheng and Zheng, Wei-Long and Si, Xujie and Ellis, Kevin},
title = {Code repair with LLMs gives an exploration-exploitation tradeoff},
year = {2024},
isbn = {9798331314385},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Iteratively improving and repairing source code with large language models (LLMs), known as refinement, has emerged as a popular way of generating programs that would be too complex to construct in one shot. Given a bank of test cases, together with a candidate program, an LLM can improve that program by being prompted with failed test cases. But it remains an open question how to best iteratively refine code, with prior work employing simple greedy or breadth-first strategies. We show here that refinement exposes an explore-exploit tradeoff: exploit by refining the program that passes the most test cases, or explore by refining a lesser considered program. We frame this as an arm-acquiring bandit problem, which we solve with Thompson Sampling. The resulting LLM-based program synthesis algorithm is broadly applicable: Across loop invariant synthesis, visual reasoning puzzles, and competition programming problems, we find that our new method can solve more problems using fewer language model calls.},
booktitle = {Proceedings of the 38th International Conference on Neural Information Processing Systems},
articleno = {3746},
numpages = {43},
location = {Vancouver, BC, Canada},
series = {NIPS '24}
}

@inproceedings{10.1109/ICSE-Companion66252.2025.00060,
author = {Valle, Pablo},
title = {Automated Repair of Cyber-Physical Systems},
year = {2025},
isbn = {9798331536831},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-Companion66252.2025.00060},
doi = {10.1109/ICSE-Companion66252.2025.00060},
abstract = {Cyber-Physical Systems (CPS) integrate digital technologies with physical processes and are common in different domains and industries, such as robotic systems, autonomous vehicles or satellites. Debugging and verification of CPS software consumes much of the development budget as it is often purely manual. To speed up this process, Automated Program Repair (APR) has been targeted for a long time. Although there have been advances in software APR and CPS verification techniques, research specifically on APR for CPSs is limited. This Ph.D. research project aims to develop scalable APR techniques for CPSs, addressing problems of fault localization, long test execution times, and fitness function limitations. A new method combining spectrum-based fault localization (SBFL) with patch generation and advanced artificial intelligence techniques will be investigated. The approach will be validated by empirical studies on open and industrial code bases of CPSs.},
booktitle = {Proceedings of the IEEE/ACM 47th International Conference on Software Engineering: Companion Proceedings},
pages = {199–201},
numpages = {3},
keywords = {cyber-physical systems, automated program repair, fault localization, test input minimization},
location = {Ottawa, Ontario, Canada},
series = {ICSE '25}
}

@inproceedings{10.1109/ICSE48619.2023.00128,
author = {Fan, Zhiyu and Gao, Xiang and Mirchev, Martin and Roychoudhury, Abhik and Tan, Shin Hwei},
title = {Automated Repair of Programs from Large Language Models},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00128},
doi = {10.1109/ICSE48619.2023.00128},
abstract = {Large language models such as Codex, have shown the capability to produce code for many programming tasks. However, the success rate of existing models is low, especially for complex programming tasks. One of the reasons is that language models lack awareness of program semantics, resulting in incorrect programs, or even programs which do not compile. In this paper, we systematically study whether automated program repair (APR) techniques can fix the incorrect solutions produced by language models in LeetCode contests. The goal is to study whether APR techniques can enhance reliability in the code produced by large language models. Our study revealed that: (1) automatically generated code shares common programming mistakes with human-crafted solutions, indicating APR techniques may have potential to fix auto-generated code; (2) given bug location information provided by a statistical fault localization approach, the newly released Codex edit mode, which supports editing code, is similar to or better than existing Java repair tools TBar and Recoder in fixing incorrect solutions. By analyzing the experimental results generated by these tools, we provide several suggestions: (1) enhancing APR tools to surpass limitations in patch space (e.g., introducing more flexible fault localization) is desirable; (2) as large language models can derive more fix patterns by training on more data, future APR tools could shift focus from adding more fix patterns to synthesis/semantics based approaches, (3) combination of language models with APR to curate patch ingredients, is worth studying.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {1469–1481},
numpages = {13},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@article{10.1145/3696450,
author = {Huang, Kai and Xu, Zhengzi and Yang, Su and Sun, Hongyu and Li, Xuejun and Yan, Zheng and Zhang, Yuqing},
title = {Evolving Paradigms in Automated Program Repair: Taxonomy, Challenges, and Opportunities},
year = {2024},
issue_date = {February 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {57},
number = {2},
issn = {0360-0300},
url = {https://doi.org/10.1145/3696450},
doi = {10.1145/3696450},
abstract = {With the rapid development and large-scale popularity of program software, modern society increasingly relies on software systems. However, the problems exposed by software have also come to the fore. The software bug has become an important factor troubling developers. In this context, Automated Program Repair (APR) techniques have emerged, aiming to automatically fix software bug problems and reduce manual debugging work. In particular, benefiting from the advances in deep learning, numerous learning-based APR techniques have emerged in recent years, which also bring new opportunities for APR research. To give researchers a quick overview of APR techniques’ complete development and future opportunities, we review the evolution of APR techniques and discuss in depth the latest advances in APR research. In this article, the development of APR techniques is introduced in terms of four different patch generation schemes: search-based, constraint-based, template-based, and learning-based. Moreover, we propose a uniform set of criteria to review and compare each APR tool and then discuss the current state of APR development. Finally, we analyze current challenges and future directions, especially highlighting the critical opportunities that large language models bring to APR research.},
journal = {ACM Comput. Surv.},
month = oct,
articleno = {36},
numpages = {43},
keywords = {Automated program repair}
}

@article{10.1145/3641026,
author = {Moore, Robert J. and An, Sungeun and Marrese, Olivia H.},
title = {Understanding is a Two-Way Street: User-Initiated Repair on Agent Responses and Hearing in Conversational Interfaces},
year = {2024},
issue_date = {April 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {CSCW1},
url = {https://doi.org/10.1145/3641026},
doi = {10.1145/3641026},
abstract = {Although methods for repairing prior turns in natural conversation are critical for enabling mutual understanding, or successful communication, these methods are seldom built into conversational user interfaces systematically. Chatbots and voice assistants tend to ask users to paraphrase what they said if it was not understood, but users cannot do the same if they encounter trouble in understanding what the agent said. Understanding is a one-way street in most (intent-based) conversation-like interfaces. An exception to this is Moore and Arar (2019), who demonstrate nine types of user-initiated repair on agent responses that are common in natural conversation and who have shown that users will employ these repair features correctly in text-based interfaces if taught. In this small-scale study, we test these user-initiated repairs (in second position) in a voice-based interface. With understanding-oriented repairs, we found that participants employed them much the same way in text and voice. In addition, we examine some hearing- and speaking-oriented repairs that emerged from the use of our novel multi-modal interface. We found that participants used them to manage troubles specific to the voice modality. Analysis of user logs and transcripts suggests that user-initiated repair features are valuable components of conversational interfaces.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = apr,
articleno = {187},
numpages = {26},
keywords = {chatbots, conversational agents, conversational ai, conversational user interfaces, conversational ux, user-initiated repair}
}

@inproceedings{10.1609/icaps.v35i1.36131,
author = {Zaidins, Paul and Goldman, Robert P. and Kuter, Ugur and Nau, Dana and Roberts, Mark},
title = {HTN plan repair algorithms compared: strengths and weaknesses of different methods},
year = {2025},
isbn = {1-57735-903-8},
publisher = {AAAI Press},
url = {https://doi.org/10.1609/icaps.v35i1.36131},
doi = {10.1609/icaps.v35i1.36131},
abstract = {This paper provides theoretical and empirical comparisons of three recent hierarchical plan repair algorithms: SHOPFIXER, IPYHOPPER, and REWRITE. Our theoretical results show that the three algorithms correspond to three different definitions of the plan repair problem, leading to differences in the algorithms' search spaces, the repair problems they can solve, and the kinds of repairs they can make. Understanding these distinctions is important when choosing a repair method for any given application.Building on the theoretical results, we evaluate the algorithms empirically in a series of benchmark planning problems. Our empirical results provide more detailed insight into the runtime repair performance of these systems and the coverage of the repair problems solved, based on algorithmic properties such as replanning, chronological backtracking, and backjumping over plan trees.},
booktitle = {Proceedings of the Thirty-Fifth International Conference on Automated Planning and Scheduling},
articleno = {37},
numpages = {9},
location = {Melbourne, Victoria, Australia},
series = {ICAPS '25}
}

@article{10.1145/3711938,
author = {Romeo, Marta and Torre, Ilaria and Le Maguer, S\'{e}bastien and Sleat, Alexander and Cangelosi, Angelo and Leite, Iolanda},
title = {The Effect of Voice and Repair Strategy on Trust Formation and Repair in Human-Robot Interaction},
year = {2025},
issue_date = {June 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {2},
url = {https://doi.org/10.1145/3711938},
doi = {10.1145/3711938},
abstract = {Trust is essential for social interactions, including those between humans and social artificial agents, such as robots. Several factors and combinations thereof can contribute to the formation of trust and, importantly in the case of machines that work with a certain margin of error, to its maintenance and repair after it has been breached. In this article, we present the results of a study aimed at investigating the role of robot voice and chosen repair strategy on trust formation and repair in a collaborative task. People helped a robot navigate through a maze, and the robot made mistakes at pre-defined points during the navigation. Via in-game behaviour and follow-up questionnaires, we could measure people’s trust towards the robot. We found that people trusted the robot speaking with a state-of-the-art synthetic voice more than with the default robot voice in the game, even though they indicated the opposite in the questionnaires. Additionally, we found that three repair strategies that people use in human-human interaction (justification of the mistake, promise to be better and denial of the mistake) work also in human-robot interaction.},
journal = {J. Hum.-Robot Interact.},
month = feb,
articleno = {33},
numpages = {22},
keywords = {Trust, Trust repair, Robot voice, Human-Robot Interaction}
}

@inproceedings{10.1109/ICSE55347.2025.00109,
author = {Yuan, Mingyue and Chen, Jieshan and Xing, Zhenchang and Quigley, Aaron and Luo, Yuyu and Luo, Tianqi and Mohammadi, Gelareh and Lu, Qinghua and Zhu, Liming},
title = {DesignRepair: Dual-Stream Design Guideline-Aware Frontend Repair with Large Language Models},
year = {2025},
isbn = {9798331505691},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE55347.2025.00109},
doi = {10.1109/ICSE55347.2025.00109},
abstract = {The rise of Large Language Models (LLMs) has streamlined frontend interface creation through tools like Vercel's V0, yet surfaced challenges in design quality (e.g., accessibility, and usability). Current solutions, often limited by their focus, generalisability, or data dependency, fall short in addressing these complexities. Moreover, none of them examine the quality of LLM-generated UI design. In this work, we introduce DesignRepair, a novel dual-stream design guideline-aware system to examine and repair the UI design quality issues from both code aspect and rendered page aspect. We utilised the mature and popular Material Design as our knowledge base to guide this process. Specifically, we first constructed a comprehensive knowledge base encoding Google's Material Design principles into low-level component knowledge base and high-level system design knowledge base. After that, DesignRepair employs a LLM for the extraction of key components and utilizes the Playwright tool for precise page analysis, aligning these with the established knowledge bases. Finally, we integrate Retrieval-Augmented Generation with state-of-the-art LLMs like GPT-4 to holistically refine and repair frontend code through a strategic divide and conquer approach. Our extensive evaluations validated the efficacy and utility of our approach, demonstrating significant enhancements in adherence to design guidelines, accessibility, and user experience metrics.},
booktitle = {Proceedings of the IEEE/ACM 47th International Conference on Software Engineering},
pages = {2483–2494},
numpages = {12},
keywords = {frontend code repair, design guideline, UI design, large language models},
location = {Ottawa, Ontario, Canada},
series = {ICSE '25}
}

@inbook{10.1109/ICSE55347.2025.00030,
author = {Huang, Kai and Zhang, Jian and Meng, Xiangxin and Liu, Yang},
title = {Template-Guided Program Repair in the Era of Large Language Models},
year = {2025},
isbn = {9798331505691},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE55347.2025.00030},
abstract = {Recent advancements in automated program repair (APR) have been significantly driven by the application of Large Language Models (LLMs). In particular, the integration of LLMs with traditional template-based repair methods has demonstrated effective outcomes. Despite this, the synergy between the strengths of traditional methods and LLMs remains underexploited. This oversight originates from the indiscriminate use of templates and their insufficient coverage. Also, using small-scale LLMs within the zero-shot learning context proves to be suboptimal.To alleviate the limitations, we propose NTR (Neural Template Repair), a two-stage repair framework including template selection and patch generation, both of which are under the fine-tuning paradigm. In the template selection phase, we formulate it as a multiclass classification problem and fine-tune million-level LLMs for better selecting possible templates. During the patch generation phase, we leverage the chosen templates as probable directions (e.g., 'Mutate Conditional Expression') to guide the fine-tuning process of LLMs at the billion-level scale for precise patch creation. Moreover, we incorporate a unique template to signify the absence of a suitable template and employ a probability-based prioritization of templates, thereby optimizing patch generation. This framework not only effectively addresses template mismatch issues, but also enables the billion-level LLMs to explore the patch space more efficiently, despite the GPU memory constraints.We evaluate NTR with different foundational models on Defects4J V1.2 and HumanEval-Java, the framework consistently demonstrates significant effectiveness. When utilizing StarCoder as the foundational model for patch generation, NTR fixes 128 and 129 bugs in Defects4J and HumanEval, outperforming the best baseline APR tool by 14 and 59 bugs. With the larger CodeLlama model, the fixed bugs rise to 139 and 136, respectively, exceeding the baseline by 25 and 66 bugs. Notably, the performance stems not only from the foundational models but also benefits greatly from our NTR framework. Specifically, NTR's implementation with StarCoder and CodeLlama leads to 22 and 23 additional fixes, which is beyond what the models achieve on their own. This emphasizes the success of our new perspective on utilizing templates to unlock the bug-fixing potential of LLMs.},
booktitle = {Proceedings of the IEEE/ACM 47th International Conference on Software Engineering},
pages = {1895–1907},
numpages = {13}
}

@inproceedings{10.1145/3721238.3730671,
author = {Wen, Huibiao and He, Guilong and Xu, Rui and Chen, Shuangmin and Xin, Shiqing and Shu, Zhenyu and Komura, Taku and Feng, Jieqing and Wang, Wenping and Tu, Changhe},
title = {Feature-Preserving Mesh Repair via Restricted Power Diagram},
year = {2025},
isbn = {9798400715402},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3721238.3730671},
doi = {10.1145/3721238.3730671},
abstract = {Mesh repair is a critical process in 3D geometry processing aimed at correcting errors and imperfections in polygonal meshes to produce watertight, manifold, and feature-preserving meshes suitable for downstream tasks. While errors such as degeneracies, duplication, holes, and overlaps can be addressed through standard repair processes, cracks along trimmed curves require special attention and should ideally be repaired to align with sharp feature lines.In this paper, we present a unified framework for repairing diverse mesh imperfections by leveraging a manifold wrap surface as a mediating agent. The primary role of the wrap surface is to define spatial connections between points on the original surface, thereby decoupling the challenges of edge connectivity and point relocation during repair. Throughout the process, our algorithm operates on the dual objects: the original defective mesh and the manifold wrap surface. The implementation begins by extracting a set of samples from the wrap surface and projecting them onto the original surface. These projected samples are optimized by minimizing the quadratic error relative to the tangent planes of neighboring points on the original surface. Notably, samples far from feature lines remain unchanged, while samples near feature lines converge to those lines even when the input surface lacks correct mesh topology. We then assign an adaptive weight to each sample based on the squared moving distance. By introducing this weight setting, we observe that the restricted power diagram prioritizes connectivity along feature lines, thereby effectively preserving sharp features. Through extensive experiments, we demonstrate the superiority of our proposed algorithm over existing methodologies in terms of manifoldness, watertightness, topological correctness, triangle quality, and feature preservation.},
booktitle = {Proceedings of the Special Interest Group on Computer Graphics and Interactive Techniques Conference Conference Papers},
articleno = {150},
numpages = {11},
keywords = {mesh repair, restricted power diagram, wrap surface, manifold and watertight},
location = {
},
series = {SIGGRAPH Conference Papers '25}
}

@inproceedings{10.1145/3643656.3643900,
author = {Chen, Yang and Jabbarvand, Reyhaneh},
title = {Can ChatGPT Repair Non-Order-Dependent Flaky Tests?},
year = {2024},
isbn = {9798400705588},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643656.3643900},
doi = {10.1145/3643656.3643900},
abstract = {Regression testing helps developers check whether the latest code changes break software functionality. Flaky tests, which can non-deterministically pass or fail on the same code version, may mislead developers' concerns, resulting in missing some bugs or spending time pinpointing bugs that do not exist. Existing flakiness detection and mitigation techniques have primarily focused on general order-dependent (OD) and implementation-dependent (ID) flaky tests. There is also a dearth of research on repairing test flakiness, out of which, mostly have focused on repairing OD flaky tests, and a few have explored repairing a subcategory of non-order-dependent (NOD) flaky tests that are caused by asynchronous waits. As a result, there is a demand for devising techniques to reproduce, detect, and repair NOD flaky tests. Large language models (LLMs) have shown great effectiveness in several programming tasks. To explore the potential of LLMs in addressing NOD flakiness, this paper investigates the possibility of using ChatGPT to repair different categories of NOD flaky tests. Our comprehensive study on 118 from the IDoFT dataset shows that ChatGPT, despite as a leading LLM with notable success in multiple code generation tasks, is ineffective in repairing NOD test flakiness, even by following the best practices for prompt crafting. We investigated the reasons behind the failure of using ChatGPT in repairing NOD tests, which provided us valuable insights about the next step to advance the field of NOD test flakiness repair.},
booktitle = {Proceedings of the 1st International Workshop on Flaky Tests},
pages = {22–29},
numpages = {8},
keywords = {software testing, test flakiness, large language models},
location = {Lisbon, Portugal},
series = {FTW '24}
}

@inbook{10.1109/ICSE55347.2025.00257,
author = {Rinard, Martin C.},
title = {Research in Program Repair and Approximate Computing: A Retrospective},
year = {2025},
isbn = {9798331505691},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE55347.2025.00257},
abstract = {This paper and accompanying talk trace the trajectory of my research in program repair and approximate computing. The prevailing value system in the field at the time focused on program correctness as a fundamental goal. This research, in contrast, was driven by a new perspective that emphasized acceptable (but not necessarily fully correct) survival through errors and the automatic identification and exploitation of performance versus accuracy tradeoff spaces implicitly present in computations coded to operate at only a single point in this space.Because the research challenged the prevailing value system at the time, it met with some skepticism despite empirical results highlighting its effectiveness. The following quote from an anonymous reviewer may give some idea of the reaction:"The basic idea—to assist incorrect programs in their efforts to emit incorrect output—is an abomination and if adopted would likely usher in a new dark age."As the research progressed, we gained a deeper understanding of the reasons behind the surprising — at least to us — phenomena we observed. We were able to formalize this understanding to generate source code patches and obtain performance, accuracy, and acceptability guarantees for computations that leveraged our techniques, bringing the research full circle to once again focus on reasoning statically about program behavior but with different reasoning techniques and guarantees.Finally, I discuss lessons learned and future relevance of the principles, perspectives, and concepts that this research pioneered.},
booktitle = {Proceedings of the IEEE/ACM 47th International Conference on Software Engineering},
pages = {1–15},
numpages = {15}
}

@article{10.1007/s10515-025-00570-0,
author = {Zhang, Ruxin and Zhang, Shanxin and Xie, Linbo},
title = {A systematic exploration of C-to-rust code translation based on large language models: prompt strategies and automated repair},
year = {2025},
issue_date = {Dec 2025},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {33},
number = {1},
issn = {0928-8910},
url = {https://doi.org/10.1007/s10515-025-00570-0},
doi = {10.1007/s10515-025-00570-0},
abstract = {C is widely used in system programming due to its low-level flexibility. However, as demands for memory safety and code reliability grow, Rust has become a more favorable alternative owing to its modern design principles. Migrating existing C code to Rust has therefore emerged as a key approach for enhancing the security and maintainability of software systems. Nevertheless, automating such migrations remains challenging due to fundamental differences between the two languages in terms of language design philosophy, type systems, and levels of abstraction. Most current code transformation tools focus on mappings of basic data types and syntactic replacements, such as handling pointers or conversion of lock mechanisms. These approaches often fail to deeply model the semantic features and programming paradigms of the target language. To address this limitation, this paper proposes RustFlow, a C-to-Rust code translation framework based on large language models (LLMs), designed to generate idiomatic and semantically accurate Rust code. This framework employs a multi-stage progressive architecture, which decomposes the overall translation task into several sequential stages, namely translation, validation, and repair. During the translation phase, a collaborative prompting strategy is employed to guide the LLM in achieving cross-language semantic alignment, thereby improving the accuracy of the generated code. Subsequently, a validation mechanism is introduced to perform syntactic and semantic checks on the generated output, and a conversational iterative repair strategy is employed to further enhance the quality of the final result. Experimental results show that RustFlow outperforms most of the latest baseline approaches, achieving an average improvement of 50.67\% in translation performance compared to the base LLM. This work offers a novel technical approach and practical support for efficient and reliable cross-language code migration.},
journal = {Automated Software Engg.},
month = oct,
numpages = {34},
keywords = {Large language models, Code translation, Prompt engineering, Empirical research}
}

@inproceedings{10.1145/3639478.3641227,
author = {Chen, Yang},
title = {Flakiness Repair in the Era of Large Language Models},
year = {2024},
isbn = {9798400705021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639478.3641227},
doi = {10.1145/3639478.3641227},
abstract = {Flaky tests can non-deterministically pass or fail regardless of any change to the code, which negatively impacts the effectiveness of the regression testing. Prior repair techniques for flaky tests mainly leverage program analysis techniques to mitigate test flakiness, which only focus on Order-Dependent (OD) and Implementation-Dependent (ID) flakiness with known flakiness patterns and root causes. In this paper, we propose an approach to repair flaky tests with the power of Large Language Models (LLMs). Our approach successfully repaired 79\% of OD tests and 58\% of ID tests in an extensive evaluation using 666 flaky tests from 222 projects. We submitted pull requests to fix 61 flaky tests; at the time of submission, 19 tests have already been accepted. However, we observed that currently LLMs are ineffective in adequately repairing Non-Order-Dependent (NOD) flaky tests by analyzing 118 of such tests from 11 projects.},
booktitle = {Proceedings of the 2024 IEEE/ACM 46th International Conference on Software Engineering: Companion Proceedings},
pages = {441–443},
numpages = {3},
keywords = {software testing, test flakiness, large language models},
location = {Lisbon, Portugal},
series = {ICSE-Companion '24}
}

@article{10.1007/s10458-021-09515-9,
author = {Kox, E. S. and Kerstholt, J. H. and Hueting, T. F. and de Vries, P. W.},
title = {Trust repair in human-agent teams: the effectiveness of explanations and expressing regret},
year = {2021},
issue_date = {Oct 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {35},
number = {2},
issn = {1387-2532},
url = {https://doi.org/10.1007/s10458-021-09515-9},
doi = {10.1007/s10458-021-09515-9},
abstract = {The role of intelligent agents becomes more social as they are expected to act in direct interaction, involvement and/or interdependency with humans and other artificial entities, as in Human-Agent Teams (HAT). The highly interdependent and dynamic nature of teamwork demands correctly calibrated trust among team members. Trust violations are an inevitable aspect of the cycle of trust and since repairing damaged trust proves to be more difficult than building trust initially, effective trust repair strategies are needed to ensure durable and successful team performance. The aim of this study was to explore the effectiveness of different trust repair strategies from an intelligent agent by measuring the development of human trust and advice taking in a Human-Agent Teaming task. Data for this study were obtained using a task environment resembling a first-person shooter game. Participants carried out a mission in collaboration with their artificial team member. A trust violation was provoked when the agent failed to detect an approaching enemy. After this, the agent offered one of four trust repair strategies, composed of the apology components explanation and expression of regret (either one alone, both or neither). Our results indicated that expressing regret was crucial for effective trust repair. After trust declined due to the violation by the agent, trust only significantly recovered when an expression of regret was included in the apology. This effect was stronger when an explanation was added. In this context, the intelligent agent was the most effective in its attempt of rebuilding trust when it provided an apology that was both affective, and informational. Finally, the implications of our findings for the design and study of Human-Agent trust repair are discussed.},
journal = {Autonomous Agents and Multi-Agent Systems},
month = oct,
numpages = {20},
keywords = {Trust repair, Trust, Intelligent agents, Human, Agent interaction, Apology, Social abilities, Collaboration, Competence}
}

@article{10.1007/s10515-025-00501-z,
author = {Hanna, Carol and Blot, Aymeric and Petke, Justyna},
title = {Reinforcement learning for mutation operator selection in automated program repair: Reinforcement learning for mutation operator selection in automated program repair},
year = {2025},
issue_date = {Aug 2025},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {32},
number = {2},
issn = {0928-8910},
url = {https://doi.org/10.1007/s10515-025-00501-z},
doi = {10.1007/s10515-025-00501-z},
abstract = {Automated program repair techniques aim to aid software developers with the challenging task of fixing bugs. In heuristic-based program repair, a search space of mutated program variants is explored to find potential patches for bugs. Most commonly, every selection of a mutation operator during search is performed uniformly at random, which can generate many buggy, even uncompilable programs. Our goal is to reduce the generation of variants that do not compile or break intended functionality which waste considerable resources. In this paper, we investigate the feasibility of a reinforcement learning-based approach for the selection of mutation operators in heuristic-based program repair. Our proposed approach is programming language, granularity-level, and search strategy agnostic and allows for easy augmentation into existing heuristic-based repair tools. We conducted an extensive empirical evaluation of four operator selection techniques, two reward types, two credit assignment strategies, two integration methods, and three sets of mutation operators using 30,080 independent repair attempts. We evaluated our approach on 353 real-world bugs from the Defects4J benchmark. The reinforcement learning-based mutation operator selection results in a higher number of test-passing variants, but does not exhibit a noticeable improvement in the number of bugs patched in comparison with the baseline, uniform random selection. While reinforcement learning has been previously shown to be successful in improving the search of evolutionary algorithms, often used in heuristic-based program repair, it has yet to demonstrate such improvements when applied to this area of research.},
journal = {Automated Software Engg.},
month = mar,
numpages = {33},
keywords = {Automated program repair, Machine learning, Mutation operators, Genetic improvement, Reinforcement learning}
}

@inproceedings{10.1145/3597503.3608132,
author = {Peng, Yun and Gao, Shuzheng and Gao, Cuiyun and Huo, Yintong and Lyu, Michael},
title = {Domain Knowledge Matters: Improving Prompts with Fix Templates for Repairing Python Type Errors},
year = {2024},
isbn = {9798400702174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597503.3608132},
doi = {10.1145/3597503.3608132},
abstract = {As a dynamic programming language, Python has become increasingly popular in recent years. Although the dynamic type system of Python facilitates the developers in writing Python programs, it also brings type errors at run-time which are prevalent yet not easy to fix. There exist rule-based approaches for automatically repairing Python type errors. The approaches can generate accurate patches for the type errors covered by manually defined templates, but they require domain experts to design patch synthesis rules and suffer from low template coverage of real-world type errors. Learning-based approaches alleviate the manual efforts in designing patch synthesis rules and have become prevalent due to the recent advances in deep learning. Among the learning-based approaches, the prompt-based approach which leverages the knowledge base of code pre-trained models via pre-defined prompts, obtains state-of-the-art performance in general program repair tasks. However, such prompts are manually defined and do not involve any specific clues for repairing Python type errors, resulting in limited effectiveness. How to automatically improve prompts with the domain knowledge for type error repair is challenging yet under-explored.In this paper, we present TypeFix, a novel prompt-based approach with fix templates incorporated for repairing Python type errors. TypeFix first mines generalized fix templates via a novel hierarchical clustering algorithm. The identified fix templates indicate the common edit patterns and contexts of existing type error fixes. TypeFix then generates code prompts for code pre-trained models by employing the generalized fix templates as domain knowledge, in which the masks are adaptively located for each type error instead of being pre-determined. Experiments on two benchmarks, including BugsInPy and TypeBugs, show that TypeFix successfully repairs 26 and 55 type errors, outperforming the best baseline approach by 9 and 14, respectively. Besides, the proposed fix template mining approach can cover 75\% of developers' patches in both benchmarks, increasing the best rule-based approach PyTER by more than 30\%.},
booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
articleno = {4},
numpages = {13},
location = {Lisbon, Portugal},
series = {ICSE '24}
}

@article{10.1145/3631974,
author = {Zhang, Quanjun and Fang, Chunrong and Ma, Yuxiang and Sun, Weisong and Chen, Zhenyu},
title = {A Survey of Learning-based Automated Program Repair},
year = {2023},
issue_date = {February 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {33},
number = {2},
issn = {1049-331X},
url = {https://doi.org/10.1145/3631974},
doi = {10.1145/3631974},
abstract = {Automated program repair (APR) aims to fix software bugs automatically and plays a crucial role in software development and maintenance. With the recent advances in deep learning (DL), an increasing number of APR techniques have been proposed to leverage neural networks to learn bug-fixing patterns from massive open-source code repositories. Such learning-based techniques usually treat APR as a neural machine translation (NMT) task, where buggy code snippets (i.e., source language) are translated into fixed code snippets (i.e., target language) automatically. Benefiting from the powerful capability of DL to learn hidden relationships from previous bug-fixing datasets, learning-based APR techniques have achieved remarkable performance.In this article, we provide a systematic survey to summarize the current state-of-the-art research in the learning-based APR community. We illustrate the general workflow of learning-based APR techniques and detail the crucial components, including fault localization, patch generation, patch ranking, patch validation, and patch correctness phases. We then discuss the widely adopted datasets and evaluation metrics and outline existing empirical studies. We discuss several critical aspects of learning-based APR techniques, such as repair domains, industrial deployment, and the open science issue. We highlight several practical guidelines on applying DL techniques for future APR studies, such as exploring explainable patch generation and utilizing code features. Overall, our article can help researchers gain a comprehensive understanding about the achievements of the existing learning-based APR techniques and promote the practical application of these techniques. Our artifacts are publicly available at the repository: .},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = dec,
articleno = {55},
numpages = {69},
keywords = {Automatic program repair, deep learning, neural machine translation, AI and software engineering}
}

@article{10.1145/3787105,
author = {Yang, Canghai and Zhong, Kan and Tan, Yujuan and Ren, Ao and Qiao, Lei and Liu, Duo},
title = {Boosting Aggregation Repair with All Available Nodes in Erasure-Coded Storage},
year = {2026},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1544-3566},
url = {https://doi.org/10.1145/3787105},
doi = {10.1145/3787105},
abstract = {Distributed storage systems ensure data availability through fault-tolerant mechanisms, with erasure coding being widely adopted for its low storage overhead. However, erasure coding generates significant repair traffic during data recovery, which can severely degrade performance. Recent repair algorithms aim to alleviate network bottlenecks at congested nodes, but they mainly focus on downlink bottlenecks, neglecting uplink constraints that fundamentally limit repair efficiency. Moreover, these algorithms lack a systematic approach to handling diverse failure scenarios, complicating the recovery process. In this paper, we propose RAN, an aggregation-based repair algorithm that alleviates both uplink and downlink bottlenecks by optimizing bandwidth utilization across all available nodes and aggregating transfers via programmable network devices. RAN systematically maximizes repair performance across diverse failure scenarios through a unified procedure. We further optimize its scalability and availability, and analyze its theoretical performance and improvements. The system prototype of RAN is implemented and optionally integrated as middleware into HDFS. Experiments on Amazon EC2 show that RAN improves repair throughput by up to 68.9\% for degraded read and 266.6\% for full-node recovery compared to state-of-the-art algorithms.},
note = {Just Accepted},
journal = {ACM Trans. Archit. Code Optim.},
month = jan,
keywords = {Distributed Storage, Erasure Coding, Repair Algorithms, Network Transfer, Fault Tolerance}
}

@inproceedings{10.1609/aaai.v39i1.32046,
author = {Orvalho, Pedro and Janota, Mikol\'{a}\v{s} and Manquinho, Vasco M.},
title = {Counterexample guided program repair using zero-shot learning and MaxSAT-based fault localization},
year = {2025},
isbn = {978-1-57735-897-8},
publisher = {AAAI Press},
url = {https://doi.org/10.1609/aaai.v39i1.32046},
doi = {10.1609/aaai.v39i1.32046},
abstract = {Automated Program Repair (APR) for introductory programming assignments (IPAS) is motivated by the large number of student enrollments in programming courses each year. Since providing feedback on programming assignments requires substantial time and effort from faculty, personalized automated feedback often involves suggesting repairs to students' programs. Symbolic semantic repair approaches, which rely on Formal Methods (FM), check a program's execution against a test suite or reference solution, are effective but limited. These tools excel at identifying buggy parts but can only fix programs if the correct implementation and the faulty one share the same control flow graph. Conversely, Large Language Models (LLMS) are used for program repair but often make extensive rewrites instead of minimal adjustments. This tends to lead to more invasive fixes, making it harder for students to learn from their mistakes. In summary, LLMS excel at completing strings, while FM-based fault localization excel at identifying buggy parts of a program.In this paper, we propose a novel approach that combines the strengths of both FM-based fault localization and LLMS, via zero-shot learning, to enhance APR for IPAS. Our method uses MaxSAT-based fault localization to identify buggy parts of a program, then presents the LLM with a program sketch devoid of these buggy statements. This hybrid approach follows a Counterexample Guided Inductive Synthesis (CEGIS) loop to iteratively refine the program. We ask the LLM to synthesize the missing parts, which are then checked against a test suite. If the suggested program is incorrect, a counterexample from the test suite is fed back to the LLM for revised synthesis. Our experiments on 1,431 incorrect student programs show that our counterexample guided approach, using MaxSAT-based bug-free program sketches, significantly improves the repair capabilities of all six evaluated LLMS. This method allows LLMS to repair more programs and produce smaller fixes, outperforming other configurations and state-of-the-art symbolic program repair tools. Code — https://doi.org/10.5281/zenodo.14517771},
booktitle = {Proceedings of the Thirty-Ninth AAAI Conference on Artificial Intelligence and Thirty-Seventh Conference on Innovative Applications of Artificial Intelligence and Fifteenth Symposium on Educational Advances in Artificial Intelligence},
articleno = {73},
numpages = {9},
series = {AAAI'25/IAAI'25/EAAI'25}
}

@article{10.1145/3715716,
author = {Wang, Xu and Zhang, Mingming and Meng, Xiangxin and Zhang, Jian and Liu, Yang and Hu, Chunming},
title = {Element-Based Automated DNN Repair with Fine-Tuned Masked Language Model},
year = {2025},
issue_date = {July 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {FSE},
url = {https://doi.org/10.1145/3715716},
doi = {10.1145/3715716},
abstract = {Deep Neural Networks (DNNs) are prevalent across a wide range of applications. Despite their success, the complexity and opaque nature of DNNs pose significant challenges in debugging and repairing DNN models, limiting their reliability and broader adoption. In this paper, we propose MLM4DNN, an element-based automated DNN repair method. Unlike previous techniques that focus on post-training adjustments or rely heavily on predefined bug patterns, MLM4DNN repairs DNNs by leveraging a fine-tuned Masked Language Model (MLM) to predict correct fixes for nine predefined key elements in DNNs. We construct a large-scale dataset by masking nine key elements from the correct DNN source code and then force the MLM to restore the correct elements to learn the deep semantics that ensure the normal functionalities of DNNs. Afterwards, a light-weight static analysis tool is designed to filter out low-quality patches to enhance the repair efficiency. We introduce a patch validation method specifically for DNN repair tasks, which consists of three evaluation metrics from different aspects to model the effectiveness of generated patches. We construct a benchmark, BenchmarkAPR4DNN, including 51 buggy DNN models and an evaluation tool that outputs the three metrics. We evaluate MLM4DNN against six baselines on BenchmarkAPR4DNN, and results show that MLM4DNN outperforms all state-of-the-art baselines, including two dynamic-based and four zero-shot learning-based methods. After applying the fine-tuned MLM design to several prevalent Large Language Models (LLMs), we consistently observe improved performance in DNN repair tasks compared to the original LLMs, which demonstrates the effectiveness of the method proposed in this paper.},
journal = {Proc. ACM Softw. Eng.},
month = jun,
articleno = {FSE006},
numpages = {24},
keywords = {Deep Neural Network, Fine-Tune, Masked Language Model, Program Repair}
}

@article{10.1145/3631972,
author = {Zirak, Armin and Hemmati, Hadi},
title = {Improving Automated Program Repair with Domain Adaptation},
year = {2024},
issue_date = {March 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {33},
number = {3},
issn = {1049-331X},
url = {https://doi.org/10.1145/3631972},
doi = {10.1145/3631972},
abstract = {Automated Program Repair (APR) is defined as the process of fixing a bug/defect in the source code, by an automated tool. APR tools have recently experienced promising results by leveraging state-of-the-art Neural Language Processing (NLP) techniques. APR tools such as TFix and CodeXGLUE that combine text-to-text transformers with software-specific techniques are outperforming alternatives, these days. However, in most APR studies, the train and test sets are chosen from the same set of projects (i.e., when APR fixes a bug in the test set from project A, the model has already seen example fixed bugs from project A in the training set). In the real world, however, APR models are meant to be generalizable to new and different projects. Therefore, there is a potential threat that reported APR models with high effectiveness perform poorly when the characteristics of the new project or its bugs are different than the training set’s (“Domain Shift”).In this study, we first define the problem of domain shift in automated program repair. Next, we measure the potential damage of domain shift on two recent APR models (TFix and CodeXGLUE). Based on this observation, we then propose a domain adaptation framework that can adapt an APR model for a given target project. We conduct an empirical study with three domain adaptation methods FullFineTuning, TuningWithLightWeightAdapterLayers, and CurriculumLearning and two APR models on 2,672 bugs from 12 projects.The results show that our proposed framework on average can improve the effectiveness of TFix by 13.05\% and CodeXGLUE by 48.78\%, in terms of “Exact Match”. Through experiments, we also show that the framework provides high efficiency and reliability (in terms of “Exposure Bias”). Using synthetic data to domain adapt TFix and CodeXGLUE on the projects with no data (Zero-shot learning), also results in an average improvement of 5.76\% and 17.62\% for TFix and CodeXGLUE, respectively.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = mar,
articleno = {65},
numpages = {43},
keywords = {Automated program repair, deep learning, neural machine translation, transformers, CodeBERT, domain adaptation}
}

@article{10.1016/j.cosrev.2025.100732,
author = {Ochoa, Lina and Hammad, Muhammad and Giray, G\"{o}rkem and Babur, \"{O}nder and Bennin, Kwabena},
title = {Characterising harmful API uses and repair techniques: Insights from a systematic review},
year = {2025},
issue_date = {Aug 2025},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {57},
number = {C},
issn = {1574-0137},
url = {https://doi.org/10.1016/j.cosrev.2025.100732},
doi = {10.1016/j.cosrev.2025.100732},
journal = {Comput. Sci. Rev.},
month = aug,
numpages = {26},
keywords = {Application Programming Interface (API), Harmful API use, Client repair, Program repair, Survey, Systematic literature review}
}

@article{10.1145/3704997,
author = {Renzullo, Joseph and Reiter, Pemma and Weimer, Westley and Forrest, Stephanie},
title = {Automated Program Repair: Emerging Trends Pose and Expose Problems for Benchmarks},
year = {2025},
issue_date = {August 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {57},
number = {8},
issn = {0360-0300},
url = {https://doi.org/10.1145/3704997},
doi = {10.1145/3704997},
abstract = {Machine learning (ML) pervades the field of Automated Program Repair (APR). Algorithms deploy neural machine translation and large language models (LLMs) to generate software patches, among other tasks. But, there are important differences between these applications of ML and earlier work, which complicates the task of ensuring that results are valid and likely to generalize. A challenge is that the most popular APR evaluation benchmarks were not designed with ML techniques in mind. This is especially true for LLMs, whose large and often poorly-disclosed training datasets may include problems on which they are evaluated.This article reviews work in APR published in the field’s top five venues since 2018, emphasizing emerging trends in the field, including the dramatic rise of ML models, including LLMs. ML-based articles are categorized along structural and functional dimensions, and a variety of issues are identified that these new methods raise. Importantly, data leakage and contamination concerns arise from the challenge of validating ML-based APR using existing benchmarks, which were designed before these techniques were popular. We discuss inconsistencies in evaluation design and performance reporting and offer pointers to solutions where they are available. Finally, we highlight promising new directions that the field is already taking.},
journal = {ACM Comput. Surv.},
month = mar,
articleno = {208},
numpages = {18},
keywords = {automated program repair, machine learning, benchmarks, patch quality}
}

@inproceedings{10.1145/3597503.3639108,
author = {Cao, Shaoheng and Pan, Minxue and Pei, Yu and Yang, Wenhua and Zhang, Tian and Wang, Linzhang and Li, Xuandong},
title = {Comprehensive Semantic Repair of Obsolete GUI Test Scripts for Mobile Applications},
year = {2024},
isbn = {9798400702174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597503.3639108},
doi = {10.1145/3597503.3639108},
abstract = {Graphical User Interface (GUI) testing is one of the primary approaches for testing mobile apps. Test scripts serve as the main carrier of GUI testing, yet they are prone to obsolescence when the GUIs change with the apps' evolution. Existing repair approaches based on GUI layouts or images prove effective when the GUI changes between the base and updated versions are minor, however, they may struggle with substantial changes. In this paper, a novel approach named COSER is introduced as a solution to repairing broken scripts, which is capable of addressing larger GUI changes compared to existing methods. COSER incorporates both external semantic information from the GUI elements and internal semantic information from the source code to provide a unique and comprehensive solution. The efficacy of COSER was demonstrated through experiments conducted on 20 Android apps, resulting in superior performance when compared to the state-of-the-art tools METER and GUIDER. In addition, a tool that implements the COSER approach is available for practical use and future research.},
booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
articleno = {90},
numpages = {13},
keywords = {GUI test script repair, Android testing, regression testing},
location = {Lisbon, Portugal},
series = {ICSE '24}
}

@inproceedings{10.1145/3639478.3647633,
author = {Chen, Yuxiao and Wu, Jingzheng and Ling, Xiang and Li, Changjiang and Rui, Zhiqing and Luo, Tianyue and Wu, Yanjun},
title = {When Large Language Models Confront Repository-Level Automatic Program Repair: How Well They Done?},
year = {2024},
isbn = {9798400705021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639478.3647633},
doi = {10.1145/3639478.3647633},
abstract = {In recent years, large language models (LLMs) have demonstrated substantial potential in addressing automatic program repair (APR) tasks. However, the current evaluation of these models for APR tasks focuses solely on the limited context of the single function or file where the bug is located, overlooking the valuable information in the repository-level context. This paper investigates the performance of popular LLMs in handling repository-level repair tasks. We introduce RepoBugs, a new benchmark comprising 124 typical repository-level bugs from open-source repositories. Preliminary experiments using GPT3.5 based on the function where the error is located, reveal that the repair rate on RepoBugs is only 22.58\%, significantly diverging from the performance of GPT3.5 on function-level bugs in related studies. This underscores the importance of providing repository-level context when addressing bugs at this level. However, the repository-level context offered by the preliminary method often proves redundant and imprecise and easily exceeds the prompt length limit of LLMs. To solve the problem, we propose a simple and universal repository-level context extraction method (RLCE) designed to provide more precise context for repository-level code repair tasks. Evaluations of three mainstream LLMs show that RLCE significantly enhances the ability to repair repository-level bugs. The improvement reaches a maximum of 160\% compared to the preliminary method. Additionally, we conduct a comprehensive analysis of the effectiveness and limitations of RLCE, along with the capacity of LLMs to address repository-level bugs, offering valuable insights for future research.},
booktitle = {Proceedings of the 2024 IEEE/ACM 46th International Conference on Software Engineering: Companion Proceedings},
pages = {459–471},
numpages = {13},
keywords = {large language models, automatic program repair, repository-level bugs, context, static analysis},
location = {Lisbon, Portugal},
series = {ICSE-Companion '24}
}

@inproceedings{10.24963/ijcai.2025/1152,
author = {Bercher, Pascal and Sreedharan, Sarath and Vallati, Mauro},
title = {A survey on model repair in ai planning},
year = {2025},
isbn = {978-1-956792-06-5},
url = {https://doi.org/10.24963/ijcai.2025/1152},
doi = {10.24963/ijcai.2025/1152},
abstract = {Accurate planning models are a prerequisite for the appropriate functioning of AI planning applications. Creating these models is, however, a tedious and error-prone task - even for planning experts. This makes the provision of automated modeling support essential. In this work, we differentiate between approaches that learn models from scratch (called domain model acquisition) and those that repair flawed or incomplete ones. We survey approaches for the latter, including those that can be used for domain repair but have been developed for other applications, discuss possible optimization metrics (i.e., which repaired model to aim at), and conclude with lines of research we believe deserve more attention.},
booktitle = {Proceedings of the Thirty-Fourth International Joint Conference on Artificial Intelligence},
articleno = {1152},
numpages = {10},
location = {Montreal, Canada},
series = {IJCAI '25}
}

@inproceedings{10.1145/3618305.3623587,
author = {Ribeiro, Francisco},
title = {Large Language Models for Automated Program Repair},
year = {2023},
isbn = {9798400703843},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3618305.3623587},
doi = {10.1145/3618305.3623587},
abstract = {This paper introduces two methods for automated program repair (APR) utilizing pre-trained language models. The first method demonstrates program repair as a code completion task and is validated on a dataset of Java programs. The second method, Mentat, leverages OCaml’s parser and type system as fault localization techniques to generate prompts for GPT-3, producing candidate patches. Evaluation results show promising repair rates, with 27\% and 39.2\% effectiveness, respectively. For OCaml, a comparative study employing an automated validation strategy is presented in which the technique outperforms other tools. Language models are effective at APR, enhancing bug fixing and freeing developers to focus on other critical aspects of software engineering.},
booktitle = {Companion Proceedings of the 2023 ACM SIGPLAN International Conference on Systems, Programming, Languages, and Applications: Software for Humanity},
pages = {7–9},
numpages = {3},
keywords = {automated program repair, code generation, fault localization, type systems},
location = {Cascais, Portugal},
series = {SPLASH 2023}
}

@inproceedings{10.1007/978-3-032-07106-4_10,
author = {Yuviler, Tom and Drachsler-Cohen, Dana},
title = {Enhancing Neural Network Robustness via&nbsp;Synthesis of&nbsp;Repair Programs},
year = {2025},
isbn = {978-3-032-07105-7},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-032-07106-4_10},
doi = {10.1007/978-3-032-07106-4_10},
abstract = {Adversarial examples undermine the reliability of neural networks. To defend against attacks, multiple approaches have been proposed. However, many of them introduce high training overhead or high inference overhead, some significantly decrease the network’s accuracy or insufficiently increase the network’s robustness, and others do not scale to deep networks. To mitigate all these shortcomings, we propose a new form of defense: optimal program synthesis of&nbsp;short repair programs, integrated into a trained network. A repair program modifies a few neurons by using a few other neurons. The challenge is to identify the most successful combination of neurons to enhance the network’s robustness while maintaining high accuracy. We introduce DefEnSyn, a stochastic synthesizer of repair programs. To cope with the exponential number of neuron combinations, DefEnSyn learns the effective combinations by synthesizing repair programs of increasing length. We evaluate DefEnSyn on classifiers for ImageNet and CIFAR-10 and show it enhances the robustness of networks to L∞-, L2-, and L0- black-box adversarial example attacks and to backdoor attacks. DefEnSyn ’s repair programs enhance the networks’ robustness on average by +40\% and up to +71\%. DefEnSyn decreases the network’s accuracy by only ≈-1\%. We demonstrate that DefEnSyn outperforms existing state-of-the-art defenses based on adversarial training, randomization, and repair, in both robustness and accuracy.},
booktitle = {Static Analysis: 32nd International Symposium, SAS 2025, Singapore, Singapore,  October 13–14, 2025, Proceedings},
pages = {221–248},
numpages = {28},
keywords = {Neural Network Robustness, Program Synthesis},
location = {Singapore, Singapore}
}

@article{10.1145/3705895,
author = {Yang, Jiahui and Nan, Fulin and Shen, Zhirong and Chen, Zhisheng and Cai, Yuhui and Kaplun, Dmitrii and Wang, Xiaoli and Xu, Quanqing and Yang, Chuanhui and Shu, Jiwu},
title = {TPRepair: Tree-based Pipelined Repair in Clustered Storage Systems},
year = {2025},
issue_date = {March 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {1},
issn = {1544-3566},
url = {https://doi.org/10.1145/3705895},
doi = {10.1145/3705895},
abstract = {Erasure coding is an effective technique for guaranteeing data reliability for storage systems, yet it incurs a high repair penalty with amplified repair traffic. The repair becomes more intricate in clustered storage systems with the bandwidth diversity property. We present TPRepair, a Tree-based Pipelined Repair approach, aiming to expedite the overall repair process with the tailored pipelined repair procedure. TPRepair first prioritizes selecting racks with the current minimum load to participate in the repair process. It subsequently formulates tree-based links, tailored to align seamlessly with the pipelined repair procedure. TPRepair further designs an optimization algorithm to reduce the bottleneck load when repairing multiple chunks. Large-scale simulations demonstrate that TPRepair can increase 13.8\%–41.3\% of the balance ratio without amplifying cross-rack traffic. Meanwhile, Alibaba Cloud ECS experiments indicate that TPRepair can increase repair throughput by 11.3\% to 72.9\%.},
journal = {ACM Trans. Archit. Code Optim.},
month = mar,
articleno = {31},
numpages = {25},
keywords = {Erasure coding, tree-based pipelined repair, clustered storage systems}
}

@article{10.1145/3712187,
author = {Pian, Weiguo and Li, Yinghua and Tian, Haoye and Sun, Tiezhu and Song, Yewei and Tang, Xunzhu and Habib, Andrew and Klein, Jacques and Bissyand\'{e}, Tegawend\'{e} F.},
title = {You Don’t Have to Say Where to Edit! jLED—Joint Learning to Localize and Edit Source Code},
year = {2025},
issue_date = {July 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {34},
number = {6},
issn = {1049-331X},
url = {https://doi.org/10.1145/3712187},
doi = {10.1145/3712187},
abstract = {Learning to edit code automatically is becoming more and more feasible. Thanks to recent advances in Neural Machine Translation (NMT), various case studies are being investigated where patches are automatically produced and assessed either automatically (using test suites) or by developers themselves. An appealing setting remains when the developer must provide a natural language input of the requirement for the code change. A recent proof of concept in the literature showed that it is indeed feasible to translate these natural language requirements into code changes. A recent advancement, MODIT, has shown promising results in code editing by leveraging natural language, code context, and location information as input. However, it struggles when location information is unavailable. While several studies have demonstrated the ability to edit source code without explicitly specifying the edit location, they still tend to generate edits with less accuracy at the line level. In this work, we address the challenge of generating code edits without precise location information, a scenario we consider crucial for the practical adoption of NMT in code development. To that end, we develop a novel joint training approach for both localization and source code editions. Building a benchmark based on over 70k commits (patches and messages), we demonstrate that our joint Localize and EDit (jLED)  approach is effective. An ablation study further demonstrates the importance of our design choice in joint training.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = jul,
articleno = {164},
numpages = {27},
keywords = {Source Code Edition, Joint Learning, Automated Programming, Neural Machine Translation}
}

@inproceedings{10.1609/aaai.v38i18.29984,
author = {Caglar, Turgay and Belhaj, Sirine and Chakraborti, Tathagata and Katz, Michael and Sreedharan, Sarath},
title = {Can LLMs fix issues with reasoning models? towards more likely models for AI planning},
year = {2024},
isbn = {978-1-57735-887-9},
publisher = {AAAI Press},
url = {https://doi.org/10.1609/aaai.v38i18.29984},
doi = {10.1609/aaai.v38i18.29984},
abstract = {This is the first work to look at the application of large language models (LLMs) for the purpose of model space edits in automated planning tasks. To set the stage for this union, we explore two different flavors of model space problems that have been studied in the AI planning literature and explore the effect of an LLM on those tasks. We empirically demonstrate how the performance of an LLM contrasts with combinatorial search (CS) - an approach that has been traditionally used to solve model space tasks in planning, both with the LLM in the role of a standalone model space reasoner as well as in the role of a statistical signal in concert with the CS approach as part of a two-stage process. Our experiments show promising results suggesting further forays of LLMs into the exciting world of model space reasoning for planning tasks in the future.},
booktitle = {Proceedings of the Thirty-Eighth AAAI Conference on Artificial Intelligence and Thirty-Sixth Conference on Innovative Applications of Artificial Intelligence and Fourteenth Symposium on Educational Advances in Artificial Intelligence},
articleno = {2236},
numpages = {9},
series = {AAAI'24/IAAI'24/EAAI'24}
}

@article{10.1145/3773990,
author = {Li, Xiaolu and Yuan, Han and Liu, Xuan and Zhang, Junlong and Lee, Patrick P. C. and Hu, Yuchong and Feng, Dan},
title = {Harnessing Parallelism for Fast Data Repair in MSR-Coded Storage},
year = {2026},
issue_date = {February 2026},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {1},
issn = {1553-3077},
url = {https://doi.org/10.1145/3773990},
doi = {10.1145/3773990},
abstract = {Minimum-storage regenerating (MSR) codes are provably optimal erasure codes that minimize the repair bandwidth (i.e., the amount of traffic being transferred during a repair operation), while minimizing storage redundancy, in distributed storage systems. However, the practical repair performance of MSR codes still has significant room for improvements, as their mathematical structure makes repair operations difficult to parallelize. In this article, we present HyperParaRC, a parallel repair framework for MSR codes. HyperParaRC leverages the sub-packetization nature of MSR codes to parallelize the repair of sub-blocks and balance repair load (i.e., the amount of traffic sent or received by a node) across available nodes. We first demonstrate that there exists a trade-off between repair bandwidth and maximum repair load. We then propose an affinity-based heuristic for HyperParaRC, which approximately minimizes the maximum repair load by examining the bandwidth incurred during sub-block computations and significantly reduces the search time for large coding parameters compared with our earlier work, ParaRC. Based on our affinity-based heuristic, we further design a full-node recovery mechanism for HyperParaRC that combines both intra-stripe and inter-stripe parallel repair scheduling to repair multiple lost blocks in a failed node. We prototype HyperParaRC on Hadoop HDFS and evaluate it on Alibaba Cloud. Our evaluation results show that HyperParaRC reduces both single-block repair and full-node recovery times compared with state-of-the-art repair approaches.},
journal = {ACM Trans. Storage},
month = jan,
articleno = {10},
numpages = {38},
keywords = {Erasure coding, Distributed Storage Systems}
}

@inproceedings{10.1145/3742876.3742881,
author = {Hemati Moghadam, Iman and Lijzenga, Oebele and Zaytsev, Vadim},
title = {Comparative Analysis of Pre-trained Code Language Models for Automated Program Repair via Code Infill Generation},
year = {2025},
isbn = {9798400719950},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3742876.3742881},
doi = {10.1145/3742876.3742881},
abstract = {Automated Program Repair (APR) has advanced significantly with the emergence of pre-trained Code Language Models (CLMs), enabling the generation of high-quality patches. However, selecting the most suitable CLM for APR remains challenging due to a range of factors, including accuracy, efficiency, and scalability, among others. These factors are interdependent and interact in complex ways, making the selection of a CLM for APR a multifaceted problem.    This study systematically evaluates 20 pre-trained CLMs, ranging from 60M to 16B parameters, on the HumanEval-Java benchmark (163 buggy Java methods). The evaluation examines bug-fixing accuracy, resource consumption, compilability, patch diversity, and sampling strategies (beam search vs. nucleus sampling).    Results indicate that larger models such as CodeLLaMA-13B and StarCoder generally perform better in bug fixing and compiler error handling, but scale alone does not guarantee effectiveness, as some (e.g., CodeGen2) perform poorly despite their size. Notably, memory usage increases with model size, but time consumption does not exhibit a clear correlation, suggesting that efficiency is influenced by architecture rather than scale alone. Additionally, nucleus sampling slightly outperforms beam search, though the difference is not statistically significant. Since no single CLM fixes all bugs, these findings highlight the potential of hybrid or ensemble-based CLM-driven APR approaches for more robust bug-fixing.},
booktitle = {Proceedings of the 24th ACM SIGPLAN International Conference on Generative Programming: Concepts and Experiences},
pages = {13–26},
numpages = {14},
keywords = {Automated program repair, Java, empirical study, pre-trained code language model, zero-shot learning},
location = {Bergen, Norway},
series = {GPCE '25}
}

@inproceedings{10.1145/3643788.3648019,
author = {Yuan, Yuan},
title = {ARJA-e for the First International Competition on Automated Program Repair},
year = {2024},
isbn = {9798400705779},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643788.3648019},
doi = {10.1145/3643788.3648019},
abstract = {ARJA-e is an enhanced repair system based on ARJA. Here we briefly introduces ARJA-e and report the results of ARJA-e for the First International Competition on Automated Program Repair on two competition tracks, including the AI Generated Code track and the Functional Errors track.},
booktitle = {Proceedings of the 5th ACM/IEEE International Workshop on Automated Program Repair},
pages = {50–52},
numpages = {3},
keywords = {program repair, genetic programming},
location = {Lisbon, Portugal},
series = {APR '24}
}

@inproceedings{10.1609/aaai.v38i18.29984,
author = {Caglar, Turgay and Belhaj, Sirine and Chakraborti, Tathagata and Katz, Michael and Sreedharan, Sarath},
title = {Can LLMs fix issues with reasoning models? towards more likely models for AI planning},
year = {2024},
isbn = {978-1-57735-887-9},
publisher = {AAAI Press},
url = {https://doi.org/10.1609/aaai.v38i18.29984},
doi = {10.1609/aaai.v38i18.29984},
abstract = {This is the first work to look at the application of large language models (LLMs) for the purpose of model space edits in automated planning tasks. To set the stage for this union, we explore two different flavors of model space problems that have been studied in the AI planning literature and explore the effect of an LLM on those tasks. We empirically demonstrate how the performance of an LLM contrasts with combinatorial search (CS) - an approach that has been traditionally used to solve model space tasks in planning, both with the LLM in the role of a standalone model space reasoner as well as in the role of a statistical signal in concert with the CS approach as part of a two-stage process. Our experiments show promising results suggesting further forays of LLMs into the exciting world of model space reasoning for planning tasks in the future.},
booktitle = {Proceedings of the Thirty-Eighth AAAI Conference on Artificial Intelligence and Thirty-Sixth Conference on Innovative Applications of Artificial Intelligence and Fourteenth Symposium on Educational Advances in Artificial Intelligence},
articleno = {2236},
numpages = {9},
series = {AAAI'24/IAAI'24/EAAI'24}
}

@article{10.1145/3664604,
author = {Li, Yuechen and Pei, Hanyu and Huang, Linzhi and Yin, Beibei and Cai, Kai-Yuan},
title = {Automatic Repair of Quantum Programs via Unitary Operation},
year = {2024},
issue_date = {July 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {33},
number = {6},
issn = {1049-331X},
url = {https://doi.org/10.1145/3664604},
doi = {10.1145/3664604},
abstract = {With the continuous advancement of quantum computing (QC), the demand for high-quality quantum programs (QPs) is growing. To avoid program failure, in software engineering, the technology of automatic program repair (APR) employs appropriate patches to remove potential bugs without the intervention of a human. However, the method tailored for repairing defective QPs is still absent. This article proposes, to the best of our knowledge, a new APR method named UnitAR that can repair QPs via unitary operation automatically. Based on the characteristics of superposition and entanglement in QC, the article constructs an algebraic model and adopts a generate-and-validate approach for the repair procedure. Furthermore, the article presents two schemes that can respectively promote the efficiency of generating patches and guarantee the effectiveness of applying patches. For the purpose of evaluating the proposed method, the article selects 29 mutated versions as well as five real-world buggy programs as the objects and introduces two traditional APR approaches GenProg and TBar as baselines. According to the experiments, UnitAR can fix 23 buggy programs, and this method demonstrates the highest efficiency and effectiveness among three APR approaches. Besides, the experimental results further manifest the crucial roles of two constituents involved in the framework of UnitAR.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = jun,
articleno = {154},
numpages = {43},
keywords = {Quantum computing, automatic program repair, quantum software engineering, unitary operation, software cybernetics, S-ADA}
}

@inproceedings{10.1145/3597503.3623310,
author = {Zhong, Wenkang and Li, Chuanyi and Liu, Kui and Xu, Tongtong and Ge, Jidong and Bissyande, Tegawende F. and Luo, Bin and Ng, Vincent},
title = {Practical Program Repair via Preference-based Ensemble Strategy},
year = {2024},
isbn = {9798400702174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597503.3623310},
doi = {10.1145/3597503.3623310},
abstract = {To date, over 40 Automated Program Repair (APR) tools have been designed with varying bug-fixing strategies, which have been demonstrated to have complementary performance in terms of being effective for different bug classes. Intuitively, it should be feasible to improve the overall bug-fixing performance of APR via assembling existing tools. Unfortunately, simply invoking all available APR tools for a given bug can result in unacceptable costs on APR execution as well as on patch validation (via expensive testing). Therefore, while assembling existing tools is appealing, it requires an efficient strategy to reconcile the need to fix more bugs and the requirements for practicality. In light of this problem, we propose a Preference-based Ensemble Program Repair framework (P-EPR), which seeks to effectively rank APR tools for repairing different bugs. P-EPR is the first non-learning-based APR ensemble method that is novel in its exploitation of repair patterns as a major source of knowledge for ranking APR tools and its reliance on a dynamic update strategy that enables it to immediately exploit and benefit from newly derived repair results. Experimental results show that P-EPR outperforms existing strategies significantly both in flexibility and effectiveness.},
booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
articleno = {5},
numpages = {13},
keywords = {program repair, ensemble strategy},
location = {Lisbon, Portugal},
series = {ICSE '24}
}

@inproceedings{10.1145/3568813.3600130,
author = {Koutcheme, Charles and Sarsa, Sami and Leinonen, Juho and Haaranen, Lassi and Hellas, Arto},
title = {Evaluating Distance Measures for Program Repair},
year = {2023},
isbn = {9781450399760},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3568813.3600130},
doi = {10.1145/3568813.3600130},
abstract = {Background and Context: Struggling with programming assignments while learning to program is a common phenomenon in programming courses around the world. Supporting struggling students is a common theme in Computing Education Research (CER), where a wide variety of support methods have been created and evaluated. An important stream of research here focuses on program repair, where methods for automatically fixing erroneous code are used for supporting students as they debug their code. Work in this area has so far assessed the performance of the methods by evaluating the closeness of the proposed fixes to the original erroneous code. The evaluations have mainly relied on the use of edit distance measures such as the sequence edit distance and there is a lack of research on which distance measure is the most appropriate. Objectives: Provide insight into measures for quantifying the distance between erroneous code written by a student and a proposed change. We conduct the evaluation in an introductory programming context, where insight into the distance measures can provide help in choosing a suitable metric that can inform which fixes should be suggested to novices. Method: A team of five experts annotated a subset of the Dublin dataset, creating solutions for over a thousand erroneous programs written by students. We evaluated how the prominent edit distance measures from the CER literature compare against measures used in Natural Language Processing (NLP) tasks for retrieving the experts’ solutions from a pool of proposed solutions. We also evaluated how the expert-generated solutions compare against the solutions proposed by common program repair algorithms. The annotated dataset and the evaluation code are published as part of the work. Findings: Our results highlight that the ROUGE score, classically used for evaluating the performance of machine summarization tasks, performs well as an evaluation and selection metric for program repair. We also highlight the practical utility of NLP metrics, which allow an easier interpretation and comparison of the performance of repair techniques when compared to the classic methods used in the CER literature. Implications: Our study highlights the variety of distance metrics used for comparing source codes. We find issues with the classically used distance measures that can be combated by using NLP metrics. Based on our findings, we recommend including NLP metrics, and in particular, the ROUGE metric, in evaluations when considering new program repair methodologies. We also suggest incorporating NLP metrics into other areas where source codes are compared, including plagiarism detection.},
booktitle = {Proceedings of the 2023 ACM Conference on International Computing Education Research - Volume 1},
pages = {495–507},
numpages = {13},
keywords = {BLEU, ROUGE, automated program repair, automatic program repair, bug fixing, computing education, dataset, distance measures, distance metrics, educational data mining, feedback, natural language processing, program repair},
location = {Chicago, IL, USA},
series = {ICER '23}
}

@inproceedings{10.1145/3744333.3747832,
author = {Lan, Lan and Cai, Yunhao and Chu, Yueying and Tang, Wenting and Chen, Yuchu and Liu, Peng},
title = {Examining Cross-Cultural Differences in Intelligent Vehicle Agents: Repair Strategies after Their Failures},
year = {2025},
isbn = {9798400720130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3744333.3747832},
doi = {10.1145/3744333.3747832},
abstract = {Anthropomorphic design in intelligent vehicle agents (IVAs) is crucial for driving safety and user experience. Cultural background may shape user preferences, as evidenced by Chinese car manufacturers offering more anthropomorphic IVAs (e.g., physical robots, human-like virtual agents) than their Western counterparts. While prior research has examined cross-cultural differences in visual anthropomorphism, behavioral anthropomorphism remains understudied. Here we evaluated the performance of eight IVAs (five from Chinese brands, three from Western brands) in responding to user requests and their social repair behaviors (e.g., apology and promise) following request failures. Overall, Chinese and Western IVAs did not differ in their corrective responses or likelihood of employing repair behaviors. However, Chinese IVAs were more likely to use combined behaviors rather than single ones and to incorporate intimacy expressions in their repair behaviors. Our findings highlight cultural design nuances in behavioral anthropomorphism, with implications for the culturally adaptive design of IVAs.},
booktitle = {Proceedings of the 17th International Conference on Automotive User Interfaces and Interactive Vehicular Applications},
pages = {351–361},
numpages = {11},
keywords = {Behavioral anthropomorphism, Cross-cultural differences, Intelligent vehicle agents, Social repair behaviors},
location = {
},
series = {AutomotiveUI '25}
}

@article{10.1016/j.eswa.2023.121234,
author = {Papadopoulos, George and Bastas, Alevizos and Vouros, George A. and Crook, Ian and Andrienko, Natalia and Andrienko, Gennady and Cordero, Jose Manuel},
title = {Deep reinforcement learning in service of air traffic controllers to resolve tactical conflicts},
year = {2024},
issue_date = {Feb 2024},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {236},
number = {C},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2023.121234},
doi = {10.1016/j.eswa.2023.121234},
journal = {Expert Syst. Appl.},
month = feb,
numpages = {19},
keywords = {Air traffic control, Conflict detection and resolution, Graph convolutional reinforcement learning, Transparency}
}

@inproceedings{10.1007/978-3-032-09044-7_9,
author = {Pineda, Moises and Luna, Diego and Esquivel, Mariana and Bours, Jes\'{u}s and Salazar, Juan and Flores-Araiza, Dainel and Hinojosa, Salvador},
title = {Beyond SWE-Bench: A Compiler-Assisted Pipeline for&nbsp;Multi-language Automated Program Repair},
year = {2025},
isbn = {978-3-032-09043-0},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-032-09044-7_9},
doi = {10.1007/978-3-032-09044-7_9},
abstract = {Automated program repair (APR) research predominantly focuses on Python environments, creating significant infrastructure gaps for compiled languages like C, C++, and Java that dominate production systems. We present the first systematic pipeline addressing multi-language APR infrastructure limitations through compiler-assisted dataset curation and paradigm-aware evaluation frameworks. Our approach combines a DFA-based code classification system achieving 92.4\% accuracy in programming paradigm detection with systematic dataset filtering that processes over 3 million samples to extract 30,000 high-quality object-oriented examples. Initial evaluation on Qwen3-14B using LoRA fine-tuning reveals critical adaptation thresholds: effective multi-language adaptation requires modification of approximately 1.2\% or more model parameters, with lighter fine-tuning underperforming baseline models. Our open-source pipeline provides end-to-end infrastructure from compiler-assisted dataset curation to cloud deployment, enabling systematic research advancement in multi-language automated program repair and establishing methodological foundations for compiler-assisted machine learning across diverse programming environments.},
booktitle = {Advances in Soft Computing: 24th Mexican International Conference on Artificial Intelligence, MICAI 2025, Guanajuato, Mexico, November 3, 2025, Proceedings, Part II},
pages = {115–127},
numpages = {13},
keywords = {automated program repair, multi-language systems, dataset curation, compiler techniques, AI for Software Engineering},
location = {Guanajuato, Mexico}
}

@inproceedings{10.1145/3611643.3616243,
author = {First, Emily and Rabe, Markus N. and Ringer, Talia and Brun, Yuriy},
title = {Baldur: Whole-Proof Generation and Repair with Large Language Models},
year = {2023},
isbn = {9798400703270},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3611643.3616243},
doi = {10.1145/3611643.3616243},
abstract = {Formally verifying software is a highly desirable but labor-intensive task.   Recent work has developed methods to automate formal verification using proof assistants, such as Coq and Isabelle/HOL, e.g., by training a model to predict one proof step at a time and using that model to search through the space of possible proofs.   This paper introduces a new method to automate formal verification: We use large language models, trained on natural language and code and fine-tuned on proofs, to generate whole proofs at once.   We then demonstrate that a model fine-tuned to repair generated proofs further increasing proving power.   This paper:   (1) Demonstrates that whole-proof generation using transformers is possible and is as effective but more efficient than search-based techniques.   (2) Demonstrates that giving the learned model additional context, such as a prior failed proof attempt and the ensuing error message, results in proof repair that further improves automated proof generation.   (3) Establishes, together with prior work, a new state of the art for fully automated proof synthesis.   We reify our method in a prototype, Baldur, and evaluate it on a benchmark of 6,336 Isabelle/HOL theorems and their proofs,   empirically showing the effectiveness of whole-proof generation, repair, and added context. We also show that Baldur complements the state-of-the-art tool, Thor, by automatically generating proofs for an additional 8.7\% of the theorems. Together, Baldur and Thor can prove 65.7\% of the theorems fully automatically. This paper paves the way for new research into using large language models for automating formal verification.},
booktitle = {Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {1229–1241},
numpages = {13},
keywords = {Proof assistants, automated formal verification, large language models, machine learning, proof repair, proof synthesis},
location = {San Francisco, CA, USA},
series = {ESEC/FSE 2023}
}

@inproceedings{10.1007/978-3-030-77772-2_7,
author = {Rebensky, Summer and Carmody, Kendall and Ficke, Cherrise and Nguyen, Daniel and Carroll, Meredith and Wildman, Jessica and Thayer, Amanda},
title = {Whoops! Something Went Wrong: Errors, Trust, and Trust Repair Strategies in Human Agent Teaming},
year = {2021},
isbn = {978-3-030-77771-5},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-77772-2_7},
doi = {10.1007/978-3-030-77772-2_7},
abstract = {Human interactions with computerized systems are shifting from using computers as tools, into collaborating with them as teammates via autonomous capabilities. Modern technological advances will inevitably lead to the integration of autonomous systems and will consequently increase the need for effective human agent teaming (HAT). One of the most paramount ideals human operators must discern is their perception of autonomous agents as equal team members. In order to instill this trust within human operators, it is necessary for HAT missions to apply the proper trust repair strategies after a team member commits a trust violation. Identifying the correct trust repair strategy is critical to advancing HAT and preventing degrading team performance or potential misuse. Based on the current literature, this paper addresses key components necessary for effective trust repair and the numerous variables that can further improve upcoming HAT operations. The impacting factors of HAT trust, trust repair strategies, and needed areas of future research are presented.},
booktitle = {Artificial Intelligence in HCI: Second International Conference, AI-HCI 2021, Held as Part of the 23rd HCI International Conference, HCII 2021, Virtual Event, July 24–29, 2021, Proceedings},
pages = {95–106},
numpages = {12},
keywords = {Human-agent teaming, Trust violations, Trust repair}
}

@article{10.1145/3637229,
author = {Guo, Hanyang and Chen, Yingye and Chen, Xiangping and Huang, Yuan and Zheng, Zibin},
title = {Smart Contract Code Repair Recommendation based on Reinforcement Learning and Multi-metric Optimization},
year = {2024},
issue_date = {May 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {33},
number = {4},
issn = {1049-331X},
url = {https://doi.org/10.1145/3637229},
doi = {10.1145/3637229},
abstract = {A smart contract is a kind of code deployed on the blockchain that executes automatically once an event triggers a clause in the contract. Since smart contracts involve businesses such as asset transfer, they are more vulnerable to attacks, so it is crucial to ensure the security of smart contracts. Because a smart contract cannot be tampered with once deployed on the blockchain, for smart contract developers, it is necessary to fix vulnerabilities before deployment. Compared with many vulnerability detection tools for smart contracts, the amount of automatic fix approaches for smart contracts is relatively limited. These approaches mainly use defined pattern-based methods or heuristic search algorithms for vulnerability repairs. In this article, we propose RLRep, a reinforcement learning-based approach to provide smart contract repair recommendations for smart contract developers automatically. This approach adopts an agent to provide repair action suggestions based on the vulnerable smart contract without any supervision, which can solve the problem of missing labeled data in machine learning-based repair methods. We evaluate our approach on a dataset containing 853 smart contract programs (programming language: Solidity) with different kinds of vulnerabilities. We split them into training and test sets. The result shows that our approach can provide 54.97\% correct repair recommendations for smart contracts.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = apr,
articleno = {106},
numpages = {31},
keywords = {Repair recommendation, smart contract}
}

@inproceedings{10.1145/3524459.3527347,
author = {Ribeiro, Francisco and Abreu, Rui and Saraiva, Jo\~{a}o},
title = {Framing program repair as code completion},
year = {2022},
isbn = {9781450392853},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3524459.3527347},
doi = {10.1145/3524459.3527347},
abstract = {Many techniques have contributed to the advancement of automated program repair, such as: generate and validate approaches, constraint-based solvers and even neural machine translation. Simultaneously, artificial intelligence has allowed the creation of general-purpose pre-trained models that support several down-stream tasks. In this paper, we describe a technique that takes advantage of a generative model --- CodeGPT --- to automatically repair buggy programs by making use of its code completion capabilities. We also elaborate on where to perform code completion in a buggy line and how we circumvent the open-ended nature of code generation to appropriately fit the new code in the original program. Furthermore, we validate our approach on the ManySStuBs4J dataset containing real-world open-source projects and show that our tool is able to fix 1739 programs out of 6415 --- a 27\% repair rate. The repaired programs range from single-line changes to multiple line modifications. In fact, our technique is able to fix programs which were missing relatively complex expressions prior to being analyzed. In the end, we present case studies that showcase different scenarios our technique was able to handle.},
booktitle = {Proceedings of the Third International Workshop on Automated Program Repair},
pages = {38–45},
numpages = {8},
keywords = {code completion, code generation, program repair},
location = {Pittsburgh, Pennsylvania},
series = {APR '22}
}

@inproceedings{10.1145/3715275.3732017,
author = {Hopkins, Aspen and Struckman, Isabella and Klyman, Kevin and Silbey, Susan S.},
title = {Recourse, Repair, Reparation, \&amp; Prevention: A Stakeholder Analysis of AI Supply Chains},
year = {2025},
isbn = {9798400714825},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3715275.3732017},
doi = {10.1145/3715275.3732017},
abstract = {The AI industry is exploding in popularity, with increasing attention to potential harms and unwanted consequences. In the current digital ecosystem, AI deployments are often the product of AI supply chains (AISC): networks of outsourced models, data, and tooling through which multiple entities contribute to AI development and distribution. AI supply chains lack the modularity, redundancies, or conventional supply chain practices that enable identification, isolation, and easy correction of failures, exacerbating the already difficult processes of responding to ML-generated harms. As the stakeholders participating in and impacted by AISCs have scaled and diversified, so too have the risks they face. In this stakeholder analysis of AI supply chains, we consider who participates in AISCs, what harms they face, where sources of harm lie, and how market dynamics and power differentials inform the type and probability of remedies. Because AI supply chains are purposely invented and implemented, they may be designed to account for, rather than ignore, the complexities, consequences, and risks of deploying AI systems. To enable responsible design and management of AISCs, we offer a typology of responses to AISC-induced harms: recourse, repair, reparation or prevention. We apply this typology to stakeholders participating in a health-care AISC across three stylized markets—vertical integration, horizontal integration, free market—to illustrate how stakeholder positioning and power within an AISC may shape responses to an experienced harm.},
booktitle = {Proceedings of the 2025 ACM Conference on Fairness, Accountability, and Transparency},
pages = {209–227},
numpages = {19},
keywords = {AI Supply Chains, AI Value Chains, Stakeholders, Recourse, Reparation, Repair, Redress, AI Safety, AI Harms, Markets, AI Supply Chain Participation},
location = {
},
series = {FAccT '25}
}

@article{10.1155/2024/7135765,
author = {Zhang, Rui and Qiao, Ziyue and Yu, Yong and Vocaturo, Eugenio},
title = {Security Analysis of Large Language Models on API Misuse Programming Repair},
year = {2024},
issue_date = {2024},
publisher = {John Wiley and Sons Ltd.},
address = {GBR},
volume = {2024},
issn = {0884-8173},
url = {https://doi.org/10.1155/2024/7135765},
doi = {10.1155/2024/7135765},
abstract = {Application programming interface (API) misuse refers to misconceptions or carelessness in the anticipated usage of APIs, threatening the software system’s security. Moreover, API misuses demonstrate significant concealment and are challenging to uncover. Recent advancements have explored enhanced LLMs in a variety of software engineering (SE) activities, such as code repair. Nonetheless, the security implications of using LLMs for these purposes remain underexplored, particularly concerning the issue of API misuse. In this paper, we present an empirical study to observe the bug-fixing capabilities of LLMs in addressing API misuse related to monitoring resource management (MRM API misuse). Initially, we propose APImisRepair, a real-world benchmark for repairing MRM API misuse, including buggy programs, corresponding fixed programs, and descriptions of API misuse. Subsequently, we assess the performance of several LLMs using the APImisRepair benchmark. Findings reveal the vulnerabilities of LLMs in repairing MRM API misuse and find several reasons, encompassing factors such as fault localization and a lack of awareness regarding API misuse. Additionally, we have insights on improving LLMs in terms of their ability to fix MRM API misuse and introduce a crafted approach, APImisAP. Experimental results demonstrate that APImisAP exhibits a certain degree of improvement in the security of LLMs.},
journal = {Int. J. Intell. Syst.},
month = jan,
numpages = {18}
}

@article{10.1145/3654441,
author = {Wang, Xu and Yu, Hongwei and Meng, Xiangxin and Cao, Hongliang and Zhang, Hongyu and Sun, Hailong and Liu, Xudong and Hu, Chunming},
title = {MTL-TRANSFER: Leveraging Multi-task Learning and Transferred Knowledge for Improving Fault Localization and Program Repair},
year = {2024},
issue_date = {July 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {33},
number = {6},
issn = {1049-331X},
url = {https://doi.org/10.1145/3654441},
doi = {10.1145/3654441},
abstract = {Fault localization (FL) and automated program repair (APR) are two main tasks of automatic software debugging. Compared with traditional methods, deep learning-based approaches have been demonstrated to achieve better performance in FL and APR tasks. However, the existing deep learning-based FL methods ignore the deep semantic features or only consider simple code representations. And for APR tasks, existing template-based APR methods are weak in selecting the correct fix templates for more effective program repair, which are also not able to synthesize patches via the embedded end-to-end code modification knowledge obtained by training models on large-scale bug-fix code pairs. Moreover, in most of FL and APR methods, the model designs and training phases are performed separately, leading to ineffective sharing of updated parameters and extracted knowledge during the training process. This limitation hinders the further improvement in the performance of FL and APR tasks. To solve the above problems, we propose a novel approach called MTL-TRANSFER, which leverages a multi-task learning strategy to extract deep semantic features and transferred knowledge from different perspectives. First, we construct a large-scale open-source bug datasets and implement 11 multi-task learning models for bug detection and patch generation sub-tasks on 11 commonly used bug types, as well as one multi-classifier to learn the relevant semantics for the subsequent fix template selection task. Second, an MLP-based ranking model is leveraged to fuse spectrum-based, mutation-based and semantic-based features to generate a sorted list of suspicious statements. Third, we combine the patches generated by the neural patch generation sub-task from the multi-task learning strategy with the optimized fix template selecting order gained from the multi-classifier mentioned above. Finally, the more accurate FL results, the optimized fix template selecting order, and the expanded patch candidates are combined together to further enhance the overall performance of APR tasks. Our extensive experiments on widely-used benchmark Defects4J show that MTL-TRANSFER outperforms all baselines in FL and APR tasks, proving the effectiveness of our approach. Compared with our previously proposed FL method TRANSFER-FL (which is also the state-of-the-art statement-level FL method), MTL-TRANSFER increases the faults hit by 8/11/12 on Top-1/3/5 metrics (92/159/183 in total). And on APR tasks, the number of successfully repaired bugs of MTL-TRANSFER under the perfect localization setting reaches 75, which is 8 more than our previous APR method TRANSFER-PR. Furthermore, another experiment to simulate the actual repair scenarios shows that MTL-TRANSFER can successfully repair 15 and 9 more bugs (56 in total) compared with TBar and TRANSFER, which demonstrates the effectiveness of the combination of our optimized FL and APR components.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = jun,
articleno = {148},
numpages = {31},
keywords = {Fault localization, automated program repair, deep neural networks, transfer learning, multi-task learning, neural machine translation}
}

@inproceedings{10.1145/3597926.3598101,
author = {Kim, YoungJae and Han, Seungheon and Khamit, Askar Yeltayuly and Yi, Jooyong},
title = {Automated Program Repair from Fuzzing Perspective},
year = {2023},
isbn = {9798400702211},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597926.3598101},
doi = {10.1145/3597926.3598101},
abstract = {In this work, we present a novel approach that connects two closely-related topics: fuzzing and automated program repair (APR). The paper is divided into two parts. In the first part, we describe the similarities between fuzzing and APR both of which can be viewed as a search problem. In the second part, we introduce a new patch-scheduling algorithm called Casino, which is designed from a fuzzing perspective to enhance search efficiency. Our experiments demonstrate that Casino outperforms existing algorithms. We also promote open science by sharing SimAPR, a simulation tool that can be used to evaluate new patch-scheduling algorithms.},
booktitle = {Proceedings of the 32nd ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {854–866},
numpages = {13},
keywords = {Automated Program Repair, Fuzzing, Multi-Armed Bandit, Patch Scheduling},
location = {Seattle, WA, USA},
series = {ISSTA 2023}
}

@article{10.1145/3705302,
author = {Zhang, Lehuan and Guo, Shikai and Guo, Yi and Li, Hui and Chai, Yu and Chen, Rong and Li, Xiaochen and Jiang, He},
title = {Context-based Transfer Learning for Structuring Fault Localization and Program Repair Automation},
year = {2025},
issue_date = {May 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {34},
number = {4},
issn = {1049-331X},
url = {https://doi.org/10.1145/3705302},
doi = {10.1145/3705302},
abstract = {Automated software debugging plays a crucial role in aiding software developers to swiftly identify and attempt to rectify faults, thereby significantly reducing developers’ workload. Previous researches have predominantly relied on simplistic semantic deep learning or statistical analysis methods to locate faulty statements in diverse projects. However, code repositories often consist of lengthy sequences with long-distance dependencies, posing challenges for accurately modeling fault localization using these methods. In addition, the lack of joint reasoning among various faults prevents existing models from deeply capturing fault information. To address these challenges, we propose a method named CodeHealer to achieve accurate fault localization and program repair. CodeHealer comprises three components: a Deep Semantic Information Extraction Component that effectively extracts deep semantic features from suspicious code statements using classifiers based on Joint-attention mechanisms; a Suspicious Statement Ranking Component that combines various fault localization features and employs multilayer perceptrons to derive multidimensional vectors of suspicion values; and a Fault Repair Component that, based on ranked suspicious statements generated by fault localization, adopts a top-down approach using multiple classifiers based on Co-teaching mechanisms to select repair templates and generate patches. The experimental results indicate that when applied to fault localization, CodeHealer outperforms the best baseline method with improvements of 11.4\%, 2.7\%, and 1.6\% on Top-1/3/5 metrics, respectively. It also reduces the MFR and MAR by 9.8\% and 2.1\%, where lower values denote better fault localization effectiveness. Additionally, in automated software debugging, CodeHealer fixes an additional 6 faults compared to the current best method, totaling 53 faults repaired.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = apr,
articleno = {96},
numpages = {32},
keywords = {Software debugging, Fault Localization, Transfer learning}
}

@inproceedings{10.1109/ICSE48619.2023.00129,
author = {Xia, Chunqiu Steven and Wei, Yuxiang and Zhang, Lingming},
title = {Automated Program Repair in the Era of Large Pre-Trained Language Models},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00129},
doi = {10.1109/ICSE48619.2023.00129},
abstract = {Automated Program Repair (APR) aims to help developers automatically patch software bugs. However, current state-of-the-art traditional and learning-based APR techniques face the problem of limited patch variety, failing to fix complicated bugs. This is mainly due to the reliance on bug-fixing datasets to craft fix templates (traditional) or directly predict potential patches (learning-based). Large Pre-Trained Language Models (LLMs), trained using billions of text/code tokens, can potentially help avoid this issue. Very recently, researchers have directly leveraged LLMs for APR without relying on any bug-fixing datasets. Meanwhile, such existing work either failed to include state-of-the-art LLMs or was not evaluated on realistic datasets. Thus, the true power of modern LLMs on the important APR problem is yet to be revealed.In this work, we perform the first extensive study on directly applying LLMs for APR. We select 9 recent state-of-the-art LLMs, including both generative and infilling models, ranging from 125M to 20B in size. We designed 3 different repair settings to evaluate the different ways we can use LLMs to generate patches: 1) generate the entire patch function, 2) fill in a chunk of code given the prefix and suffix 3) output a single line fix. We apply the LLMs under these repair settings on 5 datasets across 3 different languages and compare different LLMs in the number of bugs fixed, generation speed and compilation rate. We also compare the LLMs against recent state-of-the-art APR tools. Our study demonstrates that directly applying state-of-the-art LLMs can already substantially outperform all existing APR techniques on all our datasets. Among the studied LLMs, the scaling effect exists for APR where larger models tend to achieve better performance. Also, we show for the first time that suffix code after the buggy line (adopted in infilling-style APR) is important in not only generating more fixes but more patches with higher compilation rate. Besides patch generation, the LLMs consider correct patches to be more natural than other ones, and can even be leveraged for effective patch ranking or patch correctness checking. Lastly, we show that LLM-based APR can be further substantially boosted via: 1) increasing the sample size, and 2) incorporating fix template information.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {1482–1494},
numpages = {13},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@article{10.1145/3563327,
author = {Bavishi, Rohan and Joshi, Harshit and Cambronero, Jos\'{e} and Fariha, Anna and Gulwani, Sumit and Le, Vu and Radi\v{c}ek, Ivan and Tiwari, Ashish},
title = {Neurosymbolic repair for low-code formula languages},
year = {2022},
issue_date = {October 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {OOPSLA2},
url = {https://doi.org/10.1145/3563327},
doi = {10.1145/3563327},
abstract = {Most users of low-code platforms, such as Excel and PowerApps, write programs in domain-specific formula languages to carry out nontrivial tasks. Often users can write most of the program they want, but introduce small mistakes that yield broken formulas. These mistakes, which can be both syntactic and semantic, are hard for low-code users to identify and fix, even though they can be resolved with just a few edits. We formalize the problem of producing such edits as the last-mile repair problem. To address this problem, we developed LaMirage, a LAst-MIle RepAir-engine GEnerator that combines symbolic and neural techniques to perform last-mile repair in low-code formula languages. LaMirage takes a grammar and a set of domain-specific constraints/rules, which jointly approximate the target language, and uses these to generate a repair engine that can fix formulas in that language. To tackle the challenges of localizing errors and ranking candidate repairs, LaMirage leverages neural techniques, whereas it relies on symbolic methods to generate candidate edits. This combination allows LaMirage to find repairs that satisfy the provided grammar and constraints, and then pick the most natural repair. We compare LaMirage to state-of-the-art neural and symbolic approaches on 400 real Excel and Power Fx formulas, where LaMirage outperforms all baselines. We release these benchmarks to encourage subsequent work in low-code domains.},
journal = {Proc. ACM Program. Lang.},
month = oct,
articleno = {164},
numpages = {30},
keywords = {Low-Code, Neurosymbolic, Program Repair}
}

@article{10.1016/j.eswa.2023.121823,
author = {Chen, Weijie and Zhang, Tao and Liu, Yuanqing and Tang, Yuxiu},
title = {Exploring the microscopic mechanism of credit repair knowledge dissemination: A complex network-based approach},
year = {2024},
issue_date = {Mar 2024},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {238},
number = {PC},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2023.121823},
doi = {10.1016/j.eswa.2023.121823},
journal = {Expert Syst. Appl.},
month = mar,
numpages = {13},
keywords = {Credit repair, Knowledge dissemination, Evolutionary dynamics, Complex network, Agent-based model}
}

@article{10.1145/3703408,
author = {Murali, Pavithra Sripathanallur},
title = {Leveraging Large Language Models for Automated Program Repair in Programming Education},
year = {2025},
issue_date = {Winter 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {31},
number = {2},
issn = {1528-4972},
url = {https://doi.org/10.1145/3703408},
doi = {10.1145/3703408},
journal = {XRDS},
month = jan,
pages = {58–60},
numpages = {3}
}

@inproceedings{10.1007/978-3-031-72244-8_4,
author = {Fel, Leo},
title = {A Framework for Voters’ Trust Repair in Internet Voting},
year = {2024},
isbn = {978-3-031-72243-1},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-72244-8_4},
doi = {10.1007/978-3-031-72244-8_4},
abstract = {Research on voters’ trust in i-voting has been exclusively related to building trust in the process of i-voting adoption, with no work addressing the question of trust repair. This article introduces a framework for trust repair in i-voting by integrating insights from trust repair in other research areas, as well as concepts developed for and used in the e-voting literature. The article traces the process of trust repair from the different beliefs influencing voters’ trust in both the human and technological dimensions of an i-voting system, through the influence of the internal and external stakeholders, to trust violations and the i-voting organisers’ strategies for trust repair and the ‘arsenal’ of measures at their disposal. The article highlights the importance of detecting the emergence of events that may violate trust among voters, understanding the severity and dimensions of trust violation, and strategically navigating trust repair. It also outlines open questions and identifies avenues for future research.},
booktitle = {Electronic Voting: 9th International Joint Conference, E-Vote-ID 2024, Tarragona, Spain, October 2–4, 2024, Proceedings},
pages = {54–71},
numpages = {18},
keywords = {i-voting, voters, trust, trust repair, framework},
location = {Tarragona, Spain}
}

@inproceedings{10.1109/ICSE48619.2023.00112,
author = {Parasaram, Nikhil and Barr, Earl T. and Mechtaev, Sergey},
title = {Rete: Learning Namespace Representation for Program Repair},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00112},
doi = {10.1109/ICSE48619.2023.00112},
abstract = {A key challenge of automated program repair is finding correct patches in the vast search space of candidate patches. Real-world programs define large namespaces of variables that considerably contributes to the search space explosion. Existing program repair approaches neglect information about the program namespace, which makes them inefficient and increases the chance of test-overfitting. We propose Rete, a new program repair technique, that learns project-independent information about program namespace and uses it to navigate the search space of patches. Rete uses a neural network to extract project-independent information about variable CDU chains, defuse chains augmented with control flow. Then, it ranks patches by jointly ranking variables and the patch templates into which the variables are inserted. We evaluated Rete on 142 bugs extracted from two datasets, ManyBugs and BugsInPy. Our experiments demonstrate that Rete generates six new correct patches that fix bugs that previous tools did not repair, an improvement of 31\% and 59\% over the existing state of the art.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {1264–1276},
numpages = {13},
keywords = {program repair, deep learning, patch prioritisation, variable representation},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1145/3756681.3757021,
author = {Bucaioni, Alessio and Gualandi, Gabriele and Toma, Johan},
title = {Benchmarking Large Language Models for Autonomous Run-time Error Repair: Toward Self-Healing Software Systems},
year = {2025},
isbn = {9798400713859},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3756681.3757021},
doi = {10.1145/3756681.3757021},
abstract = {As software systems grow in complexity and become integral to daily operations, traditional approaches to software testing, maintenance, and evolution are increasingly inadequate. Recent advances in artificial intelligence, particularly in large language models, offer promising avenues for achieving self-healing software—software capable of autonomously detecting, diagnosing, and repairing faults without human intervention. However, while much of the existing literature focuses on on code repair of vulnerabilities or repository-level bugs, the application of large language models for autonomously repairing run-time errors—which require dynamic analysis and execution context awareness—remains largely uncharted.In this study, we empirically benchmark ten distinct large language models—ChatGPT-4o, ChatGPT-4o-mini, Claude 3.5 Sonnet, Claude 3.5 Haiku, Gemini 1.5 Flash, Llama 3.2, Mistral Nemo, Grok Beta, Command R+, and Jamba 1.5 Large—to assess their ability to repair run-time errors in code. We conducted our evaluation on a dataset of 76 programming problems manually sourced from Leetcode, implemented in C++ (48 problems) and Java (28 problems). Each model was provided with a single opportunity to generate a corrected solution, which was then evaluated based on its ability to pass all associated test cases.Our experimental results provide early empirical evidence of the potential of large language models to drive a paradigm shift in artificial intelligence-driven software engineering. The findings reveal that while certain large language models demonstrate strong code-fixing capabilities, others struggle, highlighting significant performance disparities across models. This work not only fills a critical gap in empirical software engineering, but also opens avenues for refining artificial intelligence-driven software engineering, particularly for self-healing software.},
booktitle = {Proceedings of the 29th International Conference on Evaluation and Assessment in Software Engineering},
pages = {641–646},
numpages = {6},
keywords = {Self-healing code, code repair, large language models, benchmarking},
location = {
},
series = {EASE '25}
}

@inproceedings{10.1145/3597503.3623337,
author = {Ye, He and Monperrus, Martin},
title = {ITER: Iterative Neural Repair for Multi-Location Patches},
year = {2024},
isbn = {9798400702174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597503.3623337},
doi = {10.1145/3597503.3623337},
abstract = {Automated program repair (APR) has achieved promising results, especially using neural networks. Yet, the overwhelming majority of patches produced by APR tools are confined to one single location. When looking at the patches produced with neural repair, most of them fail to compile, while a few uncompilable ones go in the right direction. In both cases, the fundamental problem is to ignore the potential of partial patches. In this paper, we propose an iterative program repair paradigm called ITER founded on the concept of improving partial patches until they become plausible and correct. First, ITER iteratively improves partial single-location patches by fixing compilation errors and further refining the previously generated code. Second, ITER iteratively improves partial patches to construct multi-location patches, with fault localization re-execution. ITER is implemented for Java based on battle-proven deep neural networks and code representation. ITER is evaluated on 476 bugs from 10 open-source projects in Defects4J 2.0. ITER succeeds in repairing 15.5\% of them, including 9 uniquely repaired multi-location bugs.},
booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
articleno = {10},
numpages = {13},
location = {Lisbon, Portugal},
series = {ICSE '24}
}

@article{10.1145/3591238,
author = {Tao, Zhe and Nawas, Stephanie and Mitchell, Jacqueline and Thakur, Aditya V.},
title = {Architecture-Preserving Provable Repair of Deep Neural Networks},
year = {2023},
issue_date = {June 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {PLDI},
url = {https://doi.org/10.1145/3591238},
doi = {10.1145/3591238},
abstract = {Deep neural networks (DNNs) are becoming increasingly important components   of software, and are considered the state-of-the-art solution for a number   of problems, such as image recognition. However, DNNs are far from   infallible, and incorrect behavior of DNNs can have disastrous real-world   consequences. This paper addresses the problem of architecture-preserving   V-polytope provable repair of DNNs.   A V-polytope defines a convex bounded polytope using its vertex representation.   V-polytope provable repair guarantees that the repaired DNN   satisfies the given specification on the infinite set of points in the given V-polytope.  An architecture-preserving repair only modifies the parameters of the DNN, without  modifying its architecture. The repair has the flexibility to   modify multiple layers of the DNN, and runs in polynomial time.   It supports DNNs with activation functions that have some linear pieces,   as well as fully-connected, convolutional, pooling and residual layers.   To the best our knowledge, this is the first provable repair approach that   has all of these features.   We implement our approach in a tool called APRNN. Using   MNIST, ImageNet, and ACAS Xu DNNs, we show that   it has better efficiency, scalability, and generalization   compared to PRDNN and REASSURE, prior provable repair methods that are   not architecture preserving.},
journal = {Proc. ACM Program. Lang.},
month = jun,
articleno = {124},
numpages = {25},
keywords = {Bug fixing, Deep Neural Networks, Repair, Synthesis}
}

@article{10.1145/3720510,
author = {Wang, Ruixin and Zhao, Zhongkai and Fang, Le and Jiang, Nan and Lou, Yiling and Tan, Lin and Zhang, Tianyi},
title = {Show Me Why It’s Correct: Saving 1/3 of Debugging Time in Program Repair with Interactive Runtime Comparison},
year = {2025},
issue_date = {April 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {OOPSLA1},
url = {https://doi.org/10.1145/3720510},
doi = {10.1145/3720510},
abstract = {Automated Program Repair (APR) holds the promise of alleviating the burden of debugging and fixing software bugs. Despite this, developers still need to manually inspect each patch to confirm its correctness, which is tedious and time-consuming. This challenge is exacerbated in the presence of plausible patches, which accidentally pass test cases but may not correctly fix the bug. To address this challenge, we propose an interactive approach called iFix to facilitate patch understanding and comparison based on their runtime difference. iFix performs static analysis to identify runtime variables related to the buggy statement and captures their runtime values during execution for each patch. These values are then aligned across different patch candidates, allowing users to compare and contrast their runtime behavior. To evaluate iFix, we conducted a within-subjects user study with 28 participants. Compared with manual inspection and a state-of-the-art interactive patch filtering technique, iFix reduced participants’ task completion time by 36\% and 33\% while also improving their confidence by 50\% and 20\%, respectively. Besides, quantitative experiments demonstrate that iFix improves the ranking of correct patches by at least 39\% compared with other patch ranking methods and is generalizable to different APR tools.},
journal = {Proc. ACM Program. Lang.},
month = apr,
articleno = {145},
numpages = {27},
keywords = {Automatic Program Repair, Interaction Support, User Trust}
}

@inproceedings{10.1109/ICSE55347.2025.00089,
author = {Yang, Aidan Z. H. and Kolak, Sophia and Hellendoorn, Vincent and Martins, Ruben and Goues, Claire Le},
title = {Revisiting Unnaturalness for Automated Program Repair in the Era of Large Language Models},
year = {2025},
isbn = {9798331505691},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE55347.2025.00089},
doi = {10.1109/ICSE55347.2025.00089},
abstract = {The problem of software quality has motivated the development of a variety of techniques for Automatic Program Repair (APR). Meanwhile, recent advances in AI and Large Language Models (LLMs) have produced orders of magnitude performance improvements over previous code generation techniques, affording promising opportunities for program repair and its constituent subproblems (e.g., fault localization, patch generation). Because models are trained on large volumes of code in which defects are relatively rare, they tend to both simultaneously perceive faulty code as unlikely (or "unnatural") and to produce generally correct code (which is more "natural"). This paper comprehensively revisits the idea of (un)naturalness for program repair. We argue that, fundamentally, LLMs can only go so far on their own in reasoning about and fixing buggy code. This motivates the incorporation of traditional tools, which compress useful contextual and analysis information, as a complement to LLMs for repair. We interrogate the role of entropy at every stage of traditional repair, and show that it is indeed usefully complementary to classic techniques. We show that combining measures of naturalness with class Spectrum-Based Fault Localization (SBFL) approaches improves Top-5 scoring by 50\% over SBFL alone. We show that entropy delta, or change in entropy induced by a candidate patch, can improve patch generation efficiency by 24 test suite executions per repair, on average, on our dataset. Finally, we show compelling results that entropy delta for patch classification is highly effective at distinguishing correct from overfitting patches. Overall, our results suggest that LLMs can effectively complement classic techniques for analysis and transformation, producing more efficient and effective automated repair techniques overall.},
booktitle = {Proceedings of the IEEE/ACM 47th International Conference on Software Engineering},
pages = {2561–2573},
numpages = {13},
keywords = {program repair, deep learning, large language models},
location = {Ottawa, Ontario, Canada},
series = {ICSE '25}
}

@article{10.1287/mnsc.2022.4401,
author = {Jin, Chen and Yang, Luyi and Zhu, Cungen},
title = {Right to Repair: Pricing, Welfare, and Environmental Implications},
year = {2023},
issue_date = {February 2023},
publisher = {INFORMS},
address = {Linthicum, MD, USA},
volume = {69},
number = {2},
issn = {0025-1909},
url = {https://doi.org/10.1287/mnsc.2022.4401},
doi = {10.1287/mnsc.2022.4401},
abstract = {The “right-to-repair” (RTR) movement calls for government legislation that requires manufacturers to provide repair information, tools, and parts so that consumers can independently repair their own products with more ease. The initiative has gained global traction in recent years. Repair advocates argue that such legislation would break manufacturers’ monopoly on the repair market and benefit consumers. They further contend that it would reduce the environmental impact by reducing e-waste and new production. Yet the RTR legislation may also trigger a price response in the product market as manufacturers try to mitigate the profit loss. This paper employs an analytical model to study the pricing, welfare, and environmental implications of RTR. We find that, as the RTR legislation continually lowers the independent repair cost, manufacturers may initially cut the new product price and then raise it. This nonmonotone price adjustment may further induce a nonmonotone change in consumer surplus, social welfare, and the environmental impact. Strikingly, the RTR legislation can potentially lead to a lose–lose–lose outcome that compromises manufacturer profit, reduces consumer surplus, and increases the environmental impact despite repair being made easier and more affordable.This paper was accepted by Charles Corbett, operations management.Funding: Chen Jin gratefully acknowledges the Singapore Ministry of Education Academic Research Fund [Tier 1, Grant R-253-000-144-133].Supplemental Material: The online appendix is available at .},
journal = {Manage. Sci.},
month = feb,
pages = {1017–1036},
numpages = {20},
keywords = {sustainable operations, repair, durable goods, after-sales service, pricing, extended producer responsibility, consumer surplus, environmental impact}
}

@inproceedings{10.1145/3611643.3616256,
author = {Wang, Weishi and Wang, Yue and Joty, Shafiq and Hoi, Steven C.H.},
title = {RAP-Gen: Retrieval-Augmented Patch Generation with CodeT5 for Automatic Program Repair},
year = {2023},
isbn = {9798400703270},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3611643.3616256},
doi = {10.1145/3611643.3616256},
abstract = {Automatic program repair (APR) is crucial to reduce manual debugging efforts for developers and improve software reliability. While conventional search-based techniques typically rely on heuristic rules or a redundancy assumption to mine fix patterns, recent years have witnessed the surge of deep learning (DL) based approaches to automate the program repair process in a data-driven manner. However, their performance is often limited by a fixed set of parameters to model the highly complex search space of APR. To ease such burden on the parametric models, in this work, we propose a novel Retrieval-Augmented Patch Generation framework (RAP-Gen) by explicitly leveraging relevant fix patterns retrieved from a codebase of previous bug-fix pairs. Specifically, we build a hybrid patch retriever to account for both lexical and semantic matching based on the raw source code in a language-agnostic manner, which does not rely on any code-specific features. In addition, we adapt a code-aware language model CodeT5 as our foundation model to facilitate both patch retrieval and generation tasks in a unified manner. We adopt a stage-wise approach where the patch retriever first retrieves a relevant external bug-fix pair to augment the buggy input for the CodeT5 patch generator, which synthesizes a ranked list of repair patch candidates. Notably, RAP-Gen is a generic APR framework that can flexibly integrate different patch retrievers and generators to repair various types of bugs. We thoroughly evaluate RAP-Gen on three benchmarks in two programming languages, including the TFix benchmark in JavaScript, and Code Refinement and Defects4J benchmarks in Java, where the bug localization information may or may not be provided. Experimental results show that RAP-Gen significantly outperforms previous state-of-the-art (SoTA) approaches on all benchmarks, e.g., boosting the accuracy of T5-large on TFix from 49.70\% to 54.15\% (repairing 478 more bugs) and repairing 15 more bugs on 818 Defects4J bugs. Further analysis reveals that our patch retriever can search for relevant fix patterns to guide the APR systems.},
booktitle = {Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {146–158},
numpages = {13},
keywords = {Automated program repair, Neural networks, Pretrained language models, Retrieval-augmented generation},
location = {San Francisco, CA, USA},
series = {ESEC/FSE 2023}
}

@inproceedings{10.1145/3691620.3695602,
author = {Kim, YoungJae and Park, Yechan and Han, Seungheon and Yi, Jooyong},
title = {Enhancing the Efficiency of Automated Program Repair via Greybox Analysis},
year = {2024},
isbn = {9798400712487},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3691620.3695602},
doi = {10.1145/3691620.3695602},
abstract = {In this paper, we pay attention to the efficiency of automated program repair (APR). Recently, an efficient patch scheduling algorithm, Casino, has been proposed to improve APR efficiency. Inspired by fuzzing, Casino adaptively chooses the next patch candidate to evaluate based on the results of previous evaluations. However, we observe that Casino utilizes only the test results, treating the patched program as a black box. Inspired by greybox fuzzing, we propose a novel patch-scheduling algorithm, Gresino, which leverages the internal state of the program to further enhance APR efficiency. Specifically, Gresino monitors the hit counts of branches observed during the execution of the program and uses them to guide the search for a valid patch. Our experimental evaluation on the Defects4J benchmark and eight APR tools demonstrates the efficacy of our approach.},
booktitle = {Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering},
pages = {1719–1731},
numpages = {13},
keywords = {automated program repair, patch scheduling, greybox analysis},
location = {Sacramento, CA, USA},
series = {ASE '24}
}

@article{10.1016/j.cie.2025.111256,
author = {Lv, Lingling and Fan, Jiaxin and Zhang, Chunjiang and Shen, Weiming},
title = {Schedule repair for flexible job shops under machine breakdowns by deep reinforcement learning},
year = {2025},
issue_date = {Sep 2025},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {207},
number = {C},
issn = {0360-8352},
url = {https://doi.org/10.1016/j.cie.2025.111256},
doi = {10.1016/j.cie.2025.111256},
journal = {Comput. Ind. Eng.},
month = sep,
numpages = {11},
keywords = {Flexible job shop rescheduling, Deep reinforcement learning (DRL), Machine breakdowns, Graph attention network (GAT)}
}

@inproceedings{10.1145/3533767.3534219,
author = {Yuan, Wei and Zhang, Quanjun and He, Tieke and Fang, Chunrong and Hung, Nguyen Quoc Viet and Hao, Xiaodong and Yin, Hongzhi},
title = {CIRCLE: continual repair across programming languages},
year = {2022},
isbn = {9781450393799},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3533767.3534219},
doi = {10.1145/3533767.3534219},
abstract = {Automatic Program Repair (APR) aims at fixing buggy source code with less manual debugging efforts, which plays a vital role in improving software reliability and development productivity. Recent APR works have achieved remarkable progress via applying deep learning (DL), particularly neural machine translation (NMT) techniques. However, we observe that existing DL-based APR models suffer from at least two severe drawbacks: (1) Most of them can only generate patches for a single programming language, as a result, to repair multiple languages, we have to build and train many repairing models. (2) Most of them are developed offline. Therefore, they won’t function when there are new-coming requirements. To address the above problems, a T5-based APR framework equipped with continual learning ability across multiple programming languages is proposed, namely ContInual Repair aCross Programming LanguagEs (CIRCLE). Specifically, (1) CIRCLE utilizes a prompting function to narrow the gap between natural language processing (NLP) pre-trained tasks and APR. (2) CIRCLE adopts a difficulty-based rehearsal strategy to achieve lifelong learning for APR without access to the full historical data. (3) An elastic regularization method is employed to strengthen CIRCLE’s continual learning ability further, preventing it from catastrophic forgetting. (4) CIRCLE applies a simple but effective re-repairing method to revise generated errors caused by crossing multiple programming languages. We train CIRCLE for four languages (i.e., C, JAVA, JavaScript, and Python) and evaluate it on five commonly used benchmarks. The experimental results demonstrate that CIRCLE not only effectively and efficiently repairs multiple programming languages in continual learning settings, but also achieves state-of-the-art performance (e.g., fixes 64 Defects4J bugs) with a single repair model.},
booktitle = {Proceedings of the 31st ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {678–690},
numpages = {13},
keywords = {AI and Software Engineering, Automatic Program Repair, Lifelong Learning, Neural Machine Translation},
location = {Virtual, South Korea},
series = {ISSTA 2022}
}

@inproceedings{10.1007/978-981-96-4566-4_13,
author = {Gong, Huimin and Shen, Zongliang and Zhang, Hua and Qiao, Lei and Wang, Huawei and Zhang, Chi},
title = {Construction of an AI Code Defect Detection and Repair Dataset Based on Chain of Thought},
year = {2025},
isbn = {978-981-96-4565-7},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-981-96-4566-4_13},
doi = {10.1007/978-981-96-4566-4_13},
abstract = {When detecting and repairing code defects, enhancing the generalization ability and detection accuracy of models is a key challenge. This paper proposes a data fine-tuning method based on Chain of Thought (CoT) fine-tuning to improve the capabilities of models on defect detection in AI code. We constructed a dataset that includes the CrossVul dataset and a manually created dataset of AI code defects and repairs, improving data quality through techniques like context free removal. In the experiments, we used the Codeshell-7B, Qwencoder2.5-7B and Llama3.1-7B as the base and trained them using LoRA fine-tuning techniques. We compared different datasets and training methods to verify the model's effectiveness in detecting and repairing AI code defects. The results show that the CoT fine-tuning model outperforms models without CoT fine-tuning in all aspects of handling code defect tasks. Additionally, the specialized dataset we created for AI code defects and repairs significantly enhances the model's accuracy and repair rate in AI code detection. Our experiments highlight the importance of constructing targeted datasets for AI code defects and employing CoT fine-tuning strategies in improving code defect detection.},
booktitle = {Machine Learning for Cyber Security: 6th International Conference, ML4CS 2024, Hangzhou, China, December 27–29, 2024, Proceedings},
pages = {184–196},
numpages = {13},
keywords = {Chain of Thought, Code defect detection, Dataset construction, AI code, Fine-tuning},
location = {Hangzhou, China}
}

@inproceedings{10.1109/ICSE48619.2023.00126,
author = {Zhu, Qihao and Sun, Zeyu and Zhang, Wenjie and Xiong, Yingfei and Zhang, Lu},
title = {Tare: Type-Aware Neural Program Repair},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00126},
doi = {10.1109/ICSE48619.2023.00126},
abstract = {Automated program repair (APR) aims to reduce the effort of software development. With the development of deep learning, lots of DL-based APR approaches have been proposed using an encoder-decoder architecture. Despite the promising performance, these models share the same limitation: generating lots of untypable patches. The main reason for this phenomenon is that the existing models do not consider the constraints of code captured by a set of typing rules.In this paper, we propose, Tare, a type-aware model for neural program repair to learn the typing rules. To encode an individual typing rule, we introduce three novel components: (1) a novel type of grammars, T-Grammar, that integrates the type information into a standard grammar, (2) a novel representation of code, T-Graph, that integrates the key information needed for type checking an AST, and (3) a novel type-aware neural program repair approach, Tare, that encodes the T-Graph and generates the patches guided by T-Grammar.The experiment was conducted on three benchmarks, 393 bugs from Defects4J v1.2, 444 additional bugs from Defects4J v2.0, and 40 bugs from QuixBugs. Our results show that Tare repairs 62, 32, and 27 bugs on these benchmarks respectively, and outperforms the existing APR approaches on all benchmarks. Further analysis also shows that Tare tends to generate more compilable patches than the existing DL-based APR approaches with the typing rule information.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {1443–1445},
numpages = {3},
keywords = {program repair, neural networks},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1145/3640543.3645198,
author = {Qian, Crystal and Wexler, James},
title = {Take It, Leave It, or Fix It: Measuring Productivity and Trust in Human-AI Collaboration},
year = {2024},
isbn = {9798400705083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3640543.3645198},
doi = {10.1145/3640543.3645198},
abstract = {Although recent developments in generative AI have greatly enhanced the capabilities of conversational agents such as Google’s Bard or OpenAI’s ChatGPT, it’s unclear whether the usage of these agents aids users across various contexts. To better understand how access to conversational AI affects productivity and trust, we conducted a mixed-methods, task-based user study, observing 76 software engineers (N=76) as they completed a programming exam with and without access to Bard. Effects on performance, efficiency, satisfaction, and trust vary depending on user expertise, question type (open-ended "solve" questions vs. definitive "search" questions), and measurement type (demonstrated vs. self-reported). Our findings include evidence of automation complacency, increased reliance on the AI over the course of the task, and increased performance for novices on “solve”-type questions when using the AI. We discuss common behaviors, design recommendations, and impact considerations to improve collaborations with conversational AI.},
booktitle = {Proceedings of the 29th International Conference on Intelligent User Interfaces},
pages = {370–384},
numpages = {15},
location = {Greenville, SC, USA},
series = {IUI '24}
}

@inproceedings{10.1007/978-3-031-72344-5_27,
author = {Xu, Zhenyu and Sheng, Victor S.},
title = {Towards Minimal Edits in&nbsp;Automated Program Repair: A Hybrid Framework Integrating Graph Neural Networks and&nbsp;Large Language Models},
year = {2024},
isbn = {978-3-031-72343-8},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-72344-5_27},
doi = {10.1007/978-3-031-72344-5_27},
abstract = {Large Language Models trained on Code (LLMCs) have&nbsp;been shown to be effective in Automated Program Repair (APR) tasks, introducing new innovations to the field. Typically, LLMCs do&nbsp;not engage in error localization for APR tasks, instead treating&nbsp;APR more as a code refinement task. This approach often results&nbsp;in larger edit distances, altering the program’s original structure. The principle of making minimal edits is crucial in certain scenarios, such as when correcting student programming assignments or software group development, where it’s better to preserve&nbsp;the original intent of the code with as few changes as possible.&nbsp;To address these challenges, we introduce a hybrid framework&nbsp;for automated program repair that combines graph neural networks&nbsp;and large language models, which we refer to as HFRepair. HFRepair leverages the precise error localization capability of DrRepair&nbsp;for C programs, combining it with LLMCs to perform the line-level&nbsp;APR task based on the code context, aiming for minimal edits.&nbsp;Our experimental results demonstrate that HFRepair significantly outperforms previous state-of-the-art methods in benchmark tests. For instance, on the DeepFix dataset, HFRepair improves the&nbsp;full repair rate from 67.9\% and 71.4\% (achieved by DrRepair and BIFI, respectively) to 82.2\%, while reducing average edit distance&nbsp;from 33.4 and 27.7 to 11.6.},
booktitle = {Artificial Neural Networks and Machine Learning – ICANN 2024: 33rd International Conference on Artificial Neural Networks, Lugano, Switzerland, September 17–20, 2024, Proceedings, Part V},
pages = {402–416},
numpages = {15},
keywords = {Automated Program Repair, Graph Neural Networks, Large Language Models},
location = {Lugano, Switzerland}
}

@inproceedings{10.1007/978-3-031-36272-9_74,
author = {Koutcheme, Charles and Sarsa, Sami and Leinonen, Juho and Hellas, Arto and Denny, Paul},
title = {Automated Program Repair Using Generative Models for&nbsp;Code Infilling},
year = {2023},
isbn = {978-3-031-36271-2},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-36272-9_74},
doi = {10.1007/978-3-031-36272-9_74},
abstract = {In educational settings, automated program repair techniques serve as a feedback mechanism to guide students working on their programming assignments. Recent work has investigated using large language models (LLMs) for program repair. In this area, most of the attention has been focused on using proprietary systems accessible through APIs. However, the limited access and control over these systems remain a block to their adoption and usage in education. The present work studies the repairing capabilities of open large language models. In particular, we focus on a recent family of generative models, which, on top of standard left-to-right program synthesis, can also predict missing spans of code at any position in a program. We experiment with one of these models on four programming datasets and show that we can obtain good repair performance even without additional training.},
booktitle = {Artificial Intelligence in Education: 24th International Conference, AIED 2023, Tokyo, Japan, July 3–7, 2023, Proceedings},
pages = {798–803},
numpages = {6},
keywords = {Program repair, Large Language Models, Computer Science Education},
location = {Tokyo, Japan}
}

@article{10.1145/3563330,
author = {Sakkas, Georgios and Endres, Madeline and Guo, Philip J. and Weimer, Westley and Jhala, Ranjit},
title = {Seq2Parse: neurosymbolic parse error repair},
year = {2022},
issue_date = {October 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {OOPSLA2},
url = {https://doi.org/10.1145/3563330},
doi = {10.1145/3563330},
abstract = {We present Seq2Parse, a language-agnostic neurosymbolic approach to automatically repairing parse errors. Seq2Parse is based on the insight that Symbolic Error Correcting (EC) Parsers can, in principle, synthesize repairs, but, in practice, are overwhelmed by the many error-correction rules that are not relevant to the particular program that requires repair. In contrast, Neural approaches are fooled by the large space of possible sequence level edits, but can precisely pinpoint the set of EC-rules that are relevant to a particular program. We show how to combine their complementary strengths by using neural methods to train a sequence classifier that predicts the small set of relevant EC-rules for an ill-parsed program, after which, the symbolic EC-parsing algorithm can make short work of generating useful repairs. We train and evaluate our method on a dataset of 1,100,000 Python programs, and show that Seq2Parse is accurate and efficient: it can parse 94\% of our tests within 2.1 seconds, while generating the exact user fix in 1 out 3 of the cases; and useful: humans perceive both Seq2Parse-generated error locations and repairs to be almost as good as human-generated ones in a statistically-significant manner.},
journal = {Proc. ACM Program. Lang.},
month = oct,
articleno = {167},
numpages = {27},
keywords = {Automated Program Repair, Error-Correcting Parsers, Machine Learning}
}

@inproceedings{10.1145/3611643.3616341,
author = {So, Sunbeom and Oh, Hakjoo},
title = {SmartFix: Fixing Vulnerable Smart Contracts by Accelerating Generate-and-Verify Repair using Statistical Models},
year = {2023},
isbn = {9798400703270},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3611643.3616341},
doi = {10.1145/3611643.3616341},
abstract = {We present SmartFix, a new technique for repairing vulnerable smart contracts. There is an urgent need to develop automatic bug-repair techniques for smart contracts, as smart contracts are safety-critical software and manual debugging is burdensome and error-prone. While several repair approaches have been proposed recently, they are unsatisfactory since no existing techniques can achieve high repairability, full automation, and safety guarantee at the same time, posing significant problems for practical use. SmartFix aims to address these shortcomings by using a “generate-and-verify” approach that iteratively enumerates candidate patches while validating their correctness by invoking a safety verifier. However, in this approach, a technical challenge arises as the search space is huge and the verification-based patch validation is expensive. To address this challenge, we present a novel technique for accelerating the generate-and-verify repair procedure using statistical models derived from the verifier’s feedback. Experimental results on real-world Ethereum smart contracts show that SmartFix is able to achieve a fix success rate of 94.8\% for critical classes of vulnerabilities, far outperforming sGuard, the existing state-of-the-art technique whose success rate is 65.4\%.},
booktitle = {Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {185–197},
numpages = {13},
keywords = {generate-and-verify repair, smart contract, statistical model},
location = {San Francisco, CA, USA},
series = {ESEC/FSE 2023}
}

@inproceedings{10.1109/ASE56229.2023.00063,
author = {Zhang, Quanjun and Fang, Chunrong and Zhang, Tongke and Yu, Bowen and Sun, Weisong and Chen, Zhenyu},
title = {Gamma: Revisiting Template-based Automated Program Repair via Mask Prediction},
year = {2024},
isbn = {9798350329964},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASE56229.2023.00063},
doi = {10.1109/ASE56229.2023.00063},
abstract = {Automated program repair (APR) aims to fix software bugs without manual debugging efforts and plays a crucial role in software development and maintenance. Template-based APR has been widely investigated and shown promising results. However, it is challenging for template-based APR to select the appropriate donor code, which is an important repair ingredient for generating candidate patches. Inappropriate donor code may cause plausible but incorrect patch generation even with correct fix patterns, limiting the repair performance.In this paper, we aim to revisit template-based APR, and propose Gamma, to directly leverage large pre-trained language models for donor code generation. Our main insight is that instead of retrieving donor code in the local buggy file, we can directly predict the correct code tokens based on the context code snippets and repair patterns by a cloze task. Specifically, (1) Gamma revises a variety of fix templates from state-of-the-art template-based APR techniques (i.e., TBar) and transforms them into mask patterns. (2) Gamma adopts a pre-trained language model to predict the correct code for masked code as a fill-in-the-blank task. Although our idea is general and can be built on various existing pre-trained language models, we have implemented Gamma as a practical APR tool based on the recent UniXcoder model. The experimental results demonstrate that Gamma correctly repairs 82 bugs on Defects4J-v1.2, which achieves 20.59\% (14 bugs) and 26.15\% (17 bugs) improvement over the previous state-of-the-art template-based approach TBar and learning-based one Recoder. Furthermore, Gamma repairs 45 bugs and 22 bugs from the additional Defects4J-v2.0 and QuixBugs, indicating the generalizability of Gamma in addressing the dataset overfitting issue. We also prove that adopting other pre-trained language models can provide substantial advancement, e.g., CodeBERT-based and ChatGPT-based Gamma is able to fix 80 and 67 bugs on Defects4J-v1.2, indicating the scalability of Gamma. Overall, our study highlights the promising future of adopting pre-trained models to generate correct patches on top of fix patterns in practice.},
booktitle = {Proceedings of the 38th IEEE/ACM International Conference on Automated Software Engineering},
pages = {535–547},
numpages = {13},
keywords = {automated program repair, fix pattern, pre-trained model, LLM4SE},
location = {Echternach, Luxembourg},
series = {ASE '23}
}

@inproceedings{10.1145/3650212.3680326,
author = {Xie, Linna and Li, Chongmin and Pei, Yu and Zhang, Tian and Pan, Minxue},
title = {BRAFAR: Bidirectional Refactoring, Alignment, Fault Localization, and Repair for Programming Assignments},
year = {2024},
isbn = {9798400706127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3650212.3680326},
doi = {10.1145/3650212.3680326},
abstract = {The problem of automated feedback generation for introductory programming assignments (IPAs) has attracted significant attention with the increasing demand for programming education. While existing approaches, like Refactory, that employ the ”block-by-block” repair strategy have produced promising results, they suffer from two limitations. 		First, Refactory randomly applies refactoring and mutation operations to correct and buggy programs, respectively, to align their control-flow structures (CFSs), which, however, has a relatively low success rate and often complicates the original repairing tasks.                                                                            		Second, Refactory generates repairs for each basic block of the buggy program when its semantics differs from the counterpart in the correct program, which, however, ignores the different roles that basic blocks play in the programs and often produces unnecessary repairs.                                                              		To overcome these limitations, we propose the Brafar approach to feedback generation for IPAs.                                                                         		The core innovation of Brafar lies in its novel bidirectional refactoring algorithm and coarse-to-fine fault localization.                                            		The former aligns the CFSs of buggy and correct programs by applying semantics-preserving refactoring operations to both programs in a guided manner,                       		while the latter identifies basic blocks that truly need repairs based on the semantics of their enclosing statements and themselves.                                     		In our experimental evaluation on 1783 real-life incorrect student submissions from a publicly available dataset, Brafar significantly outperformed Refactory and Clara, generating correct repairs for more incorrect programs with smaller patch sizes in a shorter time.},
booktitle = {Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {856–868},
numpages = {13},
keywords = {Program Repair, Programming Education, Software Refactoring},
location = {Vienna, Austria},
series = {ISSTA 2024}
}

@inproceedings{10.1109/ICSE48619.2023.00125,
author = {Jiang, Nan and Liu, Kevin and Lutellier, Thibaud and Tan, Lin},
title = {Impact of Code Language Models on Automated Program Repair},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00125},
doi = {10.1109/ICSE48619.2023.00125},
abstract = {Automated program repair (APR) aims to help developers improve software reliability by generating patches for buggy programs. Although many code language models (CLM) are developed and effective in many software tasks such as code completion, there has been little comprehensive, in-depth work to evaluate CLMs' fixing capabilities and to fine-tune CLMs for the APR task.Firstly, this work is the first to evaluate ten CLMs on four APR benchmarks, which shows that surprisingly, the best CLM, as is, fixes 72\% more bugs than the state-of-the-art deep-learning (DL)-based APR techniques. Secondly, one of the four APR benchmarks was created by us in this paper to avoid data leaking for a fair evaluation. Thirdly, it is the first work to fine-tune CLMs with APR training data, which shows that fine-tuning brings 31\%--1,267\% improvement to CLMs and enables them to fix 46\%--164\% more bugs than existing DL-based APR techniques. Fourthly, this work studies the impact of buggy lines, showing that CLMs, as is, cannot make good use of the buggy lines to fix bugs, yet fine-tuned CLMs could potentially over-rely on buggy lines. Lastly, this work analyzes the size, time, and memory efficiency of different CLMs.This work shows promising directions for the APR domain, such as fine-tuning CLMs with APR-specific designs, and also raises awareness of fair and comprehensive evaluations of CLMs and calls for more transparent reporting of open-source repositories used in the pre-training data to address the data leaking problem.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {1430–1442},
numpages = {13},
keywords = {automated program repair, code language model, fine-tuning, deep learning},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1145/3510454.3517063,
author = {Grishina, Anastasiia},
title = {Enabling automatic repair of source code vulnerabilities using data-driven methods},
year = {2022},
isbn = {9781450392235},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3510454.3517063},
doi = {10.1145/3510454.3517063},
abstract = {Users around the world rely on software-intensive systems in their day-to-day activities. These systems regularly contain bugs and security vulnerabilities. To facilitate bug fixing, data-driven models of automatic program repair use pairs of buggy and fixed code to learn transformations that fix errors in code. However, automatic repair of security vulnerabilities remains under-explored. In this work, we propose ways to improve code representations for vulnerability repair from three perspectives: input data type, data-driven models, and downstream tasks. The expected results of this work are improved code representations for automatic program repair and, specifically, fixing security vulnerabilities.},
booktitle = {Proceedings of the ACM/IEEE 44th International Conference on Software Engineering: Companion Proceedings},
pages = {275–277},
numpages = {3},
keywords = {ML4Code, automatic program repair, graph-based machine learning, natural language processing, software security, static analysis},
location = {Pittsburgh, Pennsylvania},
series = {ICSE '22}
}

@inproceedings{10.1145/3540250.3549101,
author = {Xia, Chunqiu Steven and Zhang, Lingming},
title = {Less training, more repairing please: revisiting automated program repair via zero-shot learning},
year = {2022},
isbn = {9781450394130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3540250.3549101},
doi = {10.1145/3540250.3549101},
abstract = {Due to the promising future of Automated Program Repair (APR), researchers have proposed various APR techniques, including heuristic-based, template-based, and constraint-based techniques. Among such classic APR techniques, template-based techniques have been widely recognized as state of the art. However, such template-based techniques require predefined templates to perform repair, and their effectiveness is thus limited. To this end, researchers have leveraged the recent advances in Deep Learning to further improve APR. Such learning-based techniques typically view APR as a Neural Machine Translation problem, using the buggy/fixed code snippets as the source/target languages for translation. In this way, such techniques heavily rely on large numbers of high-quality bug-fixing commits, which can be extremely costly/challenging to construct and may limit their edit variety and context representation. In this paper, we aim to revisit the learning-based APR problem, and propose AlphaRepair, the first cloze-style (or infilling-style) APR approach to directly leveraging large pre-trained code models for APR without any fine-tuning/retraining on historical bug fixes. Our main insight is instead of modeling what a repair edit should look like (i.e., a NMT task), we can directly predict what the correct code is based on the context information (i.e., a cloze or text infilling task). Although our approach is general and can be built on various pre-trained code models, we have implemented AlphaRepair as a practical multilingual APR tool based on the recent CodeBERT model. Our evaluation of AlphaRepair on the widely used Defects4J benchmark shows for the first time that learning-based APR without any history bug fixes can already outperform state-of-the-art APR techniques. We also studied the impact of different design choices and show that AlphaRepair performs even better on a newer version of Defects4J (2.0) with 3.3X more fixes than best performing baseline, indicating that AlphaRepair can potentially avoid the dataset-overfitting issue of existing techniques. Additionally, we demonstrate the multilingual repair ability of AlphaRepair by evaluating on the QuixBugs dataset where AlphaRepair achieved the state-of-the-art results on both Java and Python versions.},
booktitle = {Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {959–971},
numpages = {13},
keywords = {Automated Program Repair, Deep Learning, Zero-shot Learning},
location = {Singapore, Singapore},
series = {ESEC/FSE 2022}
}

@article{10.1016/j.cie.2025.111020,
author = {Rojas Trejos, Carlos Alberto and Meisel, Jose D. and Adarme-Jaimes, Wilson and Orejuela Cabrera, Juan Pablo},
title = {Repair resources scheduling for attention of transitory road disruptions in humanitarian aid networks},
year = {2025},
issue_date = {May 2025},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {203},
number = {C},
issn = {0360-8352},
url = {https://doi.org/10.1016/j.cie.2025.111020},
doi = {10.1016/j.cie.2025.111020},
journal = {Comput. Ind. Eng.},
month = may,
numpages = {13},
keywords = {Humanitarian logistics, Repair scheduling, Mathematical model, Access restoration}
}

@inproceedings{10.1145/3650215.3650256,
author = {Liu, Wenbo and Gao, Yan and Chen, Ming},
title = {A Network Application Fault Repair Method Based on Reinforcement Learning},
year = {2024},
isbn = {9798400709449},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3650215.3650256},
doi = {10.1145/3650215.3650256},
abstract = {With the continuous expansion of enterprise networks and the growing emphasis on security requirements, many enterprises have implemented multi-level security equipment (MLSE) within their networks. However, this deployment has led to network application failures and reduced overall network availability. This paper conducts an analysis of the underlying mechanisms responsible for such network application failures in MLSE enterprise networks. Subsequently, a Fault Location and Recovery (FDR) method is proposed, utilizing Q-learning path search with an experience storage (ESQL) mechanism. To validate the proposed approach, a prototype MLSE enterprise network system is constructed using NFV technology, and a series of verification experiments are performed on the FDR method. The experimental results demonstrate that the proposed approach successfully reproduces the failure phenomena and accurately locates and expedites the repair of network application failures through the FDR method.},
booktitle = {Proceedings of the 2023 4th International Conference on Machine Learning and Computer Application},
pages = {232–238},
numpages = {7},
location = {Hangzhou, China},
series = {ICMLCA '23}
}

@inproceedings{10.1145/3643834.3661559,
author = {Lee, Christine P and Praveena, Pragathi and Mutlu, Bilge},
title = {REX: Designing User-centered Repair and Explanations to Address Robot Failures},
year = {2024},
isbn = {9798400705830},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643834.3661559},
doi = {10.1145/3643834.3661559},
abstract = {Robots in real-world environments continuously engage with multiple users and encounter changes that lead to unexpected conflicts in fulfilling user requests. Recent technical advancements (e.g., large-language models (LLMs), program synthesis) offer various methods for automatically generating repair plans that address such conflicts. In this work, we understand how automated repair and explanations can be designed to improve user experience with robot failures through two user studies. In our first, online study (n = 162), users expressed increased trust, satisfaction, and utility with the robot performing automated repair and explanations. However, we also identified risk factors—safety, privacy, and complexity—that require adaptive repair strategies. The second, in-person study (n = 24) elucidated distinct repair and explanation strategies depending on the level of risk severity and type. Using a design-based approach, we explore automated repair with explanations as a solution for robots to handle conflicts and failures, complemented by adaptive strategies for risk factors. Finally, we discuss the implications of incorporating such strategies into robot designs to achieve seamless operation among changing user needs and environments.},
booktitle = {Proceedings of the 2024 ACM Designing Interactive Systems Conference},
pages = {2911–2925},
numpages = {15},
keywords = {failures, human-robot interaction, program repair, robot, user-centered design, vignette study},
location = {Copenhagen, Denmark},
series = {DIS '24}
}

@inproceedings{10.1109/ICSE48619.2023.00111,
author = {Jiang, Nan and Lutellier, Thibaud and Lou, Yiling and Tan, Lin and Goldwasser, Dan and Zhang, Xiangyu},
title = {KNOD: Domain Knowledge Distilled Tree Decoder for Automated Program Repair},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00111},
doi = {10.1109/ICSE48619.2023.00111},
abstract = {Automated Program Repair (APR) improves software reliability by generating patches for a buggy program automatically. Recent APR techniques leverage deep learning (DL) to build models to learn to generate patches from existing patches and code corpora. While promising, DL-based APR techniques suffer from the abundant syntactically or semantically incorrect patches in the patch space. These patches often disobey the syntactic and semantic domain knowledge of source code and thus cannot be the correct patches to fix a bug.We propose a DL-based APR approach KNOD, which incorporates domain knowledge to guide patch generation in a direct and comprehensive way. KNOD has two major novelties, including (1) a novel three-stage tree decoder, which directly generates Abstract Syntax Trees of patched code according to the inherent tree structure, and (2) a novel domain-rule distillation, which leverages syntactic and semantic rules and teacher-student distributions to explicitly inject the domain knowledge into the decoding procedure during both the training and inference phases.We evaluate KNOD on three widely-used benchmarks. KNOD fixes 72 bugs on the Defects4J v1.2, 25 bugs on the QuixBugs, and 50 bugs on the additional Defects4J v2.0 benchmarks, outperforming all existing APR tools.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {1251–1263},
numpages = {13},
keywords = {automated program repair, abstract syntax tree, deep learning},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@article{10.1007/s10458-021-09508-8,
author = {van den Berg, Line and Atencia, Manuel and Euzenat, J\'{e}r\^{o}me},
title = {A logical model for the ontology alignment repair game},
year = {2021},
issue_date = {Oct 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {35},
number = {2},
issn = {1387-2532},
url = {https://doi.org/10.1007/s10458-021-09508-8},
doi = {10.1007/s10458-021-09508-8},
abstract = {Ontology alignments enable agents to communicate while preserving heterogeneity in their knowledge. Alignments may not be provided as input and should be able to evolve when communication fails or when new information contradicting the alignment is acquired. The Alignment Repair Game (ARG) has been proposed for agents to simultaneously communicate and repair their alignments through adaptation operators when communication failures occur. ARG has been evaluated experimentally and the experiments showed that agents converge towards successful communication and improve their alignments. However, whether the adaptation operators are formally correct, complete or redundant could not be established by experiments. We introduce a logical model, Dynamic Epistemic Ontology Logic (DEOL), that enables us to answer these questions. This framework allows us (1) to express the ontologies and alignments used via a faithful translation from ARG to DEOL, (2) to model the ARG adaptation operators as dynamic modalities and (3) to formally define and establish the correctness, partial redundancy and incompleteness of the adaptation operators in ARG.},
journal = {Autonomous Agents and Multi-Agent Systems},
month = oct,
numpages = {34},
keywords = {Ontology alignment, Alignment repair, Multi-agent systems, Agent communication, Dynamic epistemic logic}
}

@inproceedings{10.1109/ICSE48619.2023.00111,
author = {Jiang, Nan and Lutellier, Thibaud and Lou, Yiling and Tan, Lin and Goldwasser, Dan and Zhang, Xiangyu},
title = {KNOD: Domain Knowledge Distilled Tree Decoder for Automated Program Repair},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00111},
doi = {10.1109/ICSE48619.2023.00111},
abstract = {Automated Program Repair (APR) improves software reliability by generating patches for a buggy program automatically. Recent APR techniques leverage deep learning (DL) to build models to learn to generate patches from existing patches and code corpora. While promising, DL-based APR techniques suffer from the abundant syntactically or semantically incorrect patches in the patch space. These patches often disobey the syntactic and semantic domain knowledge of source code and thus cannot be the correct patches to fix a bug.We propose a DL-based APR approach KNOD, which incorporates domain knowledge to guide patch generation in a direct and comprehensive way. KNOD has two major novelties, including (1) a novel three-stage tree decoder, which directly generates Abstract Syntax Trees of patched code according to the inherent tree structure, and (2) a novel domain-rule distillation, which leverages syntactic and semantic rules and teacher-student distributions to explicitly inject the domain knowledge into the decoding procedure during both the training and inference phases.We evaluate KNOD on three widely-used benchmarks. KNOD fixes 72 bugs on the Defects4J v1.2, 25 bugs on the QuixBugs, and 50 bugs on the additional Defects4J v2.0 benchmarks, outperforming all existing APR tools.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {1251–1263},
numpages = {13},
keywords = {automated program repair, abstract syntax tree, deep learning},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1109/ASE56229.2023.00181,
author = {Huang, Kai and Meng, Xiangxin and Zhang, Jian and Liu, Yang and Wang, Wenjie and Li, Shuhao and Zhang, Yuqing},
title = {An Empirical Study on Fine-Tuning Large Language Models of Code for Automated Program Repair},
year = {2024},
isbn = {9798350329964},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASE56229.2023.00181},
doi = {10.1109/ASE56229.2023.00181},
abstract = {The advent of large language models (LLMs) has opened up new opportunities for automated program repair (APR). In particular, some recent studies have explored how to leverage large language models of code (LLMCs) for program repair tasks and show promising results. However, most of them adopt the zero/few-shot learning paradigm for APR, which directly use LLMCs to generate the possibly correct code given its surrounding context. Though effective, the repair capabilities of LLMCs based on the fine-tuning paradigm have yet to be extensively explored. Also, it remains unknown whether LLMCs have the potential to repair more complicated bugs (e.g., multi-hunk bugs). To fill the gap, in this work, we conduct a comprehensive study on the program repair capability of LLMCs in the fine-tuning paradigm. We select 5 popular LLMCs with representative pre-training architectures, including CodeBERT, GraphCodeBERT, PLBART, CodeT5, and UniXcoder. We consider 3 typical program repair scenarios (i.e., bugs, vulnerabilities, and errors) involving 3 programming languages (i.e., Java, C/C++, and JavaScript). Notably, we take both single-hunk and multi-hunk bugs/vulnerabilities into account. We then fine-tune them on widely-used datasets and compare them with existing state-of-the-art APR tools. We also investigate the impact of different design choices, which include code abstractions, code representations, and model evaluation metrics. Our experimental results show that LLMCs in the fine-tuning paradigm can significantly outperform previous state-of-the-art APR tools. Through in-depth analysis, we provide insights into choosing appropriate strategies to guide LLMCs for better performance. Lastly, we reveal several limitations of LLMCs for APR and make suggestions for future research on LLMC-based APR.},
booktitle = {Proceedings of the 38th IEEE/ACM International Conference on Automated Software Engineering},
pages = {1162–1174},
numpages = {13},
keywords = {automated program repair, large language models of code, neural machine translation, fine-tuning},
location = {Echternach, Luxembourg},
series = {ASE '23}
}

@article{10.1145/3672450,
author = {Lou, Yiling and Yang, Jun and Benton, Samuel and Hao, Dan and Tan, Lin and Chen, Zhenpeng and Zhang, Lu and Zhang, Lingming},
title = {When Automated Program Repair Meets Regression Testing—An Extensive Study on Two Million Patches},
year = {2024},
issue_date = {September 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {33},
number = {7},
issn = {1049-331X},
url = {https://doi.org/10.1145/3672450},
doi = {10.1145/3672450},
abstract = {In recent years, Automated Program Repair (APR) has been extensively studied in academia and even drawn wide attention from the industry. However, APR techniques can be extremely time consuming since (1) a large number of patches can be generated for a given bug, and (2) each patch needs to be executed on the original tests to ensure its correctness. In the literature, various techniques (e.g., based on learning, mining, and constraint solving) have been proposed/studied to reduce the number of patches. Intuitively, every patch can be treated as a software revision during regression testing; thus, traditional Regression Test Selection (RTS) techniques can be leveraged to only execute the tests affected by each patch (as the other tests would keep the same outcomes) to further reduce patch execution time. However, few APR systems actually adopt RTS and there is still a lack of systematic studies demonstrating the benefits of RTS and the impact of different RTS strategies on APR. To this end, this article presents the first extensive study of widely used RTS techniques at different levels (i.e., class/method/statement levels) for 12 state-of-the-art APR systems on over 2M patches. Our study reveals various practical guidelines for bridging the gap between APR and regression testing, including: (1) the number of patches widely used for measuring APR efficiency can incur skewed conclusions, and the use of inconsistent RTS configurations can further skew the conclusions; (2) all studied RTS techniques can substantially improve APR efficiency and should be considered in future APR work; (3) method- and statement-level RTS outperform class-level RTS substantially and should be preferred; (4) RTS techniques can substantially outperform state-of-the-art test prioritization techniques for APR, and combining them can further improve APR efficiency; and (5) traditional Regression Test Prioritization (RTP) widely studied in regression testing performs even better than APR-specific test prioritization when combined with most RTS techniques. Furthermore, we also present the detailed impact of different patch categories and patch validation strategies on our findings.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = sep,
articleno = {180},
numpages = {23},
keywords = {Test selection, program repair, patch validation}
}

@article{10.1145/3743694,
author = {Kox, Esther and Hennekens, Milou and Metcalfe, Jason and Kerstholt, Jos\'{e}},
title = {Trust Violations due to Error or Choice: The Differential Effects on Trust Repair in Human–Human and Human–Robot Interaction},
year = {2025},
issue_date = {December 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {4},
url = {https://doi.org/10.1145/3743694},
doi = {10.1145/3743694},
abstract = {Many decisions in life involve tradeoffs: To gain something, one often has to lose something in return. As robots become more autonomous, their decisions will extend beyond mere assessments (e.g., detecting a threat) to making choices (e.g., taking the faster or the safer route). The aim of the current research was to study perceived trustworthiness in scenarios involving adverse consequences due to (1) an assessment error versus (2) a choice. Perceived trustworthiness (ability, benevolence, integrity) was measured repeatedly during a computer task simulating a military mission. Participants teamed with either a virtual human or a robotic partner who led the way and warned for potential danger. After encountering a hazard, the partner explained that it (1) failed to detect the threat (error) or (2) prioritized the mission and chose the fastest route despite the risk (choice). Results showed that: (a) the error-explanation repaired all trustworthiness dimensions, (b) the choice-explanation only repaired perceptions of ability, not benevolence or integrity, (c) no differences were found between human and robotic partners. Our findings suggest that trust violations due to choices are harder to repair than those due to errors. Implications and future research directions are discussed.},
journal = {J. Hum.-Robot Interact.},
month = aug,
articleno = {75},
numpages = {27},
keywords = {Human-Robot Interaction, Trust, Trust violations, Trust repair, Error, Choice}
}

@inproceedings{10.1145/3563657.3595991,
author = {Subbaraman, Blair and Peek, Nadya},
title = {3D Printers Don’t Fix Themselves: How Maintenance is Part of Digital Fabrication},
year = {2023},
isbn = {9781450398930},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3563657.3595991},
doi = {10.1145/3563657.3595991},
abstract = {Digital fabrication practice such as 3D printing has increasingly moved into home and hobbyist environments. Beyond running machines, practitioners in these settings undertake maintenance and repair. However, acquiring the skills necessary for machine maintenance is a non-trivial process contingent on experience, equipment, and materials. We seek to better understand how practitioners develop the skills necessary to maintain their 3D printers. We collect interview and survey data from active members of online 3D printing communities to conceptualize themes to characterize current maintenance practice. We find that maintenance is core to our participants’ 3D printing practice: practitioners develop maintenance routines that formalize tacit understandings of fabrication processes, advance expertise during required acts of repair, and rely on hands-on testing to reconcile differences between physical prints and digital models. Given our findings, we argue for considering maintenance as a core part of digital fabrication, and discuss implications for the design of future digital fabrication systems.},
booktitle = {Proceedings of the 2023 ACM Designing Interactive Systems Conference},
pages = {2050–2065},
numpages = {16},
keywords = {3D Printing, Digital Fabrication, Embodiment, Maintenance, Repair},
location = {Pittsburgh, PA, USA},
series = {DIS '23}
}

@inproceedings{10.1145/3746027.3762074,
author = {Patamia, Rutherford Agbeshi and Dinh, Ha Pham Thien and Liu, Ming and Cosgun, Akansel},
title = {Beyond Technical Failures: Multimodal Time-Series Modelling for Detecting Social Breakdowns and User Repair Attempts in Human-Robot Interaction},
year = {2025},
isbn = {9798400720352},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3746027.3762074},
doi = {10.1145/3746027.3762074},
abstract = {Reliable detection of conversational errors and user-initiated corrections is critical for effective human-robot interaction (HRI). In this study, we present a comprehensive multimodal approach leveraging temporal window processing, targeted feature engineering, and a MiniRocket + Ridge classification pipeline to address the challenges introduced by the ERR@HRI 2.0 dataset. Our methodology systematically integrates multimodal data streams, including facial expressions, acoustic features, and linguistic embeddings, to predict robot failures and user reactions. Experimental results demonstrate significant improvements over baseline models in event-level detection performance. Notably, linguistic features derived from transcript embeddings emerged as the most informative modality, substantially enhancing model performance. However, we observed challenges associated with managing false positives at the event level, suggesting avenues for future refinement in adaptive thresholding and sequential post-processing techniques. Our findings underscore the importance of careful feature selection and robust temporal modelling in developing effective real-time error detection systems for conversational robots. Our code is available online. https://github.com/Ruddy202/err-hri-2.0-armas.git.},
booktitle = {Proceedings of the 33rd ACM International Conference on Multimedia},
pages = {14136–14142},
numpages = {7},
keywords = {feature engineering, human-robot interaction, minirocket, ridge classifier, robot error, user correction},
location = {Dublin, Ireland},
series = {MM '25}
}

@inproceedings{10.1145/3630106.3658924,
author = {Pareek, Saumya and Velloso, Eduardo and Goncalves, Jorge},
title = {Trust Development and Repair in AI-Assisted Decision-Making during Complementary Expertise},
year = {2024},
isbn = {9798400704505},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3630106.3658924},
doi = {10.1145/3630106.3658924},
abstract = {Leveraging Artificial Intelligence to support human decision-makers requires harnessing the unique strengths of both entities, where human expertise often complements AI capabilities. However, human decision-makers must accurately discern when to trust the AI. In situations with complementary Human-AI expertise, identifying AI inaccuracies becomes challenging for humans, hindering their ability to rely on the AI only when warranted. Even when AI performance improves post-errors, this inability to assess accuracy can hinder trust recovery. Through two experimental tasks, we investigate trust development, erosion, and recovery during AI-assisted decision-making, examining explicit Trust Repair Strategies (TRSs) – Apology, Denial, Promise, and Model Update. Our participants classified familiar and unfamiliar stimuli with an AI with varying accuracy. We find that participants leveraged AI accuracy in familiar tasks as a heuristic to dynamically calibrate their trust during unfamiliar tasks. Further, once trust in the AI was eroded, trust restored through Model Update surpassed initial trust values, followed by Apology, Promise, and the baseline (no repair), with Denial being least effective. We empirically demonstrate how trust calibration occurs during complementary expertise, highlighting factors influencing the different effectiveness of TRSs despite identical AI accuracy, and offering implications for effectively restoring trust in Human-AI collaborations.},
booktitle = {Proceedings of the 2024 ACM Conference on Fairness, Accountability, and Transparency},
pages = {546–561},
numpages = {16},
keywords = {Human-AI decision-making, complementary expertise, trust development, trust repair},
location = {Rio de Janeiro, Brazil},
series = {FAccT '24}
}

@inproceedings{10.1145/3597503.3623311,
author = {Tappler, Martin and Pferscher, Andrea and Aichernig, Bernhard K. and K\"{o}nighofer, Bettina},
title = {Learning and Repair of Deep Reinforcement Learning Policies from Fuzz-Testing Data},
year = {2024},
isbn = {9798400702174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597503.3623311},
doi = {10.1145/3597503.3623311},
abstract = {Reinforcement learning from demonstrations (RLfD) is a promising approach to improve the exploration efficiency of reinforcement learning (RL) by learning from expert demonstrations in addition to interactions with the environment. In this paper, we propose a framework that combines techniques from search-based testing with RLfD with the goal to raise the level of dependability of RL policies and to reduce human engineering effort. Within our framework, we provide methods for efficiently training, evaluating, and repairing RL policies. Instead of relying on the costly collection of demonstrations from (human) experts, we automatically compute a diverse set of demonstrations via search-based fuzzing methods and use the fuzz demonstrations for RLfD. To evaluate the safety and robustness of the trained RL agent, we search for safety-critical scenarios in the black-box environment. Finally, when unsafe behavior is detected, we compute demonstrations through fuzz testing that represent safe behavior and use them to repair the policy. Our experiments show that our framework is able to efficiently learn high-performing and safe policies without requiring any expert knowledge.},
booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
articleno = {6},
numpages = {13},
keywords = {deep reinforcement learning, reinforcement learning from demonstrations, search-based software testing, policy repair},
location = {Lisbon, Portugal},
series = {ICSE '24}
}

@inproceedings{10.5555/3737916.3739179,
author = {Yu, Yongsheng and Zeng, Ziyun and Hua, Hang and Fu, Jianlong and Luo, Jiebo},
title = {PromptFix: you prompt and we fix the photo},
year = {2024},
isbn = {9798331314385},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Diffusion models equipped with language models demonstrate excellent controllability in image generation tasks, allowing image processing to adhere to human instructions. However, the lack of diverse instruction-following data hampers the development of models that effectively recognize and execute user-customized instructions, particularly in low-level tasks. Moreover, the stochastic nature of the diffusion process leads to deficiencies in image generation or editing tasks that require the detailed preservation of the generated images. To address these limitations, we propose PromptFix, a comprehensive framework that enables diffusion models to follow human instructions to perform a wide variety of image-processing tasks. First, we construct a large-scale instruction-following dataset that covers comprehensive image-processing tasks, including low-level tasks, image editing, and object creation. Next, we propose a high-frequency guidance sampling method to explicitly control the denoising process and preserve high-frequency details in unprocessed areas. Finally, we design an auxiliary prompting adapter, utilizing Vision-Language Models (VLMs) to enhance text prompts and improve the model's task generalization. Experimental results show that PromptFix outperforms previous methods in various image-processing tasks. Our proposed model also achieves comparable inference efficiency with these baseline models and exhibits superior zero-shot capabilities in blind restoration and combination tasks. The dataset and code are available at https://www.yongshengyu.com/PromptFix-Page.},
booktitle = {Proceedings of the 38th International Conference on Neural Information Processing Systems},
articleno = {1263},
numpages = {32},
location = {Vancouver, BC, Canada},
series = {NIPS '24}
}

@inproceedings{10.1145/3555776.3577762,
author = {Kim, Jisung and Lee, Byungjeong},
title = {MCRepair: Multi-Chunk Program Repair via Patch Optimization with Buggy Block},
year = {2023},
isbn = {9781450395175},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3555776.3577762},
doi = {10.1145/3555776.3577762},
abstract = {Automated program repair (APR) is a technology that identifies and repairs bugs automatically. However, repairing multi-chunk bugs remains a long-standing and challenging problem because an APR technique must consider dependencies and then reduce the large patch space. In addition, little is known about how to combine individual candidate patches even though multi-chunk bugs require combinations. Therefore, we propose a novel APR technique called multi-code repair (MCRepair), which applies a buggy block, patch optimization, and CodeBERT to target multi-chunk bugs. A buggy block is a novel method that binds buggy chunks into a multi-buggy chunk and preprocesses the chunk with its buggy contexts for patch space reduction and dependency problems. Patch optimization is a novel strategy that effectively combines the generated candidate patches with patch space reduction. In addition, CodeBERT, a BERT for source code datasets, is fine-tuned to address the lack of datasets and out-of-vocabulary problems. We conducted several experiments to evaluate our approach on six project modules of Defects4J. In the experiments using Defects4J, MCRepair repaired 66 bugs, including 22 multi-chunk bugs. Moreover, it fixed 19 unique bugs, including nine multi-chunk bugs, and improved 45--266\% performance than the baselines.},
booktitle = {Proceedings of the 38th ACM/SIGAPP Symposium on Applied Computing},
pages = {1508–1515},
numpages = {8},
keywords = {automated program repair, buggy block, patch optimization, deep learning},
location = {Tallinn, Estonia},
series = {SAC '23}
}

@inproceedings{10.1145/3551349.3559519,
author = {Ghanbari, Ali and Marcus, Andrian (Andi)},
title = {Shibboleth: Hybrid Patch Correctness Assessment in Automated Program Repair},
year = {2023},
isbn = {9781450394758},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3551349.3559519},
doi = {10.1145/3551349.3559519},
abstract = {Test-based generate-and-validate automated program repair (APR) systems generate many patches that pass the test suite without fixing the bug. The generated patches must be manually inspected by the developers, a task that tends to be time-consuming, thereby diminishing the role of APR in reducing debugging costs. We present the design and implementation of a novel tool, named Shibboleth, for automatic assessment of the patches generated by test-based generate-and-validate APR systems. Shibboleth leverages lightweight static and dynamic heuristics from both test and production code to rank and classify the patches. Shibboleth is based on the idea that the buggy program is almost correct and the bugs are small mistakes that require small changes to fix and specifically the fix does not remove the code implementing correct functionality of the program. Thus, the tool measures the impact of patches on both production code (via syntactic and semantic similarity) and test code (via code coverage) to separate the patches that result in similar programs and that do not remove desired program elements. We have evaluated Shibboleth on 1,871 patches, generated by 29 Java-based APR systems for Defects4J programs. The technique outperforms state-of-the-art raking and classification techniques. Specifically, in our ranking data set, in 66\% of the cases, Shibboleth ranks the correct patch in top-1 or top-2 positions and, in our classification data set, it achieves an accuracy and F1-score of 0.887 and 0.852, respectively, in classification mode. A demo video of the tool is available at https://bit.ly/3NvYJN8.},
booktitle = {Proceedings of the 37th IEEE/ACM International Conference on Automated Software Engineering},
articleno = {166},
numpages = {4},
keywords = {Automated Program Repair, Branch Coverage, Patch Correctness Assessment, Similarity},
location = {Rochester, MI, USA},
series = {ASE '22}
}

@inproceedings{10.1145/3613904.3642900,
author = {Wenzel, Kimi and Kaufman, Geoff},
title = {Designing for Harm Reduction: Communication Repair for Multicultural Users' Voice Interactions},
year = {2024},
isbn = {9798400703300},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613904.3642900},
doi = {10.1145/3613904.3642900},
abstract = {Voice assistants’ inability to serve people-of-color and non-native English speakers has largely been documented as a quality-of-service harm. However, little work has investigated what downstream harms propagate from this poor service. How does poor usability materially manifest and affect users’ lives? And what interaction designs might help users recover from these effects? We identify 6 downstream harms that propagate from quality-of-service harms in voice assistants. Through interviews and design activities with 16 multicultural participants, we unveil these 6 harms, outline how multicultural users uniquely personify their voice assistant, and suggest how these harms and personifications may affect their interactions. Lastly, we employ techniques from psychology on communication repair to contribute suggestions for harm-reducing repair that may be implemented in voice technologies. Our communication repair strategies include: identity affirmations (intermittent frequency), cultural sensitivity, and blame redirection. This work shows potential for a harm-repair framework to positively influence voice interactions.},
booktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},
articleno = {879},
numpages = {17},
keywords = {Automated Speech Recognition, Communication Breakdown, Communication Repair, Conversational Repair, Conversational User Interface, Harm, Harm-Reduction, Language Technology, Multiculture, Multilingual, Voice Assistants},
location = {Honolulu, HI, USA},
series = {CHI '24}
}

@article{10.1016/j.compbiomed.2025.110838,
author = {Franz, Juliana and Barbieri, Fabian and Barink, Marco and Groth, Alexandra and Reinthaler, Markus and Kasner, Mario and Br\"{u}ning, Jan and Lavezzo, Valentina and Waechter-Stehle, Irina and Landmesser, Ulf and Kuehne, Titus and Goubergrits, Leonid and Vellguth, Katharina},
title = {Influences of mitral valve shape on transmitral hemodynamics before and after edge-to-edge repair: development of a reduced-order model},
year = {2025},
issue_date = {Sep 2025},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {196},
number = {PB},
issn = {0010-4825},
url = {https://doi.org/10.1016/j.compbiomed.2025.110838},
doi = {10.1016/j.compbiomed.2025.110838},
journal = {Comput. Biol. Med.},
month = sep,
numpages = {14},
keywords = {Reduced-order model, Computational fluid dynamics, Lumped parameter model, Mitral regurgitation, Interventional echocardiography, Transcatheter edge-to-edge repair, TEER}
}

@inproceedings{10.1145/3551349.3560422,
author = {Li, Xueyang and Liu, Shangqing and Feng, Ruitao and Meng, Guozhu and Xie, Xiaofei and Chen, Kai and Liu, Yang},
title = {TransRepair: Context-aware Program Repair for Compilation Errors},
year = {2023},
isbn = {9781450394758},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3551349.3560422},
doi = {10.1145/3551349.3560422},
abstract = {Automatically fixing compilation errors can greatly raise the productivity of software development, by guiding the novice or AI programmers to write and debug code. Recently, learning-based program repair has gained extensive attention and became the state-of-the-art in practice. But it still leaves plenty of space for improvement. In this paper, we propose an end-to-end solution&nbsp;TransRepair to locate the error lines and create the correct substitute for a C program simultaneously. Superior to the counterpart, our approach takes into account the context of erroneous code and diagnostic compilation feedback. Then we devise a Transformer-based neural network to learn the ways of repair from the erroneous code as well as its context and the diagnostic feedback. To increase the effectiveness of TransRepair, we summarize 5 types and 74 fine-grained sub-types of compilations errors from two real-world program datasets and the Internet. Then a program corruption technique is developed to synthesize a large dataset with 1,821,275 erroneous C programs. Through the extensive experiments, we demonstrate that TransRepair outperforms the state-of-the-art in both single repair accuracy and full repair accuracy. Further analysis sheds light on the strengths and weaknesses in the contemporary solutions for future improvement.},
booktitle = {Proceedings of the 37th IEEE/ACM International Conference on Automated Software Engineering},
articleno = {108},
numpages = {13},
keywords = {Program repair, compilation error, context-aware, deep learning},
location = {Rochester, MI, USA},
series = {ASE '22}
}

@inproceedings{10.1145/3743049.3743050,
author = {Alghamdi, Essam and Halvey, Martin and Nicol, Emma},
title = {The Effect of Interaction Language on Preferences for Communication Repair Strategies in Digital Voice Assistants (DVAs): A Comparative Study},
year = {2025},
isbn = {9798400715822},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3743049.3743050},
doi = {10.1145/3743049.3743050},
abstract = {As digital voice assistants (DVAs) become increasingly prevalent, understanding user preferences for communication repair strategies across various language contexts is crucial. Previous research has focused on conversational user interfaces, but seldom on how interaction language and user characteristics such as prior experience and computer self-efficacy influence these preferences. This study quantitatively explored the impact of interaction language on repair strategy preferences in DVAs using a pairwise comparison method (N=99). Findings indicate that while apologies and direct repeat requests are favoured by both native Arabic (AL1) and non-native English (EL2) speakers, EL2 speakers show a stronger preference for confirmation strategies due to linguistic limitations. Additionally, explanations for breakdowns are more favoured by native speakers; however, qualitative insights reveal that the preference of this strategy depends on context and its timing within the interaction sequence. These results underscore the need for tailored repair mechanisms to enhance effective and personalized DVA interactions.},
booktitle = {Proceedings of the Mensch Und Computer 2025},
pages = {146–162},
numpages = {17},
keywords = {conversational agent (CA), conversational user interface (CUI), digital voice assistant (DVA), voice-enabled assistant (VA), intelligent virtual assistant (IVA), virtual personal assistant (VPA)},
location = {
},
series = {MuC '25}
}

@inproceedings{10.1145/3540250.3558953,
author = {Winter, Emily Rowan and Nowack, Vesna and Bowes, David and Counsell, Steve and Hall, Tracy and Haraldsson, S\ae{}mundur and Woodward, John and Kirbas, Serkan and Windels, Etienne and McBello, Olayori and Atakishiyev, Abdurahman and Kells, Kevin and Pagano, Matthew},
title = {Towards developer-centered automatic program repair: findings from Bloomberg},
year = {2022},
isbn = {9781450394130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3540250.3558953},
doi = {10.1145/3540250.3558953},
abstract = {This paper reports on qualitative research into automatic program repair (APR) at Bloomberg. Six focus groups were conducted with a total of seventeen participants (including both developers of the APR tool and developers using the tool) to consider: the development at Bloomberg of a prototype APR tool (Fixie); developers’ early experiences using the tool; and developers’ perspectives on   how they would like to interact with the tool in future. APR is developing rapidly and it is important to understand in greater detail developers' experiences using this emerging technology. In this paper, we provide in-depth, qualitative data from an industrial setting. We found that the development of APR at Bloomberg had become increasingly user-centered, emphasising how fixes were presented to developers, as well as particular features, such as customisability. From the focus groups with developers who had used Fixie, we found particular concern with the pragmatic aspects of APR, such as how and when fixes were presented to them. Based on our findings, we make a series of recommendations to inform future APR development, highlighting how APR tools should 'start small', be customisable, and fit with developers' workflows. We also suggest that APR tools should capitalise on the promise of repair bots and draw on advances in explainable AI.},
booktitle = {Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {1578–1588},
numpages = {11},
keywords = {automatic program repair, human factors, qualitative methods},
location = {Singapore, Singapore},
series = {ESEC/FSE 2022}
}

@article{10.1016/j.jss.2024.112083,
author = {Gharibi, Reza and Sadreddini, Mohammad Hadi and Fakhrahmad, Seyed Mostafa},
title = {T5APR: Empowering automated program repair across languages through checkpoint ensemble},
year = {2024},
issue_date = {Aug 2024},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {214},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2024.112083},
doi = {10.1016/j.jss.2024.112083},
journal = {J. Syst. Softw.},
month = aug,
numpages = {19},
keywords = {Automated program repair, Neural program repair, Deep learning, Transformer}
}

@article{10.1016/j.engappai.2023.106275,
author = {Babli, Mohannad and Sapena, \'{O}scar and Onaindia, Eva},
title = {Plan commitment: Replanning versus plan repair},
year = {2023},
issue_date = {Aug 2023},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {123},
number = {PA},
issn = {0952-1976},
url = {https://doi.org/10.1016/j.engappai.2023.106275},
doi = {10.1016/j.engappai.2023.106275},
journal = {Eng. Appl. Artif. Intell.},
month = aug,
numpages = {16},
keywords = {Automated planning, Plan repair, Plan distance}
}

@article{10.1287/opre.2020.2018,
author = {Tian, Feng and Sun, Peng and Duenyas, Izak},
title = {Optimal Contract for Machine Repair and Maintenance},
year = {2021},
issue_date = {May-June 2021},
publisher = {INFORMS},
address = {Linthicum, MD, USA},
volume = {69},
number = {3},
issn = {0030-364X},
url = {https://doi.org/10.1287/opre.2020.2018},
doi = {10.1287/opre.2020.2018},
abstract = {Maintenance outsourcing is quite common in industries that rely on complex and critical equipment. Instead of investing in the maintenance facilities, firms outsource maintenance activities to specialized companies. However, it may be hard for firms (i.e., principal) to observe whether maintenance companies (i.e., agent) put sufficient resources into providing the best service, which gives rise to agency issues. In a dynamic environment in which an agent is responsible for both maintenance and repair of a critical machine, how the principal uses payments and termination to tackle agency issues is a challenging problem. In “Optimal Contract for Machine Repair and Maintenance,” F. Tian, P. Sun, and I. Duenyas provide theoretical guidance on designing the optimal contract to induce efforts from an agent to efficiently operate a machine. Although they consider the very general contract forms, the optimal contracts demonstrate simple and intuitive structures, making them easy to describe and implement in practice.A principal hires an agent to repair a machine when it is down and maintain it when it is up and earns a revenue flow when the machine is up. Both the up- and downtimes follow exponential distributions. If the agent exerts effort, the downtime is shortened, and uptime is prolonged. Effort, however, is costly to the agent and unobservable to the principal. We study optimal dynamic contracts that always induce the agent to exert effort while maximizing the principal’s profits. We formulate the contract design problem as a stochastic optimal control model with incentive constraints in continuous time over an infinite horizon. Although we consider the contract space that allows payments and potential contract termination time to take general forms, the optimal contracts demonstrate simple and intuitive structures, making them easy to describe and implement in practice.},
journal = {Oper. Res.},
month = may,
pages = {916–949},
numpages = {34},
keywords = {Dynamic programming/optimal control: models; facilities/equipment planning: maintenance/replacement; games/group decisions: stochastic, Stochastic Models, dynamic, moral hazard, optimal control, jump process, maintenance}
}

@inproceedings{10.1145/3543829.3543833,
author = {Jaber, Razan and McMillan, Donald},
title = {Cross-Modal Repair: Gaze and Speech Interaction for List Advancement},
year = {2022},
isbn = {9781450397391},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3543829.3543833},
doi = {10.1145/3543829.3543833},
abstract = {Interacting with long lists of instructions or ingredients continues to be a challenge for conversational interaction. In this paper, we conducted a user study to experiment with the use of ‘cued-gaze’ – waiting for the user’s visual attention – to manage the delivery of instructions with a voice agent. In a Wizard-of-Oz setting, 12 participants were instructed to build a simple Lego tower by a conversational agent and were able to advance in the list using either speech interaction, or gaze interaction. The increasing use of speech agents in real-world cause users to encounter failures in interactions, so in this task the agent was designed to fail when providing the list of instruction to explore how the participants proceeded to recover from common failures. This showed that, for this use case, cross-modality repair was more effective than reformulation of speech.},
booktitle = {Proceedings of the 4th Conference on Conversational User Interfaces},
articleno = {25},
numpages = {11},
keywords = {conversational user interface, gaze interaction, speech interaction, user study},
location = {Glasgow, United Kingdom},
series = {CUI '22}
}

@article{10.1016/j.csl.2023.101603,
author = {Alloatti, Francesca and Grasso, Francesca and Ferrod, Roger and Siragusa, Giovanni and Di Caro, Luigi and Cena, Federica},
title = {A tag-based methodology for the detection of user repair strategies in task-oriented conversational agents},
year = {2024},
issue_date = {Jun 2024},
publisher = {Academic Press Ltd.},
address = {GBR},
volume = {86},
number = {C},
issn = {0885-2308},
url = {https://doi.org/10.1016/j.csl.2023.101603},
doi = {10.1016/j.csl.2023.101603},
journal = {Comput. Speech Lang.},
month = jun,
numpages = {17},
keywords = {Conversational agents, Repair strategies detection, Human-centred artificial intelligence, Human–computer interaction}
}

@inproceedings{10.1145/3640794.3665558,
author = {Alghamdi, Essam and Halvey, Martin and Nicol, Emma},
title = {System and User Strategies to Repair Conversational Breakdowns of Spoken Dialogue Systems: A Scoping Review},
year = {2024},
isbn = {9798400705113},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3640794.3665558},
doi = {10.1145/3640794.3665558},
abstract = {Spoken Dialogue Systems (SDSs) are critical in facilitating natural and efficient human-machine interaction through speech. SDSs frequently encounter challenges in managing complex dialogues, resulting in communication breakdowns, which include misunderstandings— where the system misunderstands user input— and non-understandings— where the system fails to interpret the input at all. Strategies to repair these breakdowns have been investigated across multiple disciplines; despite this interest, the findings from these studies are inconsistent and hinder comparative analysis due to the use of diverse methodologies and terminologies. To address this gap, this scoping review systematically examines SDS and user repair strategies within a broad spectrum of literature. Based on 36 papers out of 818 found, we provide two comprehensive frameworks: one categorising SDS system-repair strategies into six distinct categories and the other user-repair strategies into five categories. Our analysis reveals a disparity in the literature’s focus on repair strategies, highlighting, in particular, the lack of research on less explored strategies, such as Information and Disclosure repair strategies, providing potential avenues for future research directions in this area.},
booktitle = {Proceedings of the 6th ACM Conference on Conversational User Interfaces},
articleno = {28},
numpages = {13},
keywords = {Conversational Agents, Conversational Breakdowns, Recovery Strategies, Repair Strategies, Spoken Dialogue System},
location = {Luxembourg, Luxembourg},
series = {CUI '24}
}

@inproceedings{10.1007/978-3-031-80853-1_15,
author = {Efrosinin, Dmitry and Vishnevsky, Vladimir and Stepanova, Natalia},
title = {Simulation-Based Optimization for&nbsp;Resource Allocation Problem in&nbsp;Finite-Source Queue with&nbsp;Heterogeneous Repair Facility},
year = {2025},
isbn = {978-3-031-80852-4},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-80853-1_15},
doi = {10.1007/978-3-031-80853-1_15},
abstract = {The paper deals with an optimal allocation problem in a finite-source queuing system where the repair facility consists of multiple heterogeneous servers. A threshold-based allocation policy prescribes the usage of slower servers according to given threshold levels of the queue lengths. This problem under markovian settings can be treated as a continuous-time Markov decision problem which was efficiently solved by dynamic programming algorithms. However, under conditions of uncertainty, when there is no information about the transient characteristics of the system and, in addition, the total number of states is too large, the simulation-based optimization methods must be applied. We use both the reinforcement learning methods and the random search method based on simulated annealing to solve the discrete optimization problem. Experimental results are compared with an actual solution obtained by policy iteration. Advantages and disadvantages of the methods and the peculiarities of their use for controllable queueing system are discussed.},
booktitle = {Distributed Computer and Communication Networks: 27th International Conference, DCCN 2024, Moscow, Russia, September 23–27, 2024, Revised Selected Papers},
pages = {187–202},
numpages = {16},
keywords = {Finite-source queue, Heterogeneous servers, Threshold policy, Simulation-based optimization, Reinforcement learning algorithms, Random search},
location = {Moscow, Russia}
}

@article{10.1145/3767729,
author = {Nagy, Timea Noemi and Rezaei Khavas, Zahra and Kotturu, Monish Reddy and Liefooghe, Baptist and Robinette, Paul and De Graaf, Maartje},
title = {A Robot Should Compensate for Its Mistakes: An Exploration of the Dynamics of Trust Violation and Repair Strategies in Human-Robot Collaboration},
year = {2025},
issue_date = {January 2026},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {1},
url = {https://doi.org/10.1145/3767729},
doi = {10.1145/3767729},
abstract = {Human-robot interactions are becoming prevalent in a varied number of fields, with trust being essential for efficient collaboration between humans and robots. Robots, just like humans, are bound to make mistakes leading to a violation of trust. Research investigating how to repair this broken trust has produced mixed results. This work investigates the effects of five communicative trust repair strategies (apology, denial, explanation, compensation, and silence) on participants’ trust in the robot, following trust violations of two kinds (moral and performance violation). In an online between-subjects experiment, participants engaged in a collaborative task with a robot that repeatedly committed trust violating acts and responded with a repair message. The findings indicate the higher severity of moral violations on moral trust and willingness to collaborate in the future, with compensation showing to be the most effective repair strategy, enhancing trust and willingness to collaborate, while also reducing discomfort. This work advances the understanding of trust relationships in collaborative HRI contexts.},
journal = {J. Hum.-Robot Interact.},
month = oct,
articleno = {22},
numpages = {34},
keywords = {Human-Robot Interaction, Collaborative HRI, Trust, Trust violation, Trust repair}
}

@inproceedings{10.1145/3472307.3484170,
author = {Hald, Kasper and Weitz, Katharina and Andr\'{e}, Elisabeth and Rehm, Matthias},
title = {“An Error Occurred!” - Trust Repair With Virtual Robot Using Levels of Mistake Explanation},
year = {2021},
isbn = {9781450386203},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3472307.3484170},
doi = {10.1145/3472307.3484170},
abstract = {Human-robot collaboration in industrial settings is an expanding research field in robotics. When working together, robot mistakes are an important factor to decrease trust and therefore interferes with cooperation. It is unclear whether explanations help to restore human-robot trust after a mistake. In our study, we investigate whether system explanations as a trust-repairing action after a robot makes a mistake in a collaborative task is helpful. Our pilot study revealed that users are more interested in solutions to errors than they are in just why the error happened. Therefore, in our main study, we evaluated three levels of mistake explanations (no explanation, explanation, and explanation with solution) after a robot in VR made a mistake in executing a shared objective. After testing with 30 participants we found that the robot making a mistake significantly affects trust toward the robot, compared to it completing the task successfully. While participants found the explanations helpful to trust or distrust the robot, the levels of the explanation did not lead to an increase in trust towards the robot after a mistake. In addition, we found no significant impact of explanations on self-efficacy and the emotional state of the participants. Our results show that explanations alone are not sufficient to increase human-computer trust after robot mistakes.},
booktitle = {Proceedings of the 9th International Conference on Human-Agent Interaction},
pages = {218–226},
numpages = {9},
keywords = {XAI, human-robot collaboration, human-robot trust, proximity, robot mistakes, virtual reality},
location = {Virtual Event, Japan},
series = {HAI '21}
}

@inproceedings{10.1007/978-3-030-78292-4_1,
author = {Abhinav, Kumar and Sharvani, Vijaya and Dubey, Alpana and D’Souza, Meenakshi and Bhardwaj, Nitish and Jain, Sakshi and Arora, Veenu},
title = {RepairNet: Contextual Sequence-to-Sequence Network for Automated Program Repair},
year = {2021},
isbn = {978-3-030-78291-7},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-78292-4_1},
doi = {10.1007/978-3-030-78292-4_1},
abstract = {Compile-time errors can wreak havoc for programmers – seasoned and novice. Often developers spend a lot of time debugging them. An automated system to repair such errors can be a useful aid to the developers for their productivity. In this work, we propose a deep generative model, RepairNet, that automatically repairs programs that fail at compile time. RepairNet is based on sequence-to-sequence modeling and uses both code and error messages to repair the program. We evaluated the effectiveness of our system on 6,971 erroneous submissions for 93 programming tasks. RepairNet outperforms the existing state-of-the-art technique, MACER, with 17\% relative improvement of repair accuracy. Our approach can fix 66.4\% of the erroneous submissions completely and 14.2\% partially.},
booktitle = {Artificial Intelligence in Education: 22nd International Conference, AIED 2021, Utrecht, The Netherlands, June 14–18, 2021, Proceedings, Part I},
pages = {3–15},
numpages = {13},
keywords = {Program repair, Sequence modeling, Bug fixing},
location = {Utrecht, The Netherlands}
}

@article{10.1007/s00500-022-07590-y,
author = {Abuaddous, Hayfa Y. and Kaur, Goldendeep and Jyoti, Kiran and Mittal, Nitin and Mahajan, Shubham and Pandit, Amit Kant and Alsoud, Anas Ratib and Abualigah, Laith},
title = {Repulsion-based grey wolf optimizer with improved exploration and exploitation capabilities to localize sensor nodes in 3D wireless sensor network},
year = {2022},
issue_date = {Apr 2023},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {27},
number = {7},
issn = {1432-7643},
url = {https://doi.org/10.1007/s00500-022-07590-y},
doi = {10.1007/s00500-022-07590-y},
abstract = {In recent years, localization turns out to be a crucial aspect in the realm of wireless sensor networks (WSNs) sparking a lot of research interest among researchers. It is the procedure of discovering the locality of target nodes concerning the installed anchor nodes whose placements are well known as they have a GPS component integrated into them. But as GPS is incompatible with indoor and/or aquatic situations, all sensor nodes are often not set up with it. If all the nodes are fitted with GPS, a network becomes too expensive and uses extra energy, which is a key disadvantage of WSNs. In the literature, various localization strategies have been presented; however, the majority of research ideas focus on 2D applications. In 3D implementations, however, the region under consideration in the sensing environment may be complicated. The determination of node placement in a 3D environment necessitates an optimal algorithm. In this research, we proposed a repulsion-based improved grey wolf optimizer (R-GWO) for the sensor nodes localization that outperforms the traditional GWO in terms of exploration and exploitation abilities. The suggested R-GWO has been evaluated on the WSN Localization problem and has shown to have the lowest localization error when contrasted to the other strategies used in 3D environments.},
journal = {Soft Comput.},
month = nov,
pages = {3869–3885},
numpages = {17},
keywords = {Localization, WSN, GWO, R-GWO}
}

@inproceedings{10.1145/3643788.3648015,
author = {Shariffdeen, Ridwan and Noller, Yannic and Mirchev, Martin and Ruan, Haifeng and Xiang, Gao and Costa, Andreea and Duck, Gregory J and Roychoudhury, Abhik},
title = {APR Competition 2024},
year = {2024},
isbn = {9798400705779},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643788.3648015},
doi = {10.1145/3643788.3648015},
abstract = {This report outlines the objectives, methodology, challenges, and results of the first Automated Program Repair Competition held at the APR Workshop 2024. The competition utilized Cerberus, a program repair framework, to evaluate the program repair tools using different repair configurations for each track in the competition. The competition was organized in three phases: first the participants integrated their tools with Cerberus, second the integrated tools were tested using public benchmarks and participants were able to fix any identified issues. In the last phase, the submitted tools and baseline comparison tools were evaluated against private benchmark programs.},
booktitle = {Proceedings of the 5th ACM/IEEE International Workshop on Automated Program Repair},
pages = {46–49},
numpages = {4},
location = {Lisbon, Portugal},
series = {APR '24}
}

@inproceedings{10.5555/3523760.3523825,
author = {van Waveren, Sanne and Pek, Christian and Tumova, Jana and Leite, Iolanda},
title = {Correct Me If I'm Wrong: Using Non-Experts to Repair Reinforcement Learning Policies},
year = {2022},
publisher = {IEEE Press},
abstract = {Reinforcement learning has shown great potential for learning sequential decision-making tasks. Yet, it is difficult to anticipate all possible real-world scenarios during training, causing robots to inevitably fail in the long run. Many of these failures are due to variations in the robot's environment. Usually experts are called to correct the robot's behavior; however, some of these failures do not necessarily require an expert to solve them. In this work, we query non-experts online for help and explore 1) if/how non-experts can provide feedback to the robot after a failure and 2) how the robot can use this feedback to avoid such failures in the future by generating shields that restrict or correct its high-level actions. We demonstrate our approach on common daily scenarios of a simulated kitchen robot. The results indicate that non-experts can indeed understand and repair robot failures. Our generated shields accelerate learning and improve data-efficiency during retraining.},
booktitle = {Proceedings of the 2022 ACM/IEEE International Conference on Human-Robot Interaction},
pages = {493–501},
numpages = {9},
keywords = {non-experts, policy repair, robot failure, shielded reinforcement learning},
location = {Sapporo, Hokkaido, Japan},
series = {HRI '22}
}

@inproceedings{10.1145/3610977.3634948,
author = {Axelsson, Minja and Spitale, Micol and Gunes, Hatice},
title = {"Oh, Sorry, I Think I Interrupted You": Designing Repair Strategies for Robotic Longitudinal Well-being Coaching},
year = {2024},
isbn = {9798400703225},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3610977.3634948},
doi = {10.1145/3610977.3634948},
abstract = {Robotic well-being coaches have been shown to successfully promote people's mental well-being. To provide successful coaching, a robotic coach should have the capability to repair the mistakes it makes. Past investigations of robot mistakes are limited to game or task-based, one-off and in-lab studies. This paper presents a 4-phase design process to design repair strategies for robotic longitudinal well-being coaching with the involvement of real-world stakeholders: 1) designing repair strategies with a professional well-being coach; 2) a longitudinal study with the involvement of experienced users (i.e., who had already interacted with a robotic coach) to investigate the repair strategies defined in (1); 3) a design workshop with users from the study in (2) to gather their perspectives on the robotic coach's repair strategies; 4) discussing the results obtained in (2) and (3) with the mental well-being professional to reflect on how to design repair strategies for robotic coaching. Our results show that users have different expectations for a robotic coach than a human coach, which influences how repair strategies should be designed. We show that different repair strategies (e.g., apologizing, explaining, or repairing empathically) are appropriate in different scenarios, and that preferences for repair strategies change during longitudinal interactions with the robotic coach.},
booktitle = {Proceedings of the 2024 ACM/IEEE International Conference on Human-Robot Interaction},
pages = {13–22},
numpages = {10},
keywords = {coaching, design research, human-robot interaction, interaction ruptures, robot mistakes, socially assistive robotics, well-being},
location = {Boulder, CO, USA},
series = {HRI '24}
}

@article{10.1287/trsc.2022.0434,
author = {Pham, Dai T. and Kiesm\"{u}ller, Gudrun P.},
title = {Hybrid Value Function Approximation for Solving the Technician Routing Problem with Stochastic Repair Requests},
year = {2024},
issue_date = {March-April 2024},
publisher = {INFORMS},
address = {Linthicum, MD, USA},
volume = {58},
number = {2},
issn = {1526-5447},
url = {https://doi.org/10.1287/trsc.2022.0434},
doi = {10.1287/trsc.2022.0434},
abstract = {We investigate the combined planning problem involving the routing of technicians and the stocking of spare parts for servicing geographically distributed repair tasks. The problem incorporates many operational uncertainties, such as future repair requests and the required spare parts to replace malfunctioned components. We model the problem as a sequential decision problem where decisions are made at the end of each day about the next day’s technician route and spare part inventory in the van. We show that exact methods are intractable because of the inherent high-dimensional state, decision, and transition spaces involved. To overcome these challenges, we present two novel algorithmic techniques. First, we suggest a hybrid value function approximation method that combines a genetic search with a graph neural network capable of reasoning, learning, and decision making in high-dimensional, discrete decision spaces. Second, we introduce a unique state-encoding method that employs multiattribute graphs and spatial markers, eliminating the need for manually designed basis functions and allowing efficient learning. We illustrate the general adaptive learning capacity by solving a variety of instance settings without instance-specific hyperparameter tuning. An extensive numerical study demonstrates that our hybrid learning technique outperforms other benchmark policies and adapts well to changes in the environment. We also generate a wide range of insights that not only shed light on the algorithmic components but also offer guidance on how to execute on-site repair tasks more efficiently. The techniques showcased are versatile and hold potential for application in other dynamic and stochastic problems, particularly in the realm of transportation planning.Funding: This work was supported by Deutsche Forschungsgemeinschaft (DFG). The Research Training Group 2201 [Grant 277991500], “Advanced Optimization in a Networked Economy,” funded by the DFG, has provided partial support for this work.Supplemental Material: The online appendix is available at .},
journal = {Transportation Science},
month = mar,
pages = {499–519},
numpages = {21},
keywords = {graph neural network-based value function approximation, multiperiod stochastic problem, combinatorial decision space}
}

@inproceedings{10.1145/3514197.3549641,
author = {Asisof, Alina},
title = {Towards a comprehensive repair framework for human-chatbot interaction: the case of rephrasing},
year = {2022},
isbn = {9781450392488},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3514197.3549641},
doi = {10.1145/3514197.3549641},
abstract = {Chatbots are becoming a regular part of service offerings. However, failures in human-chatbot interactions are common and mitigating them with appropriate strategies is an integral part of the dialogue. For instance, chatbots can be designed to prompt a rephrase, although, due to the complexity of user reactions, this is not always successful. Research has called for taxonomies to categorize user reactions to compute meaningful responses that encourage dialogue continuation. We suggest a framework of strategies based on prior research and test its validity, focusing on how users rephrase across dialogue turns. We find that users rephrase problems formally (49\%), by changing the number of words, altering syntax, or using synonyms and to a lesser extent by altering informational value (25\%). We suggest training chatbots along this behavior and designing better prompts that guide users' next actions.},
booktitle = {Proceedings of the 22nd ACM International Conference on Intelligent Virtual Agents},
articleno = {34},
numpages = {3},
keywords = {chatbots, repair strategies, user behavior},
location = {Faro, Portugal},
series = {IVA '22}
}

@inproceedings{10.5555/3737916.3738430,
author = {Ji, Xiayan and Xue, Anton and Wong, Eric and Sokolsky, Oleg and Lee, Insup},
title = {AR-Pro: counterfactual explanations for anomaly repair with formal properties},
year = {2024},
isbn = {9798331314385},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Anomaly detection is widely used for identifying critical errors and suspicious behaviors, but current methods lack interpretability. We leverage common properties of existing methods and recent advances in generative models to introduce counterfactual explanations for anomaly detection. Given an input, we generate its counterfactual as a diffusion-based repair that shows what a non-anomalous version should have looked like. A key advantage of this approach is that it enables a domain-independent formal specification of explainability desiderata, offering a unified framework for generating and evaluating explanations. We demonstrate the effectiveness of our anomaly explainability framework, AR-Pro, on vision (MVTec, VisA) and time-series (SWaT, WADI, HAI) anomaly datasets. The code used for the experiments is accessible at: https://github.com/xjiae/arpro.},
booktitle = {Proceedings of the 38th International Conference on Neural Information Processing Systems},
articleno = {514},
numpages = {27},
location = {Vancouver, BC, Canada},
series = {NIPS '24}
}

@article{10.1007/s10270-022-00983-5,
author = {Barriga, Angela and Rutle, Adrian and Heldal, Rogardt},
title = {AI-powered model repair: an experience report—lessons learned, challenges, and opportunities},
year = {2022},
issue_date = {Jun 2022},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {21},
number = {3},
issn = {1619-1366},
url = {https://doi.org/10.1007/s10270-022-00983-5},
doi = {10.1007/s10270-022-00983-5},
abstract = {Artificial intelligence has already proven to be a powerful tool to automate and improve how we deal with software development processes. The application of artificial intelligence to model-driven engineering projects is becoming more and more popular; however, within the model repair field, the use of this technique remains mostly an open challenge. In this paper, we explore some existing approaches in the field of AI-powered model repair. From the existing approaches in this field, we identify a series of challenges which the community needs to overcome. In addition, we present a number of research opportunities by taking inspiration from other fields which have successfully used artificial intelligence, such as code repair. Moreover, we discuss the connection between the existing approaches and the opportunities with the identified challenges. Finally, we present the outcomes of our experience of applying artificial intelligence to model repair.},
journal = {Softw. Syst. Model.},
month = jun,
pages = {1135–1157},
numpages = {23},
keywords = {Opportunities, Challenges, Model repair, Artificial intelligence}
}

@inproceedings{10.1007/978-3-031-42283-6_16,
author = {Law, Effie Lai-Chong and van As, Nena and F\o{}lstad, Asbj\o{}rn},
title = {Effects of Prior Experience, Gender, and Age on Trust in a Banking Chatbot With(Out) Breakdown and Repair},
year = {2023},
isbn = {978-3-031-42282-9},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-42283-6_16},
doi = {10.1007/978-3-031-42283-6_16},
abstract = {Trust is an attitudinal construct that can be sensitive to prior experience, gender, and age. In our study, we explored how trust in a banking chatbot might be shaped by these user characteristics. Statistical analysis of 251 participants, who interacted with one of six chatbots defined by humanlikeness (high/low) and conversational performance (no breakdown, breakdown with repaired, breakdown without repair), showed that the user characteristics of gender and age did not significantly impact trust, but prior experience did. Trust resilience was found across the gender and age groups. The effect of users’ prior experience on their trust in a chatbot which they have never used holds implications for research and practice. Future studies on the effect of cultural context, longer interaction episodes, and more diverse application contexts on trust in chatbots are recommended.},
booktitle = {Human-Computer Interaction – INTERACT 2023: 19th IFIP TC13 International Conference, York, UK, August 28 – September 1, 2023, Proceedings, Part II},
pages = {277–296},
numpages = {20},
keywords = {Repair, Breakdown, Prior experience, Gender, Age, Trust, Artificial Intelligence (AI), Chatbot},
location = {York, United Kingdom}
}

@inproceedings{10.1145/3533767.3534368,
author = {Ghanbari, Ali and Marcus, Andrian},
title = {Patch correctness assessment in automated program repair based on the impact of patches on production and test code},
year = {2022},
isbn = {9781450393799},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3533767.3534368},
doi = {10.1145/3533767.3534368},
abstract = {Test-based generate-and-validate automated program repair (APR) systems often generate many patches that pass the test suite without fixing the bug.   The generated patches must be manually inspected by the developers, so previous research proposed various techniques for automatic correctness assessment of APR-generated patches.   Among them, dynamic patch correctness assessment techniques rely on the assumption that, when running the originally passing test cases, the correct patches will not alter the program behavior in a significant way, e.g., removing the code implementing correct functionality of the program.   In this paper, we propose and evaluate a novel technique, named Shibboleth, for automatic correctness assessment of the patches generated by test-based generate-and-validate APR systems.   Unlike existing works, the impact of the patches is captured along three complementary facets, allowing more effective patch correctness assessment.   Specifically, we measure the impact of patches on both production code (via syntactic and semantic similarity) and test code (via code coverage of passing tests) to separate the patches that result in similar programs and that do not delete desired program elements.   Shibboleth assesses the correctness of patches via both ranking and classification.  We evaluated Shibboleth on 1,871 patches, generated by 29 Java-based APR systems for Defects4J programs. The technique outperforms state-of-the-art ranking and classification techniques.   Specifically, in our ranking data set, in 43\% (66\%) of the cases, Shibboleth ranks the correct patch in top-1 (top-2) positions, and in classification mode applied on our classification data set, it achieves an accuracy and F1-score of 0.887 and 0.852, respectively.},
booktitle = {Proceedings of the 31st ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {654–665},
numpages = {12},
keywords = {Similarity, Patch Correctness Assessment, Branch Coverage, Automated Program Repair},
location = {Virtual, South Korea},
series = {ISSTA 2022}
}

@inproceedings{10.1007/978-3-031-19849-6_17,
author = {Dam, Khanh Huu The and Duchene, Fabien and Given-Wilson, Thomas and Cordy, Maxime and Legay, Axel},
title = {Automated Repair of&nbsp;Security Errors in&nbsp;C Programs via&nbsp;Statistical Model Checking: A Proof of&nbsp;Concept},
year = {2022},
isbn = {978-3-031-19848-9},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-19849-6_17},
doi = {10.1007/978-3-031-19849-6_17},
abstract = {One major challenge in software development is finding and repairing programming errors. Recently, formal methods such as model checking have become a popular approach to finding errors due to their formal guarantees about error status and evidence of the error in the form of a trace. Another recently growing area, automated program repair, aims to fix errors using automated approaches that do not require programmer intervention. This paper gives a proof of concept that one can combine these two areas using state-of-the-art approaches in both: the discovery of program errors and traces exhibiting these errors; and automated program repair building on test cases generated from traces. This naturally links together the discovery, learning, repairing, and validation of repair in a single package.},
booktitle = {Leveraging Applications of Formal Methods, Verification and Validation. Verification Principles: 11th International Symposium, ISoLA 2022, Rhodes, Greece, October 22–30, 2022, Proceedings, Part I},
pages = {279–298},
numpages = {20},
location = {Rhodes, Greece}
}

@inproceedings{10.1007/978-3-030-76663-4_1,
author = {Tam\`{o}-Larrieux, Aurelia and Zihlmann, Zaira and Garcia, Kimberly and Mayer, Simon},
title = {The Right to Customization: Conceptualizing the Right to Repair for Informational Privacy},
year = {2021},
isbn = {978-3-030-76662-7},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-76663-4_1},
doi = {10.1007/978-3-030-76663-4_1},
abstract = {Terms of use of a digital service are often framed in a binary way: Either one agrees to the service provider's data processing practices, and is granted access to the service, or one does not, and is denied the service. Many scholars have lamented these ‘take-it-or-leave-it’ situations, as this goes against the ideals of data protection law. To address this inadequacy, computer scientists and legal scholars have tried to come up with approaches to enable more privacy-friendly products and services. In this article, we call for a right to customize the processing of user data. Our arguments build upon technology-driven approaches as well as on the ideals of privacy by design and the now codified data protection by design and default norm within the General Data Protection Regulation. In addition, we draw upon the right to repair that is propagated to empower consumers and enable a more circular economy. We propose two technologically-oriented approaches, termed ‘variants’ and ‘alternatives’ that could enable the technical implementation of a right to customization. We posit that these approaches cannot be demanded without limitation, and that restrictions will depend on how reasonable a customization demand is.},
booktitle = {Privacy Technologies and Policy: 9th Annual Privacy Forum, APF 2021, Oslo, Norway, June 17–18, 2021, Proceedings},
pages = {3–22},
numpages = {20},
keywords = {Right to customization, Right to repair, Consent, GDPR, Informational privacy}
}

@inproceedings{10.1145/3581641.3584067,
author = {Ning, Zheng and Zhang, Zheng and Sun, Tianyi and Tian, Yuan and Zhang, Tianyi and Li, Toby Jia-Jun},
title = {An Empirical Study of Model Errors and User Error Discovery and Repair Strategies in Natural Language Database Queries},
year = {2023},
isbn = {9798400701061},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3581641.3584067},
doi = {10.1145/3581641.3584067},
abstract = {Recent advances in machine learning (ML) and natural language processing (NLP) have led to significant improvement in natural language interfaces for structured databases (NL2SQL). Despite the great strides, the overall accuracy of NL2SQL models is still far from being perfect (∼ 75\% on the Spider benchmark). In practice, this requires users to discern incorrect SQL queries generated by a model and manually fix them when using NL2SQL models. Currently, there is a lack of comprehensive understanding about the common errors in auto-generated SQLs and the effective strategies to recognize and fix such errors. To bridge the gap, we (1) performed an in-depth analysis of errors made by three state-of-the-art NL2SQL models; (2) distilled a taxonomy of NL2SQL model errors; and (3) conducted a within-subjects user study with 26 participants to investigate the effectiveness of three representative interactive mechanisms for error discovery and repair in NL2SQL. Findings from this paper shed light on the design of future error discovery and repair strategies for natural language data query interfaces.},
booktitle = {Proceedings of the 28th International Conference on Intelligent User Interfaces},
pages = {633–649},
numpages = {17},
keywords = {Empirical study, database systems, human-computer interaction},
location = {Sydney, NSW, Australia},
series = {IUI '23}
}

@article{10.3233/JHS-210661,
author = {Niu, Xin and Jiang, Jingjing and Souri, Alireza and Chen, Mu-Yen},
title = {Single node repair algorithm for a multimedia cloud storage system based on network coding},
year = {2021},
issue_date = {2021},
publisher = {IOS Press},
address = {NLD},
volume = {27},
number = {3},
issn = {0926-6801},
url = {https://doi.org/10.3233/JHS-210661},
doi = {10.3233/JHS-210661},
abstract = {Multimedia is inconvenient to use, difficult to maintain, and redundant in data storage. In order to solve the above problems and apply cloud storage to the integration of university teaching resources, this paper designs a virtualized cloud storage platform for university multimedia classrooms. The platform has many advantages, such as reducing the initial investment in multimedia classrooms, simplifying management tasks, making maximum use of actual resources and easy access to resources. Experiments and analysis show the feasibility and effectiveness of the platform. Aiming at the problems of the single-node repair algorithm of the existing multimedia cloud storage system, the limited domain is large, the codec complexity is high, the disk I/O (Input/Output) cost is high, the storage overhead and the repair bandwidth are unbalanced, and a network coding-based approach is proposed. Multimedia cloud storage. System single node repair algorithm. The algorithm stores the grouped multimedia file data in groups in the system, and performs XOR (exclusive OR) on the data in the group on the GF(2) finite field. When some nodes fail, the new node only needs to be connected. Two to three non-faulty nodes in the same group can accurately repair the data in the failed node. Theoretical analysis and simulation results show that the algorithm can reduce the complexity and repair of the codec, and reduce the disk I/O overhead. In this case, the storage cost of the algorithm is consistent with the storage cost based on the minimum storage regeneration code algorithm, and the repair bandwidth cost is close to the minimum bandwidth regeneration code algorithm.},
journal = {J. High Speed Netw.},
month = jan,
pages = {205–214},
numpages = {10},
keywords = {Internet, single node repair, network coding, cloud storage, Multimedia}
}

@article{10.1016/j.procs.2021.01.363,
author = {Diaz, Rafael and Smith, Katherine and Acero, Beatriz and Longo, Francesco and Padovano, Antonio},
title = {Developing an Artificial Intelligence Framework to Assess Shipbuilding and Repair Sub-Tier Supply Chains Risk},
year = {2021},
issue_date = {2021},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {180},
number = {C},
issn = {1877-0509},
url = {https://doi.org/10.1016/j.procs.2021.01.363},
doi = {10.1016/j.procs.2021.01.363},
journal = {Procedia Comput. Sci.},
month = jan,
pages = {996–1002},
numpages = {7},
keywords = {Type your keywords here, separated by semicolons}
}

@inproceedings{10.1007/978-3-031-06053-3_11,
author = {Feng, Yushun and Tan, Hao},
title = {Comfort or Promise? Investigating the Effect of Trust Repair Strategies of Intelligent Vehicle System on Trust and Intention to Use from a Perspective of Social Cognition},
year = {2022},
isbn = {978-3-031-06052-6},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-06053-3_11},
doi = {10.1007/978-3-031-06053-3_11},
abstract = {The trust issue between intelligent system and humans is faced with more complex challenges. As the intelligent systems are gaining more autonomy, new modes and frameworks are required to investigate human-computer interaction in the days to come. This article aims to explore the trust repair strategies of intelligent vehicle system after a traffic accident from the perspective of social cognition, specifically, by comparing the impact of different trust repair strategies including admission and denial, comfort and promise, on the participants’ perceived warmth, perceived competence, trust and intention to use. An online research based on video material with 432 participates in total was conducted. As indicated by the results, trust repair strategies had a significant impact on the perception and attitudes of users. Comforting and apologizing after the accident increased the perceived warmth of participants on the vehicle system. More importantly, their trust and intention to use were supported. Promising to avoid the accident in the future increased the perceived competence, which supported users’ intention to use it indirectly. The trust repair strategy of intelligent vehicle system based on the social cognition theory has a significant influence on the cognition and attitudes of users, which provides not only a new perspective for the relevant practitioners in the field of human-computer interaction but also direct reference for the design of personified and anthropomorphic vehicle system agent.},
booktitle = {Cross-Cultural Design. Product and Service Design, Mobility and Automotive Design, Cities, Urban Areas, and Intelligent Environments Design: 14th International Conference, CCD 2022, Held as Part of the 24th HCI International Conference, HCII 2022, Virtual Event, June 26 – July 1, 2022, Proceedings, Part IV},
pages = {154–166},
numpages = {13},
keywords = {Trust repair, Social cognition theory, Human-computer interaction, Intelligent vehicle system}
}

@inproceedings{10.1145/3510003.3510177,
author = {Li, Yi and Wang, Shaohua and Nguyen, Tien N.},
title = {DEAR: a novel deep learning-based approach for automated program repair},
year = {2022},
isbn = {9781450392211},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3510003.3510177},
doi = {10.1145/3510003.3510177},
abstract = {The existing deep learning (DL)-based automated program repair (APR) models are limited in fixing general software defects. We present DEAR, a DL-based approach that supports fixing for the general bugs that require dependent changes at once to one or multiple consecutive statements in one or multiple hunks of code. We first design a novel fault localization (FL) technique for multi-hunk, multi-statement fixes that combines traditional spectrum-based (SB) FL with deep learning and data-flow analysis. It takes the buggy statements returned by the SBFL model, detects the buggy hunks to be fixed at once, and expands a buggy statement s in a hunk to include other suspicious statements around s. We design a two-tier, tree-based LSTM model that incorporates cycle training and uses a divide-and-conquer strategy to learn proper code transformations for fixing multiple statements in the suitable fixing context consisting of surrounding subtrees. We conducted several experiments to evaluate DEAR on three datasets: Defects4J (395 bugs), BigFix (+26k bugs), and CPatMiner (+44k bugs). On Defects4J dataset, DEAR outperforms the baselines from 42\%--683\% in terms of the number of auto-fixed bugs with only the top-1 patches. On BigFix dataset, it fixes 31--145 more bugs than existing DL-based APR models with the top-1 patches. On CPatMiner dataset, among 667 fixed bugs, there are 169 (25.3\%) multi-hunk/multi-statement bugs. DEAR fixes 71 and 164 more bugs, including 52 and 61 more multi-hunk/multi-statement bugs, than the state-of-the-art, DL-based APR models.},
booktitle = {Proceedings of the 44th International Conference on Software Engineering},
pages = {511–523},
numpages = {13},
keywords = {automated program repair, deep learning, fault localization},
location = {Pittsburgh, Pennsylvania},
series = {ICSE '22}
}

@inproceedings{10.5555/3585938.3585940,
author = {Li, Xiaolu and Cheng, Keyun and Tang, Kaicheng and Lee, Patrick P. C. and Hu, Yuchong and Feng, Dan and Li, Jie and Wu, Ting-Yi},
title = {ParaRC: embracing sub-packetization for repair parallelization in MSR-coded storage},
year = {2023},
isbn = {978-1-939133-32-8},
publisher = {USENIX Association},
address = {USA},
abstract = {Minimum-storage regenerating (MSR) codes are provably optimal erasure codes that minimize the repair bandwidth (i.e., the amount of traffic being transferred during a repair operation), with the minimum storage redundancy, in distributed storage systems. However, the practical repair performance of MSR codes still has significant room to improve, as the mathematical structure of MSR codes makes their repair operations difficult to parallelize. We present ParaRC, a parallel repair framework for MSR codes. ParaRC exploits the sub-packetization nature of MSR codes to parallelize the repair of sub-blocks and balance the repair load (i.e., the amount of traffic sent or received by a node) across the available nodes. We show that there exists a trade-off between the repair bandwidth and the maximum repair load, and further propose a fast heuristic that approximately minimizes the maximum repair load with limited search time for large coding parameters. We prototype our heuristic in ParaRC and show that ParaRC reduces the degraded read and full-node recovery times over the conventional centralized repair approach in MSR codes by up to 59.3\% and 39.2\%, respectively.},
booktitle = {Proceedings of the 21st USENIX Conference on File and Storage Technologies},
articleno = {2},
numpages = {15},
location = {Santa Clara, CA, USA},
series = {FAST'23}
}

@inproceedings{10.1109/ICSE43902.2021.00106,
author = {Henkel, Jordan and Silva, Denini and Teixeira, Leopoldo and d'Amorim, Marcelo and Reps, Thomas},
title = {Shipwright: A Human-in-the-Loop System for Dockerfile Repair},
year = {2021},
isbn = {9781450390859},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE43902.2021.00106},
doi = {10.1109/ICSE43902.2021.00106},
abstract = {Docker is a tool for lightweight OS-level virtualization. Docker images are created by performing a build, controlled by a source-level artifact called a Dockerfile. We studied Dockerfiles on GitHub, and---to our great surprise--- found that over a quarter of the examined Dockerfiles failed to build (and thus to produce images). To address this problem, we propose Shipwright, a human-in-the-loop system for finding repairs to broken Dockerfiles. Shipwright uses a modified version of the BERT language model to embed build logs and to cluster broken Dockerfiles. Using these clusters and a search-based procedure, we were able to design 13 rules for making automated repairs to Dockerfiles. With the aid of Shipwright, we submitted 45 pull requests (with a 42.2\% acceptance rate) to GitHub projects with broken Dockerfiles. Furthermore, in a "time-travel" analysis of broken Dockerfiles that were later fixed, we found that Shipwright proposed repairs that were equivalent to human-authored patches in 22.77\% of the cases we studied. Finally, we compared our work with recent, state-of-the-art, static Dockerfile analyses, and found that, while static tools detected possible build-failure-inducing issues in 20.6-33.8\% of the files we examined, Shipwright was able to detect possible issues in 73.25\% of the files and, additionally, provide automated repairs for 18.9\% of the files.},
booktitle = {Proceedings of the 43rd International Conference on Software Engineering},
pages = {1148–1160},
numpages = {13},
keywords = {DevOps, Docker, Repair},
location = {Madrid, Spain},
series = {ICSE '21}
}

@inproceedings{10.1109/ICSE43902.2021.00107,
author = {Jiang, Nan and Lutellier, Thibaud and Tan, Lin},
title = {CURE: Code-Aware Neural Machine Translation for Automatic Program Repair},
year = {2021},
isbn = {9781450390859},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE43902.2021.00107},
doi = {10.1109/ICSE43902.2021.00107},
abstract = {Automatic program repair (APR) is crucial to improve software reliability. Recently, neural machine translation (NMT) techniques have been used to fix software bugs automatically. While promising, these approaches have two major limitations. Their search space often does not contain the correct fix, and their search strategy ignores software knowledge such as strict code syntax. Due to these limitations, existing NMT-based techniques underperform the best template-based approaches.We propose CURE, a new NMT-based APR technique with three major novelties. First, CURE pre-trains a programming language (PL) model on a large software codebase to learn developer-like source code before the APR task. Second, CURE designs a new code-aware search strategy that finds more correct fixes by focusing on compilable patches and patches that are close in length to the buggy code. Finally, CURE uses a subword tokenization technique to generate a smaller search space that contains more correct fixes.Our evaluation on two widely-used benchmarks shows that CURE correctly fixes 57 Defects4J bugs and 26 QuixBugs bugs, outperforming all existing APR techniques on both benchmarks.},
booktitle = {Proceedings of the 43rd International Conference on Software Engineering},
pages = {1161–1173},
numpages = {13},
keywords = {automatic program repair, software reliability},
location = {Madrid, Spain},
series = {ICSE '21}
}

@inproceedings{10.5555/3540261.3542029,
author = {Chen, Zimin and Hellendoorn, Vincent J. and Maniatis, Petros and Lamblin, Pascal and Manzagol, Pierre-Antoine and Tarlow, Daniel and Moitra, Subhodeep},
title = {PLUR: a unifying, graph-based view of program learning, understanding, and repair},
year = {2021},
isbn = {9781713845393},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Machine learning for understanding and editing source code has recently attracted significant interest, with many developments in new models, new code representations, and new tasks. This proliferation can appear disparate and disconnected, making each approach seemingly unique and incompatible, thus obscuring the core machine learning challenges and contributions. In this work, we demonstrate that the landscape can be significantly simplified by taking a general approach of mapping a graph to a sequence of tokens and pointers. Our main result is to show that 16 recently published tasks of different shapes can be cast in this form, based on which a single model architecture achieves near or above state-of-the-art results on nearly all tasks, outperforming custom models like code2seq and alternative generic models like Transformers. This unification further enables multitask learning and a series of cross-cutting experiments about the importance of different modeling choices for code understanding and repair tasks. The full framework, called PLUR, is easily extensible to more tasks, and will be open-sourced},
booktitle = {Proceedings of the 35th International Conference on Neural Information Processing Systems},
articleno = {1768},
numpages = {13},
series = {NIPS '21}
}

@inproceedings{10.1145/3568294.3580122,
author = {Kraus, Johannes Maria and Merger, Julia and Gr\"{o}ner, Felix and P\"{a}tz, Jessica},
title = { 'Sorry' Says the Robot: The Tendency to Anthropomorphize and Technology Affinity Affect Trust in Repair Strategies after Error},
year = {2023},
isbn = {9781450399708},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3568294.3580122},
doi = {10.1145/3568294.3580122},
abstract = {This research investigates how six different trust repair strategies (apologies, explanations and denial) of a robot packing lunch bags affect trust after an error and how user dispositions predict trust in the repair strategies. In an online experiment, the perceived trustworthiness was assessed in a within-subjects design (N = 604) in which all strategies were evaluated in direct comparison. Higher trustworthiness of an apology (vs. no apology) and of a technical explanation for an error (vs. an empty and an anthropomorphic explanation) was found. In line with theoretical considerations, user personality was found to be associated with trust in specific strategies. A higher tendency to anthropomorphize technology was associated with higher trust in an anthropomorphic explanation and technological affinity was associated with a higher trust in the technical explanation. Taken together, personalization of trust repair strategies is a promising direction for individualized design to foster trustworthy human-robot interaction.},
booktitle = {Companion of the 2023 ACM/IEEE International Conference on Human-Robot Interaction},
pages = {436–441},
numpages = {6},
keywords = {errors, personality, reliability, trust, trust repair strategies},
location = {Stockholm, Sweden},
series = {HRI '23}
}

@inproceedings{10.1145/3643788.3648018,
author = {Mechtaev, Sergey and Tan, Shin Hwei},
title = {F1X at APR-COMP 2024},
year = {2024},
isbn = {9798400705779},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643788.3648018},
doi = {10.1145/3643788.3648018},
abstract = {Automated program repair aims to generate patches for buggy programs, a task often hindered by the cost of test executions in large projects. F1X introduces a novel methodology relying on test-equivalence relations, defining if two programs yield indistinguishable results for a specific test. By leveraging two test-equivalence relations based on runtime values and dependencies, F1X' algorithm categorises patches into test-equivalence classes, which helps to significantly reduce the number of required test execution to generate a patch without any information loss. Experiments on real-world programs from the ManyBugs benchmark demonstrated a substantial reduction in test executions, leading to efficiency gains over the previous methods, while retaining the patch quality. The efficiency and effectiveness of F1X was further shown in APR-COMP 2024, where it received the highest score in the Functional-C track.},
booktitle = {Proceedings of the 5th ACM/IEEE International Workshop on Automated Program Repair},
pages = {56–57},
numpages = {2},
location = {Lisbon, Portugal},
series = {APR '24}
}

@inproceedings{10.1145/3643788.3648020,
author = {Jiang, Nan and Wu, Yi},
title = {RepairCAT: Applying Large Language Model to Fix Bugs in AI-Generated Programs},
year = {2024},
isbn = {9798400705779},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643788.3648020},
doi = {10.1145/3643788.3648020},
abstract = {Automated program repair has been a crucial and popular domain for years, and with the development of large language models (LLMs) and the trend of using LLMs for code generation, there comes the new challenge of fixing bugs in LLM-generated (AI-generated) programs. In this work, we introduce RepairCAT, a simple and neat framework for fine-tuning large language models for automated repairing Python programs. Our experiments built on StarCoder-1B successfully generated patches fixing the failed test cases for 14 out of 100 bugs in the Python programs, 2 of which passed all the public test cases and were considered plausible.},
booktitle = {Proceedings of the 5th ACM/IEEE International Workshop on Automated Program Repair},
pages = {58–60},
numpages = {3},
keywords = {automated program repair, large language model},
location = {Lisbon, Portugal},
series = {APR '24}
}

@article{10.1145/3708522,
author = {Zhou, Xin and Cao, Sicong and Sun, Xiaobing and Lo, David},
title = {Large Language Model for Vulnerability Detection and Repair: Literature Review and the Road Ahead},
year = {2025},
issue_date = {June 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {34},
number = {5},
issn = {1049-331X},
url = {https://doi.org/10.1145/3708522},
doi = {10.1145/3708522},
abstract = {The significant advancements in Large Language Models (LLMs) have resulted in their widespread adoption across various tasks within Software Engineering (SE), including vulnerability detection and repair. Numerous studies have investigated the application of LLMs to enhance vulnerability detection and repair tasks. Despite the increasing research interest, there is currently no existing survey that focuses on the utilization of LLMs for vulnerability detection and repair. In this paper, we aim to bridge this gap by offering a systematic literature review of approaches aimed at improving vulnerability detection and repair through the utilization of LLMs. The review encompasses research work from leading SE, AI, and Security conferences and journals, encompassing 43 papers published across 25 distinct venues, along with 15 high-quality preprint papers, bringing the total to 58 papers. By answering three key research questions, we aim to (1) summarize the LLMs employed in the relevant literature, (2) categorize various LLM adaptation techniques in vulnerability detection, and (3) classify various LLM adaptation techniques in vulnerability repair. Based on our findings, we have identified a series of limitations of existing studies. Additionally, we have outlined a roadmap highlighting potential opportunities that we believe are pertinent and crucial for future research endeavors.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = may,
articleno = {145},
numpages = {31},
keywords = {Literature review, vulnerability detection, vulnerability repair, large language models}
}

@inproceedings{10.1145/3691620.3695066,
author = {Li, Guochang and Zhi, Chen and Chen, Jialiang and Han, Junxiao and Deng, Shuiguang},
title = {Exploring Parameter-Efficient Fine-Tuning of Large Language Model on Automated Program Repair},
year = {2024},
isbn = {9798400712487},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3691620.3695066},
doi = {10.1145/3691620.3695066},
abstract = {Automated Program Repair (APR) aims to fix bugs by generating patches. And existing work has demonstrated that "pre-training and fine-tuning" paradigm enables Large Language Models (LLMs) improve fixing capabilities on APR. However, existing work mainly focuses on Full-Model Fine-Tuning (FMFT) for APR and limited research has been conducted on the execution-based evaluation of Parameter-Efficient Fine-Tuning (PEFT) for APR. Comparing to FMFT, PEFT can reduce computing resource consumption without compromising performance and has been widely adopted to other software engineering tasks.To fill this gap, we enhance the existing APR dataset by employing prompt engineering to create an instruction dataset, APR-Instruction, at first. Secondly, we fine-tune four pre-trained LLMs using four different PEFT methods with APR-Instruction. The best fine-tuned model fixes 58\% more bugs than the state-of-the-art LLM-based APR techniques. The results also show that (IA)3 improves the creativity of LLMs more effectively through fine-tuning and achieves the highest fixing capability compared to the other three PEFT methods. Thirdly, we explore the optimal configuration of PEFT hyperparameters, and assess the impact of instruction dataset size, showing that a larger number of parameters and a larger training dataset do not necessarily result in better performance for PEFT. Lastly, we analyze peak memory usage and trainable parameters to show the efficiency of PEFT.This work provides a comprehensive exploration of PEFT on APR and suggests potentially promising directions for extension to other software engineering downstream tasks. APR-Instruction, PEFT weights, and the fine-tuning code are publicly available as open-source resources.},
booktitle = {Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering},
pages = {719–731},
numpages = {13},
keywords = {automated program repair, parameter-effective fine-tuning, large language model, execution-based evaluation},
location = {Sacramento, CA, USA},
series = {ASE '24}
}

@article{10.1145/3715738,
author = {Yang, Xu and Zhu, Wenhan and Pacheco, Michael and Zhou, Jiayuan and Wang, Shaowei and Hu, Xing and Liu, Kui},
title = {Code Change Intention, Development Artifact, and History Vulnerability: Putting Them Together for Vulnerability Fix Detection by LLM},
year = {2025},
issue_date = {July 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {FSE},
url = {https://doi.org/10.1145/3715738},
doi = {10.1145/3715738},
abstract = {Detecting vulnerability fix commits in open-source software is crucial for maintaining software security. To help OSS identify vulnerability fix commits, several automated approaches are developed. However, existing approaches like VulFixMiner and CoLeFunDa, focus solely on code changes, neglecting essential context from development artifacts. Tools like Vulcurator, which integrates issue reports, fail to leverage semantic associations between different development artifacts (e.g., pull requests and history vulnerability fixes). Moreover, they miss vulnerability fixes in tangled commits and lack explanations, limiting practical use. Hence to address those limitations, we propose LLM4VFD, a novel framework that leverages Large Language Models (LLMs) enhanced with Chain-of-Thought reasoning and In-Context Learning to improve the accuracy of vulnerability fix detection. LLM4VFD comprises three components: (1) Code Change Intention, which analyzes commit summaries, purposes, and implications using Chain-of-Thought reasoning; (2) Development Artifact, which incorporates context from related issue reports and pull requests; (3) Historical Vulnerability, which retrieves similar past vulnerability fixes to enrich context. More importantly, on top of the prediction, LLM4VFD also provides a detailed analysis and explanation to help security experts understand the rationale behind the decision. We evaluated LLM4VFD against state-of-the-art techniques, including Pre-trained Language Model-based approaches and vanilla LLMs, using a newly collected dataset, BigVulFixes. Experimental results demonstrate that LLM4VFD significantly outperforms the best-performed existing approach by 68.1\%--145.4\%. Furthermore, We conducted a user study with security experts, showing that the analysis generated by LLM4VFD improves the efficiency of vulnerability fix identification.},
journal = {Proc. ACM Softw. Eng.},
month = jun,
articleno = {FSE023},
numpages = {22},
keywords = {Large Language Model, Vulnerability Fix Detection}
}

@article{10.1145/3770581,
author = {Feng, Qiong and Ma, Xiaotian and Sheng, Jiayi and Feng, Ziyuan and Song, Wei and Liang, Peng},
title = {Integrating Various Software Artifacts for Better LLM-based Bug Localization and Program Repair},
year = {2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1049-331X},
url = {https://doi.org/10.1145/3770581},
doi = {10.1145/3770581},
abstract = {LLMs have garnered considerable attention for their potential to streamline Automated Program Repair (APR). LLM-based approaches can either insert the correct code using an infilling-style technique or directly generate patches when provided with buggy methods, aiming for plausible patches to pass all tests. However, most of LLM-based APR methods rely on a single type of software information, such as issue descriptions or error stack traces, without fully leveraging a combination of diverse software artifacts. Human developers, in contrast, often use a range of information — such as debugging data, issue discussions, and error stack traces — to diagnose and fix bugs. Despite this, many LLM-based approaches do not explore which specific types of software information best assist in localizing and repairing software bugs. Addressing this gap is crucial for advancing LLM-based APR techniques.To investigate this and mimic the way human developers fix bugs, we propose DEVLoRe (short for DEVeloper Localization and Repair). In this framework, LLMs first use issue content (description and discussion) and stack error traces to localize buggy methods, then rely on debug information in buggy methods and issue content and stack error to localize buggy lines and generate valid patches. We evaluated the effectiveness of issue content, error stack traces, and debugging information in bug localization and automatic program repair. Our results show that while issue content and error stack is particularly effective in assisting LLMs with fault localization and program repair respectively, different types of software artifacts complement each other in addressing various bugs. By incorporating these three types of artifacts and using the Defects4J v2.0 dataset for evaluation, DEVLoRe successfully localizes 49.3\% of single-method bugs and generates 56.0\% plausible patches. Additionally, DEVLoRe can localize 47.6\% of non-single-method bugs and generates 14.5\% plausible patches. Moreover, our framework streamlines the end-to-end process from buggy source code to a complete repair, and achieves a 39.7\% and 17.1\% of single-method and non-single-method bug repair rate, outperforming current state-of-the-art APR methods. Furthermore, we re-implemented and evaluated our framework, demonstrating its effectiveness in resolving 9 unique issues compared to other state-of-the-art frameworks using the same or more advanced models on SWE-bench Lite. We also discussed whether a leading framework for Python code can be directly applied to Java code, or vice versa. The source code and experimental results of this work for replication are available at .},
note = {Just Accepted},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = oct,
keywords = {Large Language Model, Automatic Program Repair, Fault Localization}
}

@article{10.1145/3771930,
author = {Wang, Dong and Yu, Junji and Shu, Honglin and Fu, Michael and Tantithamthavorn, Chakkrit and Kamei, Yasutaka and Chen, Junjie},
title = {On the Evaluation of Large Language Models in Multilingual Vulnerability Repair},
year = {2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1049-331X},
url = {https://doi.org/10.1145/3771930},
doi = {10.1145/3771930},
abstract = {Various Deep Learning-based approaches with pre-trained language models have been proposed for automatically repairing software vulnerabilities. However, these approaches are limited to a specific programming language (C/C++). Recent advances in large language models (LLMs) offer language-agnostic capabilities and strong semantic understanding, exhibiting potential to overcome multilingual vulnerability limitation. Although some work has begun to explore LLM’s repair performance, their effectiveness is unsatisfactory. To address these limitations, we conducted a large-scale empirical study to investigate the performance of automated vulnerability repair approaches and state-of-the-art LLMs across seven programming languages. Results show GPT-4o, instruction-tuned with few-shot prompting, performs competitively against the leading approach, VulMaster. Additionally, the LLM-based approach shows superior performance in repairing unique vulnerabilities and is more likely to repair the most dangerous vulnerabilities. Instruction-tuned GPT-4o demonstrates strong generalization on vulnerabilities in previously unseen language, outperforming existing approaches. Analysis shows that Go consistently achieves the highest effectiveness across all model types, while C/C++ performs the worst. Based on findings, we discuss the promising of LLM on multilingual vulnerability repair and reasons behind LLM failed cases. This work takes the first look at repair approaches and LLMs across multiple languages, highlighting the promising future of adopting LLMs to multilingual vulnerability repair.},
note = {Just Accepted},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = oct,
keywords = {Multilingual Vulnerability, Vulnerability Repair, Large Language Model}
}

@inproceedings{10.1145/3691620.3695349,
author = {Wang, Che and Zhang, Jiashuo and Gao, Jianbo and Xia, Libin and Guan, Zhi and Chen, Zhong},
title = {ContractTinker: LLM-Empowered Vulnerability Repair for Real-World Smart Contracts},
year = {2024},
isbn = {9798400712487},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3691620.3695349},
doi = {10.1145/3691620.3695349},
abstract = {Smart contracts are susceptible to being exploited by attackers, especially when facing real-world vulnerabilities. To mitigate this risk, developers often rely on third-party audit services to identify potential vulnerabilities before project deployment. Nevertheless, repairing the identified vulnerabilities is still complex and laborintensive, particularly for developers lacking security expertise. Moreover, existing pattern-based repair tools mostly fail to address real-world vulnerabilities due to their lack of high-level semantic understanding. To fill this gap, we propose ContractTinker, a Large Language Models (LLMs)-empowered tool for real-world vulnerability repair. The key insight is our adoption of the Chain-of-Thought approach to break down the entire generation task into subtasks. Additionally, to reduce hallucination, we integrate program static analysis to guide the LLM. We evaluate ContractTinker on 48 high-risk vulnerabilities. The experimental results show that among the patches generated by ContractTinker, 23 (48\%) are valid patches that fix the vulnerabilities, while 10 (21\%) require only minor modifications. A video of ContractTinker is available at https://youtu.be/HWFVi-YHcPE.},
booktitle = {Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering},
pages = {2350–2353},
numpages = {4},
keywords = {program repair, smart contract, large language model},
location = {Sacramento, CA, USA},
series = {ASE '24}
}

@inproceedings{10.1145/3643788.3648021,
author = {Lajko, Mark and Csuvik, Viktor and Gyimothy, Tibor and Vidacs, Laszlo},
title = {Automated Program Repair with the GPT Family, including GPT-2, GPT-3 and CodeX},
year = {2024},
isbn = {9798400705779},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643788.3648021},
doi = {10.1145/3643788.3648021},
abstract = {Automated Program Repair (APR) is a promising approach for addressing software defects and improving software reliability. There are various approaches to APR, including using Machine Learning (ML) techniques such as neural networks and evolutionary algorithms, as well as more traditional methods such as static analysis and symbolic execution. In recent years, there has been growing interest in using ML techniques for APR, including the use of large language models such as GPT-2 and GPT-3. These models have the ability to generate human-like text and code, making them well-suited for tasks such as generating repair patches for defective programs. In this paper, we explore the use of the GPT family (including GPT-2, GPT-J-6B, GPT-3 and Codex) for APR of JavaScript programs and evaluate their performance in terms of the number and quality of repair patches generated. Our results show that these state-of-the-art language models are able to generate repair patches that successfully fix the defects in the JavaScript programs, with Codex performing slightly better overall. To be precise, in our self-assembled dataset, Codex was able to generate 108 repair patches that are exactly the same as the developer fix for the first try. If we consider multiple patch generations, up to 201 buggy programs are being repaired automatically from the 1559 evaluation dataset (12.89\%).},
booktitle = {Proceedings of the 5th ACM/IEEE International Workshop on Automated Program Repair},
pages = {34–41},
numpages = {8},
keywords = {automated program repair, transformers, GPT-3, codex, JavaScript},
location = {Lisbon, Portugal},
series = {APR '24}
}

@inproceedings{10.1109/ICSE55347.2025.00169,
author = {Xu, Junjielong and Fu, Ying and Tan, Shin Hwei and He, Pinjia},
title = {Aligning the Objective of LLM-Based Program Repair},
year = {2025},
isbn = {9798331505691},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE55347.2025.00169},
doi = {10.1109/ICSE55347.2025.00169},
abstract = {Large language models (LLMs) have achieved decent results on automated program repair (APR). However, the next token prediction training objective of decoder-only LLMs (e.g., GPT-4) is misaligned with the masked span prediction objective of current infilling-style methods, which impedes LLMs from fully leveraging pre-trained knowledge for program repair. In addition, while some LLMs can locate and repair bugs in certain functions using the related artifacts (e.g., test cases), existing methods still depend on statement-level fault localization methods to provide a list of buggy hunks for repair. This restriction hinders LLMs from exploring potential patches beyond the given locations.In this paper, we investigate a new approach to adapt LLMs to program repair. Our core insight is that LLM's APR capability can be greatly improved by simply aligning the output to their training objective and allowing them to refine the whole program without first identifying faulty statements. Based on this insight, we designed D4C, a straightforward prompting framework for APR. D4C can repair 180 bugs correctly in Defects4J, with each patch being sampled only 10 times. This surpasses the SOTA APR methods with perfect fault localization by 10\% and reduces the patch sampling number by 90\%. Our findings reveal that (1) objective alignment is crucial for fully exploiting LLM's pre-trained capability, and (2) replacing the traditional localize-buggy-hunks-then-repair workflow with direct debugging is more effective for LLM-based APR methods. Thus, we believe this paper introduces a new mindset for harnessing LLMs in APR.},
booktitle = {Proceedings of the IEEE/ACM 47th International Conference on Software Engineering},
pages = {2548–2560},
numpages = {13},
keywords = {automated program repair, large language model, objective alignment},
location = {Ottawa, Ontario, Canada},
series = {ICSE '25}
}

@article{10.1007/s10515-025-00568-8,
author = {Lin, Renze and Wang, Ran and Hu, Guanghuan and Xu, Xianghua},
title = {LMFuzz: Program repair fuzzing based on large language models},
year = {2025},
issue_date = {Dec 2025},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {33},
number = {1},
issn = {0928-8910},
url = {https://doi.org/10.1007/s10515-025-00568-8},
doi = {10.1007/s10515-025-00568-8},
abstract = {Generating programs using large language models (LLMs) for fuzz testing has emerged as a significant testing methodology. While traditional fuzzers can produce correct programs, their effectiveness is limited by excessive constraints and restricted API combinations, resulting in insufficient coverage of the target system’s code and impacting testing efficiency. Unlike traditional methods, large language model based fuzzers can generate more diverse code, effectively addressing key issues of conventional fuzzers. However, the lack of constraints on API combinations during the generation process often leads to reduced program validity. Therefore, a crucial challenge is to enhance the validity of generated code while maintaining its diversity. To address this issue, we propose a novel and universal fuzzer, LMFuzz. To ensure the fuzzer’s generation capability, we utilize a large language model as the primary generator and model the operator selection problem within the fuzzing loop as a multi-armed bandit problem. We introduce the Thompson Sampling algorithm to enhance both the diversity and validity of program generation. To improve the validity of the generated code, we incorporate a program repair loop that iteratively corrects the generated programs, thereby reducing errors caused by the lack of API combination constraints. Experimental results demonstrate that LMFuzz significantly surpasses existing state-of-the-art large language model based fuzzers in terms of coverage and validity, and also exhibits notable advantages in generating diverse programs. Furthermore, LMFuzz has identified 24 bugs across five popular programming languages and their corresponding systems.},
journal = {Automated Software Engg.},
month = oct,
numpages = {27},
keywords = {Fuzzing, Large language model, Code generation, Thompson sampling algorithm, Program Repair, Software security}
}

@inproceedings{10.1007/978-3-031-78386-9_28,
author = {Hori, Shota and Matsumoto, Shinsuke and Higo, Yoshiki and Kusumoto, Shinji and Yasuda, Kazuya and Ito, Shinji and Huyen, Phan Thi Thanh},
title = {The Effects of&nbsp;Semantic Information on&nbsp;LLM-Based Program Repair},
year = {2024},
isbn = {978-3-031-78385-2},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-78386-9_28},
doi = {10.1007/978-3-031-78386-9_28},
abstract = {Large Language Model-based Automated Program Repair (LLM-APR) has recently received significant attention as a debugging assistance. Our objective is to improve the performance of LLM-APR.&nbsp;In this study, we focus on semantic information contained in the source code. Semantic information refers to elements used by the programmer to understand the source code, which does not contribute&nbsp;to compilation or execution. We picked out specification, method&nbsp;names and variable names as semantic information. In the investigation,&nbsp;we prepared eight prompts, each consisting of all combinations of&nbsp;three types of semantic information. The experimental results showed&nbsp;that all semantic information improves the performance of LLM-APR,&nbsp;and variable names are particularly significant.},
booktitle = {Product-Focused Software Process Improvement: 25th International Conference, PROFES 2024, Tartu, Estonia, December 2–4, 2024, Proceedings},
pages = {377–385},
numpages = {9},
keywords = {Large language model (LLM), automated program repair (APR), semantic information, prompt engineering, ChatGPT},
location = {Tartu, Estonia}
}

@inproceedings{10.1145/3650212.3652106,
author = {Shan, Shiwen and Huo, Yintong and Su, Yuxin and Li, Yichen and Li, Dan and Zheng, Zibin},
title = {Face It Yourselves: An LLM-Based Two-Stage Strategy to Localize Configuration Errors via Logs},
year = {2024},
isbn = {9798400706127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3650212.3652106},
doi = {10.1145/3650212.3652106},
abstract = {Configurable software systems are prone to configuration errors, resulting in significant losses to companies. However, diagnosing these errors is challenging due to the vast and complex configuration space. These errors pose significant challenges for both experienced maintainers and new end-users, particularly those without access to the source code of the software systems. Given that logs are easily accessible to most end-users, we conduct a preliminary study to outline the challenges and opportunities of utilizing logs in localizing configuration errors. Based on the insights gained from the preliminary study, we propose an LLM-based two-stage strategy for end-users to localize the root-cause configuration properties based on logs. We further implement a tool, LogConfigLocalizer, aligned with the design of the aforementioned strategy, hoping to assist end-users in coping with configuration errors through log analysis. To the best of our knowledge, this is the first work to localize the root-cause configuration properties for end-users based on Large Language Models (LLMs) and logs. We evaluate the proposed strategy on Hadoop by LogConfigLocalizer and prove its efficiency with an average accuracy as high as 99.91\%. Additionally, we also demonstrate the effectiveness and necessity of different phases of the methodology by comparing it with two other variants and a baseline tool. Moreover, we validate the proposed methodology through a practical case study to demonstrate its effectiveness and feasibility.},
booktitle = {Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {13–25},
numpages = {13},
keywords = {Configuration Errors, Large Language Model, Log Analysis},
location = {Vienna, Austria},
series = {ISSTA 2024}
}

@article{10.1145/3719345,
author = {Kong, Jiaolong and Xie, Xiaofei and Cheng, Mingfei and Liu, Shangqing and Du, Xiaoning and Guo, Qi},
title = {ContrastRepair: Enhancing Conversation-Based Automated Program Repair via Contrastive Test Case Pairs},
year = {2025},
issue_date = {November 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {34},
number = {8},
issn = {1049-331X},
url = {https://doi.org/10.1145/3719345},
doi = {10.1145/3719345},
abstract = {Automated Program Repair (APR) aims to automatically generate patches for rectifying software bugs. Recent strides in Large Language Models (LLM), such as ChatGPT, have yielded encouraging outcomes in APR, especially within the conversation-driven APR framework. Nevertheless, the efficacy of conversation-driven APR is contingent on the quality of the feedback information. In this article, we propose ContrastRepair, a novel conversation-based APR approach that augments conversation-driven APR by providing LLMs with contrastive test pairs. A test pair consists of a failing test and a passing test, which offer contrastive feedback to the LLM. Our key insight is to minimize the difference between the generated passing test and the given failing test, which can better isolate the root causes of bugs. By providing such informative feedback, ContrastRepair enables the LLM to produce effective bug fixes. The implementation of ContrastRepair is based on the state-of-the-art LLM, ChatGPT, and it iteratively interacts with ChatGPT until plausible patches are generated. We evaluate ContrastRepair on multiple benchmark datasets, including Defects4J, QuixBugs, and HumanEval-Java. The results demonstrate that ContrastRepair significantly outperforms existing methods, achieving a new state-of-the-art in program repair. For instance, among Defects4J 1.2 and 2.0, ContrastRepair correctly repairs 143 out of all 337 bug cases, while the best-performing baseline fixes 124 bugs.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = oct,
articleno = {216},
numpages = {31},
keywords = {Program Repair, Large Language Model}
}

@article{10.1016/j.csi.2024.103951,
author = {Zubair, Fida and Al-Hitmi, Maryam and Catal, Cagatay},
title = {The use of large language models for program repair},
year = {2025},
issue_date = {Apr 2025},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {93},
number = {C},
issn = {0920-5489},
url = {https://doi.org/10.1016/j.csi.2024.103951},
doi = {10.1016/j.csi.2024.103951},
journal = {Comput. Stand. Interfaces},
month = apr,
numpages = {12},
keywords = {Large language model, Program repair, Software engineering, Automated program repair, SLR, LLM, AI, GPT, BERT, APR, F1, BLEU, GLUE, SQuAD, RQ, C1}
}

@inproceedings{10.1145/3650212.3680328,
author = {Yang, Boyang and Tian, Haoye and Pian, Weiguo and Yu, Haoran and Wang, Haitao and Klein, Jacques and Bissyand\'{e}, Tegawend\'{e} F. and Jin, Shunfu},
title = {CREF: An LLM-Based Conversational Software Repair Framework for Programming Tutors},
year = {2024},
isbn = {9798400706127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3650212.3680328},
doi = {10.1145/3650212.3680328},
abstract = {With the proven effectiveness of Large Language Models (LLMs) in code-related tasks, researchers have explored their potential for program repair. However, existing repair benchmarks might have influenced LLM training data, potentially causing data leakage. To evaluate LLMs’ realistic repair capabilities, (i) we introduce an extensive, non-crawled benchmark TutorCode, comprising 1,239 C++ defect codes and associated information such as tutor guidance, solution description, failing test cases, and the corrected code. Our work assesses LLM’s repair performance on TutorCode, measuring repair correctness (TOP-5 and AVG-5) and patch precision (RPSR). (ii) We then provide a comprehensive investigation into which types of extra information can help LLMs improve their repair performance. Among these types, tutor guidance was the most effective information. To fully harness LLMs’ conversational capabilities and the benefits of augmented information, (iii) we introduce a novel conversational semi-automatic repair framework CREF assisting human programming tutors. It demonstrates a remarkable AVG-5 improvement of 17.2\%-24.6\% compared to the baseline, achieving an impressive AVG-5 of 76.6\% when utilizing GPT-4. These results highlight the potential for enhancing LLMs’ repair capabilities through tutor interactions and historical conversations. The successful application of CREF in a real-world educational setting demonstrates its effectiveness in reducing tutors’ workload and improving students’ learning experience, showing promise for code review and other software engineering tasks.},
booktitle = {Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {882–894},
numpages = {13},
keywords = {Large Language Model, Open Source, Program Repair},
location = {Vienna, Austria},
series = {ISSTA 2024}
}

@inproceedings{10.1145/3671016.3674823,
author = {Liu, Xingpeng and Liu, Hengzhu and Yi, Xiaodong and Wang, Ji},
title = {LLM-Enhanced Theorem Proving with Term Explanation and Tactic Parameter Repair✱},
year = {2024},
isbn = {9798400707056},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3671016.3674823},
doi = {10.1145/3671016.3674823},
abstract = {There has been emerging researches on leveraging large language models (LLMs) to improve the automation of theorem proving. However, they are still suffering from low accuracy and efficiency. In this paper, we propose to strengthen the existing approach by enhancing a language agent, which provides automatic explanation of terms and repair of tactics parameters. Term explanation explains terms specific to the proof obligations formally and tactic parameter repair complements the potentially correct proof tactics as much as possible. Similar to the existing approach, the agent uses GPT-4 as query objects in a search policy. During the search, we add term explanation to the prompt, and then the policy selects a proof tactic and repairs it. The repaired tactics interact with the theorem prover (Coq), and the execution result is fed back to build the prompt for the next policy invocation. We evaluate our approach on subsets of the CompCert project implemented using Coq. Our approach proves 8.11\% more theorems than the existing language agent COPRA, and demonstrates faster search and proof speed. Besides, when term explanation and tactic parameter repair are applied, the performance of the SOTA method PROVERBOT9001 can be also improved.},
booktitle = {Proceedings of the 15th Asia-Pacific Symposium on Internetware},
pages = {21–30},
numpages = {10},
keywords = {Coq, Language Agent, Large Language Model, Theorem Proving},
location = {Macau, China},
series = {Internetware '24}
}

@article{10.1145/3735129,
author = {Yang, Boyang and Tian, Haoye and Ren, Jiadong and Zhang, Hongyu and Klein, Jacques and Bissyand\'{e}, Tegawend\'{e} F. and Goues, Claire Le and Jin, Shunfu},
title = {MORepair: Teaching LLMs to Repair Code via Multi-Objective Fine-Tuning},
year = {2026},
issue_date = {February 2026},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {35},
number = {2},
issn = {1049-331X},
url = {https://doi.org/10.1145/3735129},
doi = {10.1145/3735129},
abstract = {Within the realm of software engineering, specialized tasks on code, such as program repair, present unique challenges, necessitating fine-tuning Large language models (LLMs) to unlock state-of-the-art performance. Fine-tuning approaches proposed in the literature for LLMs on program repair tasks generally overlook the need to reason about the logic behind code changes, beyond syntactic patterns in the data. High-performing fine-tuning experiments also usually come at very high computational costs. With MORepair, we propose a novel perspective on the learning focus of LLM fine-tuning for program repair: we not only adapt the LLM parameters to the syntactic nuances of the task of code transformation (objective ➊), but we also specifically fine-tune the LLM with respect to the logical reason behind the code change in the training data (objective ➋). Such a multi-objective fine-tuning will instruct LLMs to generate high-quality patches.We apply MORepair to fine-tune four open-source LLMs with different sizes and architectures. Experimental results on function-level and repository-level repair benchmarks show that the implemented fine-tuning effectively boosts LLM repair performance by 11.4\% to 56.0\%. We further show that our fine-tuning strategy yields superior performance compared to the state-of-the-art approaches, including standard fine-tuning, Fine-tune-CoT, and RepairLLaMA.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = jan,
articleno = {38},
numpages = {38},
keywords = {Program Repair, Fine-tuning, Large Language Model, Open Source}
}

@article{10.1145/3771922,
author = {Vallecillos Ruiz, Fernando and Grishina, Anastasiia and Hort, Max and Moonen, Leon},
title = {Assessing the Latent Automated Program Repair Capabilities of Large Language Models using Round-Trip Translation},
year = {2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1049-331X},
url = {https://doi.org/10.1145/3771922},
doi = {10.1145/3771922},
abstract = {Research shows that errors in natural language can be corrected by translating texts to another language and back using language models. We explore to what extent this latent correction capability extends to Automated Program Repair (APR) by investigating Round-Trip Translation (RTT): translating code from one programming language into another programming or natural language and back, using Large Language Models (LLMs). We hypothesize that RTT restores patterns most commonly seen in the LLM’s training corpora through regression toward the mean, replacing infrequent bugs with more frequent, natural, bug-free code. To test this hypothesis, we employ nine LLMs and four common APR benchmarks in Java, and perform a detailed quantitative and qualitative analysis of RTT-generated patches. We find that RTT through English generates plausible patches for 100 of 164 bugs with GPT-4 on the HumanEval-Java benchmark, and 97 are found to be correct in our manual assessment. Moreover, RTT uniquely generates plausible patches for 46 bugs that were missed by LLMs specifically fine-tuned for APR. While this demonstrates the viability of RTT for APR, we also observe limitations, such as a lower overall bug fix rate than the state-of-the-art and diluting the original coding style. We analyze the impact of these limitations and discuss the potential of using RTT as a complementary component in APR frameworks.},
note = {Just Accepted},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = oct,
keywords = {automated program repair, large language model, machine translation}
}

@inproceedings{10.1145/3756681.3757078,
author = {Antal, G\'{a}bor and Bogenf\"{u}rst, Bence and Ferenc, Rudolf and Hegedus, P\'{e}ter},
title = {Identifying Helpful Context for LLM-based Vulnerability Repair: A Preliminary Study},
year = {2025},
isbn = {9798400713859},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3756681.3757078},
doi = {10.1145/3756681.3757078},
abstract = {Recent advancements in large language models (LLMs) have shown promise for automated vulnerability detection and repair in software systems. This paper investigates the performance of GPT-4o in repairing Java vulnerabilities from a widely used dataset (Vul4J), exploring how different contextual information affects automated vulnerability repair (AVR) capabilities. We compare the latest GPT-4o’s performance against previous results with GPT-4 using identical prompts. We evaluated nine additional prompts crafted by us that contain various contextual information such as CWE or CVE information, and manually extracted code contexts. Each prompt was executed three times on 42 vulnerabilities, and the resulting fix candidates were validated using Vul4J’s automated testing framework. Our results show that GPT-4o performed 11.9\% worse on average than GPT-4 with the same prompt, but was able to fix 10.5\% more distinct vulnerabilities in the three runs together. CVE information significantly improved repair rates, while the length of the task description had minimal impact. Combining CVE guidance with manually extracted code context resulted in the best performance. Using our Top-3 prompts together, GPT-4o repaired 26 (62\%) vulnerabilities at least once, outperforming both the original baseline (40\%) and its reproduction (45\%), suggesting that ensemble prompt strategies could improve vulnerability repair in zero-shot settings.},
booktitle = {Proceedings of the 29th International Conference on Evaluation and Assessment in Software Engineering},
pages = {696–700},
numpages = {5},
keywords = {LLM, vulnerability repair, contextual information, prompt engineering, software security, automated program repair},
location = {
},
series = {EASE '25}
}

@article{10.1145/3731754,
author = {Zhong, Renyi and Li, Yichen and Kuang, Jinxi and Gu, Wenwei and Huo, Yintong and Lyu, Michael R.},
title = {LogUpdater: Automated Detection and Repair of Specific Defects in Logging Statements},
year = {2025},
issue_date = {January 2026},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {35},
number = {1},
issn = {1049-331X},
url = {https://doi.org/10.1145/3731754},
doi = {10.1145/3731754},
abstract = {Developers write logging statements to monitor software runtime behaviors and system state. However, poorly constructed or misleading log messages can inadvertently obfuscate actual program execution patterns, thereby impeding effective software maintenance. Existing research on analyzing issues within logging statements is limited, primarily focusing on detecting a singular type of defect and relying on manual intervention for fixes rather than automated solutions.To address the limitation, we initiate a systematic study that pinpoints four specific types of defects in logging statements (i.e., statement code inconsistency, static dynamic inconsistency, temporal relation inconsistency, and readability issues) through the analysis of real-world log-centric changes. We then propose LogUpdater, a two-stage framework for automatically detecting and updating logging statements for these specific defects. In the offline stage, LogUpdater constructs a similarity-based classifier on a set of synthetic defective logging statements to identify specific defect types. During the online testing phase, this classifier first evaluates logging statements in a given code snippet to determine the necessity and type of improvements required. Then, LogUpdater constructs type-aware prompts from historical logging update changes for an LLM-based recommendation framework to suggest updates addressing these specific defects.We evaluate the effectiveness of LogUpdater on a dataset containing real-world logging changes, a synthetic dataset, and a new real-world project dataset. The results indicate that our approach is highly effective in detecting logging defects, achieving an F1-score of 0.625. Additionally, it exhibits significant improvements in suggesting precise static text and dynamic variables, with enhancements of 48.12\% and 24.90\%, respectively. Furthermore, LogUpdater achieves a 61.49\% success rate in recommending correct updates on new real-world projects. We reported 40 problematic logging statements and their fixes to GitHub via pull requests, resulting in 25 changes confirmed and merged across 11 different projects.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = dec,
articleno = {16},
numpages = {31},
keywords = {Logging Statement, Logging Practice, Large Language Model}
}

@inproceedings{10.1109/ICSE55347.2025.00157,
author = {Bouzenia, Islem and Devanbu, Premkumar and Pradel, Michael},
title = {RepairAgent: An Autonomous, LLM-Based Agent for Program Repair},
year = {2025},
isbn = {9798331505691},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE55347.2025.00157},
doi = {10.1109/ICSE55347.2025.00157},
abstract = {Automated program repair has emerged as a powerful technique to mitigate the impact of software bugs on system reliability and user experience. This paper introduces RepairAgent, the first work to address the program repair challenge through an autonomous agent based on a large language model (LLM). Unlike existing deep learning-based approaches, which prompt a model with a fixed prompt or in a fixed feedback loop, our work treats the LLM as an agent capable of autonomously planning and executing actions to fix bugs by invoking suitable tools. RepairAgent freely interleaves gathering information about the bug, gathering repair ingredients, and validating fixes, while deciding which tools to invoke based on the gathered information and feedback from previous fix attempts. Key contributions that enable RepairAgent include a set of tools that are useful for program repair, a dynamically updated prompt format that allows the LLM to interact with these tools, and a finite state machine that guides the agent in invoking the tools. Our evaluation on the popular Defects4J dataset demonstrates RepairAgent's effectiveness in autonomously repairing 164 bugs, including 39 bugs not fixed by prior techniques. Interacting with the LLM imposes an average cost of 270k tokens per bug, which, under the current pricing of OpenAI's GPT-3.5 model, translates to 14 cents per bug. To the best of our knowledge, this work is the first to present an autonomous, LLM-based agent for program repair, paving the way for future agent-based techniques in software engineering.},
booktitle = {Proceedings of the IEEE/ACM 47th International Conference on Software Engineering},
pages = {2188–2200},
numpages = {13},
location = {Ottawa, Ontario, Canada},
series = {ICSE '25}
}

@inproceedings{10.1145/3663529.3663815,
author = {Wu, Yonghao and Li, Zheng and Zhang, Jie M. and Liu, Yong},
title = {ConDefects: A Complementary Dataset to Address the Data Leakage Concern for LLM-Based Fault Localization and Program Repair},
year = {2024},
isbn = {9798400706585},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3663529.3663815},
doi = {10.1145/3663529.3663815},
abstract = {With the growing interest on Large Language Models (LLMs) for fault localization and program repair, ensuring the integrity and generalizability of the LLM-based methods becomes paramount. The code in existing widely-adopted benchmarks for these tasks was written before the bloom of LLMs and may be included in the training data of existing popular LLMs, thereby suffering from the threat of data leakage, leading to misleadingly optimistic performance metrics.  To address this issue, we introduce ConDefects, a dataset developed as a complement to existing datasets, meticulously curated with real faults to eliminate such overlap. ConDefects contains 1,254 Java faulty programs and 1,625 Python faulty programs. All these programs are sourced from the online competition platform AtCoder and were produced between October 2021 and September 2023. We pair each fault with fault locations and the corresponding repaired code versions, making it tailored for fault localization and program repair related research. We also provide interfaces for selecting subsets based on different time windows and coding task difficulties. While inspired by LLM-based tasks, ConDefects can be adopted for benchmarking ALL types of fault localization and program repair methods. The dataset is publicly available, and a demo video can be found at https://www.youtube.com/watch?v=22j15Hj5ONk.},
booktitle = {Companion Proceedings of the 32nd ACM International Conference on the Foundations of Software Engineering},
pages = {642–646},
numpages = {5},
keywords = {Dataset, Fault Localization, Large Language Model, Program Repair},
location = {Porto de Galinhas, Brazil},
series = {FSE 2024}
}

@inproceedings{10.1145/3650212.3680323,
author = {Xia, Chunqiu Steven and Zhang, Lingming},
title = {Automated Program Repair via Conversation: Fixing 162 out of 337 Bugs for $0.42 Each using ChatGPT},
year = {2024},
isbn = {9798400706127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3650212.3680323},
doi = {10.1145/3650212.3680323},
abstract = {Automated Program Repair (APR) aims to automatically generate patches for buggy programs. Traditional APR techniques suffer from a lack of patch variety as they rely heavily on handcrafted or mined bug fixing patterns and cannot easily generalize to other bug/fix types. To address this limitation, recent APR work has been focused on leveraging modern Large Language Models (LLMs) to directly generate patches for APR. Such LLM-based APR tools work by first constructing an input prompt built using the original buggy code and then querying the LLM to either fill-in (cloze-style APR) the correct code at the bug location or to produce a completely new code snippet as the patch. While the LLM-based APR tools are able to achieve state-of-the-art results, they still follow the classic Generate and Validate (GV) repair paradigm of first generating lots of patches by sampling from the same initial prompt and then validating each one afterwards. This not only leads to many repeated patches that are incorrect, but also misses the crucial and yet previously ignored information in test failures as well as in plausible patches.        To address these aforementioned limitations, we propose ChatRepair, the first fully automated conversation-driven APR approach that interleaves patch generation with instant feedback to perform APR in a conversational style. ChatRepair first feeds the LLM with relevant test failure information to start with, and then learns from both failures and successes of earlier patching attempts of the same bug for more powerful APR. For earlier patches that failed to pass all tests, we combine the incorrect patches with their corresponding relevant test failure information to construct a new prompt for the LLM to generate the next patch. In this way, we can avoid making the same    mistakes. For earlier patches that passed all the tests (i.e., plausible patches), we further ask the LLM to generate alternative variations of the original plausible patches. In this way, we can further build on and learn from earlier successes to generate more plausible patches to increase the chance of having correct patches. While our approach is general, we implement ChatRepair using state-of-the-art dialogue-based LLM – ChatGPT. Our evaluation on the widely studied Defects4j dataset shows that ChatRepair is able to achieve the new state-of-the-art in repair performance, achieving 114 and 48 correct fixes on Defects4j 1.2 and 2.0 respectively. By calculating the cost    of accessing ChatGPT, we can fix 162 out of 337 bugs for $0.42 each!},
booktitle = {Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {819–831},
numpages = {13},
keywords = {Automated Program Repair, Large Language Model},
location = {Vienna, Austria},
series = {ISSTA 2024}
}

@article{10.1145/3715004,
author = {Li, Fengjie and Jiang, Jiajun and Sun, Jiajun and Zhang, Hongyu},
title = {Hybrid Automated Program Repair by Combining Large Language Models and Program Analysis},
year = {2025},
issue_date = {September 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {34},
number = {7},
issn = {1049-331X},
url = {https://doi.org/10.1145/3715004},
doi = {10.1145/3715004},
abstract = {Automated Program Repair (APR) has garnered significant attention due to its potential to streamline the bug repair process for human developers. Recently, LLM-based APR methods have shown promise in repairing real-world bugs. However, existing APR methods often utilize patches generated by LLMs without further optimization, resulting in reduced effectiveness due to the lack of program-specific knowledge. Furthermore, the evaluations of these APR methods have typically been conducted under the assumption of perfect fault localization, which may not accurately reflect their real-world effectiveness. To address these limitations, this article introduces an innovative APR approach called GiantRepair. Our approach leverages the insight that LLM-generated patches, although not necessarily correct, offer valuable guidance for the patch generation process. Based on this insight, GiantRepair first constructs patch skeletons from LLM-generated patches to confine the patch space, and then generates high-quality patches tailored to specific programs through context-aware patch generation by instantiating the skeletons. To evaluate the performance of our approach, we conduct two large-scale experiments. The results demonstrate that GiantRepair not only effectively repairs more bugs (an average of 27.78\% on Defects4J v1.2 and 23.40\% on Defects4J v2.0) than using LLM-generated patches directly, but also outperforms state-of-the-art APR methods by repairing at least 42 and 7 more bugs under perfect and automated fault localization scenarios, respectively.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = aug,
articleno = {202},
numpages = {28},
keywords = {Program Repair, Large Language Model, Program Synthesis}
}

@inbook{10.1109/ICSE55347.2025.00246,
author = {Ouyang, Shuyin and Zhang, Jie M. and Sun, Zeyu and Penuela, Albert Merono},
title = {Knowledge-Enhanced Program Repair for Data Science Code},
year = {2025},
isbn = {9798331505691},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE55347.2025.00246},
abstract = {This paper introduces DSrepair, a knowledge-enhanced program repair approach designed to repair the buggy code generated by LLMs in the data science domain. DSrepair uses knowledge graph based RAG for API knowledge retrieval and bug knowledge enrichment to construct repair prompts for LLMs. Specifically, to enable knowledge graph-based API retrieval, we construct DS-KG (Data Science Knowledge Graph) for widely used data science libraries. For bug knowledge enrichment, we employ an abstract syntax tree (AST) to localize errors at the AST node level. We evaluate DSrepair's effectiveness against five state-of-the-art LLM-based repair baselines using four advanced LLMs on the DS-1000 dataset. The results show that DSrepair outperforms all five baselines. Specifically, when compared to the second-best baseline, DSrepair achieves substantial improvements, fixing 44.4\%, 14.2\%, 20.6\%, and 32.1\% more buggy code snippets for each of the four evaluated LLMs, respectively. Additionally, it achieves greater efficiency, reducing the number of tokens required per code task by 17.49\%, 34.24\%, 24.71\%, and 17.59\%, respectively.},
booktitle = {Proceedings of the IEEE/ACM 47th International Conference on Software Engineering},
pages = {898–910},
numpages = {13}
}

@inproceedings{10.1145/3650212.3680359,
author = {Yin, Xin and Ni, Chao and Wang, Shaohua and Li, Zhenhao and Zeng, Limin and Yang, Xiaohu},
title = {ThinkRepair: Self-Directed Automated Program Repair},
year = {2024},
isbn = {9798400706127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3650212.3680359},
doi = {10.1145/3650212.3680359},
abstract = {Though many approaches have been proposed for Automated Program Repair (APR) and indeed achieved remarkable performance, they still have limitations in fixing bugs that require analyzing and reasoning about the logic of the buggy program. Recently, large language models (LLMs) instructed by prompt engineering have attracted much attention for their powerful ability to address many kinds of tasks including bug-fixing. However, the quality of the prompt will highly affect the ability of LLMs and manually constructing high-quality prompts is a costly endeavor.    To address this limitation, we propose a self-directed LLM-based automated program repair, ThinkRepair, with two main phases: collection phase and fixing phase. The former phase automatically collects various chains of thoughts that constitute pre-fixed knowledge by instructing LLMs with the Chain-of-Thought (CoT) prompt. The latter phase targets fixing a bug by first selecting examples for few-shot learning and second automatically interacting with LLMs, optionally appending with feedback of testing information.    Evaluations on two widely studied datasets (Defects4J and QuixBugs) by comparing ThinkRepair with 12 SOTA APRs indicate the priority of ThinkRepair in fixing bugs. Notably, ThinkRepair fixes 98 bugs and improves baselines by 27\%∼344.4\% on Defects4J V1.2. On Defects4J V2.0, ThinkRepair fixes 12∼65 more bugs than the SOTA APRs. Additionally, ThinkRepair also makes a considerable improvement on QuixBugs (31 for Java and 21 for Python at most).},
booktitle = {Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {1274–1286},
numpages = {13},
keywords = {Automated Program Repair, Large Language Model, Prompt Engineering},
location = {Vienna, Austria},
series = {ISSTA 2024}
}

@inproceedings{10.1145/3524459.3527351,
author = {Prenner, Julian Aron and Babii, Hlib and Robbes, Romain},
title = {Can OpenAI's codex fix bugs? an evaluation on QuixBugs},
year = {2022},
isbn = {9781450392853},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3524459.3527351},
doi = {10.1145/3524459.3527351},
abstract = {OpenAI's Codex, a GPT-3 like model trained on a large code corpus, has made headlines in and outside of academia. Given a short user-provided description, it is capable of synthesizing code snippets that are syntactically and semantically valid in most cases. In this work, we want to investigate whether Codex is able to localize and fix bugs, two important tasks in automated program repair. Our initial evaluation uses the multi-language QuixBugs benchmark (40 bugs in both Python and Java). We find that, despite not being trained for APR, Codex is surprisingly effective, and competitive with recent state of the art techniques. Our results also show that Codex is more successful at repairing Python than Java, fixing 50\% more bugs in Python.},
booktitle = {Proceedings of the Third International Workshop on Automated Program Repair},
pages = {69–75},
numpages = {7},
keywords = {QuixBugs, automatic program repair, codex, deep learning},
location = {Pittsburgh, Pennsylvania},
series = {APR '22}
}

@inproceedings{10.1145/3716368.3735171,
author = {Fu, Weimin and Li, Shijie and Jin, Yier and Guo, Xiaolong},
title = {HWFixBench: Benchmarking Tools for Hardware Understanding and Fault Repair},
year = {2025},
isbn = {9798400714962},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3716368.3735171},
doi = {10.1145/3716368.3735171},
abstract = {Large Language Models (LLMs) have demonstrated remarkable potential in automating coding tasks, making their application in Electronic Design Automation (EDA) an increasingly popular research direction. However, existing benchmarks for evaluating LLMs’ hardware-related capabilities are overly simplistic and fail to capture the complexity of real-world hardware projects. To address this gap, we constructed HWFixBench, a comprehensive benchmark derived from 500 pull requests and 1,481 bug fixes across 12 widely-used open-source hardware projects. HWFixBench reflects the challenges and complexity of real-world hardware tasks, providing a rigorous testbed for assessing whether LLMs can truly automate hardware development. We evaluate general-purpose LLMs, hardware-specialized LLMs, and prompt-based methods on HWFixBench, offering insights into their performance and limitations. To facilitate further research, we open-source the dataset and provide full hardware simulation support, enabling robust cross-category comparisons of solutions.},
booktitle = {Proceedings of the Great Lakes Symposium on VLSI 2025},
pages = {427–434},
numpages = {8},
keywords = {Large Language Model, Hardware Benchmarking, Semi-Synthetic Dataset, Electronic Design Automation, Metric},
location = {
},
series = {GLSVLSI '25}
}

@inproceedings{10.1145/3643788.3648012,
author = {Santos, Sofia and Saraiva, Jo\~{a}o and Ribeiro, Francisco},
title = {Large Language Models in Automated Repair of Haskell Type Errors},
year = {2024},
isbn = {9798400705779},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643788.3648012},
doi = {10.1145/3643788.3648012},
abstract = {This paper introduces a new method of Automated Program Repair that relies on a combination of the GPT-4 Large Language Model and automatic type checking of Haskell programs. This method identifies the source of a type error and asks GPT-4 to fix that specific portion of the program. Then, QuickCheck is used to automatically generate a large set of test cases to validate whether the generated repair behaves as the correct solution. Our publicly available experiments revealed a success rate of 88.5\% in normal conditions. However, more detailed testing should be performed to more accurately evaluate this form of APR.},
booktitle = {Proceedings of the 5th ACM/IEEE International Workshop on Automated Program Repair},
pages = {42–45},
numpages = {4},
keywords = {automated program repair, large language model, fault localization, code generation, type checking, automatic testing},
location = {Lisbon, Portugal},
series = {APR '24}
}

@article{10.1007/s10515-025-00546-0,
author = {Li, Mengliang and Shen, Qiang and Ren, Xiaoxue and Fu, Han and Li, Zhuo and Sun, Jianling},
title = {HMF: Enhancing reentrancy vulnerability detection and repair with a hybrid model framework},
year = {2025},
issue_date = {Dec 2025},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {33},
number = {1},
issn = {0928-8910},
url = {https://doi.org/10.1007/s10515-025-00546-0},
doi = {10.1007/s10515-025-00546-0},
abstract = {Smart contracts have revolutionized the credit landscape. However, their security remains intensely scrutinized due to numerous hacking incidents and inherent logical challenges. One well-known issue is reentrancy vulnerability, exemplified by DAO attacks that lead to substantial economic losses. Previous approaches have employed rule-based and deep learning-based (DL) algorithms to detect and repair reentrancy vulnerability. Large language models (LLM) have been distinguished in recent years for their excellent understanding of text and code. However, less attention has been paid to LLM-based reentrancy vulnerability detection and repair, and direct prompt-based approaches often suffer from inefficiencies and high false positives. To overcome the above shortcomings, this paper proposes a hybrid model framework combining LLM with DL to enhance the detection and repair of reentrancy vulnerabilities. This unified framework comprises three crucial phases: the data processing phase, the vulnerability detection phase, and the vulnerability repair phase. Extensive experimental results validate the superiority of our approach over state-of-the-art baselines, and ablation studies demonstrate the effectiveness of each component. Our approach demonstrates significant improvements in vulnerability detection, with increases of 3.51\% in accuracy, 2.31\% in recall, 0.42\% in precision, and 0.85\% in F1-score. Furthermore, our approach can achieve a notable 9.62\% enhancement in the repair rate. Finally, we also conducted a user study to emphasize its potential to fortify the security of smart contracts.},
journal = {Automated Software Engg.},
month = sep,
numpages = {28},
keywords = {Smart Contract, Vulnerability Detection, Vulnerability Repair, LLM, Reentrancy}
}

@inproceedings{10.1145/3643795.3648390,
author = {Jiang, Shengbei and Zhang, Jiabao and Chen, Wei and Wang, Bo and Zhou, Jianyi and Zhang, Jie},
title = {Evaluating Fault Localization and Program Repair Capabilities of Existing Closed-Source General-Purpose LLMs},
year = {2024},
isbn = {9798400705793},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643795.3648390},
doi = {10.1145/3643795.3648390},
abstract = {Automated debugging is an emerging research field that aims to automatically find and repair bugs. In this field, Fault Localization (FL) and Automated Program Repair (APR) gain the most research efforts. Most recently, researchers have adopted pre-trained Large Language Models (LLMs) to facilitate FL and APR and their results are promising. However, the LLMs they used either vanished (such as Codex) or outdated (such as early versions of GPT). In this paper, we evaluate the performance of recent commercial closed-source general-purpose LLMs on FL and APR, i.e., ChatGPT 3.5, ERNIE Bot 3.5, and IFlytek Spark 2.0. We select three popular LLMs and evaluate them on 120 real-world Java bugs from the benchmark Defects4J. For FL and APR, we designed three kinds of prompts for each, considering different kinds of information. The results show that these LLMs could successfully locate 53.3\% and correctly fix 12.5\% of these bugs.},
booktitle = {Proceedings of the 1st International Workshop on Large Language Models for Code},
pages = {75–78},
numpages = {4},
keywords = {large language model, fault localization, program repair, software debugging},
location = {Lisbon, Portugal},
series = {LLM4Code '24}
}

@article{10.1145/3715775,
author = {Imtiaz, Sayem Mohammad and Singh, Astha and Batole, Fraol and Rajan, Hridesh},
title = {IRepair: An Intent-Aware Approach to Repair Data-Driven Errors in Large Language Models},
year = {2025},
issue_date = {July 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {FSE},
url = {https://doi.org/10.1145/3715775},
doi = {10.1145/3715775},
abstract = {Not a day goes by without hearing about the impressive feats of large language models (LLMs), and equally, not a day passes without hearing about their challenges. LLMs are notoriously vulnerable to biases in their dataset, leading to issues such as toxicity, harmful responses, and factual inaccuracies. While domain-adaptive training has been employed to mitigate these issues, these techniques often address all model parameters indiscriminately during the repair process, resulting in poor repair quality and reduced model versatility. In this paper, drawing inspiration from fault localization via program slicing, we introduce a novel dynamic slicing-based intent-aware LLM repair strategy, IRepair. This approach selectively targets the most error-prone sections of the model for repair. Specifically, we propose dynamically slicing the model’s most sensitive layers that require immediate attention, concentrating repair efforts on those areas. This method enables more effective repairs with potentially less impact on the model’s overall versatility by altering a smaller portion of the model. Furthermore, dynamic selection allows for a more nuanced and precise model repair compared to a fixed selection strategy. We evaluated our technique on three models from the GPT2 and GPT-Neo families, with parameters ranging from 800M to 1.6B, in a toxicity mitigation setup. Our results show that IRepair repairs errors 43.6\% more effectively while causing 46\% less disruption to general performance compared to the closest baseline, direct preference optimization. Our empirical analysis also reveals that errors are more concentrated in a smaller section of the model, with the top 20\% of layers exhibiting 773\% more error density than the remaining 80\%. This highlights the need for selective repair. Additionally, we demonstrate that a dynamic selection approach is essential for addressing errors dispersed throughout the model, ensuring a robust and efficient repair.},
journal = {Proc. ACM Softw. Eng.},
month = jun,
articleno = {FSE056},
numpages = {23},
keywords = {Detoxification, Dynamic Program Slicing, Fault Localization, Large Language Model, Program Repair, SE4AI}
}

@inproceedings{10.1145/3746276.3760467,
author = {Li, Xuan and Sun, Fengzhao and Yu, Jun and Zhang, Yunxiang},
title = {See, Localize and Verify: A GRPO-Powered Framework for Enhancing Factual Accuracy in Multimodal Models},
year = {2025},
isbn = {9798400720581},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3746276.3760467},
doi = {10.1145/3746276.3760467},
abstract = {Multimodal Large Language Models (MLLMs) have demonstrated exceptional capabilities in integrating vision and language. However, their widespread practical deployment is severely hindered by the phenomenon of hallucination, that the model's generated textual output is inconsistent with the provided visual content or lacks factual grounding. Existing research indicates that the hallucination and fact-checking problem in MLLMs is primarily caused by the misalignment between visual and text modalities and the neglect of visual tokens in deep attention layers. Consequently, guiding the model to enhance its attention to visual signals within the deep network, balance its focus across both modalities, and achieve faithful cross-modal reasoning has become a critical pathway to addressing this issue. This paper explores the use of reinforcement learning to guide the model to increase its focus on the visual modality, thereby enabling faithful cross-modal reasoning and mitigating the hallucination phenomenon in MLLMs. Specifically, we design a reward function for cross-modal reasoning, tailored for the GRPO reinforcement learning method. This function guides the model to first locate the coordinates of objects in the image that are associated with the descriptive text, and subsequently detect hallucinations in the text by referencing grounded visual evidence. Through end-to-end reinforcement learning with this method, our model, with only 7B parameters, surpassed its 32B counterpart on hallucination detection and fact-checking tasks, and significantly outperformed models trained with supervised fine-tuning method. Our code will be released on https://github.com/TioeAre/SLAV.},
booktitle = {Proceedings of the 1st International Workshop on MLLM for Unified Comprehension and Generation},
pages = {1–7},
numpages = {7},
keywords = {multimodal large language model, hallucination detection, fact checking, reinforcement learning, visual grounding},
location = {Ireland},
series = {MUCG '25}
}

@inproceedings{10.1145/3611643.3616271,
author = {Wei, Yuxiang and Xia, Chunqiu Steven and Zhang, Lingming},
title = {Copiloting the Copilots: Fusing Large Language Models with Completion Engines for Automated Program Repair},
year = {2023},
isbn = {9798400703270},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3611643.3616271},
doi = {10.1145/3611643.3616271},
abstract = {During Automated Program Repair (APR), it can be challenging&nbsp;to synthesize correct patches for real-world systems in general-purpose programming languages. Recent Large Language Models&nbsp;(LLMs) have been shown to be helpful “copilots” in assisting developers with various coding tasks, and have also been directly&nbsp;applied for patch synthesis. However, most LLMs treat programs as&nbsp;sequences of tokens, meaning that they are ignorant of the underlying semantics constraints of the target programming language. This&nbsp;results in plenty of statically invalid generated patches, impeding&nbsp;the practicality of the technique. Therefore, we propose Repilot,&nbsp;a framework to further copilot the AI “copilots” (i.e., LLMs) by&nbsp;synthesizing more valid patches during the repair process. Our key&nbsp;insight is that many LLMs produce outputs autoregressively (i.e.,&nbsp;token by token), resembling human writing programs, which can&nbsp;be significantly boosted and guided through a Completion Engine.&nbsp;Repilot synergistically synthesizes a candidate patch through the&nbsp;interaction between an LLM and a Completion Engine, which 1)&nbsp;prunes away infeasible tokens suggested by the LLM and 2) proactively completes the token based on the suggestions provided by the&nbsp;Completion Engine. Our evaluation on a subset of the widely-used&nbsp;Defects4j 1.2 and 2.0 datasets shows that Repilot fixes 66 and 50&nbsp;bugs, respectively, surpassing the best-performing baseline by 14&nbsp;and 16 bugs fixed. More&nbsp;importantly, Repilot is capable of producing more valid and correct patches than the base LLM when given&nbsp;the same generation budget.},
booktitle = {Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {172–184},
numpages = {13},
keywords = {Completion Engine, Large Language Model, Program Repair},
location = {San Francisco, CA, USA},
series = {ESEC/FSE 2023}
}

@article{10.1007/s10515-025-00492-x,
author = {Cao, Jialun and Li, Meiziniu and Wen, Ming and Cheung, Shing-Chi},
title = {A study on prompt design, advantages and limitations of ChatGPT for deep learning program repair},
year = {2025},
issue_date = {May 2025},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {32},
number = {1},
issn = {0928-8910},
url = {https://doi.org/10.1007/s10515-025-00492-x},
doi = {10.1007/s10515-025-00492-x},
abstract = {The emergence of large language models (LLMs) such as ChatGPT has revolutionized many fields. In particular, recent advances in LLMs have triggered various studies examining the use of these models for software development tasks, such as program repair, code understanding, and code generation. Prior studies have shown the capability of ChatGPT in repairing conventional programs. However, debugging deep learning (DL) programs poses unique challenges since the decision logic is not directly encoded in the source code. This requires LLMs to not only parse the source code syntactically but also understand the intention of DL programs. Therefore, ChatGPT’s capability in repairing DL programs remains unknown. To fill this gap, our study aims to answer three research questions: (1) Can ChatGPT debug DL programs effectively? (2) How can ChatGPT’s repair performance be improved by prompting? (3) In which way can dialogue help facilitate the repair? Our study analyzes the typical information that is useful for prompt design and suggests enhanced prompt templates that are more efficient for repairing DL programs. On top of them, we summarize the dual perspectives (i.e., advantages and disadvantages) of ChatGPT’s ability, such as its handling of API misuse and recommendation, and its shortcomings in identifying default parameters. Our findings indicate that ChatGPT has the potential to repair DL programs effectively and that prompt engineering and dialogue can further improve its performance by providing more code intention. We also identified the key intentions that can enhance ChatGPT’s program repairing capability.},
journal = {Automated Software Engg.},
month = mar,
numpages = {29},
keywords = {Large language model, LLM4SE, Automatic program repair, Deep learning program repair}
}

@article{10.1016/j.jss.2025.112528,
author = {Cao, Xiansheng and Wang, Junfeng and Wu, Peng},
title = {Enhancing vulnerability repair through the extraction and matching of repair patterns},
year = {2025},
issue_date = {Dec 2025},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {230},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2025.112528},
doi = {10.1016/j.jss.2025.112528},
journal = {J. Syst. Softw.},
month = dec,
numpages = {13},
keywords = {Deep learning, Vulnerability repair, Repair patterns, Optimal matching}
}

@inbook{10.5555/3766078.3766306,
author = {Hu, Yiwei and Li, Zhen and Shu, Kedie and Guan, Shenghua and Zou, Deqing and Xu, Shouhuai and Yuan, Bin and Jin, Hai},
title = {SoK: automated vulnerability repair: methods, tools, and assessments},
year = {2025},
isbn = {978-1-939133-52-6},
publisher = {USENIX Association},
address = {USA},
abstract = {The increasing complexity of software has led to the steady growth of vulnerabilities. Vulnerability repair investigates how to fix software vulnerabilities. Manual vulnerability repair is labor-intensive and time-consuming because it relies on human experts, highlighting the importance of Automated Vulnerability Repair (AVR). In this SoK, we present the systematization of AVR methods through the three steps of AVR workflow: vulnerability analysis, patch generation, and patch validation. We assess AVR tools for C/C++ and Java programs as they have been widely studied by the community. Since existing AVR tools for C/C++ programs are evaluated with different datasets' which often consist of a few vulnerabilities' we construct the first C/C++ vulnerability repair benchmark dataset' dubbed VUL4C, which contains 144 vulnerabilities as well as their exploits and patches. We use VUL4C to evaluate seven AVR tools for C/C++ programs and use the third-party VUL4J dataset to evaluate two AVR tools for Java programs. We also discuss future research directions.},
booktitle = {Proceedings of the 34th USENIX Conference on Security Symposium},
articleno = {228},
numpages = {20}
}

@article{10.1007/s10515-025-00512-w,
author = {Li, Yingling and Cai, Muxin and Chen, Junjie and Xu, Yang and Huang, Lei and Li, Jianping},
title = {Context-aware prompting for LLM-based program repair: Context-aware prompting for LLM-based program repair},
year = {2025},
issue_date = {Aug 2025},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {32},
number = {2},
issn = {0928-8910},
url = {https://doi.org/10.1007/s10515-025-00512-w},
doi = {10.1007/s10515-025-00512-w},
abstract = {Automated program repair (APR) plays a crucial role in ensuring the quality of software code, as manual bug-fixing is extremely time-consuming and labor-intensive. Traditional APR tools (e.g., template-based approaches) face the challenge of generalizing to different bug patterns, while deep learning (DL)-based methods heavily rely on training datasets and struggle to fix unseen bugs. Recently, large language models (LLMs) have shown great potential in APR due to their ability to generate patches, having achieved promising results. However, their effectiveness is still constrained by the casually-determined context (e.g., being unable to adaptively select the specific context according to the situation of each defect). Therefore, a more effective APR approach is highly needed, which provides more precise and comprehensive context for the given defect to enhance the robustness of LLM-based APRs. In this paper, we propose a context-aware APR approach named CodeCorrector, which designs a Chain-of-Thought (CoT) approach to follow developers’ program repair behaviors. Given a failing test and its buggy file, CodeCorrector first analyzes why the test fails based on the failure message to infer repair direction; then selects the relevant context information to this repair direction; finally builds the context-aware repair prompt to guide LLMs for patch generation. Our motivation is to offer a novel perspective for enhancing LLM-based program repair through context-aware prompting, which adaptively selects specific context for a given defect. The evaluation on the widely-used Defects4J (i.e., v1.2 and v2.0) benchmark shows that overall, by executing a small number of repairs (i.e., as few as ten rounds), CodeCorrector outperforms all the state-of-the-art baselines on the more complex defects in Defects4J v2.0 and the defects without fine-grained defect localization information in Defects4J v1.2. Specifically, a total of 38 defects are fixed by only CodeCorrector. We further analyze the contributions of two core components (i.e., repair directions, global context selection) to the performance of CodeCorrector, especially repair directions, which improve CodeCorrector by 112\% in correct patches and 78\% in plausible patches on Defects4J v1.2. Moreover, CodeCorrector generates more valid and correct patches, achieving a 377\% improvement over the base LLM GPT-3.5 and a 268\% improvement over GPT-4.},
journal = {Automated Software Engg.},
month = apr,
numpages = {34},
keywords = {APR, LLMs, Repair directions, Adaptive context selection}
}

@article{10.1145/3763097,
author = {Xie, Linna and Li, Zhong and Pei, Yu and Wen, Zhongzhen and Liu, Kui and Zhang, Tian and Li, Xuandong},
title = {PReMM: LLM-Based Program Repair for Multi-method Bugs via Divide and Conquer},
year = {2025},
issue_date = {October 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {OOPSLA2},
url = {https://doi.org/10.1145/3763097},
doi = {10.1145/3763097},
abstract = {Large-language models (LLMs) have been leveraged to enhance the capability of automated program repair techniques in recent research.   While existing LLM-based program repair techniques compared favorably to other techniques based on heuristics, constraint-solving, and learning in producing high-quality patches,  they mainly target bugs that can be corrected by changing a single faulty method,  which greatly limits the effectiveness of such techniques in repairing bugs that demand patches spanning across multiple methods.   In this work, we propose the PReMM technique to effectively propose patches changing multiple methods.   PReMM builds on three core component techniques:   the faulty method clustering technique to partition the faulty methods into clusters based on the dependence relationship among them,   enabling a divide-and-conquer strategy for the repairing task;   the fault context extraction technique to gather extra information about the fault context which can be utilized to better guide the diagnosis of the fault and the generation of correct patches;   the dual-agent-based patch generation technique that employs two LLM-based agents with different roles to analyze the fault more precisely and generate patches of higher-quality.  We have implemented the PReMM technique into a tool with the same name and applied the tool to repair real-world bugs from datasets Defects4J V1.2 and V2.0.   PReMM produced correct patches for 307 bugs in total.   Compared with ThinkRepair, the state-of-the-art LLM-based program repair technique,  PReMM correctly repaired 102 more bugs, achieving an improvement of 49.8\%.},
journal = {Proc. ACM Program. Lang.},
month = oct,
articleno = {319},
numpages = {29},
keywords = {Automated Program Repair, Context-Aware Repair, Divide and Conquer, Large Language Models, Multi-method Bugs}
}

@article{10.1007/s10515-025-00579-5,
author = {Wang, Ruoke and Li, Zongjie and Gao, Cuiyun and Wang, Chaozheng and Xiao, Yang and Wang, Xuan},
title = {SPVR: syntax-to-prompt vulnerability repair based on large language models},
year = {2025},
issue_date = {Dec 2025},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {33},
number = {1},
issn = {0928-8910},
url = {https://doi.org/10.1007/s10515-025-00579-5},
doi = {10.1007/s10515-025-00579-5},
abstract = {Purpose: In the field of vulnerability repair, previous research has leveraged pre-trained models and LLM-based prompt engineering, among which LLM-based approaches show better generalizability and achieve the best performance. However, the LLM-based approaches generally regard vulnerability repair as a sequence-to-sequence task, and do not explicitly capture the syntax patterns for different vulnerability types, leading to limited accuracy. We aim to create a method that ensures the specificity of prompts targeting vulnerable code while also leveraging the generative capabilities of Large Language Models. Methods: We propose SPVR (Syntax-to-Prompt Vulnerability Repair), a novel framework that collects information from syntax trees, and generates corresponding prompts. Our method consists of three steps: rule design, prompt generation, and patch generation. In the rule design step, our method parses code patches and designs rules to extract relevant contextual information. These rules aid in identifying vulnerability-related issues. In the prompt generation step, our method extracts information from vulnerable code with pre-defined rules, automatically converting them into prompts. We also incorporate the description of CWE (Common Weakness Enumeration) as known information into the prompts. Finally, in the patch generation step, this prompt will serve as input to any conversational LLM to obtain code patches. Results: Extensive experiments validate that our method achieves excellent results in assisting LLMs to fix vulnerabilities accurately. We utilize multiple Large Language Models to validate the effectiveness of our work, repairing 143 of 547 vulnerable code using ChatGPT-4. We conducted a comparison of our approach against several existing vulnerability repair approaches (including fine-tuning-based and prompt-based), across multiple metrics. Conclusion: Our method is a novel framework that combines the Abstract Syntax Tree structure of code, providing targeted prompts of repair code for vulnerabilities. Our method demonstrates promising potential for real-world code vulnerability repair.},
journal = {Automated Software Engg.},
month = dec,
numpages = {33},
keywords = {Deep learning, Automated program repair, Common weakness enumeration, Generative AI, Automated code analysis}
}

@inproceedings{10.1145/3719160.3737627,
author = {Castle-Green, Teresa and Castle-Green, Simon and Lindley, Joseph and Sailaja, Neelima and Lechelt, Susan},
title = {Elmo: An Embodied Conversational Assistant For Community Repair Caf\'{e}s},
year = {2025},
isbn = {9798400715273},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3719160.3737627},
doi = {10.1145/3719160.3737627},
abstract = {Repair caf\'{e}s provide a community service that supports and empowers item ‘Bringers’ to reinstate traditional values of repair in order to reduce climate impacts of waste. In this paper, we report on a pilot study exploring the use of Elmo (an embodied conversational assistant) by volunteer Repairers within repair caf\'{e} settings. Our findings show the different ways Repairers incorporated this technology probe into their work and the challenges they faced in doing so. Through this, we contribute three areas of consideration for designers looking to implement conversational assistants into community repair settings. These include, contextual awareness of time to undertake suggestions and of the related repair caf\'{e} processes, motivation and customer relations requirements of volunteer Repairers, and enhancing empowerment work through supporting current practices and demonstrations of navigating trouble in repair-resource interactions.},
booktitle = {Proceedings of the 7th ACM Conference on Conversational User Interfaces},
articleno = {7},
numpages = {6},
keywords = {Repair Assistant, Voice Interface, Embodied, Large Language Model (LLM), ChatGPT, Repair, Community, Robot, UBTech Alpha Mini},
location = {
},
series = {CUI '25}
}

@article{10.1145/3707454,
author = {Shariffdeen, Ridwan and Timperley, Christopher S. and Noller, Yannic and Le Goues, Claire and Roychoudhury, Abhik},
title = {Vulnerability Repair via Concolic Execution and Code Mutations},
year = {2025},
issue_date = {May 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {34},
number = {4},
issn = {1049-331X},
url = {https://doi.org/10.1145/3707454},
doi = {10.1145/3707454},
abstract = {Security vulnerabilities detected via techniques like greybox fuzzing are often fixed with a significant time lag. This increases the exposure of the software to vulnerabilities. Automated fixing of vulnerabilities where a tool can generate fix suggestions is thus of value. In this work, we present such a tool, called CrashRepair, to automatically generate fix suggestions using concolic execution, specification inference, and search techniques. Our approach avoids generating fix suggestions merely at the crash location because such fixes often disable the manifestation of the error instead of fixing the error. Instead, based on sanitizer-guided concolic execution, we infer desired constraints at specific program locations and then opportunistically search for code mutations that help respect those constraints. Our technique only requires a single detected vulnerability or exploit as input; it does not require any user-provided properties. Evaluation results on a wide variety of CVEs in the VulnLoc benchmark, show CrashRepair achieves greater efficacy than state-of-the-art vulnerability repair tools like Senx. The repairs suggested come in the form of a ranked set of patches at different locations, and we show that on most occasions, the desired fix is among the top-3 fixes reported by CrashRepair.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = apr,
articleno = {105},
numpages = {27},
keywords = {Automated Program Repair, Vulnerability Repair, Semantic Program Analysis, Concolic Execution}
}

@inproceedings{10.1145/3696630.3731672,
author = {Zhang, Yifan and Leach, Kevin},
title = {Leveraging Human Insights for Enhanced LLM-based Code Repair},
year = {2025},
isbn = {9798400712760},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3696630.3731672},
doi = {10.1145/3696630.3731672},
abstract = {Large Language Models (LLMs) show promise for automating code repair but often lack the nuanced, iterative reasoning and effective use of historical context employed by human developers. We propose a framework to enhance LLM-based repair by incorporating human-inspired mechanisms: mining commit histories for recurring patterns, employing dynamic feedback loops, and facilitating reasoning over historical repair experiences. By extracting abstract fix patterns, iteratively refining patches using automated feedback (from tests and static analysis) alongside optional human guidance, and leveraging a vectorized repository of past experiences for context-aware reasoning, our approach seeks to improve automated patch generation. This framework aims to increase the accuracy and efficiency of repairs by guiding LLMs with more human-like, iterative, and context-grounded problem-solving strategies.},
booktitle = {Proceedings of the 33rd ACM International Conference on the Foundations of Software Engineering},
pages = {1536–1537},
numpages = {2},
keywords = {automated code repair, large language models, software vulnerabilities},
location = {Clarion Hotel Trondheim, Trondheim, Norway},
series = {FSE Companion '25}
}

@inproceedings{10.1145/3664646.3664770,
author = {Kulsum, Ummay and Zhu, Haotian and Xu, Bowen and d'Amorim, Marcelo},
title = {A Case Study of LLM for Automated Vulnerability Repair: Assessing Impact of Reasoning and Patch Validation Feedback},
year = {2024},
isbn = {9798400706851},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3664646.3664770},
doi = {10.1145/3664646.3664770},
abstract = {Recent work in automated program repair (APR) proposes the use of reasoning and patch validation feedback to reduce the semantic gap between the LLMs and the code under analysis. The idea has been shown to perform well for general APR, but its effectiveness in other particular contexts remains underexplored.                In this work, we assess the impact of reasoning and patch validation feedback to LLMs in the context of vulnerability repair, an important and challenging task in security. To support the evaluation, we present VRpilot, an LLM-based vulnerability repair technique based on reasoning and patch validation feedback. VRpilot (1) uses a chain-of-thought prompt to reason about a vulnerability prior to generating patch candidates and (2) iteratively refines prompts according to the output of external tools (e.g., compiler, code sanitizers, test suite, etc.) on previously generated patches.                 To evaluate performance, we compare VRpilot against the state-of-the-art vulnerability repair techniques for C and Java using public datasets from the literature. Our results show that VRpilot generates, on average, 14\% and 7.6\% more correct patches than the baseline techniques on C and Java, respectively. We show, through an ablation study, that reasoning and patch validation feedback are critical. We report several lessons from this study and potential directions for advancing LLM-empowered vulnerability repair.},
booktitle = {Proceedings of the 1st ACM International Conference on AI-Powered Software},
pages = {103–111},
numpages = {9},
keywords = {Automated Vulnerability Repair, Large Language Models},
location = {Porto de Galinhas, Brazil},
series = {AIware 2024}
}

@inproceedings{10.1109/ICSE55347.2025.00226,
author = {Ke, Kaiyao},
title = {NIODebugger: A Novel Approach to Repair Non-Idempotent-Outcome Tests with LLM-Based Agent},
year = {2025},
isbn = {9798331505691},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE55347.2025.00226},
doi = {10.1109/ICSE55347.2025.00226},
abstract = {Flaky tests, characterized by inconsistent results across repeated executions, present significant challenges in software testing, especially during regression testing. Recently, there has been emerging research interest in non-idempotent-outcome (NIO) flaky tests—tests that pass on the initial run but fail on subsequent executions within the same environment. Despite progress in utilizing Large Language Models (LLMs) to address flaky tests, existing methods have not tackled NIO flaky tests. The limited context window of LLMs restricts their ability to incorporate relevant source code beyond the test method itself, often overlooking crucial information needed to address state pollution, which is the root cause of NIO flakiness.This paper introduces NIODebugger, the first framework to utilize an LLM-based agent to repair flaky tests. NIODebugger features a three-phase design: detection, exploration, and fixing. In the detection phase, dynamic analysis collects stack traces and custom test execution logs from multiple test runs, which helps in understanding accumulative state pollution. During the exploration phase, the LLM-based agent provides instructions for extracting relevant source code associated with test flakiness. In the fixing phase, NIODebugger repairs the tests using the information gathered from the previous phases. NIODebugger can be integrated with multiple LLMs, achieving patching success rates ranging from 11.63\% to 58.72\%. Its best-performing variant, NIODebugger-GPT-4, successfully generated correct patches for 101 out of 172 previously unknown NIO tests across 20 large-scale open-source projects. We submitted pull requests for all generated patches; 58 have been merged, only 1 was rejected, and the remaining 42 are pending. The Java implementation of NIODebugger is provided as a Maven plugin accessible at https://github.com/kaiyaok2/NIOInspector.},
booktitle = {Proceedings of the IEEE/ACM 47th International Conference on Software Engineering},
pages = {1014–1025},
numpages = {12},
location = {Ottawa, Ontario, Canada},
series = {ICSE '25}
}

@inproceedings{10.1145/3719027.3748270,
author = {Vigna, Giovanni},
title = {Autonomous Vulnerability Analysis, Triaging, and Repair: A Historical Perspective},
year = {2025},
isbn = {9798400715259},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3719027.3748270},
doi = {10.1145/3719027.3748270},
abstract = {The software components that support critical infrastructure are riddled with vulnerabilities, whose exploitation could cause service disruption, financial damage, and possibly loss of life. Although there are efforts, such as OSS-Fuzz, to continuously analyze these components for vulnerabilities, some categories of security bugs are still hard to detect. In addition, the creation of testing harnesses and the generation of effective patches still require substantial effort from human experts. To address these issues, researchers and practitioners alike have focused on automating the vulnerability analysis and repair process. In particular, DARPA has supported these research efforts with two challenges: the DARPA Cyber Grand Challenge (CGC) in 2016 and the AI Cyber Challenge (AIxCC) in 2025. In these two challenges, participants had to create Cyber Reasoning Systems (CRS) that, in different contexts, had to identify vulnerabilities, exploit them, and provide patches without any human involvement. In this talk, we take a historical look at these efforts that span a decade, especially in light of the recent advances in Large Language Models (LLMs), and highlight the lessons learned from participating in these competitions, as well as the challenges that still need to be addressed to achieve a completely autonomous vulnerability analysis, triaging, and repair process.},
booktitle = {Proceedings of the 2025 ACM SIGSAC Conference on Computer and Communications Security},
pages = {1},
numpages = {1},
keywords = {software patching, vulnerability analysis},
location = {Taipei, Taiwan},
series = {CCS '25}
}

@inproceedings{10.1145/3663529.3663797,
author = {Avula, Sai Krishna and Mondal, Shouvick},
title = {MineCPP: Mining Bug Fix Pairs and Their Structures},
year = {2024},
isbn = {9798400706585},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3663529.3663797},
doi = {10.1145/3663529.3663797},
abstract = {Modern software repositories serve as valuable sources of information for understanding and addressing software bugs. In this paper, we present MineCPP, a tool designed for large-scale bug fixing dataset generation, extending the capabilities of a recently proposed approach, namely Minecraft. MineCPP not only captures bug locations and types across multiple programming languages but introduces novel features like offset of a bug in a buggy source file, the sequence of syntactic constructs up to and including the location of the bug, etc. We discuss architectural and operational aspects of MineCPP, and show how it can be used to automatically mine GitHub repositories. A Graphical User Interface (GUI) further enhances user experience by providing interactive visualizations and quantitative analyses, facilitating fine-grained insights about the structure of bug fix pairs. MineCPP serves as a helpful solution for researchers, practitioners, and developers seeking comprehensive bug-fixing datasets and insights into coding practices. Tool demonstration is available at https://youtu.be/ln99irvbADE.},
booktitle = {Companion Proceedings of the 32nd ACM International Conference on the Foundations of Software Engineering},
pages = {552–556},
numpages = {5},
keywords = {Bug Fixes, Coding Effort, LLMs, Mining Software Repositories},
location = {Porto de Galinhas, Brazil},
series = {FSE 2024}
}

@article{10.1145/3632746,
author = {Fu, Michael and Nguyen, Van and Tantithamthavorn, Chakkrit and Phung, Dinh and Le, Trung},
title = {Vision Transformer Inspired Automated Vulnerability Repair},
year = {2024},
issue_date = {March 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {33},
number = {3},
issn = {1049-331X},
url = {https://doi.org/10.1145/3632746},
doi = {10.1145/3632746},
abstract = {Recently, automated vulnerability repair approaches have been widely adopted to combat increasing software security issues. In particular, transformer-based encoder-decoder models achieve competitive results. Whereas vulnerable programs may only consist of a few vulnerable code areas that need repair, existing AVR approaches lack a mechanism guiding their model to pay more attention to vulnerable code areas during repair generation. In this article, we propose a novel vulnerability repair framework inspired by the Vision Transformer based approaches for object detection in the computer vision domain. Similar to the object queries used to locate objects in object detection in computer vision, we introduce and leverage vulnerability queries (VQs) to locate vulnerable code areas and then suggest their repairs. In particular, we leverage the cross-attention mechanism to achieve the cross-match between VQs and their corresponding vulnerable code areas. To strengthen our cross-match and generate more accurate vulnerability repairs, we propose to learn a novel vulnerability mask (VM) and integrate it into decoders’ cross-attention, which makes our VQs pay more attention to vulnerable code areas during repair generation. In addition, we incorporate our VM into encoders’ self-attention to learn embeddings that emphasize the vulnerable areas of a program. Through an extensive evaluation using the real-world 5,417 vulnerabilities, our approach outperforms all of the automated vulnerability repair baseline methods by 2.68\% to 32.33\%. Additionally, our analysis of the cross-attention map of our approach confirms the design rationale of our VM and its effectiveness. Finally, our survey study with 71 software practitioners highlights the significance and usefulness of AI-generated vulnerability repairs in the realm of software security. The training code and pre-trained models are available at https://github.com/awsm-research/VQM.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = mar,
articleno = {78},
numpages = {29},
keywords = {Software security, automated vulnerability repair}
}

@inproceedings{10.1109/ASE56229.2023.00163,
author = {He, Ye and Chen, Zimin and Goues, Claire Le},
title = {PreciseBugCollector: Extensible, Executable and Precise Bug-Fix Collection},
year = {2024},
isbn = {9798350329964},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASE56229.2023.00163},
doi = {10.1109/ASE56229.2023.00163},
abstract = {Bug datasets are vital for enabling deep learning techniques to address software maintenance tasks related to bugs. However, existing bug datasets suffer from precise and scale limitations: they are either small-scale but precise with manual validation or large-scale but imprecise with simple commit message processing. In this paper, we introduce Precise-BugCollector, a precise, multi-language bug collection approach that overcomes these two limitations. PreciseBugCollector is based on two novel components: a) A bug tracker to map the codebase repositories with external bug repositories to trace bug type information, and b) A bug injector to generate project-specific bugs by injecting noise into the correct codebases and then executing them against their test suites to obtain test failure messages.We implement PreciseBugCollector against three sources: 1) A bug tracker that links to the national vulnerability data set (NVD) to collect general-wise vulnerabilities, 2) A bug tracker that links to OSS-Fuzz to collect general-wise bugs, and 3) A bug injector based on 16 injection rules to generate project-wise bugs. To date, PreciseBugCollector comprises 1 057 818 bugs extracted from 2 968 open-source projects. Of these, 12 602 bugs are sourced from bug repositories (NVD and OSS-Fuzz), while the remaining 1 045 216 project-specific bugs are generated by the bug injector. Considering the challenge objectives, we argue that a bug injection approach is highly valuable for the industrial setting, since project-specific bugs align with domain knowledge, share the same codebase, and adhere to the coding style employed in industrial projects.},
booktitle = {Proceedings of the 38th IEEE/ACM International Conference on Automated Software Engineering},
pages = {1899–1910},
numpages = {12},
keywords = {bug datasets, program repair, software testing and debugging},
location = {Echternach, Luxembourg},
series = {ASE '23}
}

@inproceedings{10.1145/3735950.3735954,
author = {Wang, Huanting and Jacob, Dejice and Kelly, David and Elkhatib, Yehia and Singer, Jeremy and Wang, Zheng},
title = {SecureMind: A Framework for Benchmarking Large Language Models in Memory Bug Detection and Repair},
year = {2025},
isbn = {9798400716102},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3735950.3735954},
doi = {10.1145/3735950.3735954},
abstract = {Large language models (LLMs) hold great promise for automating software vulnerability detection and repair, but ensuring their correctness remains a challenge. While recent work has developed benchmarks for evaluating LLMs in bug detection and repair, existing studies rely on hand-crafted datasets that quickly become outdated. Moreover, systematic evaluation of advanced reasoning-based LLMs using chain-of-thought prompting for software security is lacking.   We introduce SecureMind, an open-source framework for evaluating LLMs in vulnerability detection and repair, focusing on memory-related vulnerabilities. SecureMind provides a user-friendly Python interface for defining test plans, which automates data retrieval, preparation, and benchmarking across a wide range of metrics.   Using SecureMind, we assess 10 representative LLMs, including 7 state-of-the-art reasoning models, on 16K test samples spanning 8 Common Weakness Enumeration (CWE) types related to memory safety violations. Our findings highlight the strengths and limitations of current LLMs in handling memory-related vulnerabilities.},
booktitle = {Proceedings of the 2025 ACM SIGPLAN International Symposium on Memory Management},
pages = {27–40},
numpages = {14},
keywords = {Bug repair, Large language models, Software bug detection},
location = {Seoul, Republic of Korea},
series = {ISMM '25}
}

@article{10.1145/3705302,
author = {Zhang, Lehuan and Guo, Shikai and Guo, Yi and Li, Hui and Chai, Yu and Chen, Rong and Li, Xiaochen and Jiang, He},
title = {Context-based Transfer Learning for Structuring Fault Localization and Program Repair Automation},
year = {2025},
issue_date = {May 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {34},
number = {4},
issn = {1049-331X},
url = {https://doi.org/10.1145/3705302},
doi = {10.1145/3705302},
abstract = {Automated software debugging plays a crucial role in aiding software developers to swiftly identify and attempt to rectify faults, thereby significantly reducing developers’ workload. Previous researches have predominantly relied on simplistic semantic deep learning or statistical analysis methods to locate faulty statements in diverse projects. However, code repositories often consist of lengthy sequences with long-distance dependencies, posing challenges for accurately modeling fault localization using these methods. In addition, the lack of joint reasoning among various faults prevents existing models from deeply capturing fault information. To address these challenges, we propose a method named CodeHealer to achieve accurate fault localization and program repair. CodeHealer comprises three components: a Deep Semantic Information Extraction Component that effectively extracts deep semantic features from suspicious code statements using classifiers based on Joint-attention mechanisms; a Suspicious Statement Ranking Component that combines various fault localization features and employs multilayer perceptrons to derive multidimensional vectors of suspicion values; and a Fault Repair Component that, based on ranked suspicious statements generated by fault localization, adopts a top-down approach using multiple classifiers based on Co-teaching mechanisms to select repair templates and generate patches. The experimental results indicate that when applied to fault localization, CodeHealer outperforms the best baseline method with improvements of 11.4\%, 2.7\%, and 1.6\% on Top-1/3/5 metrics, respectively. It also reduces the MFR and MAR by 9.8\% and 2.1\%, where lower values denote better fault localization effectiveness. Additionally, in automated software debugging, CodeHealer fixes an additional 6 faults compared to the current best method, totaling 53 faults repaired.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = apr,
articleno = {96},
numpages = {32},
keywords = {Software debugging, Fault Localization, Transfer learning}
}

@inproceedings{10.1145/3597503.3623310,
author = {Zhong, Wenkang and Li, Chuanyi and Liu, Kui and Xu, Tongtong and Ge, Jidong and Bissyande, Tegawende F. and Luo, Bin and Ng, Vincent},
title = {Practical Program Repair via Preference-based Ensemble Strategy},
year = {2024},
isbn = {9798400702174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597503.3623310},
doi = {10.1145/3597503.3623310},
abstract = {To date, over 40 Automated Program Repair (APR) tools have been designed with varying bug-fixing strategies, which have been demonstrated to have complementary performance in terms of being effective for different bug classes. Intuitively, it should be feasible to improve the overall bug-fixing performance of APR via assembling existing tools. Unfortunately, simply invoking all available APR tools for a given bug can result in unacceptable costs on APR execution as well as on patch validation (via expensive testing). Therefore, while assembling existing tools is appealing, it requires an efficient strategy to reconcile the need to fix more bugs and the requirements for practicality. In light of this problem, we propose a Preference-based Ensemble Program Repair framework (P-EPR), which seeks to effectively rank APR tools for repairing different bugs. P-EPR is the first non-learning-based APR ensemble method that is novel in its exploitation of repair patterns as a major source of knowledge for ranking APR tools and its reliance on a dynamic update strategy that enables it to immediately exploit and benefit from newly derived repair results. Experimental results show that P-EPR outperforms existing strategies significantly both in flexibility and effectiveness.},
booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
articleno = {5},
numpages = {13},
keywords = {program repair, ensemble strategy},
location = {Lisbon, Portugal},
series = {ICSE '24}
}

@article{10.1109/TSE.2025.3532759,
author = {Huang, Kai and Zhang, Jian and Bao, Xinlei and Wang, Xu and Liu, Yang},
title = {Comprehensive Fine-Tuning Large Language Models of Code for Automated Program Repair},
year = {2025},
issue_date = {April 2025},
publisher = {IEEE Press},
volume = {51},
number = {4},
issn = {0098-5589},
url = {https://doi.org/10.1109/TSE.2025.3532759},
doi = {10.1109/TSE.2025.3532759},
abstract = {Automated program repair (APR) research has entered the era of large language models (LLM), and researchers have conducted several empirical studies to explore the repair capabilities of LLMs for APR. Many studies adopt the zero/few-shot learning paradigm for APR, which directly use LLMs to generate the possibly correct code given its surrounding context. Though effective, the repair capabilities of LLMs based on the fine-tuning paradigm have yet to be extensively explored. Also, it remains unknown whether LLMs have the potential to repair more complicated bugs (e.g., multi-hunk bugs). To fill the gap, in the conference version of this work, we conduct an initial study on the program repair capability of million-level LLMs in the fine-tuning paradigm. We&nbsp;select 5 popular million-level LLMs with representative pre-training architectures, including CodeBERT, GraphCodeBERT, PLBART, CodeT5, and UniXcoder. We&nbsp;consider 3 typical program repair scenarios (i.e., bugs, vulnerabilities, and errors) involving 3 programming languages (i.e., Java, C/C++, and JavaScript). Our experimental results show that fine-tuning these LLMs can significantly outperform previous state-of-the-art APR tools. However, the repair capabilities of billion-level LLMs for APR remain largely unexplored. Moreover, their substantial model sizes significantly increase the computational cost of fine-tuning. While parameter-efficient fine-tuning (PEFT) techniques offer a promising solution, their effectiveness in repair tasks and the selection of appropriate PEFT strategies remain unclear. Similarly, many novel APR strategies have been developed for non-pre-trained models, yet their applicability and effectiveness on LLMs are still unexamined. To address these gaps, we extend our prior study through three key dimensions: 1) LLM4APR, which evaluates the repair capabilities of five billion-level LLM families (InCoder, CodeGeeX, CodeGen, StarCoder, and CodeLlama) under the fine-tuning paradigm; 2) PEFT4LLM, which compares full-parameter fine-tuning (FPFT) with three PEFT techniques (LoRA, AdaLoRA, and IA3) to determine optimal strategies that balance repair cost and performance of LLMs; and 3) APR4LLM, which investigates the potential of a basic neural machine translation (NMT) approach alongside three advanced repair strategies (TENURE, ITER, and KATANA) to enhance the repair capabilities of LLMs. Overall, our extensive results suggest that larger scale models typically have better repair capabilities. The&nbsp;LoRA technique is still the best choice for LLM4APR studies. Different repair strategies result in different repair capabilities for the foundation models, but some of the strategies that performed well on the non-pre-trained model did not show an advantage on LLMs. Besides, we released all LLMs fine-tuned with repair tasks to facilitate LLM4APR research, and we encourage researchers to develop more powerful APR tools on the basis of these repair LLMs.},
journal = {IEEE Trans. Softw. Eng.},
month = apr,
pages = {904–928},
numpages = {25}
}

@article{10.1145/3631974,
author = {Zhang, Quanjun and Fang, Chunrong and Ma, Yuxiang and Sun, Weisong and Chen, Zhenyu},
title = {A Survey of Learning-based Automated Program Repair},
year = {2023},
issue_date = {February 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {33},
number = {2},
issn = {1049-331X},
url = {https://doi.org/10.1145/3631974},
doi = {10.1145/3631974},
abstract = {Automated program repair (APR) aims to fix software bugs automatically and plays a crucial role in software development and maintenance. With the recent advances in deep learning (DL), an increasing number of APR techniques have been proposed to leverage neural networks to learn bug-fixing patterns from massive open-source code repositories. Such learning-based techniques usually treat APR as a neural machine translation (NMT) task, where buggy code snippets (i.e., source language) are translated into fixed code snippets (i.e., target language) automatically. Benefiting from the powerful capability of DL to learn hidden relationships from previous bug-fixing datasets, learning-based APR techniques have achieved remarkable performance.In this article, we provide a systematic survey to summarize the current state-of-the-art research in the learning-based APR community. We illustrate the general workflow of learning-based APR techniques and detail the crucial components, including fault localization, patch generation, patch ranking, patch validation, and patch correctness phases. We then discuss the widely adopted datasets and evaluation metrics and outline existing empirical studies. We discuss several critical aspects of learning-based APR techniques, such as repair domains, industrial deployment, and the open science issue. We highlight several practical guidelines on applying DL techniques for future APR studies, such as exploring explainable patch generation and utilizing code features. Overall, our article can help researchers gain a comprehensive understanding about the achievements of the existing learning-based APR techniques and promote the practical application of these techniques. Our artifacts are publicly available at the repository: .},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = dec,
articleno = {55},
numpages = {69},
keywords = {AI and software engineering, neural machine translation, deep learning, Automatic program repair}
}

@inproceedings{10.1145/3663529.3663815,
author = {Wu, Yonghao and Li, Zheng and Zhang, Jie M. and Liu, Yong},
title = {ConDefects: A Complementary Dataset to Address the Data Leakage Concern for LLM-Based Fault Localization and Program Repair},
year = {2024},
isbn = {9798400706585},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3663529.3663815},
doi = {10.1145/3663529.3663815},
abstract = {With the growing interest on Large Language Models (LLMs) for fault localization and program repair, ensuring the integrity and generalizability of the LLM-based methods becomes paramount. The code in existing widely-adopted benchmarks for these tasks was written before the bloom of LLMs and may be included in the training data of existing popular LLMs, thereby suffering from the threat of data leakage, leading to misleadingly optimistic performance metrics.  To address this issue, we introduce ConDefects, a dataset developed as a complement to existing datasets, meticulously curated with real faults to eliminate such overlap. ConDefects contains 1,254 Java faulty programs and 1,625 Python faulty programs. All these programs are sourced from the online competition platform AtCoder and were produced between October 2021 and September 2023. We pair each fault with fault locations and the corresponding repaired code versions, making it tailored for fault localization and program repair related research. We also provide interfaces for selecting subsets based on different time windows and coding task difficulties. While inspired by LLM-based tasks, ConDefects can be adopted for benchmarking ALL types of fault localization and program repair methods. The dataset is publicly available, and a demo video can be found at https://www.youtube.com/watch?v=22j15Hj5ONk.},
booktitle = {Companion Proceedings of the 32nd ACM International Conference on the Foundations of Software Engineering},
pages = {642–646},
numpages = {5},
keywords = {Dataset, Fault Localization, Large Language Model, Program Repair},
location = {Porto de Galinhas, Brazil},
series = {FSE 2024}
}

@inproceedings{10.1145/3524459.3527350,
author = {Lajk\'{o}, M\'{a}rk and Csuvik, Viktor and Vid\'{a}cs, L\'{a}szl\'{o}},
title = {Towards JavaScript program repair with generative pre-trained transformer (GPT-2)},
year = {2022},
isbn = {9781450392853},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3524459.3527350},
doi = {10.1145/3524459.3527350},
abstract = {The goal of Automated Program Repair (APR) is to find a fix to software bugs, without human intervention. The so-called Generate and Validate (G&amp;V) approach deemed to be the most popular method in the last few years, where the APR tool creates a patch and it is validated against an oracle. Recent years for Natural Language Processing (NLP) were of great interest, with new pre-trained models shattering records on tasks ranging from sentiment analysis to question answering. Usually these deep learning models inspire the APR community as well. These approaches usually require a large dataset on which the model can be trained (or fine-tuned) and evaluated. The criterion to accept a patch depends on the underlying dataset, but usually the generated patch should be exactly the same as the one created by a human developer. As NLP models are more and more capable to form sentences, and the sentences will form coherent paragraphs, the APR tools are also better and better at generating syntactically and semantically correct source code. As the Generative Pre-trained Transformer (GPT) model is now available to everyone thanks to the NLP and AI research community, it can be fine-tuned to specific tasks (not necessarily on natural language). In this work we use the GPT-2 model to generate source code, to the best of our knowledge, the GPT-2 model was not used for Automated Program Repair so far. The model is fine-tuned for a specific task: it has been taught to fix JavaScript bugs automatically. To do so, we trained the model on 16863 JS code snippets, where it could learn the nature of the observed programming language. In our experiments we observed that the GPT-2 model was able to learn how to write syntactically correct source code almost on every attempt, although it failed to learn good bug-fixes in some cases. Nonetheless it was able to generate the correct fixes in most of the cases, resulting in an overall accuracy up to 17.25\%.},
booktitle = {Proceedings of the Third International Workshop on Automated Program Repair},
pages = {61–68},
numpages = {8},
keywords = {machine learning, code refinement, automated program repair, JavaScript, GPT},
location = {Pittsburgh, Pennsylvania},
series = {APR '22}
}

@inproceedings{10.1145/3672608.3707774,
author = {Lijzenga, Oebele and Hemati Moghadam, Iman and Zaytsev, Vadim},
title = {Leveraging Search-Based and Pre-Trained Code Language Models for Automated Program Repair},
year = {2025},
isbn = {9798400706295},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3672608.3707774},
doi = {10.1145/3672608.3707774},
abstract = {Background. Automated Program Repair (APR) techniques often face challenges in navigating vast search space of possible patches and often rely on redundancy-based assumptions, which can restrict the diversity of generated patches. Recently, Code Language Models (CLMs) have emerged as a method for dynamically generating patch ingredients, potentially enhancing patch quality.Aim. This study aims to enhance APR by integrating search-based methods with CLMs to improve both the quality of generated patch ingredients and the efficiency of the search process.Method. We propose ARJACLM, a novel APR technique that uses a genetic algorithm for search space navigation and dynamically generates patch ingredients with the CodeLLaMA-13B model, combining redundancy-based and CLM-derived patch ingredients.Results. Testing on 176 bugs across 9 Java projects from Defect4J shows that CLM-generated patch ingredients significantly boost ARJACLM's performance, though at the cost of increased computation time. ARJACLM outperforms ARJA and GenProg, and CLM-generated patch ingredients are of higher quality than their redundancy-based counterparts. Additionally, ARJACLM performs best when redundancy-based patch ingredients are ignored.},
booktitle = {Proceedings of the 40th ACM/SIGAPP Symposium on Applied Computing},
pages = {1627–1636},
numpages = {10},
keywords = {program repair, search-based algorithm, code language model},
location = {Catania International Airport, Catania, Italy},
series = {SAC '25}
}

@inproceedings{10.1109/ICSE48619.2023.00112,
author = {Parasaram, Nikhil and Barr, Earl T. and Mechtaev, Sergey},
title = {Rete: Learning Namespace Representation for Program Repair},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00112},
doi = {10.1109/ICSE48619.2023.00112},
abstract = {A key challenge of automated program repair is finding correct patches in the vast search space of candidate patches. Real-world programs define large namespaces of variables that considerably contributes to the search space explosion. Existing program repair approaches neglect information about the program namespace, which makes them inefficient and increases the chance of test-overfitting. We propose Rete, a new program repair technique, that learns project-independent information about program namespace and uses it to navigate the search space of patches. Rete uses a neural network to extract project-independent information about variable CDU chains, defuse chains augmented with control flow. Then, it ranks patches by jointly ranking variables and the patch templates into which the variables are inserted. We evaluated Rete on 142 bugs extracted from two datasets, ManyBugs and BugsInPy. Our experiments demonstrate that Rete generates six new correct patches that fix bugs that previous tools did not repair, an improvement of 31\% and 59\% over the existing state of the art.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {1264–1276},
numpages = {13},
keywords = {variable representation, patch prioritisation, deep learning, program repair},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1109/ICSE55347.2025.00089,
author = {Yang, Aidan Z. H. and Kolak, Sophia and Hellendoorn, Vincent and Martins, Ruben and Goues, Claire Le},
title = {Revisiting Unnaturalness for Automated Program Repair in the Era of Large Language Models},
year = {2025},
isbn = {9798331505691},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE55347.2025.00089},
doi = {10.1109/ICSE55347.2025.00089},
abstract = {The problem of software quality has motivated the development of a variety of techniques for Automatic Program Repair (APR). Meanwhile, recent advances in AI and Large Language Models (LLMs) have produced orders of magnitude performance improvements over previous code generation techniques, affording promising opportunities for program repair and its constituent subproblems (e.g., fault localization, patch generation). Because models are trained on large volumes of code in which defects are relatively rare, they tend to both simultaneously perceive faulty code as unlikely (or "unnatural") and to produce generally correct code (which is more "natural"). This paper comprehensively revisits the idea of (un)naturalness for program repair. We argue that, fundamentally, LLMs can only go so far on their own in reasoning about and fixing buggy code. This motivates the incorporation of traditional tools, which compress useful contextual and analysis information, as a complement to LLMs for repair. We interrogate the role of entropy at every stage of traditional repair, and show that it is indeed usefully complementary to classic techniques. We show that combining measures of naturalness with class Spectrum-Based Fault Localization (SBFL) approaches improves Top-5 scoring by 50\% over SBFL alone. We show that entropy delta, or change in entropy induced by a candidate patch, can improve patch generation efficiency by 24 test suite executions per repair, on average, on our dataset. Finally, we show compelling results that entropy delta for patch classification is highly effective at distinguishing correct from overfitting patches. Overall, our results suggest that LLMs can effectively complement classic techniques for analysis and transformation, producing more efficient and effective automated repair techniques overall.},
booktitle = {Proceedings of the IEEE/ACM 47th International Conference on Software Engineering},
pages = {2561–2573},
numpages = {13},
keywords = {program repair, deep learning, large language models},
location = {Ottawa, Ontario, Canada},
series = {ICSE '25}
}

@inproceedings{10.1145/3611643.3616271,
author = {Wei, Yuxiang and Xia, Chunqiu Steven and Zhang, Lingming},
title = {Copiloting the Copilots: Fusing Large Language Models with Completion Engines for Automated Program Repair},
year = {2023},
isbn = {9798400703270},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3611643.3616271},
doi = {10.1145/3611643.3616271},
abstract = {During Automated Program Repair (APR), it can be challenging&nbsp;to synthesize correct patches for real-world systems in general-purpose programming languages. Recent Large Language Models&nbsp;(LLMs) have been shown to be helpful “copilots” in assisting developers with various coding tasks, and have also been directly&nbsp;applied for patch synthesis. However, most LLMs treat programs as&nbsp;sequences of tokens, meaning that they are ignorant of the underlying semantics constraints of the target programming language. This&nbsp;results in plenty of statically invalid generated patches, impeding&nbsp;the practicality of the technique. Therefore, we propose Repilot,&nbsp;a framework to further copilot the AI “copilots” (i.e., LLMs) by&nbsp;synthesizing more valid patches during the repair process. Our key&nbsp;insight is that many LLMs produce outputs autoregressively (i.e.,&nbsp;token by token), resembling human writing programs, which can&nbsp;be significantly boosted and guided through a Completion Engine.&nbsp;Repilot synergistically synthesizes a candidate patch through the&nbsp;interaction between an LLM and a Completion Engine, which 1)&nbsp;prunes away infeasible tokens suggested by the LLM and 2) proactively completes the token based on the suggestions provided by the&nbsp;Completion Engine. Our evaluation on a subset of the widely-used&nbsp;Defects4j 1.2 and 2.0 datasets shows that Repilot fixes 66 and 50&nbsp;bugs, respectively, surpassing the best-performing baseline by 14&nbsp;and 16 bugs fixed. More&nbsp;importantly, Repilot is capable of producing more valid and correct patches than the base LLM when given&nbsp;the same generation budget.},
booktitle = {Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {172–184},
numpages = {13},
keywords = {Completion Engine, Large Language Model, Program Repair},
location = {San Francisco, CA, USA},
series = {ESEC/FSE 2023}
}

@inproceedings{10.1609/aaai.v39i1.32046,
author = {Orvalho, Pedro and Janota, Mikol\'{a}\v{s} and Manquinho, Vasco M.},
title = {Counterexample guided program repair using zero-shot learning and MaxSAT-based fault localization},
year = {2025},
isbn = {978-1-57735-897-8},
publisher = {AAAI Press},
url = {https://doi.org/10.1609/aaai.v39i1.32046},
doi = {10.1609/aaai.v39i1.32046},
abstract = {Automated Program Repair (APR) for introductory programming assignments (IPAS) is motivated by the large number of student enrollments in programming courses each year. Since providing feedback on programming assignments requires substantial time and effort from faculty, personalized automated feedback often involves suggesting repairs to students' programs. Symbolic semantic repair approaches, which rely on Formal Methods (FM), check a program's execution against a test suite or reference solution, are effective but limited. These tools excel at identifying buggy parts but can only fix programs if the correct implementation and the faulty one share the same control flow graph. Conversely, Large Language Models (LLMS) are used for program repair but often make extensive rewrites instead of minimal adjustments. This tends to lead to more invasive fixes, making it harder for students to learn from their mistakes. In summary, LLMS excel at completing strings, while FM-based fault localization excel at identifying buggy parts of a program.In this paper, we propose a novel approach that combines the strengths of both FM-based fault localization and LLMS, via zero-shot learning, to enhance APR for IPAS. Our method uses MaxSAT-based fault localization to identify buggy parts of a program, then presents the LLM with a program sketch devoid of these buggy statements. This hybrid approach follows a Counterexample Guided Inductive Synthesis (CEGIS) loop to iteratively refine the program. We ask the LLM to synthesize the missing parts, which are then checked against a test suite. If the suggested program is incorrect, a counterexample from the test suite is fed back to the LLM for revised synthesis. Our experiments on 1,431 incorrect student programs show that our counterexample guided approach, using MaxSAT-based bug-free program sketches, significantly improves the repair capabilities of all six evaluated LLMS. This method allows LLMS to repair more programs and produce smaller fixes, outperforming other configurations and state-of-the-art symbolic program repair tools. Code — https://doi.org/10.5281/zenodo.14517771},
booktitle = {Proceedings of the Thirty-Ninth AAAI Conference on Artificial Intelligence and Thirty-Seventh Conference on Innovative Applications of Artificial Intelligence and Fifteenth Symposium on Educational Advances in Artificial Intelligence},
articleno = {73},
numpages = {9},
series = {AAAI'25/IAAI'25/EAAI'25}
}

@article{10.1007/s10515-025-00549-x,
author = {Wang, Bo and Deng, Ming and Chen, Mingda and Lin, Youfang and Zhou, Jianyi and Zhang, Jie M.},
title = {Assessing the effectiveness of recent closed-source large language models in fault localization and automated program repair},
year = {2025},
issue_date = {Dec 2025},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {33},
number = {1},
issn = {0928-8910},
url = {https://doi.org/10.1007/s10515-025-00549-x},
doi = {10.1007/s10515-025-00549-x},
abstract = {Large Language Models (LLMs) have made significant advancements in code-related tasks. In the field of automated debugging, fault localization (FL) and automated program repair (APR) are two prevalent topics attracting significant research effort. Recently, in the field of FL and APR, many novel LLM-based approaches have emerged. However, most existing LLM-based studies primarily focus on the GPT models from OpenAI or open-source LLMs. With the rapid development of LLMs, various internet giants have introduced new closed-source models. In addition, due to policy restrictions, some regions can only access the commercial LLMs provided by specified companies. Despite the LLMs of OpenAI, the effectiveness of the other closed-source LLMs in FL and APR remains unknown. To better understand the effectiveness of contemporary closed-source models, we conduct a large-scale empirical study on their performance with respect to FL and APR. Specifically, our study involves 4 recent commercial closed-source LLMs (i.e., GPT-4o-Mini, Ernie-3.5, Qwen-turbo, and Doubao-pro) and 1 open-source LLM (i.e., DeepSeek-V3-chat). Note that only the GPT models have region restrictions among all LLMs we studied. We designed a total of 12 distinct prompt templates, 6 each for FL and APR, incorporating various formats and information sources. We conducted experiments to evaluate the effectiveness of FL and APR on 1036 real Java bugs from two datasets, Defects4J 2.0 and ConDefects. The key findings of the experiments indicate that (1) different LLMs tend to succeed on different sets of bugs in both FL and APR, with relatively little overlap among successful cases, implying the models possess distinct strengths in handling specific kinds of bugs, (2) the effectiveness of prompt templates varies across different models, and (3) the effectiveness of FL and APR capabilities of the studied models is significantly correlated with the bug type. We summarized all 14 findings obtained into 3 implications, which could help researchers further improve the performance of LLMs on FL and APR.},
journal = {Automated Software Engg.},
month = oct,
numpages = {42},
keywords = {Large language models, Software debugging, Fault localization, Automated program repair, Empirical study}
}

@inproceedings{10.1145/3650212.3652140,
author = {Ouyang, Yicheng and Yang, Jun and Zhang, Lingming},
title = {Benchmarking Automated Program Repair: An Extensive Study on Both Real-World and Artificial Bugs},
year = {2024},
isbn = {9798400706127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3650212.3652140},
doi = {10.1145/3650212.3652140},
abstract = {As bugs are inevitable and prevalent in real-world programs, many Automated Program Repair (APR) techniques have been proposed to generate patches for them. However, due to the lack of a standard for evaluating APR techniques, prior works tend to use different settings and benchmarks in evaluation, threatening the trustworthiness of the evaluation results. Additionally, they typically only adopt plausibility and genuineness as evaluation metrics, which may potentially mask some underlying issues in APR techniques. To overcome these issues, in this paper, we conduct an extensive and multi-dimensional evaluation of nine learning-based and three traditional state-of-the-art APR techniques under the same environment and settings. We employ the widely studied Defects4J V2.0.0 benchmark and a newly constructed large-scale mutation-based benchmark named MuBench, derived from Defects4J and including 1,700 artificial bugs generated by various mutators, to uncover potential limitations in these APR techniques. We also apply multi-dimensional metrics, including compilability/plausibility/genuineness metrics, as well as SYE (SYntactic Equivalence) and TCE (Trivial Compiler Equivalence) metrics, to thoroughly analyze the 1,814,652 generated patches. This paper presents noteworthy findings from the extensive evaluation: Firstly, Large Language Model (LLM) based APR demonstrates less susceptibility to overfitting on the Defects4J V1.2.0 dataset and fixes the most number of bugs. Secondly, the study suggests a promising future for combining traditional and learning-based APR techniques, as they exhibit complementary advantages in fixing different types of bugs. Additionally, this work highlights the necessity for further enhancing patch compilability of learning-based APR techniques, despite the presence of various existing strategies attempting to improve it. The study also reveals other guidelines for enhancing APR techniques, including the need for handling unresolvable symbol compilability issues and reducing duplicate/no-op patch generation. Finally, our study uncovers seven implementation issues in the studied techniques, with five of them confirmed and fixed by the corresponding authors.},
booktitle = {Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {440–452},
numpages = {13},
keywords = {Empirical assessment, Mutation testing, Program repair},
location = {Vienna, Austria},
series = {ISSTA 2024}
}

@inproceedings{10.1145/3670474.3685953,
author = {Xu, Kangwei and Zhang, Grace Li and Yin, Xunzhao and Zhuo, Cheng and Schlichtmann, Ulf and Li, Bing},
title = {Automated C/C++ Program Repair for High-Level Synthesis via Large Language Models},
year = {2024},
isbn = {9798400706998},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3670474.3685953},
doi = {10.1145/3670474.3685953},
abstract = {In High-Level Synthesis (HLS), converting a regular C/C++ program into its HLS-compatible counterpart (HLS-C) still requires tremendous manual effort. Various program scripts have been introduced to automate this process. But the resulting codes usually contain many issues that should be manually repaired by developers. Since Large Language Models (LLMs) have the ability to automate code generation, they can also be used for automated program repair in HLS. However, due to the limited training of LLMs considering hardware and software simultaneously, hallucinations may occur during program repair using LLMs, leading to compilation failures. Besides, using LLMs for iterative repair also incurs a high cost. To address these challenges, we propose an LLM-driven program repair framework that takes regular C/C++ code as input and automatically generates its corresponding HLS-C code for synthesis while minimizing human repair effort. To mitigate the hallucinations in LLMs and enhance the prompt quality, a Retrieval-Augmented Generation (RAG) paradigm is introduced to guide the LLMs toward correct repair. In addition, we use LLMs to create a static bit width optimization program to identify the optimized bit widths for variables. Moreover, LLM-driven HLS optimization strategies are introduced to add/tune pragmas in HLS-C programs for circuit optimization. Experimental results demonstrate that the proposed LLM-driven automated framework can achieve much higher repair pass rates in 24 real-world applications compared with the traditional scripts and the direct application of LLMs for program repair. The codes are open-sourced at this link: https://github.com/code-source1/catapult.},
booktitle = {Proceedings of the 2024 ACM/IEEE International Symposium on Machine Learning for CAD},
articleno = {15},
numpages = {9},
location = {Salt Lake City, UT, USA},
series = {MLCAD '24}
}

@inproceedings{10.1007/978-3-031-36272-9_74,
author = {Koutcheme, Charles and Sarsa, Sami and Leinonen, Juho and Hellas, Arto and Denny, Paul},
title = {Automated Program Repair Using Generative Models for&nbsp;Code Infilling},
year = {2023},
isbn = {978-3-031-36271-2},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-36272-9_74},
doi = {10.1007/978-3-031-36272-9_74},
abstract = {In educational settings, automated program repair techniques serve as a feedback mechanism to guide students working on their programming assignments. Recent work has investigated using large language models (LLMs) for program repair. In this area, most of the attention has been focused on using proprietary systems accessible through APIs. However, the limited access and control over these systems remain a block to their adoption and usage in education. The present work studies the repairing capabilities of open large language models. In particular, we focus on a recent family of generative models, which, on top of standard left-to-right program synthesis, can also predict missing spans of code at any position in a program. We experiment with one of these models on four programming datasets and show that we can obtain good repair performance even without additional training.},
booktitle = {Artificial Intelligence in Education: 24th International Conference, AIED 2023, Tokyo, Japan, July 3–7, 2023, Proceedings},
pages = {798–803},
numpages = {6},
keywords = {Program repair, Large Language Models, Computer Science Education},
location = {Tokyo, Japan}
}

@inproceedings{10.1145/3742876.3742881,
author = {Hemati Moghadam, Iman and Lijzenga, Oebele and Zaytsev, Vadim},
title = {Comparative Analysis of Pre-trained Code Language Models for Automated Program Repair via Code Infill Generation},
year = {2025},
isbn = {9798400719950},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3742876.3742881},
doi = {10.1145/3742876.3742881},
abstract = {Automated Program Repair (APR) has advanced significantly with the emergence of pre-trained Code Language Models (CLMs), enabling the generation of high-quality patches. However, selecting the most suitable CLM for APR remains challenging due to a range of factors, including accuracy, efficiency, and scalability, among others. These factors are interdependent and interact in complex ways, making the selection of a CLM for APR a multifaceted problem.    This study systematically evaluates 20 pre-trained CLMs, ranging from 60M to 16B parameters, on the HumanEval-Java benchmark (163 buggy Java methods). The evaluation examines bug-fixing accuracy, resource consumption, compilability, patch diversity, and sampling strategies (beam search vs. nucleus sampling).    Results indicate that larger models such as CodeLLaMA-13B and StarCoder generally perform better in bug fixing and compiler error handling, but scale alone does not guarantee effectiveness, as some (e.g., CodeGen2) perform poorly despite their size. Notably, memory usage increases with model size, but time consumption does not exhibit a clear correlation, suggesting that efficiency is influenced by architecture rather than scale alone. Additionally, nucleus sampling slightly outperforms beam search, though the difference is not statistically significant. Since no single CLM fixes all bugs, these findings highlight the potential of hybrid or ensemble-based CLM-driven APR approaches for more robust bug-fixing.},
booktitle = {Proceedings of the 24th ACM SIGPLAN International Conference on Generative Programming: Concepts and Experiences},
pages = {13–26},
numpages = {14},
keywords = {Automated program repair, Java, empirical study, pre-trained code language model, zero-shot learning},
location = {Bergen, Norway},
series = {GPCE '25}
}

@inproceedings{10.1109/ASE56229.2023.00063,
author = {Zhang, Quanjun and Fang, Chunrong and Zhang, Tongke and Yu, Bowen and Sun, Weisong and Chen, Zhenyu},
title = {Gamma: Revisiting Template-based Automated Program Repair via Mask Prediction},
year = {2024},
isbn = {9798350329964},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASE56229.2023.00063},
doi = {10.1109/ASE56229.2023.00063},
abstract = {Automated program repair (APR) aims to fix software bugs without manual debugging efforts and plays a crucial role in software development and maintenance. Template-based APR has been widely investigated and shown promising results. However, it is challenging for template-based APR to select the appropriate donor code, which is an important repair ingredient for generating candidate patches. Inappropriate donor code may cause plausible but incorrect patch generation even with correct fix patterns, limiting the repair performance.In this paper, we aim to revisit template-based APR, and propose Gamma, to directly leverage large pre-trained language models for donor code generation. Our main insight is that instead of retrieving donor code in the local buggy file, we can directly predict the correct code tokens based on the context code snippets and repair patterns by a cloze task. Specifically, (1) Gamma revises a variety of fix templates from state-of-the-art template-based APR techniques (i.e., TBar) and transforms them into mask patterns. (2) Gamma adopts a pre-trained language model to predict the correct code for masked code as a fill-in-the-blank task. Although our idea is general and can be built on various existing pre-trained language models, we have implemented Gamma as a practical APR tool based on the recent UniXcoder model. The experimental results demonstrate that Gamma correctly repairs 82 bugs on Defects4J-v1.2, which achieves 20.59\% (14 bugs) and 26.15\% (17 bugs) improvement over the previous state-of-the-art template-based approach TBar and learning-based one Recoder. Furthermore, Gamma repairs 45 bugs and 22 bugs from the additional Defects4J-v2.0 and QuixBugs, indicating the generalizability of Gamma in addressing the dataset overfitting issue. We also prove that adopting other pre-trained language models can provide substantial advancement, e.g., CodeBERT-based and ChatGPT-based Gamma is able to fix 80 and 67 bugs on Defects4J-v1.2, indicating the scalability of Gamma. Overall, our study highlights the promising future of adopting pre-trained models to generate correct patches on top of fix patterns in practice.},
booktitle = {Proceedings of the 38th IEEE/ACM International Conference on Automated Software Engineering},
pages = {535–547},
numpages = {13},
keywords = {automated program repair, fix pattern, pre-trained model, LLM4SE},
location = {Echternach, Luxembourg},
series = {ASE '23}
}

@inproceedings{10.1109/ICSE48619.2023.00125,
author = {Jiang, Nan and Liu, Kevin and Lutellier, Thibaud and Tan, Lin},
title = {Impact of Code Language Models on Automated Program Repair},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00125},
doi = {10.1109/ICSE48619.2023.00125},
abstract = {Automated program repair (APR) aims to help developers improve software reliability by generating patches for buggy programs. Although many code language models (CLM) are developed and effective in many software tasks such as code completion, there has been little comprehensive, in-depth work to evaluate CLMs' fixing capabilities and to fine-tune CLMs for the APR task.Firstly, this work is the first to evaluate ten CLMs on four APR benchmarks, which shows that surprisingly, the best CLM, as is, fixes 72\% more bugs than the state-of-the-art deep-learning (DL)-based APR techniques. Secondly, one of the four APR benchmarks was created by us in this paper to avoid data leaking for a fair evaluation. Thirdly, it is the first work to fine-tune CLMs with APR training data, which shows that fine-tuning brings 31\%--1,267\% improvement to CLMs and enables them to fix 46\%--164\% more bugs than existing DL-based APR techniques. Fourthly, this work studies the impact of buggy lines, showing that CLMs, as is, cannot make good use of the buggy lines to fix bugs, yet fine-tuned CLMs could potentially over-rely on buggy lines. Lastly, this work analyzes the size, time, and memory efficiency of different CLMs.This work shows promising directions for the APR domain, such as fine-tuning CLMs with APR-specific designs, and also raises awareness of fair and comprehensive evaluations of CLMs and calls for more transparent reporting of open-source repositories used in the pre-training data to address the data leaking problem.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {1430–1442},
numpages = {13},
keywords = {deep learning, fine-tuning, code language model, automated program repair},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@article{10.1145/3733599,
author = {Luo, Wenqiang and Keung, Jacky and Yang, Boyang and Ye, He and Le Goues, Claire and Bissyand\'{e}, Tegawend\'{e} F. and Tian, Haoye and Le, Xuan Bach D.},
title = {When Fine-Tuning LLMs Meets Data Privacy: An Empirical Study of Federated Learning in LLM-Based Program Repair},
year = {2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1049-331X},
url = {https://doi.org/10.1145/3733599},
doi = {10.1145/3733599},
abstract = {Software systems have been evolving rapidly and inevitably introducing bugs at an increasing rate, leading to significant maintenance costs. While large language models (LLMs) have demonstrated remarkable potential in enhancing software development and maintenance practices, particularly in automated program repair (APR), they rely heavily on high-quality code repositories. Most code repositories are proprietary assets that capture the diversity and nuances of real-world industry software practices, which public datasets cannot fully represent. However, obtaining such data from various industries is hindered by data privacy concerns, as companies are reluctant to share their proprietary codebases. There has also been no in-depth investigation of collaborative software development by learning from private and decentralized data while preserving data privacy for program repair.To address the gap, we investigate federated learning as a privacy-preserving method for fine-tuning LLMs on proprietary and decentralized data to boost collaborative software development and maintenance. We use the private industrial dataset TutorCode for fine-tuning and the EvalRepair-Java benchmark for evaluation, and assess whether federated fine-tuning enhances program repair. We then further explore how code heterogeneity (i.e., variations in coding style, complexity, and embedding) and different federated learning algorithms affect bug fixing to provide practical implications for real-world software development collaboration. Our evaluation reveals that federated fine-tuning can significantly enhance program repair, achieving increases of up to 16.67\% for Top@10 and 18.44\% for Pass@10, even comparable to the bug-fixing capabilities of centralized learning. Moreover, the negligible impact of code heterogeneity implies that industries can effectively collaborate despite diverse data distributions. Different federated algorithms also demonstrate unique strengths across LLMs, suggesting that tailoring the optimization process to specific LLM characteristics can further improve program repair.},
note = {Just Accepted},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = may,
keywords = {Program Repair, Federated Learning, Large Language Models}
}

@inproceedings{10.1145/3545258.3545268,
author = {Zhong, Wenkang and Li, Chuanyi and Ge, Jidong and Luo, Bin},
title = {Neural Program Repair : Systems, Challenges and Solutions},
year = {2022},
isbn = {9781450397803},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3545258.3545268},
doi = {10.1145/3545258.3545268},
abstract = {Automated Program Repair (APR) aims to automatically fix bugs in the source code. Recently, with advances in Deep Learning (DL) field, there has been an increase of Neural Program Repair (NPR) studies that use neural networks to model the patch-generation process. NPR approaches have a significant benefit in applicability over prior APR techniques because they do not require any specifications (e.g., a test suite) when generating patches. For this reason, NPR has recently become a popular research topic. In this paper, We undertake a literature review of latest NPR systems to help interested readers understand advancements in this emerging field. We begin by introducing background information of NPR. Next, to make the various NPR systems more understandable, we split them into a four-phase pipeline and discuss various design choices for each phase. To investigate the motivations of different design choices, We further highlight a number of challenges and summarize corresponding solutions adopted by existing NPR systems. Finally, we suggest some intriguing directions for the future research.},
booktitle = {Proceedings of the 13th Asia-Pacific Symposium on Internetware},
pages = {96–106},
numpages = {11},
keywords = {Software reliability, Neural networks, Automatic program repair},
location = {Hohhot, China},
series = {Internetware '22}
}

@article{10.1145/3672450,
author = {Lou, Yiling and Yang, Jun and Benton, Samuel and Hao, Dan and Tan, Lin and Chen, Zhenpeng and Zhang, Lu and Zhang, Lingming},
title = {When Automated Program Repair Meets Regression Testing—An Extensive Study on Two Million Patches},
year = {2024},
issue_date = {September 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {33},
number = {7},
issn = {1049-331X},
url = {https://doi.org/10.1145/3672450},
doi = {10.1145/3672450},
abstract = {In recent years, Automated Program Repair (APR) has been extensively studied in academia and even drawn wide attention from the industry. However, APR techniques can be extremely time consuming since (1) a large number of patches can be generated for a given bug, and (2) each patch needs to be executed on the original tests to ensure its correctness. In the literature, various techniques (e.g., based on learning, mining, and constraint solving) have been proposed/studied to reduce the number of patches. Intuitively, every patch can be treated as a software revision during regression testing; thus, traditional Regression Test Selection (RTS) techniques can be leveraged to only execute the tests affected by each patch (as the other tests would keep the same outcomes) to further reduce patch execution time. However, few APR systems actually adopt RTS and there is still a lack of systematic studies demonstrating the benefits of RTS and the impact of different RTS strategies on APR. To this end, this article presents the first extensive study of widely used RTS techniques at different levels (i.e., class/method/statement levels) for 12 state-of-the-art APR systems on over 2M patches. Our study reveals various practical guidelines for bridging the gap between APR and regression testing, including: (1) the number of patches widely used for measuring APR efficiency can incur skewed conclusions, and the use of inconsistent RTS configurations can further skew the conclusions; (2) all studied RTS techniques can substantially improve APR efficiency and should be considered in future APR work; (3) method- and statement-level RTS outperform class-level RTS substantially and should be preferred; (4) RTS techniques can substantially outperform state-of-the-art test prioritization techniques for APR, and combining them can further improve APR efficiency; and (5) traditional Regression Test Prioritization (RTP) widely studied in regression testing performs even better than APR-specific test prioritization when combined with most RTS techniques. Furthermore, we also present the detailed impact of different patch categories and patch validation strategies on our findings.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = sep,
articleno = {180},
numpages = {23},
keywords = {Test selection, program repair, patch validation}
}

@inproceedings{10.1145/3551349.3560422,
author = {Li, Xueyang and Liu, Shangqing and Feng, Ruitao and Meng, Guozhu and Xie, Xiaofei and Chen, Kai and Liu, Yang},
title = {TransRepair: Context-aware Program Repair for Compilation Errors},
year = {2023},
isbn = {9781450394758},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3551349.3560422},
doi = {10.1145/3551349.3560422},
abstract = {Automatically fixing compilation errors can greatly raise the productivity of software development, by guiding the novice or AI programmers to write and debug code. Recently, learning-based program repair has gained extensive attention and became the state-of-the-art in practice. But it still leaves plenty of space for improvement. In this paper, we propose an end-to-end solution&nbsp;TransRepair to locate the error lines and create the correct substitute for a C program simultaneously. Superior to the counterpart, our approach takes into account the context of erroneous code and diagnostic compilation feedback. Then we devise a Transformer-based neural network to learn the ways of repair from the erroneous code as well as its context and the diagnostic feedback. To increase the effectiveness of TransRepair, we summarize 5 types and 74 fine-grained sub-types of compilations errors from two real-world program datasets and the Internet. Then a program corruption technique is developed to synthesize a large dataset with 1,821,275 erroneous C programs. Through the extensive experiments, we demonstrate that TransRepair outperforms the state-of-the-art in both single repair accuracy and full repair accuracy. Further analysis sheds light on the strengths and weaknesses in the contemporary solutions for future improvement.},
booktitle = {Proceedings of the 37th IEEE/ACM International Conference on Automated Software Engineering},
articleno = {108},
numpages = {13},
keywords = {Program repair, compilation error, context-aware, deep learning},
location = {Rochester, MI, USA},
series = {ASE '22}
}

@inproceedings{10.1109/ICSE48619.2023.00111,
author = {Jiang, Nan and Lutellier, Thibaud and Lou, Yiling and Tan, Lin and Goldwasser, Dan and Zhang, Xiangyu},
title = {KNOD: Domain Knowledge Distilled Tree Decoder for Automated Program Repair},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00111},
doi = {10.1109/ICSE48619.2023.00111},
abstract = {Automated Program Repair (APR) improves software reliability by generating patches for a buggy program automatically. Recent APR techniques leverage deep learning (DL) to build models to learn to generate patches from existing patches and code corpora. While promising, DL-based APR techniques suffer from the abundant syntactically or semantically incorrect patches in the patch space. These patches often disobey the syntactic and semantic domain knowledge of source code and thus cannot be the correct patches to fix a bug.We propose a DL-based APR approach KNOD, which incorporates domain knowledge to guide patch generation in a direct and comprehensive way. KNOD has two major novelties, including (1) a novel three-stage tree decoder, which directly generates Abstract Syntax Trees of patched code according to the inherent tree structure, and (2) a novel domain-rule distillation, which leverages syntactic and semantic rules and teacher-student distributions to explicitly inject the domain knowledge into the decoding procedure during both the training and inference phases.We evaluate KNOD on three widely-used benchmarks. KNOD fixes 72 bugs on the Defects4J v1.2, 25 bugs on the QuixBugs, and 50 bugs on the additional Defects4J v2.0 benchmarks, outperforming all existing APR tools.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {1251–1263},
numpages = {13},
keywords = {deep learning, abstract syntax tree, automated program repair},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1145/3611643.3616256,
author = {Wang, Weishi and Wang, Yue and Joty, Shafiq and Hoi, Steven C.H.},
title = {RAP-Gen: Retrieval-Augmented Patch Generation with CodeT5 for Automatic Program Repair},
year = {2023},
isbn = {9798400703270},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3611643.3616256},
doi = {10.1145/3611643.3616256},
abstract = {Automatic program repair (APR) is crucial to reduce manual debugging efforts for developers and improve software reliability. While conventional search-based techniques typically rely on heuristic rules or a redundancy assumption to mine fix patterns, recent years have witnessed the surge of deep learning (DL) based approaches to automate the program repair process in a data-driven manner. However, their performance is often limited by a fixed set of parameters to model the highly complex search space of APR. To ease such burden on the parametric models, in this work, we propose a novel Retrieval-Augmented Patch Generation framework (RAP-Gen) by explicitly leveraging relevant fix patterns retrieved from a codebase of previous bug-fix pairs. Specifically, we build a hybrid patch retriever to account for both lexical and semantic matching based on the raw source code in a language-agnostic manner, which does not rely on any code-specific features. In addition, we adapt a code-aware language model CodeT5 as our foundation model to facilitate both patch retrieval and generation tasks in a unified manner. We adopt a stage-wise approach where the patch retriever first retrieves a relevant external bug-fix pair to augment the buggy input for the CodeT5 patch generator, which synthesizes a ranked list of repair patch candidates. Notably, RAP-Gen is a generic APR framework that can flexibly integrate different patch retrievers and generators to repair various types of bugs. We thoroughly evaluate RAP-Gen on three benchmarks in two programming languages, including the TFix benchmark in JavaScript, and Code Refinement and Defects4J benchmarks in Java, where the bug localization information may or may not be provided. Experimental results show that RAP-Gen significantly outperforms previous state-of-the-art (SoTA) approaches on all benchmarks, e.g., boosting the accuracy of T5-large on TFix from 49.70\% to 54.15\% (repairing 478 more bugs) and repairing 15 more bugs on 818 Defects4J bugs. Further analysis reveals that our patch retriever can search for relevant fix patterns to guide the APR systems.},
booktitle = {Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {146–158},
numpages = {13},
keywords = {Automated program repair, Neural networks, Pretrained language models, Retrieval-augmented generation},
location = {San Francisco, CA, USA},
series = {ESEC/FSE 2023}
}

@inproceedings{10.1145/3639478.3647633,
author = {Chen, Yuxiao and Wu, Jingzheng and Ling, Xiang and Li, Changjiang and Rui, Zhiqing and Luo, Tianyue and Wu, Yanjun},
title = {When Large Language Models Confront Repository-Level Automatic Program Repair: How Well They Done?},
year = {2024},
isbn = {9798400705021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639478.3647633},
doi = {10.1145/3639478.3647633},
abstract = {In recent years, large language models (LLMs) have demonstrated substantial potential in addressing automatic program repair (APR) tasks. However, the current evaluation of these models for APR tasks focuses solely on the limited context of the single function or file where the bug is located, overlooking the valuable information in the repository-level context. This paper investigates the performance of popular LLMs in handling repository-level repair tasks. We introduce RepoBugs, a new benchmark comprising 124 typical repository-level bugs from open-source repositories. Preliminary experiments using GPT3.5 based on the function where the error is located, reveal that the repair rate on RepoBugs is only 22.58\%, significantly diverging from the performance of GPT3.5 on function-level bugs in related studies. This underscores the importance of providing repository-level context when addressing bugs at this level. However, the repository-level context offered by the preliminary method often proves redundant and imprecise and easily exceeds the prompt length limit of LLMs. To solve the problem, we propose a simple and universal repository-level context extraction method (RLCE) designed to provide more precise context for repository-level code repair tasks. Evaluations of three mainstream LLMs show that RLCE significantly enhances the ability to repair repository-level bugs. The improvement reaches a maximum of 160\% compared to the preliminary method. Additionally, we conduct a comprehensive analysis of the effectiveness and limitations of RLCE, along with the capacity of LLMs to address repository-level bugs, offering valuable insights for future research.},
booktitle = {Proceedings of the 2024 IEEE/ACM 46th International Conference on Software Engineering: Companion Proceedings},
pages = {459–471},
numpages = {13},
keywords = {large language models, automatic program repair, repository-level bugs, context, static analysis},
location = {Lisbon, Portugal},
series = {ICSE-Companion '24}
}

@inproceedings{10.1145/3540250.3558953,
author = {Winter, Emily Rowan and Nowack, Vesna and Bowes, David and Counsell, Steve and Hall, Tracy and Haraldsson, S\ae{}mundur and Woodward, John and Kirbas, Serkan and Windels, Etienne and McBello, Olayori and Atakishiyev, Abdurahman and Kells, Kevin and Pagano, Matthew},
title = {Towards developer-centered automatic program repair: findings from Bloomberg},
year = {2022},
isbn = {9781450394130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3540250.3558953},
doi = {10.1145/3540250.3558953},
abstract = {This paper reports on qualitative research into automatic program repair (APR) at Bloomberg. Six focus groups were conducted with a total of seventeen participants (including both developers of the APR tool and developers using the tool) to consider: the development at Bloomberg of a prototype APR tool (Fixie); developers’ early experiences using the tool; and developers’ perspectives on   how they would like to interact with the tool in future. APR is developing rapidly and it is important to understand in greater detail developers' experiences using this emerging technology. In this paper, we provide in-depth, qualitative data from an industrial setting. We found that the development of APR at Bloomberg had become increasingly user-centered, emphasising how fixes were presented to developers, as well as particular features, such as customisability. From the focus groups with developers who had used Fixie, we found particular concern with the pragmatic aspects of APR, such as how and when fixes were presented to them. Based on our findings, we make a series of recommendations to inform future APR development, highlighting how APR tools should 'start small', be customisable, and fit with developers' workflows. We also suggest that APR tools should capitalise on the promise of repair bots and draw on advances in explainable AI.},
booktitle = {Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {1578–1588},
numpages = {11},
keywords = {qualitative methods, human factors, automatic program repair},
location = {Singapore, Singapore},
series = {ESEC/FSE 2022}
}

@inproceedings{10.1145/3551349.3559519,
author = {Ghanbari, Ali and Marcus, Andrian (Andi)},
title = {Shibboleth: Hybrid Patch Correctness Assessment in Automated Program Repair},
year = {2023},
isbn = {9781450394758},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3551349.3559519},
doi = {10.1145/3551349.3559519},
abstract = {Test-based generate-and-validate automated program repair (APR) systems generate many patches that pass the test suite without fixing the bug. The generated patches must be manually inspected by the developers, a task that tends to be time-consuming, thereby diminishing the role of APR in reducing debugging costs. We present the design and implementation of a novel tool, named Shibboleth, for automatic assessment of the patches generated by test-based generate-and-validate APR systems. Shibboleth leverages lightweight static and dynamic heuristics from both test and production code to rank and classify the patches. Shibboleth is based on the idea that the buggy program is almost correct and the bugs are small mistakes that require small changes to fix and specifically the fix does not remove the code implementing correct functionality of the program. Thus, the tool measures the impact of patches on both production code (via syntactic and semantic similarity) and test code (via code coverage) to separate the patches that result in similar programs and that do not remove desired program elements. We have evaluated Shibboleth on 1,871 patches, generated by 29 Java-based APR systems for Defects4J programs. The technique outperforms state-of-the-art raking and classification techniques. Specifically, in our ranking data set, in 66\% of the cases, Shibboleth ranks the correct patch in top-1 or top-2 positions and, in our classification data set, it achieves an accuracy and F1-score of 0.887 and 0.852, respectively, in classification mode. A demo video of the tool is available at https://bit.ly/3NvYJN8.},
booktitle = {Proceedings of the 37th IEEE/ACM International Conference on Automated Software Engineering},
articleno = {166},
numpages = {4},
keywords = {Automated Program Repair, Branch Coverage, Patch Correctness Assessment, Similarity},
location = {Rochester, MI, USA},
series = {ASE '22}
}

@inproceedings{10.1145/3555776.3577762,
author = {Kim, Jisung and Lee, Byungjeong},
title = {MCRepair: Multi-Chunk Program Repair via Patch Optimization with Buggy Block},
year = {2023},
isbn = {9781450395175},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3555776.3577762},
doi = {10.1145/3555776.3577762},
abstract = {Automated program repair (APR) is a technology that identifies and repairs bugs automatically. However, repairing multi-chunk bugs remains a long-standing and challenging problem because an APR technique must consider dependencies and then reduce the large patch space. In addition, little is known about how to combine individual candidate patches even though multi-chunk bugs require combinations. Therefore, we propose a novel APR technique called multi-code repair (MCRepair), which applies a buggy block, patch optimization, and CodeBERT to target multi-chunk bugs. A buggy block is a novel method that binds buggy chunks into a multi-buggy chunk and preprocesses the chunk with its buggy contexts for patch space reduction and dependency problems. Patch optimization is a novel strategy that effectively combines the generated candidate patches with patch space reduction. In addition, CodeBERT, a BERT for source code datasets, is fine-tuned to address the lack of datasets and out-of-vocabulary problems. We conducted several experiments to evaluate our approach on six project modules of Defects4J. In the experiments using Defects4J, MCRepair repaired 66 bugs, including 22 multi-chunk bugs. Moreover, it fixed 19 unique bugs, including nine multi-chunk bugs, and improved 45--266\% performance than the baselines.},
booktitle = {Proceedings of the 38th ACM/SIGAPP Symposium on Applied Computing},
pages = {1508–1515},
numpages = {8},
keywords = {deep learning, patch optimization, buggy block, automated program repair},
location = {Tallinn, Estonia},
series = {SAC '23}
}

@article{10.1145/3720510,
author = {Wang, Ruixin and Zhao, Zhongkai and Fang, Le and Jiang, Nan and Lou, Yiling and Tan, Lin and Zhang, Tianyi},
title = {Show Me Why It’s Correct: Saving 1/3 of Debugging Time in Program Repair with Interactive Runtime Comparison},
year = {2025},
issue_date = {April 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {OOPSLA1},
url = {https://doi.org/10.1145/3720510},
doi = {10.1145/3720510},
abstract = {Automated Program Repair (APR) holds the promise of alleviating the burden of debugging and fixing software bugs. Despite this, developers still need to manually inspect each patch to confirm its correctness, which is tedious and time-consuming. This challenge is exacerbated in the presence of plausible patches, which accidentally pass test cases but may not correctly fix the bug. To address this challenge, we propose an interactive approach called iFix to facilitate patch understanding and comparison based on their runtime difference. iFix performs static analysis to identify runtime variables related to the buggy statement and captures their runtime values during execution for each patch. These values are then aligned across different patch candidates, allowing users to compare and contrast their runtime behavior. To evaluate iFix, we conducted a within-subjects user study with 28 participants. Compared with manual inspection and a state-of-the-art interactive patch filtering technique, iFix reduced participants’ task completion time by 36\% and 33\% while also improving their confidence by 50\% and 20\%, respectively. Besides, quantitative experiments demonstrate that iFix improves the ranking of correct patches by at least 39\% compared with other patch ranking methods and is generalizable to different APR tools.},
journal = {Proc. ACM Program. Lang.},
month = apr,
articleno = {145},
numpages = {27},
keywords = {Automatic Program Repair, Interaction Support, User Trust}
}

@inproceedings{10.1109/ICSE48619.2023.00129,
author = {Xia, Chunqiu Steven and Wei, Yuxiang and Zhang, Lingming},
title = {Automated Program Repair in the Era of Large Pre-Trained Language Models},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00129},
doi = {10.1109/ICSE48619.2023.00129},
abstract = {Automated Program Repair (APR) aims to help developers automatically patch software bugs. However, current state-of-the-art traditional and learning-based APR techniques face the problem of limited patch variety, failing to fix complicated bugs. This is mainly due to the reliance on bug-fixing datasets to craft fix templates (traditional) or directly predict potential patches (learning-based). Large Pre-Trained Language Models (LLMs), trained using billions of text/code tokens, can potentially help avoid this issue. Very recently, researchers have directly leveraged LLMs for APR without relying on any bug-fixing datasets. Meanwhile, such existing work either failed to include state-of-the-art LLMs or was not evaluated on realistic datasets. Thus, the true power of modern LLMs on the important APR problem is yet to be revealed.In this work, we perform the first extensive study on directly applying LLMs for APR. We select 9 recent state-of-the-art LLMs, including both generative and infilling models, ranging from 125M to 20B in size. We designed 3 different repair settings to evaluate the different ways we can use LLMs to generate patches: 1) generate the entire patch function, 2) fill in a chunk of code given the prefix and suffix 3) output a single line fix. We apply the LLMs under these repair settings on 5 datasets across 3 different languages and compare different LLMs in the number of bugs fixed, generation speed and compilation rate. We also compare the LLMs against recent state-of-the-art APR tools. Our study demonstrates that directly applying state-of-the-art LLMs can already substantially outperform all existing APR techniques on all our datasets. Among the studied LLMs, the scaling effect exists for APR where larger models tend to achieve better performance. Also, we show for the first time that suffix code after the buggy line (adopted in infilling-style APR) is important in not only generating more fixes but more patches with higher compilation rate. Besides patch generation, the LLMs consider correct patches to be more natural than other ones, and can even be leveraged for effective patch ranking or patch correctness checking. Lastly, we show that LLM-based APR can be further substantially boosted via: 1) increasing the sample size, and 2) incorporating fix template information.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {1482–1494},
numpages = {13},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@article{10.1145/3654441,
author = {Wang, Xu and Yu, Hongwei and Meng, Xiangxin and Cao, Hongliang and Zhang, Hongyu and Sun, Hailong and Liu, Xudong and Hu, Chunming},
title = {MTL-TRANSFER: Leveraging Multi-task Learning and Transferred Knowledge for Improving Fault Localization and Program Repair},
year = {2024},
issue_date = {July 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {33},
number = {6},
issn = {1049-331X},
url = {https://doi.org/10.1145/3654441},
doi = {10.1145/3654441},
abstract = {Fault localization (FL) and automated program repair (APR) are two main tasks of automatic software debugging. Compared with traditional methods, deep learning-based approaches have been demonstrated to achieve better performance in FL and APR tasks. However, the existing deep learning-based FL methods ignore the deep semantic features or only consider simple code representations. And for APR tasks, existing template-based APR methods are weak in selecting the correct fix templates for more effective program repair, which are also not able to synthesize patches via the embedded end-to-end code modification knowledge obtained by training models on large-scale bug-fix code pairs. Moreover, in most of FL and APR methods, the model designs and training phases are performed separately, leading to ineffective sharing of updated parameters and extracted knowledge during the training process. This limitation hinders the further improvement in the performance of FL and APR tasks. To solve the above problems, we propose a novel approach called MTL-TRANSFER, which leverages a multi-task learning strategy to extract deep semantic features and transferred knowledge from different perspectives. First, we construct a large-scale open-source bug datasets and implement 11 multi-task learning models for bug detection and patch generation sub-tasks on 11 commonly used bug types, as well as one multi-classifier to learn the relevant semantics for the subsequent fix template selection task. Second, an MLP-based ranking model is leveraged to fuse spectrum-based, mutation-based and semantic-based features to generate a sorted list of suspicious statements. Third, we combine the patches generated by the neural patch generation sub-task from the multi-task learning strategy with the optimized fix template selecting order gained from the multi-classifier mentioned above. Finally, the more accurate FL results, the optimized fix template selecting order, and the expanded patch candidates are combined together to further enhance the overall performance of APR tasks. Our extensive experiments on widely-used benchmark Defects4J show that MTL-TRANSFER outperforms all baselines in FL and APR tasks, proving the effectiveness of our approach. Compared with our previously proposed FL method TRANSFER-FL (which is also the state-of-the-art statement-level FL method), MTL-TRANSFER increases the faults hit by 8/11/12 on Top-1/3/5 metrics (92/159/183 in total). And on APR tasks, the number of successfully repaired bugs of MTL-TRANSFER under the perfect localization setting reaches 75, which is 8 more than our previous APR method TRANSFER-PR. Furthermore, another experiment to simulate the actual repair scenarios shows that MTL-TRANSFER can successfully repair 15 and 9 more bugs (56 in total) compared with TBar and TRANSFER, which demonstrates the effectiveness of the combination of our optimized FL and APR components.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = jun,
articleno = {148},
numpages = {31},
keywords = {Fault localization, automated program repair, deep neural networks, transfer learning, multi-task learning, neural machine translation}
}

@inproceedings{10.1007/978-3-031-72344-5_27,
author = {Xu, Zhenyu and Sheng, Victor S.},
title = {Towards Minimal Edits in&nbsp;Automated Program Repair: A Hybrid Framework Integrating Graph Neural Networks and&nbsp;Large Language Models},
year = {2024},
isbn = {978-3-031-72343-8},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-72344-5_27},
doi = {10.1007/978-3-031-72344-5_27},
abstract = {Large Language Models trained on Code (LLMCs) have&nbsp;been shown to be effective in Automated Program Repair (APR) tasks, introducing new innovations to the field. Typically, LLMCs do&nbsp;not engage in error localization for APR tasks, instead treating&nbsp;APR more as a code refinement task. This approach often results&nbsp;in larger edit distances, altering the program’s original structure. The principle of making minimal edits is crucial in certain scenarios, such as when correcting student programming assignments or software group development, where it’s better to preserve&nbsp;the original intent of the code with as few changes as possible.&nbsp;To address these challenges, we introduce a hybrid framework&nbsp;for automated program repair that combines graph neural networks&nbsp;and large language models, which we refer to as HFRepair. HFRepair leverages the precise error localization capability of DrRepair&nbsp;for C programs, combining it with LLMCs to perform the line-level&nbsp;APR task based on the code context, aiming for minimal edits.&nbsp;Our experimental results demonstrate that HFRepair significantly outperforms previous state-of-the-art methods in benchmark tests. For instance, on the DeepFix dataset, HFRepair improves the&nbsp;full repair rate from 67.9\% and 71.4\% (achieved by DrRepair and BIFI, respectively) to 82.2\%, while reducing average edit distance&nbsp;from 33.4 and 27.7 to 11.6.},
booktitle = {Artificial Neural Networks and Machine Learning – ICANN 2024: 33rd International Conference on Artificial Neural Networks, Lugano, Switzerland, September 17–20, 2024, Proceedings, Part V},
pages = {402–416},
numpages = {15},
keywords = {Automated Program Repair, Graph Neural Networks, Large Language Models},
location = {Lugano, Switzerland}
}

@inproceedings{10.1109/ASE56229.2023.00181,
author = {Huang, Kai and Meng, Xiangxin and Zhang, Jian and Liu, Yang and Wang, Wenjie and Li, Shuhao and Zhang, Yuqing},
title = {An Empirical Study on Fine-Tuning Large Language Models of Code for Automated Program Repair},
year = {2024},
isbn = {9798350329964},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASE56229.2023.00181},
doi = {10.1109/ASE56229.2023.00181},
abstract = {The advent of large language models (LLMs) has opened up new opportunities for automated program repair (APR). In particular, some recent studies have explored how to leverage large language models of code (LLMCs) for program repair tasks and show promising results. However, most of them adopt the zero/few-shot learning paradigm for APR, which directly use LLMCs to generate the possibly correct code given its surrounding context. Though effective, the repair capabilities of LLMCs based on the fine-tuning paradigm have yet to be extensively explored. Also, it remains unknown whether LLMCs have the potential to repair more complicated bugs (e.g., multi-hunk bugs). To fill the gap, in this work, we conduct a comprehensive study on the program repair capability of LLMCs in the fine-tuning paradigm. We select 5 popular LLMCs with representative pre-training architectures, including CodeBERT, GraphCodeBERT, PLBART, CodeT5, and UniXcoder. We consider 3 typical program repair scenarios (i.e., bugs, vulnerabilities, and errors) involving 3 programming languages (i.e., Java, C/C++, and JavaScript). Notably, we take both single-hunk and multi-hunk bugs/vulnerabilities into account. We then fine-tune them on widely-used datasets and compare them with existing state-of-the-art APR tools. We also investigate the impact of different design choices, which include code abstractions, code representations, and model evaluation metrics. Our experimental results show that LLMCs in the fine-tuning paradigm can significantly outperform previous state-of-the-art APR tools. Through in-depth analysis, we provide insights into choosing appropriate strategies to guide LLMCs for better performance. Lastly, we reveal several limitations of LLMCs for APR and make suggestions for future research on LLMC-based APR.},
booktitle = {Proceedings of the 38th IEEE/ACM International Conference on Automated Software Engineering},
pages = {1162–1174},
numpages = {13},
keywords = {automated program repair, large language models of code, neural machine translation, fine-tuning},
location = {Echternach, Luxembourg},
series = {ASE '23}
}

@inbook{10.5555/3766078.3766304,
author = {Yu, Zheng and Guo, Ziyi and Wu, Yuhang and Yu, Jiahao and Xu, Meng and Mu, Dongliang and Chen, Yan and Xing, Xinyu},
title = {PATCHAGENT: a practical program repair agent mimicking human expertise},
year = {2025},
isbn = {978-1-939133-52-6},
publisher = {USENIX Association},
address = {USA},
abstract = {Automated program repair (APR) techniques, which aim to triage and fix software bugs autonomously, have emerged as powerful tools against vulnerable code. Recent advancements in large language models (LLMs) have further shown promising results when applied to APR, especially on patch generation. However, without effective fault localization and patch validation, APR tools specialized in patching alone cannot handle a more practical and end-to-end setting—given a concrete input that triggers a vulnerability, how to patch the program without breaking existing tests?In this paper, we introduce PATCHAGENT, a novel LLM-based APR tool that seamlessly integrates fault localization, patch generation, and validation within a single autonomous agent. PATCHAGENT employs a language server, a patch verifier, and interaction optimization techniques to mimic human-like reasoning during vulnerability repair. Evaluated on a dataset of 178 real-world vulnerabilities, PATCHAGENT successfully repairs over 90\% of the cases, outperforming state-of-the-art APR tools where applicable. Our ablation study further offer insights into the how various interaction optimizations contribute to PATCHAGENT'S effectiveness.},
booktitle = {Proceedings of the 34th USENIX Conference on Security Symposium},
articleno = {226},
numpages = {20}
}

@inproceedings{10.1145/3540250.3549101,
author = {Xia, Chunqiu Steven and Zhang, Lingming},
title = {Less training, more repairing please: revisiting automated program repair via zero-shot learning},
year = {2022},
isbn = {9781450394130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3540250.3549101},
doi = {10.1145/3540250.3549101},
abstract = {Due to the promising future of Automated Program Repair (APR), researchers have proposed various APR techniques, including heuristic-based, template-based, and constraint-based techniques. Among such classic APR techniques, template-based techniques have been widely recognized as state of the art. However, such template-based techniques require predefined templates to perform repair, and their effectiveness is thus limited. To this end, researchers have leveraged the recent advances in Deep Learning to further improve APR. Such learning-based techniques typically view APR as a Neural Machine Translation problem, using the buggy/fixed code snippets as the source/target languages for translation. In this way, such techniques heavily rely on large numbers of high-quality bug-fixing commits, which can be extremely costly/challenging to construct and may limit their edit variety and context representation. In this paper, we aim to revisit the learning-based APR problem, and propose AlphaRepair, the first cloze-style (or infilling-style) APR approach to directly leveraging large pre-trained code models for APR without any fine-tuning/retraining on historical bug fixes. Our main insight is instead of modeling what a repair edit should look like (i.e., a NMT task), we can directly predict what the correct code is based on the context information (i.e., a cloze or text infilling task). Although our approach is general and can be built on various pre-trained code models, we have implemented AlphaRepair as a practical multilingual APR tool based on the recent CodeBERT model. Our evaluation of AlphaRepair on the widely used Defects4J benchmark shows for the first time that learning-based APR without any history bug fixes can already outperform state-of-the-art APR techniques. We also studied the impact of different design choices and show that AlphaRepair performs even better on a newer version of Defects4J (2.0) with 3.3X more fixes than best performing baseline, indicating that AlphaRepair can potentially avoid the dataset-overfitting issue of existing techniques. Additionally, we demonstrate the multilingual repair ability of AlphaRepair by evaluating on the QuixBugs dataset where AlphaRepair achieved the state-of-the-art results on both Java and Python versions.},
booktitle = {Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {959–971},
numpages = {13},
keywords = {Zero-shot Learning, Deep Learning, Automated Program Repair},
location = {Singapore, Singapore},
series = {ESEC/FSE 2022}
}

@inproceedings{10.1145/3293882.3330559,
author = {Ghanbari, Ali and Benton, Samuel and Zhang, Lingming},
title = {Practical program repair via bytecode mutation},
year = {2019},
isbn = {9781450362245},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3293882.3330559},
doi = {10.1145/3293882.3330559},
abstract = {Automated Program Repair (APR) is one of the most recent advances in automated debugging, and can directly fix buggy programs with minimal human intervention. Although various advanced APR techniques (including search-based or semantic-based ones) have been proposed, they mainly work at the source-code level and it is not clear how bytecode-level APR performs in practice. Also, empirical studies of the existing techniques on bugs beyond what has been reported in the original papers are rather limited. In this paper, we implement the first practical bytecode-level APR technique, PraPR, and present the first extensive study on fixing real-world bugs (e.g., Defects4J bugs) using JVM bytecode mutation. The experimental results show that surprisingly even PraPR with only the basic traditional mutators can produce genuine fixes for 17 bugs; with simple additional commonly used APR mutators, PraPR is able to produce genuine fixes for 43 bugs, significantly outperforming state-of-the-art APR, while being over 10X faster. Furthermore, we performed an extensive study of PraPR and other recent APR tools on a large number of additional real-world bugs, and demonstrated the overfitting problem of recent advanced APR tools for the first time. Lastly, PraPR has also successfully fixed bugs for other JVM languages (e.g., for the popular Kotlin language), indicating PraPR can greatly complement existing source-code-level APR.},
booktitle = {Proceedings of the 28th ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {19–30},
numpages = {12},
keywords = {Program repair, Mutation testing, JVM bytecode, Fault localization},
location = {Beijing, China},
series = {ISSTA 2019}
}

@inproceedings{10.1007/978-3-030-78292-4_1,
author = {Abhinav, Kumar and Sharvani, Vijaya and Dubey, Alpana and D’Souza, Meenakshi and Bhardwaj, Nitish and Jain, Sakshi and Arora, Veenu},
title = {RepairNet: Contextual Sequence-to-Sequence Network for Automated Program Repair},
year = {2021},
isbn = {978-3-030-78291-7},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-78292-4_1},
doi = {10.1007/978-3-030-78292-4_1},
abstract = {Compile-time errors can wreak havoc for programmers – seasoned and novice. Often developers spend a lot of time debugging them. An automated system to repair such errors can be a useful aid to the developers for their productivity. In this work, we propose a deep generative model, RepairNet, that automatically repairs programs that fail at compile time. RepairNet is based on sequence-to-sequence modeling and uses both code and error messages to repair the program. We evaluated the effectiveness of our system on 6,971 erroneous submissions for 93 programming tasks. RepairNet outperforms the existing state-of-the-art technique, MACER, with 17\% relative improvement of repair accuracy. Our approach can fix 66.4\% of the erroneous submissions completely and 14.2\% partially.},
booktitle = {Artificial Intelligence in Education: 22nd International Conference, AIED 2021, Utrecht, The Netherlands, June 14–18, 2021, Proceedings, Part I},
pages = {3–15},
numpages = {13},
keywords = {Bug fixing, Sequence modeling, Program repair},
location = {Utrecht, The Netherlands}
}

@article{10.1007/s10664-019-09770-1,
author = {Kim, Jindae and Kim, Jeongho and Lee, Eunseok and Kim, Sunghun},
title = {The effectiveness of context-based change application on automatic program repair},
year = {2020},
issue_date = {Jan 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {25},
number = {1},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-019-09770-1},
doi = {10.1007/s10664-019-09770-1},
abstract = {An Automatic Program Repair (APR) technique is an implementation of a repair model to fix a given bug by modifying program behavior. Recently, repair models which collect source code and code changes from software history and use such collected resources for patch generation became more popular. Collected resources are used to expand the patch search space and to increase the probability that correct patches for bugs are included in the space. However, it is also revealed that navigation on such expanded patch search space is difficult due to the sparseness of correct patches in the space. In this study, we evaluate the effectiveness of Context-based Change Application (CCA) technique on change selection, fix location selection and change concretization, which are the key aspects of navigating patch search space. CCA collects abstract subtree changes and their AST contexts, and applies them to fix locations only if their contexts are matched. CCA repair model can address both search space expansion and navigation issues, by expanding search space with collected changes while narrowing down search areas in the search space based on contexts. Since CCA applies changes to a fix location only if their contexts are matched, it only needs to consider the same context changes for each fix location. Also, if there is no change with the same context as a fix location, this fix location can be ignored since it means that past patches did not modify such locations. In addition, CCA uses fine-grained changes preserving changed code structures, but normalizing user-defined names. Hence change concretization can be simply done by replacing normalized names with concrete names available in buggy code. We evaluated CCA’s effectiveness with over 54K unique collected changes (221K in total) from about 5K human-written patches. Results show that using contexts, CCA correctly found 90.1\% of the changes required for test set patches, while fewer than 5\% of the changes were found without contexts. We discovered that collecting more changes is only helpful if it is supported by contexts for effective search space navigation. In addition, CCA repair model found 44-70\% of the actual fix locations of Defects4j patches more quickly compared to using SBFL techniques only. We also found that about 48\% of the patches can be fully concretized using concrete names from buggy code.},
journal = {Empirical Softw. Engg.},
month = jan,
pages = {719–754},
numpages = {36},
keywords = {Repair models, Context-based change application, Automatic program repair}
}

@inproceedings{10.1145/3533767.3534368,
author = {Ghanbari, Ali and Marcus, Andrian},
title = {Patch correctness assessment in automated program repair based on the impact of patches on production and test code},
year = {2022},
isbn = {9781450393799},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3533767.3534368},
doi = {10.1145/3533767.3534368},
abstract = {Test-based generate-and-validate automated program repair (APR) systems often generate many patches that pass the test suite without fixing the bug.   The generated patches must be manually inspected by the developers, so previous research proposed various techniques for automatic correctness assessment of APR-generated patches.   Among them, dynamic patch correctness assessment techniques rely on the assumption that, when running the originally passing test cases, the correct patches will not alter the program behavior in a significant way, e.g., removing the code implementing correct functionality of the program.   In this paper, we propose and evaluate a novel technique, named Shibboleth, for automatic correctness assessment of the patches generated by test-based generate-and-validate APR systems.   Unlike existing works, the impact of the patches is captured along three complementary facets, allowing more effective patch correctness assessment.   Specifically, we measure the impact of patches on both production code (via syntactic and semantic similarity) and test code (via code coverage of passing tests) to separate the patches that result in similar programs and that do not delete desired program elements.   Shibboleth assesses the correctness of patches via both ranking and classification.  We evaluated Shibboleth on 1,871 patches, generated by 29 Java-based APR systems for Defects4J programs. The technique outperforms state-of-the-art ranking and classification techniques.   Specifically, in our ranking data set, in 43\% (66\%) of the cases, Shibboleth ranks the correct patch in top-1 (top-2) positions, and in classification mode applied on our classification data set, it achieves an accuracy and F1-score of 0.887 and 0.852, respectively.},
booktitle = {Proceedings of the 31st ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {654–665},
numpages = {12},
keywords = {Similarity, Patch Correctness Assessment, Branch Coverage, Automated Program Repair},
location = {Virtual, South Korea},
series = {ISSTA 2022}
}

@inproceedings{10.1145/3368089.3417929,
author = {Ghanbari, Ali and Marcus, Andrian},
title = {PRF: a framework for building automatic program repair prototypes for JVM-based languages},
year = {2020},
isbn = {9781450370431},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3368089.3417929},
doi = {10.1145/3368089.3417929},
abstract = {PRF is a Java-based framework that allows researchers to build prototypes of test-based generate-and-validate automatic program repair techniques for JVM languages by simply extending it with their patch generation plugins. The framework also provides other useful components for constructing automatic program repair tools, e.g., a fault localization component that provides spectrum-based fault localization information at different levels of granularity, a configurable and safe patch validation component that is 11+X faster than vanilla testing, and a customizable post-processing component to generate fix reports. A demo video of PRF is available at https://bit.ly/3ehduSS.},
booktitle = {Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {1626–1629},
numpages = {4},
keywords = {Patch Validation, Framework, Fault Localization, Automatic Program Repair},
location = {Virtual Event, USA},
series = {ESEC/FSE 2020}
}

@inproceedings{10.1609/aaai.v39i1.31988,
author = {Dai, Zhenlong and Chen, Bingrui and Zhao, Zhuoluo and Tang, Xiu and Wu, Sai and Yao, Chang and Gao, Zhipeng and Chen, Jingyuan},
title = {Less is more: adaptive program repair with bug localization and preference learning},
year = {2025},
isbn = {978-1-57735-897-8},
publisher = {AAAI Press},
url = {https://doi.org/10.1609/aaai.v39i1.31988},
doi = {10.1609/aaai.v39i1.31988},
abstract = {Automated Program Repair (APR) is a task to automatically generate patches for the buggy code. However, most research focuses on generating correct patches while ignoring the consistency between the fixed code and the original buggy code. How to conduct adaptive bug fixing and generate patches with minimal modifications have seldom been investigated. To bridge this gap, we first introduce a novel task, namely AdaPR (Adaptive Program Repair). We then propose a two-stage approach AdaPatcher (Adaptive Patch Generator) to enhance program repair while maintaining the consistency. In the first stage, we utilize a Bug Locator with self-debug learning to accurately pinpoint bug locations. In the second stage, we train a Program Modifier to ensure consistency between the post-modified fixed code and the pre-modified buggy code. The Program Modifier is enhanced with a location-aware repair learning strategy to generate patches based on identified buggy lines, a hybrid training strategy for selective reference and an adaptive preference learning to prioritize fewer changes. The experimental results show that our approach outperforms a set of baselines by a large margin, validating the effectiveness of our two-stage framework for the newly proposed AdaPR task. Code — https://github.com/zhenlongDai/AdaPatcher},
booktitle = {Proceedings of the Thirty-Ninth AAAI Conference on Artificial Intelligence and Thirty-Seventh Conference on Innovative Applications of Artificial Intelligence and Fifteenth Symposium on Educational Advances in Artificial Intelligence},
articleno = {15},
numpages = {9},
series = {AAAI'25/IAAI'25/EAAI'25}
}

@inproceedings{10.1145/3183519.3183540,
author = {Urli, Simon and Yu, Zhongxing and Seinturier, Lionel and Monperrus, Martin},
title = {How to design a program repair bot? insights from the repairnator project},
year = {2018},
isbn = {9781450356596},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3183519.3183540},
doi = {10.1145/3183519.3183540},
abstract = {Program repair research has made tremendous progress over the last few years, and software development bots are now being invented to help developers gain productivity. In this paper, we investigate the concept of a "program repair bot" and present Repairnator. The Repairnator bot is an autonomous agent that constantly monitors test failures, reproduces bugs, and runs program repair tools against each reproduced bug. If a patch is found, Repairnator bot reports it to the developers. At the time of writing, Repairnator uses three different program repair systems and has been operating since February 2017. In total, it has studied 11 523 test failures over 1 609 open-source software projects hosted on GitHub, and has generated patches for 15 different bugs. Over months, we hit a number of hard technical challenges and had to make various design and engineering decisions. This gives us a unique experience in this area. In this paper, we reflect upon Repairnator in order to share this knowledge with the automatic program repair community.},
booktitle = {Proceedings of the 40th International Conference on Software Engineering: Software Engineering in Practice},
pages = {95–104},
numpages = {10},
location = {Gothenburg, Sweden},
series = {ICSE-SEIP '18}
}

@article{10.1016/j.jss.2024.112083,
author = {Gharibi, Reza and Sadreddini, Mohammad Hadi and Fakhrahmad, Seyed Mostafa},
title = {T5APR: Empowering automated program repair across languages through checkpoint ensemble},
year = {2024},
issue_date = {Aug 2024},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {214},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2024.112083},
doi = {10.1016/j.jss.2024.112083},
journal = {J. Syst. Softw.},
month = aug,
numpages = {19},
keywords = {Automated program repair, Neural program repair, Deep learning, Transformer}
}

@inproceedings{10.1609/aaai.v37i4.25642,
author = {Joshi, Harshit and Sanchez, Jos\'{e} Cambronero and Gulwani, Sumit and Le, Vu and Radi\v{c}ek, Ivan and Verbruggen, Gust},
title = {Repair is nearly generation: multilingual program repair with LLMs},
year = {2023},
isbn = {978-1-57735-880-0},
publisher = {AAAI Press},
url = {https://doi.org/10.1609/aaai.v37i4.25642},
doi = {10.1609/aaai.v37i4.25642},
abstract = {Most programmers make mistakes when writing code. Some of these mistakes are small and require few edits to the original program - a class of errors recently termed last mile mistakes. These errors break the flow for experienced developers and can stump novice programmers. Existing automated repair techniques targeting this class of errors are language-specific and do not easily carry over to new languages. Transferring symbolic approaches requires substantial engineering and neural approaches require data and retraining. We introduce RING, a multilingual repair engine powered by a large language model trained on code (LLMC) such as Codex. Such a multilingual engine enables a flipped model for programming assistance, one where the programmer writes code and the AI assistance suggests fixes, compared to traditional code suggestion technology. Taking inspiration from the way programmers manually fix bugs, we show that a prompt-based strategy that conceptualizes repair as localization, transformation, and candidate ranking, can successfully repair programs in multiple languages with minimal effort. We present the first results for such a multilingual repair engine by evaluating on 6 different languages and comparing performance to language-specific repair engines. We show that RING can outperform language- specific repair engines for three of these languages.},
booktitle = {Proceedings of the Thirty-Seventh AAAI Conference on Artificial Intelligence and Thirty-Fifth Conference on Innovative Applications of Artificial Intelligence and Thirteenth Symposium on Educational Advances in Artificial Intelligence},
articleno = {573},
numpages = {10},
series = {AAAI'23/IAAI'23/EAAI'23}
}

@article{10.1007/s10664-017-9562-9,
author = {Oliveira, Vinicius Paulo and Souza, Eduardo Faria and Le Goues, Claire and Camilo-Junior, Celso G.},
title = {Improved representation and genetic operators for linear genetic programming for automated program repair},
year = {2018},
issue_date = {October   2018},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {23},
number = {5},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-017-9562-9},
doi = {10.1007/s10664-017-9562-9},
abstract = {Genetic improvement for program repair can fix bugs or otherwise improve software via patch evolution. Consider GenProg, a prototypical technique of this type. GenProg's crossover and mutation operators manipulate individuals represented as patches. A patch is composed of high-granularity edits that indivisibly comprise an edit operation, a faulty location, and a fix statement used in replacement or insertions. We observe that recombination and mutation of such high-level units limits the technique's ability to effectively traverse and recombine the repair search spaces. We propose a reformulation of program repair representation, crossover, and mutation operators such that they explicitly traverse the three subspaces that underlie the search problem: the Operator, Fault and Fix Spaces. We provide experimental evidence validating our insight, showing that the operators provide considerable improvement over the baseline repair algorithm in terms of search success rate and efficiency. We also conduct a genotypic distance analysis over the various types of search, providing insight as to the influence of the operators on the program repair search problem.},
journal = {Empirical Softw. Engg.},
month = oct,
pages = {2980–3006},
numpages = {27},
keywords = {Automated program repair, Automatic software repair, Crossover operator, Genetic improvement, Genetic programming, Mutation operator}
}

@inproceedings{10.1145/3510003.3510177,
author = {Li, Yi and Wang, Shaohua and Nguyen, Tien N.},
title = {DEAR: a novel deep learning-based approach for automated program repair},
year = {2022},
isbn = {9781450392211},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3510003.3510177},
doi = {10.1145/3510003.3510177},
abstract = {The existing deep learning (DL)-based automated program repair (APR) models are limited in fixing general software defects. We present DEAR, a DL-based approach that supports fixing for the general bugs that require dependent changes at once to one or multiple consecutive statements in one or multiple hunks of code. We first design a novel fault localization (FL) technique for multi-hunk, multi-statement fixes that combines traditional spectrum-based (SB) FL with deep learning and data-flow analysis. It takes the buggy statements returned by the SBFL model, detects the buggy hunks to be fixed at once, and expands a buggy statement s in a hunk to include other suspicious statements around s. We design a two-tier, tree-based LSTM model that incorporates cycle training and uses a divide-and-conquer strategy to learn proper code transformations for fixing multiple statements in the suitable fixing context consisting of surrounding subtrees. We conducted several experiments to evaluate DEAR on three datasets: Defects4J (395 bugs), BigFix (+26k bugs), and CPatMiner (+44k bugs). On Defects4J dataset, DEAR outperforms the baselines from 42\%--683\% in terms of the number of auto-fixed bugs with only the top-1 patches. On BigFix dataset, it fixes 31--145 more bugs than existing DL-based APR models with the top-1 patches. On CPatMiner dataset, among 667 fixed bugs, there are 169 (25.3\%) multi-hunk/multi-statement bugs. DEAR fixes 71 and 164 more bugs, including 52 and 61 more multi-hunk/multi-statement bugs, than the state-of-the-art, DL-based APR models.},
booktitle = {Proceedings of the 44th International Conference on Software Engineering},
pages = {511–523},
numpages = {13},
keywords = {automated program repair, deep learning, fault localization},
location = {Pittsburgh, Pennsylvania},
series = {ICSE '22}
}

@inproceedings{10.1109/ICSE43902.2021.00107,
author = {Jiang, Nan and Lutellier, Thibaud and Tan, Lin},
title = {CURE: Code-Aware Neural Machine Translation for Automatic Program Repair},
year = {2021},
isbn = {9781450390859},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE43902.2021.00107},
doi = {10.1109/ICSE43902.2021.00107},
abstract = {Automatic program repair (APR) is crucial to improve software reliability. Recently, neural machine translation (NMT) techniques have been used to fix software bugs automatically. While promising, these approaches have two major limitations. Their search space often does not contain the correct fix, and their search strategy ignores software knowledge such as strict code syntax. Due to these limitations, existing NMT-based techniques underperform the best template-based approaches.We propose CURE, a new NMT-based APR technique with three major novelties. First, CURE pre-trains a programming language (PL) model on a large software codebase to learn developer-like source code before the APR task. Second, CURE designs a new code-aware search strategy that finds more correct fixes by focusing on compilable patches and patches that are close in length to the buggy code. Finally, CURE uses a subword tokenization technique to generate a smaller search space that contains more correct fixes.Our evaluation on two widely-used benchmarks shows that CURE correctly fixes 57 Defects4J bugs and 26 QuixBugs bugs, outperforming all existing APR techniques on both benchmarks.},
booktitle = {Proceedings of the 43rd International Conference on Software Engineering},
pages = {1161–1173},
numpages = {13},
keywords = {software reliability, automatic program repair},
location = {Madrid, Spain},
series = {ICSE '21}
}

@inproceedings{10.1145/3643788.3648012,
author = {Santos, Sofia and Saraiva, Jo\~{a}o and Ribeiro, Francisco},
title = {Large Language Models in Automated Repair of Haskell Type Errors},
year = {2024},
isbn = {9798400705779},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643788.3648012},
doi = {10.1145/3643788.3648012},
abstract = {This paper introduces a new method of Automated Program Repair that relies on a combination of the GPT-4 Large Language Model and automatic type checking of Haskell programs. This method identifies the source of a type error and asks GPT-4 to fix that specific portion of the program. Then, QuickCheck is used to automatically generate a large set of test cases to validate whether the generated repair behaves as the correct solution. Our publicly available experiments revealed a success rate of 88.5\% in normal conditions. However, more detailed testing should be performed to more accurately evaluate this form of APR.},
booktitle = {Proceedings of the 5th ACM/IEEE International Workshop on Automated Program Repair},
pages = {42–45},
numpages = {4},
keywords = {automated program repair, large language model, fault localization, code generation, type checking, automatic testing},
location = {Lisbon, Portugal},
series = {APR '24}
}

@inproceedings{10.1145/3524459.3527351,
author = {Prenner, Julian Aron and Babii, Hlib and Robbes, Romain},
title = {Can OpenAI's codex fix bugs? an evaluation on QuixBugs},
year = {2022},
isbn = {9781450392853},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3524459.3527351},
doi = {10.1145/3524459.3527351},
abstract = {OpenAI's Codex, a GPT-3 like model trained on a large code corpus, has made headlines in and outside of academia. Given a short user-provided description, it is capable of synthesizing code snippets that are syntactically and semantically valid in most cases. In this work, we want to investigate whether Codex is able to localize and fix bugs, two important tasks in automated program repair. Our initial evaluation uses the multi-language QuixBugs benchmark (40 bugs in both Python and Java). We find that, despite not being trained for APR, Codex is surprisingly effective, and competitive with recent state of the art techniques. Our results also show that Codex is more successful at repairing Python than Java, fixing 50\% more bugs in Python.},
booktitle = {Proceedings of the Third International Workshop on Automated Program Repair},
pages = {69–75},
numpages = {7},
keywords = {deep learning, codex, automatic program repair, QuixBugs},
location = {Pittsburgh, Pennsylvania},
series = {APR '22}
}

@inproceedings{10.1145/3643788.3648014,
author = {Diaz-De-Arcaya, Josu and L\'{o}pez-De-Armentia, Juan and Z\'{a}rate, Gorka and Torre-Bastida, Ana I.},
title = {Towards the self-healing of Infrastructure as Code projects using constrained LLM technologies},
year = {2024},
isbn = {9798400705779},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643788.3648014},
doi = {10.1145/3643788.3648014},
abstract = {The generalization of the use of cloud computing and edge computing solutions in industry requires innovative techniques to keep up with the complexity of these scenarios. In particular, the large heterogeneity of the infrastructural devices and the myriad of services offered by the various private and cloud providers represent a challenge. Infrastructure as Code (IaC) technologies have been adopted to reduce the complexity of these scenarios, but even IaC technologies have their drawbacks, as the errors resulting from their use often combine the complexities of the underlying layers and require a high level of expertise. In this regard, the recent upsurge of Large Language Models represents an opportunity as they are able to tackle different problems. In this article, we aspire to shed light on the automated patching of IaC projects with the help of LLMs. We evaluate the suitability of this hypothesis by using a well-known LLM that is able to solve all the scenarios we envisioned and assess the possibility of doing the same with smaller, offline LLMs, which could lead to the use of these technologies in resource-constrained environments, such as edge computing.},
booktitle = {Proceedings of the 5th ACM/IEEE International Workshop on Automated Program Repair},
pages = {22–25},
numpages = {4},
keywords = {infrastructure as code, IaC, large language models, LLMs, self-healing, automated patching},
location = {Lisbon, Portugal},
series = {APR '24}
}

@inproceedings{10.1145/3643788.3648015,
author = {Shariffdeen, Ridwan and Noller, Yannic and Mirchev, Martin and Ruan, Haifeng and Xiang, Gao and Costa, Andreea and Duck, Gregory J and Roychoudhury, Abhik},
title = {APR Competition 2024},
year = {2024},
isbn = {9798400705779},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643788.3648015},
doi = {10.1145/3643788.3648015},
abstract = {This report outlines the objectives, methodology, challenges, and results of the first Automated Program Repair Competition held at the APR Workshop 2024. The competition utilized Cerberus, a program repair framework, to evaluate the program repair tools using different repair configurations for each track in the competition. The competition was organized in three phases: first the participants integrated their tools with Cerberus, second the integrated tools were tested using public benchmarks and participants were able to fix any identified issues. In the last phase, the submitted tools and baseline comparison tools were evaluated against private benchmark programs.},
booktitle = {Proceedings of the 5th ACM/IEEE International Workshop on Automated Program Repair},
pages = {46–49},
numpages = {4},
location = {Lisbon, Portugal},
series = {APR '24}
}

@inproceedings{10.1145/3643788.3648018,
author = {Mechtaev, Sergey and Tan, Shin Hwei},
title = {F1X at APR-COMP 2024},
year = {2024},
isbn = {9798400705779},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643788.3648018},
doi = {10.1145/3643788.3648018},
abstract = {Automated program repair aims to generate patches for buggy programs, a task often hindered by the cost of test executions in large projects. F1X introduces a novel methodology relying on test-equivalence relations, defining if two programs yield indistinguishable results for a specific test. By leveraging two test-equivalence relations based on runtime values and dependencies, F1X' algorithm categorises patches into test-equivalence classes, which helps to significantly reduce the number of required test execution to generate a patch without any information loss. Experiments on real-world programs from the ManyBugs benchmark demonstrated a substantial reduction in test executions, leading to efficiency gains over the previous methods, while retaining the patch quality. The efficiency and effectiveness of F1X was further shown in APR-COMP 2024, where it received the highest score in the Functional-C track.},
booktitle = {Proceedings of the 5th ACM/IEEE International Workshop on Automated Program Repair},
pages = {56–57},
numpages = {2},
location = {Lisbon, Portugal},
series = {APR '24}
}

@inbook{10.1145/3676536.3676801,
author = {Xu, Ke and Sun, Jialin and Hu, Yuchen and Fang, Xinwei and Shan, Weiwei and Wang, Xi and Jiang, Zhe},
title = {MEIC: Re-thinking RTL Debug Automation using LLMs},
year = {2025},
isbn = {9798400710773},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3676536.3676801},
abstract = {The deployment of Large Language Models (LLMs) for code debugging (e.g., C and Python) is widespread, benefiting from their ability to understand and interpret intricate concepts. However, in the semiconductor industry, utilising LLMs to debug Register Transfer Level (RTL) code is still insufficient, largely due to the underrepre-sentation of RTL-specific data in training sets. This work introduces a novel framework, Make Each Iteration Count (MEIC), which contrasts with traditional one-shot LLM-based debugging methods that heavily rely on prompt engineering, model tuning, and model training. MEIC utilises LLMs in an iterative process to overcome the limitation of LLMs in RTL code debugging, which is suitable for identifying and correcting both syntax and function errors, while effectively managing the uncertainties inherent in LLM operations. To evaluate our framework, we provide an open-source dataset comprising 178 common RTL programming errors. The experimental results demonstrate that the proposed debugging framework achieves fix rate of 93\% for syntax errors and 78\% for function errors, with up to 48x speedup in debugging processes when compared with experienced engineers. The Repo. of dataset and code: https://github.com/SEU-ACAL/reproduce-MEIC-ICCAD.},
booktitle = {Proceedings of the 43rd IEEE/ACM International Conference on Computer-Aided Design},
articleno = {100},
numpages = {9}
}

@inproceedings{10.1007/978-3-031-98414-3_1,
author = {Phung, Tung and Choi, Heeryung and Wu, Mengyan and Singla, Adish and Brooks, Christopher},
title = {Plan More, Debug Less: Applying Metacognitive Theory to&nbsp;AI-Assisted Programming Education},
year = {2025},
isbn = {978-3-031-98413-6},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-98414-3_1},
doi = {10.1007/978-3-031-98414-3_1},
abstract = {The growing adoption of generative AI in education highlights the need to integrate established pedagogical principles into AI-assisted learning environments. This study investigates the potential of metacognitive theory to inform AI-assisted programming education through a hint system designed around the metacognitive phases of planning, monitoring, and evaluation. Upon request, the system can provide three types of AI-generated hints–planning, debugging, and optimization–to guide students at different stages of problem-solving. Through a study with 102 students in an introductory data science programming course, we find that students perceive and engage with planning hints most highly, whereas optimization hints are rarely requested. We observe a consistent association between requesting planning hints and achieving higher grades across question difficulty and student competency. However, when facing harder tasks, students seek additional debugging but not more planning support. These insights contribute to the growing field of AI-assisted programming education by providing empirical evidence on the importance of pedagogical principles in AI-assisted learning.},
booktitle = {Artificial Intelligence in Education: 26th International Conference, AIED 2025, Palermo, Italy, July 22–26, 2025, Proceedings, Part I},
pages = {3–17},
numpages = {15},
keywords = {Programming Education, Feedback Generation, Metacognitive Theory, Generative AI},
location = {Palermo, Italy}
}

@inproceedings{10.5555/3737916.3739036,
author = {Jiang, Nan and Li, Xiaopeng and Wang, Shiqi and Zhou, Qiang and Hossain, Soneya Binta and Ray, Baishakhi and Kumar, Varun and Ma, Xiaofei and Deoras, Anoop},
title = {LEDEX: training LLMs to better self-debug and explain code},
year = {2024},
isbn = {9798331314385},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {In the domain of code generation, self-debugging is crucial. It allows LLMs to refine their generated code based on execution feedback. This is particularly important because generating correct solutions in one attempt proves challenging for complex tasks. Prior works on self-debugging mostly focus on prompting methods by providing LLMs with few-shot examples, which work poorly on small open-sourced LLMs. In this work, we propose LEDEX, a training framework that significantly improves the self-debugging capability of LLMs. Intuitively, we observe that a chain of explanations on the wrong code followed by code refinement helps LLMs better analyze the wrong code and do refinement. We thus propose an automated pipeline to collect a high-quality dataset for code explanation and refinement by generating a number of explanations and refinement trajectories from the LLM itself or a larger teacher model and filtering via execution verification. We perform supervised fine-tuning (SFT) and further reinforcement learning (RL) on both success and failure trajectories with a novel reward design considering code explanation and refinement quality. SFT improves the pass@1 by up to 15.92\% and pass@10 by 9.30\% over four benchmarks. RL training brings additional up to 3.54\% improvement on pass@1 and 2.55\% improvement on pass@10. The trained LLMs show iterative refinement ability and can keep refining code continuously. Lastly, our human evaluation shows that the LLMs trained with our framework generate more useful code explanations and help developers better understand bugs in source code.},
booktitle = {Proceedings of the 38th International Conference on Neural Information Processing Systems},
articleno = {1120},
numpages = {27},
location = {Vancouver, BC, Canada},
series = {NIPS '24}
}

@inproceedings{10.5555/3523760.3523871,
author = {Favier, Anthony and Singamaneni, Phani Teja and Alami, Rachid},
title = {An Intelligent Human Avatar to Debug and Challenge Human-aware Robot Navigation Systems},
year = {2022},
publisher = {IEEE Press},
abstract = {Experimenting, testing, and debugging robot social navigation systems is a challenging task. While simulation is generally well suited for a first level of debugging and evaluation of robotics controllers and planners, the social navigation field lacks satisfactory simulators of humans which act, react and interact rationally and naturally. To facilitate the development of human-aware navigation systems, we propose a system to simulate an autonomous human agent that is both reactive and rational, specifically designed to act and interact with a robot for navigation problems and potential conflicts. Besides, it also provides some metrics to partially evaluate such interactions and data logs for further analysis. We show the limitations of over-used reactive-only approaches. Then, thanks to two different human-aware navigation planners, we show how our system can help answer the lack of intelligent human avatars for tuning and debugging social navigation systems before their final evaluation with real humans.},
booktitle = {Proceedings of the 2022 ACM/IEEE International Conference on Human-Robot Interaction},
pages = {760–764},
numpages = {5},
keywords = {human simulation, human-robot interaction, social navigation},
location = {Sapporo, Hokkaido, Japan},
series = {HRI '22}
}

@inproceedings{10.5555/3666122.3666334,
author = {Chen, Muxi and Li, Yu and Xu, Qiang},
title = {HiBug: on human-interpretable model debug},
year = {2023},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Machine learning models can frequently produce systematic errors on critical subsets (or slices) of data that share common attributes. Discovering and explaining such model bugs is crucial for reliable model deployment. However, existing bug discovery and interpretation methods usually involve heavy human intervention and annotation, which can be cumbersome and have low bug coverage.In this paper, we propose HiBug, an automated framework for interpretable model debugging. Our approach utilizes large pre-trained models, such as chatGPT, to suggest human-understandable attributes that are related to the targeted computer vision tasks. By leveraging pre-trained vision-language models, we can efficiently identify common visual attributes of underperforming data slices using human-understandable terms. This enables us to uncover rare cases in the training data, identify spurious correlations in the model, and use the interpretable debug results to select or generate new training data for model improvement. Experimental results demonstrate the efficacy of the HiBug framework. Code is available at: https://github.com/cure-lab/HiBug.},
booktitle = {Proceedings of the 37th International Conference on Neural Information Processing Systems},
articleno = {212},
numpages = {14},
location = {New Orleans, LA, USA},
series = {NIPS '23}
}

@article{10.1007/s11219-013-9199-x,
author = {Parsa, Saeed and Vahidi-Asl, Mojtaba and Asadi-Aghbolaghi, Maryam},
title = {Hierarchy-Debug: a scalable statistical technique for fault localization},
year = {2014},
issue_date = {September 2014},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {22},
number = {3},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-013-9199-x},
doi = {10.1007/s11219-013-9199-x},
abstract = {Considering the fact that faults may be revealed as undesired mutual effect of program predicates on each other, a new approach for localizing latent bugs, namely Hierarchy-Debug, is presented in this paper. To analyze the vertical effect of predicates on each other and on program termination status, the predicates are fitted into a logistic lasso model. To support scalability, a hierarchical clustering algorithm is applied to cluster the predicates according to their presence in different executions. Considering each cluster as a pseudo-predicate, a distinct lasso model is built for intermediate levels of the hierarchy. Then, we apply a majority voting technique to score the predicates according to their lasso coefficients at different levels of the hierarchy. The predicates with relatively higher scores are ranked as fault relevant predicates. To provide the context of failure, faulty sub-paths are identified as sequences of fault relevant predicates. The grouping effect of Hierarchy-Debug helps programmers to detect multiple bugs. Four case studies have been designed to evaluate the proposed approach on three well-known test suites, Space, Siemens, and Bash. The evaluations show that Hierarchy-Debug produces more precise results compared with prior fault localization techniques on the subject programs.},
journal = {Software Quality Journal},
month = sep,
pages = {427–466},
numpages = {40},
keywords = {Statistical bug localization, Multiple bugs, Majority voting, Lasso method, Hierarchical clustering, Faulty sub-paths, Fault relevant predicates}
}

@article{10.1016/j.micpro.2015.03.003,
author = {Dehbashi, Mehdi and Fey, G\"{o}rschwin},
title = {Transaction-based online debug for NoC-based multiprocessor SoCs},
year = {2015},
issue_date = {May 2015},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {39},
number = {3},
issn = {0141-9331},
url = {https://doi.org/10.1016/j.micpro.2015.03.003},
doi = {10.1016/j.micpro.2015.03.003},
abstract = {As complexity and size of Systems-on-Chip (SoC) grow, debugging becomes a bottleneck for designing IC products. In this paper, we present an approach for online debug of NoC-based multiprocessor SoCs. Our approach utilizes monitors and filters implemented in hardware. Monitors and filters observe and filter transactions at run-time. They are connected to a Debug Unit (DU). Transaction-based programmable Finite State Machines (FSMs) in the DU check assertions online to validate the correct relation of transactions at run-time. The experimental results show efficiency and performance of our approach.},
journal = {Microprocess. Microsyst.},
month = may,
pages = {157–166},
numpages = {10},
keywords = {Transaction-based online debug, System-on-Chip (SoC), Network-on-Chip (NoC)}
}

@article{10.1145/3643500,
author = {Kianpisheh, Mohammad and Mariakakis, Alex and Truong, Khai N.},
title = {exHAR: An Interface for Helping Non-Experts Develop and Debug Knowledge-based Human Activity Recognition Systems},
year = {2024},
issue_date = {March 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {1},
url = {https://doi.org/10.1145/3643500},
doi = {10.1145/3643500},
abstract = {Human activity recognition (HAR) is crucial for ubiquitous computing systems. While HAR systems are able to recognize a predefined set of activities established during the development process, they often fail to handle users' unique ways of completing these activities and changes in their behavior over time, as well as different activities. Knowledge-based HAR models have been proposed to help individuals create new activity definitions based on common-sense rules, but little research has been done to understand how users approach this task. To investigate this process, we developed and studied how people interact with an explainable knowledge-based HAR development tool called exHAR. Our tool empowers users to define their activities as a set of factual propositions. Users can debug these definitions by soliciting explanations for model predictions (why and why-not) and candidate corrections for faulty predictions (what-if and how-to). After conducting a study to evaluate the effectiveness of exHAR in helping users design accurate HAR systems, we conducted a think-aloud study to better understand people's approach to debugging and personalizing HAR systems and the challenges they may encounter. Our findings revealed why some participants had inaccurate mental models of knowledge-based HAR systems and inefficient approaches to the debugging process.},
journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
month = mar,
articleno = {11},
numpages = {30},
keywords = {Human activity recognition, end-user debugging, end-user development, explainable AI (XAI)}
}

@inproceedings{10.5555/2755753.2757110,
author = {Chen, Hsin-Chen and Wu, Cheng-Rong and Li, Katherine Shu-Min and Lee, Kuen-Jong},
title = {A breakpoint-based silicon debug technique with cycle-granularity for handshake-based SoC},
year = {2015},
isbn = {9783981537048},
publisher = {EDA Consortium},
address = {San Jose, CA, USA},
abstract = {The breakpoint-based silicon debug approach allows users to stop the normal (system) operations of the circuits under debug (CUDs), extract the internal states of the CUDs for examination, and then resume the normal operations for further debugging. However, most previous work on this approach adopts the transaction-level or handshake-level of granularity, i.e., the CUDs can be stopped only when a transaction or a handshake operation is completed. The granulations at these levels are often too coarse when a transaction or a handshake operation requires a large number of cycles to complete. In this paper, we present a novel debug mechanism, called the Protocol Agency Mechanism (PAM), which allows the breakpoint-based debug technique to be applied at the cycle- level granularity. The PAM can deal with transaction invalidation as well as protocol violation that may occur when a system is stopped and resumed. Experimental results show that the area overhead of the PAM is quite small and the performance impact on the system is negligible.},
booktitle = {Proceedings of the 2015 Design, Automation \&amp; Test in Europe Conference \&amp; Exhibition},
pages = {1281–1284},
numpages = {4},
location = {Grenoble, France},
series = {DATE '15}
}

@inproceedings{10.1145/2786572.2788712,
author = {Patra, Priyadarsan and Prudvi, Chinna},
title = {Fabrics on Die: Where Function, Debug and Test Meet},
year = {2015},
isbn = {9781450333962},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2786572.2788712},
doi = {10.1145/2786572.2788712},
abstract = {In this paper, we briefly present how packet-based networks or fabrics, have found their way into diverse usages on high-end industrial designs today. We outline the salient features, use models and challenges involved in implementation and application of these fabrics, not only in functional communication but also in power-management, silicon debug and high-volume-manufacturing test. Both debug and test hooks in SOC/NOC and some test/debug scenarios are discussed. We touch on some recent advances in functional networks and their implications to debug \&amp; test.},
booktitle = {Proceedings of the 9th International Symposium on Networks-on-Chip},
articleno = {4},
numpages = {3},
keywords = {Validation, Test, SOC, Interconnect, Fabric, Design, Debug},
location = {Vancouver, BC, Canada},
series = {NOCS '15}
}

@inproceedings{10.1145/2791321.2791346,
author = {Banerjee, Anita and Fedorova, Julia and Levy, Uri and Kurylev, Alexandr and Sharma, Sonal and Stoner, Michael and Ioffe, Robert},
title = {Propel with OpenCL: a deep dive workshop to create, debug, analyze and optimize OpenCL applications using Intel tools: a tutorial},
year = {2015},
isbn = {9781450334846},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2791321.2791346},
doi = {10.1145/2791321.2791346},
abstract = {This workshop and tutorial will allow developers to learn underlying architecture relevant to running OpenCL applications on Intel ®Processor Graphics and use Intel tools such as Intel ®OpenCL ™Code Builder and Intel ®VTune ™Amplifier to create, develop and analyze OpenCL applications for Intel ®Processor Graphics. Developers will also learn how to achieve better performance of their OpenCL programs by using detailed optimization techniques and best known methods to address issues. All the steps will be demonstrated with supportive examples and real data. Developers will also get to explore ways to utilize the new concepts of OpenCL 2.0 along with the power of the tools to create more complex and better performing OpenCL programs. During the session, developers will get a chance to walk through every stage of the lifecycle, from creating their first OpenCL application, experiencing Intel ®OpenCL ™Code Builder and Intel ®VTune ™Amplifier debugging, to optimizing capabilities in complete action.Topics that will be covered include:• Intel ®Processor Graphics Architecture Overview.• Tools workshop to create, debug, analyze and apply optimization techniques for OpenCl programs using:--- Intel ®OpenCL ™Code Builder--- Intel ®VTune ™Amplifier• Optimization guides and best known methods with supportive samples and data.},
booktitle = {Proceedings of the 3rd International Workshop on OpenCL},
articleno = {25},
numpages = {2},
location = {Palo Alto, California},
series = {IWOCL '15}
}

@inproceedings{10.1145/3691620.3695066,
author = {Li, Guochang and Zhi, Chen and Chen, Jialiang and Han, Junxiao and Deng, Shuiguang},
title = {Exploring Parameter-Efficient Fine-Tuning of Large Language Model on Automated Program Repair},
year = {2024},
isbn = {9798400712487},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3691620.3695066},
doi = {10.1145/3691620.3695066},
abstract = {Automated Program Repair (APR) aims to fix bugs by generating patches. And existing work has demonstrated that "pre-training and fine-tuning" paradigm enables Large Language Models (LLMs) improve fixing capabilities on APR. However, existing work mainly focuses on Full-Model Fine-Tuning (FMFT) for APR and limited research has been conducted on the execution-based evaluation of Parameter-Efficient Fine-Tuning (PEFT) for APR. Comparing to FMFT, PEFT can reduce computing resource consumption without compromising performance and has been widely adopted to other software engineering tasks.To fill this gap, we enhance the existing APR dataset by employing prompt engineering to create an instruction dataset, APR-Instruction, at first. Secondly, we fine-tune four pre-trained LLMs using four different PEFT methods with APR-Instruction. The best fine-tuned model fixes 58\% more bugs than the state-of-the-art LLM-based APR techniques. The results also show that (IA)3 improves the creativity of LLMs more effectively through fine-tuning and achieves the highest fixing capability compared to the other three PEFT methods. Thirdly, we explore the optimal configuration of PEFT hyperparameters, and assess the impact of instruction dataset size, showing that a larger number of parameters and a larger training dataset do not necessarily result in better performance for PEFT. Lastly, we analyze peak memory usage and trainable parameters to show the efficiency of PEFT.This work provides a comprehensive exploration of PEFT on APR and suggests potentially promising directions for extension to other software engineering downstream tasks. APR-Instruction, PEFT weights, and the fine-tuning code are publicly available as open-source resources.},
booktitle = {Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering},
pages = {719–731},
numpages = {13},
keywords = {automated program repair, parameter-effective fine-tuning, large language model, execution-based evaluation},
location = {Sacramento, CA, USA},
series = {ASE '24}
}

@article{10.1007/s10515-025-00568-8,
author = {Lin, Renze and Wang, Ran and Hu, Guanghuan and Xu, Xianghua},
title = {LMFuzz: Program repair fuzzing based on large language models},
year = {2025},
issue_date = {Dec 2025},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {33},
number = {1},
issn = {0928-8910},
url = {https://doi.org/10.1007/s10515-025-00568-8},
doi = {10.1007/s10515-025-00568-8},
abstract = {Generating programs using large language models (LLMs) for fuzz testing has emerged as a significant testing methodology. While traditional fuzzers can produce correct programs, their effectiveness is limited by excessive constraints and restricted API combinations, resulting in insufficient coverage of the target system’s code and impacting testing efficiency. Unlike traditional methods, large language model based fuzzers can generate more diverse code, effectively addressing key issues of conventional fuzzers. However, the lack of constraints on API combinations during the generation process often leads to reduced program validity. Therefore, a crucial challenge is to enhance the validity of generated code while maintaining its diversity. To address this issue, we propose a novel and universal fuzzer, LMFuzz. To ensure the fuzzer’s generation capability, we utilize a large language model as the primary generator and model the operator selection problem within the fuzzing loop as a multi-armed bandit problem. We introduce the Thompson Sampling algorithm to enhance both the diversity and validity of program generation. To improve the validity of the generated code, we incorporate a program repair loop that iteratively corrects the generated programs, thereby reducing errors caused by the lack of API combination constraints. Experimental results demonstrate that LMFuzz significantly surpasses existing state-of-the-art large language model based fuzzers in terms of coverage and validity, and also exhibits notable advantages in generating diverse programs. Furthermore, LMFuzz has identified 24 bugs across five popular programming languages and their corresponding systems.},
journal = {Automated Software Engg.},
month = oct,
numpages = {27},
keywords = {Fuzzing, Large language model, Code generation, Thompson sampling algorithm, Program Repair, Software security}
}

@article{10.1007/s10515-025-00512-w,
author = {Li, Yingling and Cai, Muxin and Chen, Junjie and Xu, Yang and Huang, Lei and Li, Jianping},
title = {Context-aware prompting for LLM-based program repair: Context-aware prompting for LLM-based program repair},
year = {2025},
issue_date = {Aug 2025},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {32},
number = {2},
issn = {0928-8910},
url = {https://doi.org/10.1007/s10515-025-00512-w},
doi = {10.1007/s10515-025-00512-w},
abstract = {Automated program repair (APR) plays a crucial role in ensuring the quality of software code, as manual bug-fixing is extremely time-consuming and labor-intensive. Traditional APR tools (e.g., template-based approaches) face the challenge of generalizing to different bug patterns, while deep learning (DL)-based methods heavily rely on training datasets and struggle to fix unseen bugs. Recently, large language models (LLMs) have shown great potential in APR due to their ability to generate patches, having achieved promising results. However, their effectiveness is still constrained by the casually-determined context (e.g., being unable to adaptively select the specific context according to the situation of each defect). Therefore, a more effective APR approach is highly needed, which provides more precise and comprehensive context for the given defect to enhance the robustness of LLM-based APRs. In this paper, we propose a context-aware APR approach named CodeCorrector, which designs a Chain-of-Thought (CoT) approach to follow developers’ program repair behaviors. Given a failing test and its buggy file, CodeCorrector first analyzes why the test fails based on the failure message to infer repair direction; then selects the relevant context information to this repair direction; finally builds the context-aware repair prompt to guide LLMs for patch generation. Our motivation is to offer a novel perspective for enhancing LLM-based program repair through context-aware prompting, which adaptively selects specific context for a given defect. The evaluation on the widely-used Defects4J (i.e., v1.2 and v2.0) benchmark shows that overall, by executing a small number of repairs (i.e., as few as ten rounds), CodeCorrector outperforms all the state-of-the-art baselines on the more complex defects in Defects4J v2.0 and the defects without fine-grained defect localization information in Defects4J v1.2. Specifically, a total of 38 defects are fixed by only CodeCorrector. We further analyze the contributions of two core components (i.e., repair directions, global context selection) to the performance of CodeCorrector, especially repair directions, which improve CodeCorrector by 112\% in correct patches and 78\% in plausible patches on Defects4J v1.2. Moreover, CodeCorrector generates more valid and correct patches, achieving a 377\% improvement over the base LLM GPT-3.5 and a 268\% improvement over GPT-4.},
journal = {Automated Software Engg.},
month = apr,
numpages = {34},
keywords = {APR, LLMs, Repair directions, Adaptive context selection}
}

@inproceedings{10.1109/ICSE55347.2025.00169,
author = {Xu, Junjielong and Fu, Ying and Tan, Shin Hwei and He, Pinjia},
title = {Aligning the Objective of LLM-Based Program Repair},
year = {2025},
isbn = {9798331505691},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE55347.2025.00169},
doi = {10.1109/ICSE55347.2025.00169},
abstract = {Large language models (LLMs) have achieved decent results on automated program repair (APR). However, the next token prediction training objective of decoder-only LLMs (e.g., GPT-4) is misaligned with the masked span prediction objective of current infilling-style methods, which impedes LLMs from fully leveraging pre-trained knowledge for program repair. In addition, while some LLMs can locate and repair bugs in certain functions using the related artifacts (e.g., test cases), existing methods still depend on statement-level fault localization methods to provide a list of buggy hunks for repair. This restriction hinders LLMs from exploring potential patches beyond the given locations.In this paper, we investigate a new approach to adapt LLMs to program repair. Our core insight is that LLM's APR capability can be greatly improved by simply aligning the output to their training objective and allowing them to refine the whole program without first identifying faulty statements. Based on this insight, we designed D4C, a straightforward prompting framework for APR. D4C can repair 180 bugs correctly in Defects4J, with each patch being sampled only 10 times. This surpasses the SOTA APR methods with perfect fault localization by 10\% and reduces the patch sampling number by 90\%. Our findings reveal that (1) objective alignment is crucial for fully exploiting LLM's pre-trained capability, and (2) replacing the traditional localize-buggy-hunks-then-repair workflow with direct debugging is more effective for LLM-based APR methods. Thus, we believe this paper introduces a new mindset for harnessing LLMs in APR.},
booktitle = {Proceedings of the IEEE/ACM 47th International Conference on Software Engineering},
pages = {2548–2560},
numpages = {13},
keywords = {automated program repair, large language model, objective alignment},
location = {Ottawa, Ontario, Canada},
series = {ICSE '25}
}

@inproceedings{10.1145/3650212.3680359,
author = {Yin, Xin and Ni, Chao and Wang, Shaohua and Li, Zhenhao and Zeng, Limin and Yang, Xiaohu},
title = {ThinkRepair: Self-Directed Automated Program Repair},
year = {2024},
isbn = {9798400706127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3650212.3680359},
doi = {10.1145/3650212.3680359},
abstract = {Though many approaches have been proposed for Automated Program Repair (APR) and indeed achieved remarkable performance, they still have limitations in fixing bugs that require analyzing and reasoning about the logic of the buggy program. Recently, large language models (LLMs) instructed by prompt engineering have attracted much attention for their powerful ability to address many kinds of tasks including bug-fixing. However, the quality of the prompt will highly affect the ability of LLMs and manually constructing high-quality prompts is a costly endeavor.    To address this limitation, we propose a self-directed LLM-based automated program repair, ThinkRepair, with two main phases: collection phase and fixing phase. The former phase automatically collects various chains of thoughts that constitute pre-fixed knowledge by instructing LLMs with the Chain-of-Thought (CoT) prompt. The latter phase targets fixing a bug by first selecting examples for few-shot learning and second automatically interacting with LLMs, optionally appending with feedback of testing information.    Evaluations on two widely studied datasets (Defects4J and QuixBugs) by comparing ThinkRepair with 12 SOTA APRs indicate the priority of ThinkRepair in fixing bugs. Notably, ThinkRepair fixes 98 bugs and improves baselines by 27\%∼344.4\% on Defects4J V1.2. On Defects4J V2.0, ThinkRepair fixes 12∼65 more bugs than the SOTA APRs. Additionally, ThinkRepair also makes a considerable improvement on QuixBugs (31 for Java and 21 for Python at most).},
booktitle = {Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {1274–1286},
numpages = {13},
keywords = {Automated Program Repair, Large Language Model, Prompt Engineering},
location = {Vienna, Austria},
series = {ISSTA 2024}
}

@inbook{10.1109/ICSE55347.2025.00246,
author = {Ouyang, Shuyin and Zhang, Jie M. and Sun, Zeyu and Penuela, Albert Merono},
title = {Knowledge-Enhanced Program Repair for Data Science Code},
year = {2025},
isbn = {9798331505691},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE55347.2025.00246},
abstract = {This paper introduces DSrepair, a knowledge-enhanced program repair approach designed to repair the buggy code generated by LLMs in the data science domain. DSrepair uses knowledge graph based RAG for API knowledge retrieval and bug knowledge enrichment to construct repair prompts for LLMs. Specifically, to enable knowledge graph-based API retrieval, we construct DS-KG (Data Science Knowledge Graph) for widely used data science libraries. For bug knowledge enrichment, we employ an abstract syntax tree (AST) to localize errors at the AST node level. We evaluate DSrepair's effectiveness against five state-of-the-art LLM-based repair baselines using four advanced LLMs on the DS-1000 dataset. The results show that DSrepair outperforms all five baselines. Specifically, when compared to the second-best baseline, DSrepair achieves substantial improvements, fixing 44.4\%, 14.2\%, 20.6\%, and 32.1\% more buggy code snippets for each of the four evaluated LLMs, respectively. Additionally, it achieves greater efficiency, reducing the number of tokens required per code task by 17.49\%, 34.24\%, 24.71\%, and 17.59\%, respectively.},
booktitle = {Proceedings of the IEEE/ACM 47th International Conference on Software Engineering},
pages = {898–910},
numpages = {13}
}

@article{10.1007/s11219-025-09728-1,
author = {Liu, Zixin and Du, Xiaozhi and Liu, Hairui},
title = {ReAPR: Automatic program repair via retrieval-augmented large language models: ReAPR: Automatic program repair via retrieval-augmented...},
year = {2025},
issue_date = {Jul 2025},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {33},
number = {3},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-025-09728-1},
doi = {10.1007/s11219-025-09728-1},
abstract = {Automatic Program Repair (APR) aims to automatically fix software defects, significantly reducing the efforts of manual debugging. Recent studies have demonstrated impressive results in utilizing Large Language Models (LLMs) for software bug fixing. Current LLM-based approaches depend solely on the pre-trained knowledge of LLMs, overlooking the prior knowledge contained in historical bug repair records, which increases the likelihood of hallucinations. To address this challenge, this paper proposes ReAPR, a retrieval-augmented framework for APR. We first curate a high-quality retrieval database by carefully compiling and filtering the existing datasets for APR. Subsequently, ReAPR leverages a retriever to fetch bug-fix pairs similar to the target bug from a retrieval database, providing contextual hints to guide the LLMs in the repair process. We then investigate two techniques to retrieve bug-fix pairs associated with the function to be fixed: BM25 and Dense Passage Retrieval (DPR). After retrieving the relevant bug-fix pair, we construct a prompt and integrate the retrieved pair into it. Besides, we also compare the proposed RAG-based approach with the parameter-efficient fine-tuning (PEFT) approaches on repair performance. To validate the effectiveness of ReAPR, we conduct extensive experiments based on the widely-used benchmark dataset Defects4j 2.0 as well as the latest benchmark GitBug-Java. The results show that ReAPR, based on the CodeLlama(7B) backbone, successfully fixes 68 and 59 bugs in the DPR and BM25 settings, respectively, in Defects4j 2.0, outperforming the best baseline approach by 18 and 9 bugs under the same repair settings.},
journal = {Software Quality Journal},
month = jul,
numpages = {31},
keywords = {Automated Program Repair, Retrieval-Augmented Generation, Large Language Models, Prompt Learning}
}

@article{10.1016/j.csi.2024.103951,
author = {Zubair, Fida and Al-Hitmi, Maryam and Catal, Cagatay},
title = {The use of large language models for program repair},
year = {2025},
issue_date = {Apr 2025},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {93},
number = {C},
issn = {0920-5489},
url = {https://doi.org/10.1016/j.csi.2024.103951},
doi = {10.1016/j.csi.2024.103951},
journal = {Comput. Stand. Interfaces},
month = apr,
numpages = {12},
keywords = {Large language model, Program repair, Software engineering, Automated program repair, SLR, LLM, AI, GPT, BERT, APR, F1, BLEU, GLUE, SQuAD, RQ, C1}
}

@inproceedings{10.1109/ICSE55347.2025.00157,
author = {Bouzenia, Islem and Devanbu, Premkumar and Pradel, Michael},
title = {RepairAgent: An Autonomous, LLM-Based Agent for Program Repair},
year = {2025},
isbn = {9798331505691},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE55347.2025.00157},
doi = {10.1109/ICSE55347.2025.00157},
abstract = {Automated program repair has emerged as a powerful technique to mitigate the impact of software bugs on system reliability and user experience. This paper introduces RepairAgent, the first work to address the program repair challenge through an autonomous agent based on a large language model (LLM). Unlike existing deep learning-based approaches, which prompt a model with a fixed prompt or in a fixed feedback loop, our work treats the LLM as an agent capable of autonomously planning and executing actions to fix bugs by invoking suitable tools. RepairAgent freely interleaves gathering information about the bug, gathering repair ingredients, and validating fixes, while deciding which tools to invoke based on the gathered information and feedback from previous fix attempts. Key contributions that enable RepairAgent include a set of tools that are useful for program repair, a dynamically updated prompt format that allows the LLM to interact with these tools, and a finite state machine that guides the agent in invoking the tools. Our evaluation on the popular Defects4J dataset demonstrates RepairAgent's effectiveness in autonomously repairing 164 bugs, including 39 bugs not fixed by prior techniques. Interacting with the LLM imposes an average cost of 270k tokens per bug, which, under the current pricing of OpenAI's GPT-3.5 model, translates to 14 cents per bug. To the best of our knowledge, this work is the first to present an autonomous, LLM-based agent for program repair, paving the way for future agent-based techniques in software engineering.},
booktitle = {Proceedings of the IEEE/ACM 47th International Conference on Software Engineering},
pages = {2188–2200},
numpages = {13},
location = {Ottawa, Ontario, Canada},
series = {ICSE '25}
}

@article{10.1145/3770581,
author = {Feng, Qiong and Ma, Xiaotian and Sheng, Jiayi and Feng, Ziyuan and Song, Wei and Liang, Peng},
title = {Integrating Various Software Artifacts for Better LLM-based Bug Localization and Program Repair},
year = {2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1049-331X},
url = {https://doi.org/10.1145/3770581},
doi = {10.1145/3770581},
abstract = {LLMs have garnered considerable attention for their potential to streamline Automated Program Repair (APR). LLM-based approaches can either insert the correct code using an infilling-style technique or directly generate patches when provided with buggy methods, aiming for plausible patches to pass all tests. However, most of LLM-based APR methods rely on a single type of software information, such as issue descriptions or error stack traces, without fully leveraging a combination of diverse software artifacts. Human developers, in contrast, often use a range of information — such as debugging data, issue discussions, and error stack traces — to diagnose and fix bugs. Despite this, many LLM-based approaches do not explore which specific types of software information best assist in localizing and repairing software bugs. Addressing this gap is crucial for advancing LLM-based APR techniques.To investigate this and mimic the way human developers fix bugs, we propose DEVLoRe (short for DEVeloper Localization and Repair). In this framework, LLMs first use issue content (description and discussion) and stack error traces to localize buggy methods, then rely on debug information in buggy methods and issue content and stack error to localize buggy lines and generate valid patches. We evaluated the effectiveness of issue content, error stack traces, and debugging information in bug localization and automatic program repair. Our results show that while issue content and error stack is particularly effective in assisting LLMs with fault localization and program repair respectively, different types of software artifacts complement each other in addressing various bugs. By incorporating these three types of artifacts and using the Defects4J v2.0 dataset for evaluation, DEVLoRe successfully localizes 49.3\% of single-method bugs and generates 56.0\% plausible patches. Additionally, DEVLoRe can localize 47.6\% of non-single-method bugs and generates 14.5\% plausible patches. Moreover, our framework streamlines the end-to-end process from buggy source code to a complete repair, and achieves a 39.7\% and 17.1\% of single-method and non-single-method bug repair rate, outperforming current state-of-the-art APR methods. Furthermore, we re-implemented and evaluated our framework, demonstrating its effectiveness in resolving 9 unique issues compared to other state-of-the-art frameworks using the same or more advanced models on SWE-bench Lite. We also discussed whether a leading framework for Python code can be directly applied to Java code, or vice versa. The source code and experimental results of this work for replication are available at .},
note = {Just Accepted},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = oct,
keywords = {Large Language Model, Automatic Program Repair, Fault Localization}
}

@article{10.1145/3719345,
author = {Kong, Jiaolong and Xie, Xiaofei and Cheng, Mingfei and Liu, Shangqing and Du, Xiaoning and Guo, Qi},
title = {ContrastRepair: Enhancing Conversation-Based Automated Program Repair via Contrastive Test Case Pairs},
year = {2025},
issue_date = {November 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {34},
number = {8},
issn = {1049-331X},
url = {https://doi.org/10.1145/3719345},
doi = {10.1145/3719345},
abstract = {Automated Program Repair (APR) aims to automatically generate patches for rectifying software bugs. Recent strides in Large Language Models (LLM), such as ChatGPT, have yielded encouraging outcomes in APR, especially within the conversation-driven APR framework. Nevertheless, the efficacy of conversation-driven APR is contingent on the quality of the feedback information. In this article, we propose ContrastRepair, a novel conversation-based APR approach that augments conversation-driven APR by providing LLMs with contrastive test pairs. A test pair consists of a failing test and a passing test, which offer contrastive feedback to the LLM. Our key insight is to minimize the difference between the generated passing test and the given failing test, which can better isolate the root causes of bugs. By providing such informative feedback, ContrastRepair enables the LLM to produce effective bug fixes. The implementation of ContrastRepair is based on the state-of-the-art LLM, ChatGPT, and it iteratively interacts with ChatGPT until plausible patches are generated. We evaluate ContrastRepair on multiple benchmark datasets, including Defects4J, QuixBugs, and HumanEval-Java. The results demonstrate that ContrastRepair significantly outperforms existing methods, achieving a new state-of-the-art in program repair. For instance, among Defects4J 1.2 and 2.0, ContrastRepair correctly repairs 143 out of all 337 bug cases, while the best-performing baseline fixes 124 bugs.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = oct,
articleno = {216},
numpages = {31},
keywords = {Program Repair, Large Language Model}
}

@inproceedings{10.1007/978-3-031-78386-9_28,
author = {Hori, Shota and Matsumoto, Shinsuke and Higo, Yoshiki and Kusumoto, Shinji and Yasuda, Kazuya and Ito, Shinji and Huyen, Phan Thi Thanh},
title = {The Effects of&nbsp;Semantic Information on&nbsp;LLM-Based Program Repair},
year = {2024},
isbn = {978-3-031-78385-2},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-78386-9_28},
doi = {10.1007/978-3-031-78386-9_28},
abstract = {Large Language Model-based Automated Program Repair (LLM-APR) has recently received significant attention as a debugging assistance. Our objective is to improve the performance of LLM-APR.&nbsp;In this study, we focus on semantic information contained in the source code. Semantic information refers to elements used by the programmer to understand the source code, which does not contribute&nbsp;to compilation or execution. We picked out specification, method&nbsp;names and variable names as semantic information. In the investigation,&nbsp;we prepared eight prompts, each consisting of all combinations of&nbsp;three types of semantic information. The experimental results showed&nbsp;that all semantic information improves the performance of LLM-APR,&nbsp;and variable names are particularly significant.},
booktitle = {Product-Focused Software Process Improvement: 25th International Conference, PROFES 2024, Tartu, Estonia, December 2–4, 2024, Proceedings},
pages = {377–385},
numpages = {9},
keywords = {Large language model (LLM), automated program repair (APR), semantic information, prompt engineering, ChatGPT},
location = {Tartu, Estonia}
}

@article{10.1145/3744900,
author = {Martinez, Matias and Mart\'{\i}nez-Fern\'{a}ndez, Silverio and Franch, Xavier},
title = {The Sustainability Face of Automated Program Repair Tools},
year = {2025},
issue_date = {November 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {34},
number = {8},
issn = {1049-331X},
url = {https://doi.org/10.1145/3744900},
doi = {10.1145/3744900},
abstract = {Automated program repair (APR) aims to automatize the process of repairing software bugs in order to reduce the cost of maintaining software programs. While APR accuracy has significantly improved in recent years, its energy impact remains unstudied. The field of green software research aims to measure the energy consumption required to develop, maintain, and use software products. Our main goal is to define the foundation for measuring the energy consumption of the APR activity. We state that an environmentally sustainable (or green) APR tool achieves a good balance between the ability to correctly repair bugs and the amount of energy consumed during such process. We measure the energy consumption of 10 traditional APR tools for Java and 11 fine-tuned large-language models (LLM) trying to repair real bugs from Defects4J. The results of this study show the existing tradeoff between energy consumption and repairability. In particular, APR tools such as TBar and RepairLlama repair more bugs than other approaches at the expense of a higher energy consumption. Other tools, such as SimFix and the LLM CodeT5-large, provide a good tradeoff between energy consumption and repairability. We also present guidelines consisting of a set of recommendations for developing greener APR.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = oct,
articleno = {237},
numpages = {37},
keywords = {Automated Program Repair, Software Sustainability, Energy Consumption of Software Tools, Green Computing}
}

@inproceedings{10.1145/3643788.3648021,
author = {Lajko, Mark and Csuvik, Viktor and Gyimothy, Tibor and Vidacs, Laszlo},
title = {Automated Program Repair with the GPT Family, including GPT-2, GPT-3 and CodeX},
year = {2024},
isbn = {9798400705779},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643788.3648021},
doi = {10.1145/3643788.3648021},
abstract = {Automated Program Repair (APR) is a promising approach for addressing software defects and improving software reliability. There are various approaches to APR, including using Machine Learning (ML) techniques such as neural networks and evolutionary algorithms, as well as more traditional methods such as static analysis and symbolic execution. In recent years, there has been growing interest in using ML techniques for APR, including the use of large language models such as GPT-2 and GPT-3. These models have the ability to generate human-like text and code, making them well-suited for tasks such as generating repair patches for defective programs. In this paper, we explore the use of the GPT family (including GPT-2, GPT-J-6B, GPT-3 and Codex) for APR of JavaScript programs and evaluate their performance in terms of the number and quality of repair patches generated. Our results show that these state-of-the-art language models are able to generate repair patches that successfully fix the defects in the JavaScript programs, with Codex performing slightly better overall. To be precise, in our self-assembled dataset, Codex was able to generate 108 repair patches that are exactly the same as the developer fix for the first try. If we consider multiple patch generations, up to 201 buggy programs are being repaired automatically from the 1559 evaluation dataset (12.89\%).},
booktitle = {Proceedings of the 5th ACM/IEEE International Workshop on Automated Program Repair},
pages = {34–41},
numpages = {8},
keywords = {automated program repair, transformers, GPT-3, codex, JavaScript},
location = {Lisbon, Portugal},
series = {APR '24}
}

@article{10.1145/3715004,
author = {Li, Fengjie and Jiang, Jiajun and Sun, Jiajun and Zhang, Hongyu},
title = {Hybrid Automated Program Repair by Combining Large Language Models and Program Analysis},
year = {2025},
issue_date = {September 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {34},
number = {7},
issn = {1049-331X},
url = {https://doi.org/10.1145/3715004},
doi = {10.1145/3715004},
abstract = {Automated Program Repair (APR) has garnered significant attention due to its potential to streamline the bug repair process for human developers. Recently, LLM-based APR methods have shown promise in repairing real-world bugs. However, existing APR methods often utilize patches generated by LLMs without further optimization, resulting in reduced effectiveness due to the lack of program-specific knowledge. Furthermore, the evaluations of these APR methods have typically been conducted under the assumption of perfect fault localization, which may not accurately reflect their real-world effectiveness. To address these limitations, this article introduces an innovative APR approach called GiantRepair. Our approach leverages the insight that LLM-generated patches, although not necessarily correct, offer valuable guidance for the patch generation process. Based on this insight, GiantRepair first constructs patch skeletons from LLM-generated patches to confine the patch space, and then generates high-quality patches tailored to specific programs through context-aware patch generation by instantiating the skeletons. To evaluate the performance of our approach, we conduct two large-scale experiments. The results demonstrate that GiantRepair not only effectively repairs more bugs (an average of 27.78\% on Defects4J v1.2 and 23.40\% on Defects4J v2.0) than using LLM-generated patches directly, but also outperforms state-of-the-art APR methods by repairing at least 42 and 7 more bugs under perfect and automated fault localization scenarios, respectively.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = aug,
articleno = {202},
numpages = {28},
keywords = {Program Repair, Large Language Model, Program Synthesis}
}

@article{10.1007/s10515-025-00501-z,
author = {Hanna, Carol and Blot, Aymeric and Petke, Justyna},
title = {Reinforcement learning for mutation operator selection in automated program repair: Reinforcement learning for mutation operator selection in automated program repair},
year = {2025},
issue_date = {Aug 2025},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {32},
number = {2},
issn = {0928-8910},
url = {https://doi.org/10.1007/s10515-025-00501-z},
doi = {10.1007/s10515-025-00501-z},
abstract = {Automated program repair techniques aim to aid software developers with the challenging task of fixing bugs. In heuristic-based program repair, a search space of mutated program variants is explored to find potential patches for bugs. Most commonly, every selection of a mutation operator during search is performed uniformly at random, which can generate many buggy, even uncompilable programs. Our goal is to reduce the generation of variants that do not compile or break intended functionality which waste considerable resources. In this paper, we investigate the feasibility of a reinforcement learning-based approach for the selection of mutation operators in heuristic-based program repair. Our proposed approach is programming language, granularity-level, and search strategy agnostic and allows for easy augmentation into existing heuristic-based repair tools. We conducted an extensive empirical evaluation of four operator selection techniques, two reward types, two credit assignment strategies, two integration methods, and three sets of mutation operators using 30,080 independent repair attempts. We evaluated our approach on 353 real-world bugs from the Defects4J benchmark. The reinforcement learning-based mutation operator selection results in a higher number of test-passing variants, but does not exhibit a noticeable improvement in the number of bugs patched in comparison with the baseline, uniform random selection. While reinforcement learning has been previously shown to be successful in improving the search of evolutionary algorithms, often used in heuristic-based program repair, it has yet to demonstrate such improvements when applied to this area of research.},
journal = {Automated Software Engg.},
month = mar,
numpages = {33},
keywords = {Automated program repair, Machine learning, Mutation operators, Genetic improvement, Reinforcement learning}
}

@inproceedings{10.1109/ICSE55347.2025.00162,
author = {Parasaram, Nikhil and Yan, Huijie and Yang, Boyu and Flahy, Zineb and Qudsi, Abriele and Ziaber, Damian and Barr, Earl T. and Mechtaev, Sergey},
title = {The Fact Selection Problem in LLM-Based Program Repair},
year = {2025},
isbn = {9798331505691},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE55347.2025.00162},
doi = {10.1109/ICSE55347.2025.00162},
abstract = {Recent research has shown that incorporating bug-related facts, such as stack traces and GitHub issues, into prompts enhances the bug-fixing capabilities of large language models (LLMs). Considering the ever-increasing context window of these models, a critical question arises: what and how many facts should be included in prompts to maximise the chance of correctly fixing bugs? To answer this question, we conducted a large-scale study, employing over 19K prompts featuring various combinations of seven diverse facts to rectify 314 bugs from open-source Python projects within the BugsInPy benchmark. Our findings revealed that each fact, ranging from simple syntactic details like code context to semantic information previously unexplored in the context of LLMs such as angelic values, is beneficial. Specifically, each fact aids in fixing some bugs that would remain unresolved or only be fixed with a low success rate without it. Importantly, we discovered that the effectiveness of program repair prompts is non-monotonic over the number of used facts; using too many facts leads to subpar outcomes. These insights led us to define the fact selection problem: determining the optimal set of facts for inclusion in a prompt to maximise LLM's performance on a given task instance. We found that there is no one-size-fits-all set of facts for bug repair. Therefore, we developed a basic statistical model, named Maniple, which selects facts specific to a given bug to include in the prompt. This model significantly surpasses the performance of the best generic fact set. To underscore the significance of the fact selection problem, we benchmarked MANIPLE against the state-of-the-art zero-shot, non-conversational LLM-based bug repair methods. On our testing dataset of 157 bugs, Maniple repairs 88 bugs, 17\% above the best configuration.},
booktitle = {Proceedings of the IEEE/ACM 47th International Conference on Software Engineering},
pages = {2574–2586},
numpages = {13},
keywords = {automated program repair, large language models, prompt engineering},
location = {Ottawa, Ontario, Canada},
series = {ICSE '25}
}

@article{10.1145/3771922,
author = {Vallecillos Ruiz, Fernando and Grishina, Anastasiia and Hort, Max and Moonen, Leon},
title = {Assessing the Latent Automated Program Repair Capabilities of Large Language Models using Round-Trip Translation},
year = {2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1049-331X},
url = {https://doi.org/10.1145/3771922},
doi = {10.1145/3771922},
abstract = {Research shows that errors in natural language can be corrected by translating texts to another language and back using language models. We explore to what extent this latent correction capability extends to Automated Program Repair (APR) by investigating Round-Trip Translation (RTT): translating code from one programming language into another programming or natural language and back, using Large Language Models (LLMs). We hypothesize that RTT restores patterns most commonly seen in the LLM’s training corpora through regression toward the mean, replacing infrequent bugs with more frequent, natural, bug-free code. To test this hypothesis, we employ nine LLMs and four common APR benchmarks in Java, and perform a detailed quantitative and qualitative analysis of RTT-generated patches. We find that RTT through English generates plausible patches for 100 of 164 bugs with GPT-4 on the HumanEval-Java benchmark, and 97 are found to be correct in our manual assessment. Moreover, RTT uniquely generates plausible patches for 46 bugs that were missed by LLMs specifically fine-tuned for APR. While this demonstrates the viability of RTT for APR, we also observe limitations, such as a lower overall bug fix rate than the state-of-the-art and diluting the original coding style. We analyze the impact of these limitations and discuss the potential of using RTT as a complementary component in APR frameworks.},
note = {Just Accepted},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = oct,
keywords = {automated program repair, large language model, machine translation}
}

@inproceedings{10.1145/3643788.3648020,
author = {Jiang, Nan and Wu, Yi},
title = {RepairCAT: Applying Large Language Model to Fix Bugs in AI-Generated Programs},
year = {2024},
isbn = {9798400705779},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643788.3648020},
doi = {10.1145/3643788.3648020},
abstract = {Automated program repair has been a crucial and popular domain for years, and with the development of large language models (LLMs) and the trend of using LLMs for code generation, there comes the new challenge of fixing bugs in LLM-generated (AI-generated) programs. In this work, we introduce RepairCAT, a simple and neat framework for fine-tuning large language models for automated repairing Python programs. Our experiments built on StarCoder-1B successfully generated patches fixing the failed test cases for 14 out of 100 bugs in the Python programs, 2 of which passed all the public test cases and were considered plausible.},
booktitle = {Proceedings of the 5th ACM/IEEE International Workshop on Automated Program Repair},
pages = {58–60},
numpages = {3},
keywords = {automated program repair, large language model},
location = {Lisbon, Portugal},
series = {APR '24}
}

@inproceedings{10.1109/ICSE-NIER66352.2025.00024,
author = {Li, Fengjie and Jiang, Jiajun and Sun, Jiajun and Zhang, Hongyu},
title = {Evaluating the Generalizability of LLMs in Automated Program Repair},
year = {2025},
isbn = {9798331537111},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-NIER66352.2025.00024},
doi = {10.1109/ICSE-NIER66352.2025.00024},
abstract = {LLM-based automated program repair methods have attracted significant attention for their state-of-the-art performance. However, they were primarily evaluated on a few well-known datasets like Defects4J, raising questions about their effectiveness on new datasets. In this study, we evaluate 11 top-performing LLMs on Defects4J-Trans, a new dataset derived from transforming Defects4J while maintaining the original semantics. Results from experiments on both Defects4J and Defects4J-Trans show that all studied LLMs have limited generalizability in APR tasks, with the average number of correct and plausible patches decreasing by 49.48\% and 42.90\%, respectively, on Defects4J-Trans. Further investigation into incorporating additional repair-relevant information in repair prompts reveals that, although this information significantly enhances the LLMs' capabilities (increasing the number of correct and plausible patches by up to 136.67\% and 121.82\%, respectively), performance still falls short of their original results. This indicates that prompt engineering alone is insufficient to substantially enhance LLMs' repair capabilities. Based on our study, we also offer several recommendations for future research.},
booktitle = {Proceedings of the IEEE/ACM 47th International Conference on Software Engineering: New Ideas and Emerging Results},
pages = {91–95},
numpages = {5},
keywords = {program repair, LLM, generalizability of LLM},
location = {Ottawa, Ontario, Canada},
series = {ICSE-NIER '25}
}

@inproceedings{10.1145/3691620.3695537,
author = {Zhao, Jiuang and Yang, Donghao and Zhang, Li and Lian, Xiaoli and Yang, Zitian and Liu, Fang},
title = {Enhancing Automated Program Repair with Solution Design},
year = {2024},
isbn = {9798400712487},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3691620.3695537},
doi = {10.1145/3691620.3695537},
abstract = {Automatic Program Repair (APR) endeavors to autonomously rectify issues within specific projects, which generally encompasses three categories of tasks: bug resolution, new feature development, and feature enhancement. Despite extensive research proposing various methodologies, their efficacy in addressing real issues remains unsatisfactory. It's worth noting that, typically, engineers have design rationales (DR) on solution--- planed solutions and a set of underlying reasons---before they start patching code. In open-source projects, these DRs are frequently captured in issue logs through project management tools like Jira. This raises a compelling question: How can we leverage DR scattered across the issue logs to efficiently enhance APR?To investigate this premise, we introduce DRCodePilot, an approach designed to augment GPT-4-Turbo's APR capabilities by incorporating DR into the prompt instruction. Furthermore, given GPT-4's constraints in fully grasping the broader project context and occasional shortcomings in generating precise identifiers, we have devised a feedback-based self-reflective framework, in which we prompt GPT-4 to reconsider and refine its outputs by referencing a provided patch and suggested identifiers. We have established a benchmark comprising 938 issue-patch pairs sourced from two open-source repositories hosted on GitHub and Jira. Our experimental results are impressive: DRCodePilot achieves a full-match ratio that is a remarkable 4.7x higher than when GPT-4 is utilized directly. Additionally, the CodeBLEU scores also exhibit promising enhancements. Moreover, our findings reveal that the standalone application of DR can yield promising increase in the full-match ratio across CodeLlama, GPT-3.5, and GPT-4 within our benchmark suite. We believe that our DRCodePilot initiative heralds a novel human-in-the-loop avenue for advancing the field of APR.},
booktitle = {Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering},
pages = {1706–1718},
numpages = {13},
keywords = {design rationale, issue logs, developer discussion, automated program repair},
location = {Sacramento, CA, USA},
series = {ASE '24}
}

@inproceedings{10.1145/3639478.3643114,
author = {Martinez, Matias and Mart\'{\i}nez-Fern\'{a}ndez, Silverio and Franch, Xavier},
title = {Energy Consumption of Automated Program Repair},
year = {2024},
isbn = {9798400705021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639478.3643114},
doi = {10.1145/3639478.3643114},
abstract = {In the last decade, following current societal needs, software sustainability has emerged as research field [2]. In this paper, we particularly focus on environmental sustainability, defined as "how software product development, maintenance, and use affect energy consumption and the consumption of other natural resources. [...] This dimension is also known as Green Software" [2].},
booktitle = {Proceedings of the 2024 IEEE/ACM 46th International Conference on Software Engineering: Companion Proceedings},
pages = {358–359},
numpages = {2},
location = {Lisbon, Portugal},
series = {ICSE-Companion '24}
}

@article{10.1145/3715751,
author = {Tan, Siwei and Lu, Liqiang and Xiang, Debin and Chu, Tianyao and Lang, Congliang and Chen, Jintao and Hu, Xing and Yin, Jianwei},
title = {HornBro: Homotopy-Like Method for Automated Quantum Program Repair},
year = {2025},
issue_date = {July 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {FSE},
url = {https://doi.org/10.1145/3715751},
doi = {10.1145/3715751},
abstract = {Quantum programs provide exponential speedups compared to classical programs in certain areas, but they also inevitably encounter logical faults. Automatically repairing quantum programs is much more challenging than repairing classical programs due to the non-replicability of data, the vast search space of program inputs, and the new programming paradigm. Existing works based on semantic-based or learning-based program repair techniques are fundamentally limited in repairing efficiency and effectiveness. In this work, we propose HornBro, an efficient framework for automated quantum program repair. The key insight of HornBro lies in the homotopy-like method, which iteratively switches between the classical and quantum parts. This approach allows the repair tasks to be efficiently offloaded to the most suitable platforms, enabling a progressive convergence toward the correct program. We start by designing an implication assertion pragma to enable rigorous specifications of quantum program behavior, which helps to generate a quantum test suite automatically. This suite leverages the orthonormal bases of quantum programs to accommodate different encoding schemes. Given a fixed number of test cases, it allows the maximum input coverage of potential counter-example candidates. Then, we develop a Clifford approximation method with an SMT-based search to transform the patch localization program into a symbolic reasoning problem. Finally, we offload the computationally intensive repair of gate parameters to quantum hardware by leveraging the differentiability of quantum gates. Experiments suggest that HornBro increases the repair success rate by more than 62.5\% compared to the existing repair techniques, supporting more types of quantum bugs. It also achieves 35.7\texttimes{} speedup in the repair and 99.9\% gate reduction of the patch.},
journal = {Proc. ACM Softw. Eng.},
month = jun,
articleno = {FSE034},
numpages = {23},
keywords = {Automated Program Repair, Program Verification, Quantum Computing}
}

@article{10.1145/3763097,
author = {Xie, Linna and Li, Zhong and Pei, Yu and Wen, Zhongzhen and Liu, Kui and Zhang, Tian and Li, Xuandong},
title = {PReMM: LLM-Based Program Repair for Multi-method Bugs via Divide and Conquer},
year = {2025},
issue_date = {October 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {OOPSLA2},
url = {https://doi.org/10.1145/3763097},
doi = {10.1145/3763097},
abstract = {Large-language models (LLMs) have been leveraged to enhance the capability of automated program repair techniques in recent research.   While existing LLM-based program repair techniques compared favorably to other techniques based on heuristics, constraint-solving, and learning in producing high-quality patches,  they mainly target bugs that can be corrected by changing a single faulty method,  which greatly limits the effectiveness of such techniques in repairing bugs that demand patches spanning across multiple methods.   In this work, we propose the PReMM technique to effectively propose patches changing multiple methods.   PReMM builds on three core component techniques:   the faulty method clustering technique to partition the faulty methods into clusters based on the dependence relationship among them,   enabling a divide-and-conquer strategy for the repairing task;   the fault context extraction technique to gather extra information about the fault context which can be utilized to better guide the diagnosis of the fault and the generation of correct patches;   the dual-agent-based patch generation technique that employs two LLM-based agents with different roles to analyze the fault more precisely and generate patches of higher-quality.  We have implemented the PReMM technique into a tool with the same name and applied the tool to repair real-world bugs from datasets Defects4J V1.2 and V2.0.   PReMM produced correct patches for 307 bugs in total.   Compared with ThinkRepair, the state-of-the-art LLM-based program repair technique,  PReMM correctly repaired 102 more bugs, achieving an improvement of 49.8\%.},
journal = {Proc. ACM Program. Lang.},
month = oct,
articleno = {319},
numpages = {29},
keywords = {Automated Program Repair, Context-Aware Repair, Divide and Conquer, Large Language Models, Multi-method Bugs}
}

@article{10.1007/s10115-025-02383-9,
author = {Dikici, Sena and Bilgin, Turgay Tugay},
title = {Advancements in automated program repair: a comprehensive review},
year = {2025},
issue_date = {Jun 2025},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {67},
number = {6},
issn = {0219-1377},
url = {https://doi.org/10.1007/s10115-025-02383-9},
doi = {10.1007/s10115-025-02383-9},
abstract = {This review paper presents a comprehensive examination of automated program repair (APR) and its significant contribution to the field of modern software engineering. It elucidates how APR methodologies markedly mitigate manual debugging needs by automating the detection and resolution of software glitches. The study encompasses an in-depth exploration of three primary categories of APR techniques: template-based, machine learning, and deep learning approaches, drawing from an exhaustive evaluation of 41 APR tools. Each category showcases distinct strategies for managing diverse software errors, underscoring the breadth and effectiveness of current APR methodologies. Template-based APR solutions utilize pre-established patterns to efficiently tackle common coding issues, while machine learning-driven approaches dynamically devise repair strategies from historical bug-fix datasets. Deep learning methods extend error rectification boundaries by delving into the semantic context of code, yielding more precise adjustments. The ongoing advancement of APR technologies necessitates researchers to address critical challenges, including the integration of semantic-syntactic analyses, mitigation of data scarcity, optimization of cross-platform tools, development of context-aware approaches, enhancement of fault localization and patch validation processes, and establishment of standardized performance evaluation metrics. This comprehensive analysis underscores the pivotal role of APR in enhancing software efficiency and reliability, representing significant progress in software development and maintenance practices.},
journal = {Knowl. Inf. Syst.},
month = mar,
pages = {4737–4783},
numpages = {47},
keywords = {Automated program repair, Software bugs, Machine learning, Deep learning, Code patterns}
}

@inbook{10.1109/ICSE55347.2025.00257,
author = {Rinard, Martin C.},
title = {Research in Program Repair and Approximate Computing: A Retrospective},
year = {2025},
isbn = {9798331505691},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE55347.2025.00257},
abstract = {This paper and accompanying talk trace the trajectory of my research in program repair and approximate computing. The prevailing value system in the field at the time focused on program correctness as a fundamental goal. This research, in contrast, was driven by a new perspective that emphasized acceptable (but not necessarily fully correct) survival through errors and the automatic identification and exploitation of performance versus accuracy tradeoff spaces implicitly present in computations coded to operate at only a single point in this space.Because the research challenged the prevailing value system at the time, it met with some skepticism despite empirical results highlighting its effectiveness. The following quote from an anonymous reviewer may give some idea of the reaction:"The basic idea—to assist incorrect programs in their efforts to emit incorrect output—is an abomination and if adopted would likely usher in a new dark age."As the research progressed, we gained a deeper understanding of the reasons behind the surprising — at least to us — phenomena we observed. We were able to formalize this understanding to generate source code patches and obtain performance, accuracy, and acceptability guarantees for computations that leveraged our techniques, bringing the research full circle to once again focus on reasoning statically about program behavior but with different reasoning techniques and guarantees.Finally, I discuss lessons learned and future relevance of the principles, perspectives, and concepts that this research pioneered.},
booktitle = {Proceedings of the IEEE/ACM 47th International Conference on Software Engineering},
pages = {1–15},
numpages = {15}
}

@article{10.1145/3716167,
author = {Le-Cong, Thanh and Nguyen, Dat and Le, Bach and Murray, Toby},
title = {Towards Reliable Evaluation of Neural Program Repair with Natural Robustness Testing},
year = {2025},
issue_date = {September 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {34},
number = {7},
issn = {1049-331X},
url = {https://doi.org/10.1145/3716167},
doi = {10.1145/3716167},
abstract = {Automated program repair (APR) has recently gained ground, with numerous research efforts being conducted in the area that have been adopted in the industry. One notable class of APR is neural program repair (NPR), which typically employs deep learning techniques that are trained on vast amounts of historical data to fix bugs that have not been seen in the past. To study the true effectiveness of NPR on existing limited datasets, recent work augments the evaluation data by employing semantics-preserving transformations to convert original buggy programs to semantically equivalent ones. Experiments show that NPR techniques are not robust; e.g., NPR cannot repair semantically equivalent counterparts of 20\%–35\% of bugs that they can repair in the original dataset. However, we found that many of these transformations are unnatural, that are unlikely to occur in real-world scenarios, leading to misleading conclusions about NPR effectiveness and misguide the improvement on unrobust behaviors, which have minimal real-world impact.In this article, we propose shifting the focus of robustness evaluation for NPR techniques towards naturally occurring data transformations. To accomplish this, we first examine the naturalness of semantic-preserving transformations through a two-stage human study. This study includes: (i) interviews with senior software developers to establish concrete criteria for evaluating the naturalness of these transformations and (ii) a survey involving 10 developers to assess the naturalness of 1,178 transformations, i.e., pairs of original and transformed programs, applied to 225 real-world bugs. Our findings show that only 60\% of these transformations are considered natural, while 20\% are considered unnatural, with strong agreement among the annotators. Moreover, the unnaturalness of these transformations significantly impacts both their applicability to benchmarks and the conclusions drawn from robustness testing.Next, we conduct natural robustness tests on NPR techniques to assess their true effectiveness against real-world data variations. Our experimental results reveal a substantial number of prediction changes in NPR techniques, leading to significant reductions in both plausible and correct patch rates when comparing performance on the original and transformed datasets. Furthermore, we observe notable differences in performance improvements between NPR techniques, suggesting potential biases in the evaluation of NPR introduced by limited datasets. Finally, we explore automating the assessment of transformation naturalness by developing a new naturalness metric, namely RNC, using large language models. This metric effectively evaluates naturalness with an AUC of 0.7, offering a promising direction for automating the naturalness assessment of code transformations.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = aug,
articleno = {213},
numpages = {44},
keywords = {Automated Program Repair, Natural Robustness, Code Naturalness, Code Transformations}
}

@inproceedings{10.1145/3638530.3664174,
author = {Bin Murtaza, Sardar and Mccoy, Aidan and Ren, Zhiyuan and Murphy, Aidan and Banzhaf, Wolfgang},
title = {LLM Fault Localisation within Evolutionary Computation Based Automated Program Repair},
year = {2024},
isbn = {9798400704956},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3638530.3664174},
doi = {10.1145/3638530.3664174},
abstract = {Repairing bugs can be a daunting task for even a human experienced in debugging, so naturally, attempting to automatically repair programs with a computer system is quite challenging. The existing methods of automated program repair leave a lot of room for improvement. Fault localization, which aims to find lines of code that are potentially buggy, minimises the search space of an automated program repair system. Recent work has shown improvement in these fault localization methods, with the use of Large Language Models. Here, we propose a system where a LLM-based fault localization tool, which we call SemiAutoFL, is used within a fully automatic program repair program, ARJA-e. We show that utilising LLM-based fault localization with ARJA-e can significantly improve its performance on real world bugs. ARJA-e with SemiAutoFL can repair 10 bugs that ARJA-e was previously unable to so do. This finding adds to our understanding of how to improve fault localization and automated program repair, highlighting the potential for more efficient and accurate fault localisation methods being applied to automated program repair.},
booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference Companion},
pages = {1824–1829},
numpages = {6},
keywords = {genetic improvement, fault localisation, large language models},
location = {Melbourne, VIC, Australia},
series = {GECCO '24 Companion}
}

@article{10.1145/3729390,
author = {Kong, Jiaolong and Xie, Xiaofei and Liu, Shangqing},
title = {Demystifying Memorization in LLM-Based Program Repair via a General Hypothesis Testing Framework},
year = {2025},
issue_date = {July 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {FSE},
url = {https://doi.org/10.1145/3729390},
doi = {10.1145/3729390},
abstract = {Large Language Models (LLMs) have achieved remarkable success in various applications, particularly in code-related tasks such as code generation and program repair, setting new performance benchmarks. However, the extensive use of large training corpora raises concerns about whether these achievements stem from genuine understanding or mere memorization of training data—a question often overlooked in current research. This paper aims to study the memorization issue within LLM-based program repair by investigating whether the correct patches generated by LLMs are the result of memorization. The key challenge lies in the absence of ground truth for confirming memorization, leading to various ad-hoc methods designed for its detection. To address this challenge, we first propose a general framework that formalizes memorization detection as a general hypothesis testing problem, where existing approaches can be unified by defining a low-probability event under the null hypothesis that the data is not memorized. The occurrence of such an event leads to the rejection of the null hypothesis, indicating potential memorization. Based on this framework, we design two specific methods (i.e., low-probability events) to detect potential memorization: 1) basic ground-truth matching, and 2) reassessment after substantial code mutation. We investigate the memorization issue in LLM-based program repair using two datasets: Defects4J, a widely used benchmark that is likely included in the training data, and GitBug-Java, a new dataset that is unlikely to be part of the training data. Our findings reveal that a significant portion of correct patches exactly match the ground truths in Defects4J (e.g., 78.83\% and 87.42\% on GPT-3.5 and CodeLlama-7b, respectively). Moreover, even after significant modifications to the buggy code, where the original repairs should not be generated, a considerable percentage of bugs (e.g., 81.82\% on GPT-3.5 and 88.24\% on CodeLlama-7b) continue to be fixed exactly as in the original bug fixes, indicating a high likelihood of memorization. Furthermore, we evaluate existing memorization detection methods and demonstrate their ineffectiveness in this context (e.g., most AUROCs are below 0.5). The theoretical analysis under our hypothesis testing framework shows that their defined events may not meet the requirements for being low-probability. The study highlights the critical need for more robust and rigorous evaluations in LLM-based software engineering research, ensuring a clear distinction between true problem-solving capabilities and mere memorization.},
journal = {Proc. ACM Softw. Eng.},
month = jun,
articleno = {FSE120},
numpages = {23},
keywords = {Code Memorization, Program Repair}
}

@inproceedings{10.1145/3626252.3630875,
author = {Ishizue, Ryosuke and Sakamoto, Kazunori and Washizaki, Hironori and Fukazawa, Yoshiaki},
title = {Improved Program Repair Methods using Refactoring with GPT Models},
year = {2024},
isbn = {9798400704239},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3626252.3630875},
doi = {10.1145/3626252.3630875},
abstract = {Teachers often utilize automatic program repair methods to provide feedback on submitted student code using model answer code. A state-of-the-art tool is Refactory, which achieves a high repair success rate and small patch size (less code repair) by refactoring code to expand the variety of correct code samples that can be referenced. However, Refactory has two major limitations. First, it cannot fix code with syntax errors. Second, it has difficulty fixing code when there are few correct submissions. Herein we propose a new method that combines Refactory and OpenAI's GPT models to address these issues and conduct a performance measurement experiment. The experiment uses a dataset consisting of 5 programming assignment problems and almost 1,800 real-life incorrect Python program submissions from 361 students for an introductory programming course at a large public university. The proposed method improves the repair success rate by 1-21\% when the set of correct code samples is sufficient and the patch size is smaller than Refactory alone in 16-45\% of the cases. When there was no set of correct code samples at all (only the model answer code was used as a reference for repair), method improves the repair success rate by 1-43\% and the patch size is smaller than Refactory alone in 42-68\% of the cases.},
booktitle = {Proceedings of the 55th ACM Technical Symposium on Computer Science Education V. 1},
pages = {569–575},
numpages = {7},
keywords = {generative ai, program repair, programming assignment},
location = {Portland, OR, USA},
series = {SIGCSE 2024}
}

@inproceedings{10.1145/3643788.3648019,
author = {Yuan, Yuan},
title = {ARJA-e for the First International Competition on Automated Program Repair},
year = {2024},
isbn = {9798400705779},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643788.3648019},
doi = {10.1145/3643788.3648019},
abstract = {ARJA-e is an enhanced repair system based on ARJA. Here we briefly introduces ARJA-e and report the results of ARJA-e for the First International Competition on Automated Program Repair on two competition tracks, including the AI Generated Code track and the Functional Errors track.},
booktitle = {Proceedings of the 5th ACM/IEEE International Workshop on Automated Program Repair},
pages = {50–52},
numpages = {3},
keywords = {program repair, genetic programming},
location = {Lisbon, Portugal},
series = {APR '24}
}

@inproceedings{10.1145/3756681.3756966,
author = {Vallecillos Ruiz, Fernando and Hort, Max and Moonen, Leon},
title = {The Art of Repair: Optimizing Iterative Program Repair with Instruction-Tuned Models},
year = {2025},
isbn = {9798400713859},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3756681.3756966},
doi = {10.1145/3756681.3756966},
abstract = {Automatic program repair (APR) aims at reducing the manual efforts required to identify and fix errors in source code. Before the rise of Large Language Model (LLM)-based agents, a common strategy was simply to increase the number of generated patches, sometimes to the thousands, which usually yielded better repair results on benchmarks. More recently, self-iterative capabilities enabled LLMs to refine patches over multiple rounds guided by feedback. However, literature often focuses on many iterations and disregards different numbers of outputs.We investigate an APR pipeline that balances these two approaches, the generation of multiple outputs and multiple rounds of iteration, while imposing a limit of 10 total patches per bug. We apply three SOTA instruction-tuned LLMs – DeepSeekCoder-Instruct, Codellama-Instruct, Llama3.1-Instruct – to the APR task. We further fine-tune each model on an APR dataset with three sizes (1K, 30K, 65K) and two techniques (Full Fine-Tuning and LoRA), allowing us to assess their repair capabilities on two APR benchmarks: HumanEval-Java and Defects4J.Our results show that by using only a fraction (&lt;1\%) of the fine-tuning dataset, we can achieve improvements of up to 78\% in the number of plausible patches generated, challenging prior studies that reported limited gains using Full Fine-Tuning. However, we find that exceeding certain thresholds leads to diminishing outcomes, likely due to overfitting. Moreover, we show that base models greatly benefit from creating patches in an iterative fashion rather than generating them all at once. In addition, the benefit of iterative strategies becomes more pronounced in complex benchmarks. Even fine-tuned models, while benefiting less from iterations, still gain advantages, particularly on complex benchmarks. The research underscores the need for balanced APR strategies that combine multi-output generation and iterative refinement.},
booktitle = {Proceedings of the 29th International Conference on Evaluation and Assessment in Software Engineering},
pages = {500–511},
numpages = {12},
keywords = {Automated Program Repair, Software Testing, Software Maintenance, Large Language Models},
location = {
},
series = {EASE '25}
}

@article{10.1145/3631972,
author = {Zirak, Armin and Hemmati, Hadi},
title = {Improving Automated Program Repair with Domain Adaptation},
year = {2024},
issue_date = {March 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {33},
number = {3},
issn = {1049-331X},
url = {https://doi.org/10.1145/3631972},
doi = {10.1145/3631972},
abstract = {Automated Program Repair (APR) is defined as the process of fixing a bug/defect in the source code, by an automated tool. APR tools have recently experienced promising results by leveraging state-of-the-art Neural Language Processing (NLP) techniques. APR tools such as TFix and CodeXGLUE that combine text-to-text transformers with software-specific techniques are outperforming alternatives, these days. However, in most APR studies, the train and test sets are chosen from the same set of projects (i.e., when APR fixes a bug in the test set from project A, the model has already seen example fixed bugs from project A in the training set). In the real world, however, APR models are meant to be generalizable to new and different projects. Therefore, there is a potential threat that reported APR models with high effectiveness perform poorly when the characteristics of the new project or its bugs are different than the training set’s (“Domain Shift”).In this study, we first define the problem of domain shift in automated program repair. Next, we measure the potential damage of domain shift on two recent APR models (TFix and CodeXGLUE). Based on this observation, we then propose a domain adaptation framework that can adapt an APR model for a given target project. We conduct an empirical study with three domain adaptation methods FullFineTuning, TuningWithLightWeightAdapterLayers, and CurriculumLearning and two APR models on 2,672 bugs from 12 projects.The results show that our proposed framework on average can improve the effectiveness of TFix by 13.05\% and CodeXGLUE by 48.78\%, in terms of “Exact Match”. Through experiments, we also show that the framework provides high efficiency and reliability (in terms of “Exposure Bias”). Using synthetic data to domain adapt TFix and CodeXGLUE on the projects with no data (Zero-shot learning), also results in an average improvement of 5.76\% and 17.62\% for TFix and CodeXGLUE, respectively.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = mar,
articleno = {65},
numpages = {43},
keywords = {Automated program repair, deep learning, neural machine translation, transformers, CodeBERT, domain adaptation}
}

@article{10.1007/s10515-025-00492-x,
author = {Cao, Jialun and Li, Meiziniu and Wen, Ming and Cheung, Shing-Chi},
title = {A study on prompt design, advantages and limitations of ChatGPT for deep learning program repair},
year = {2025},
issue_date = {May 2025},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {32},
number = {1},
issn = {0928-8910},
url = {https://doi.org/10.1007/s10515-025-00492-x},
doi = {10.1007/s10515-025-00492-x},
abstract = {The emergence of large language models (LLMs) such as ChatGPT has revolutionized many fields. In particular, recent advances in LLMs have triggered various studies examining the use of these models for software development tasks, such as program repair, code understanding, and code generation. Prior studies have shown the capability of ChatGPT in repairing conventional programs. However, debugging deep learning (DL) programs poses unique challenges since the decision logic is not directly encoded in the source code. This requires LLMs to not only parse the source code syntactically but also understand the intention of DL programs. Therefore, ChatGPT’s capability in repairing DL programs remains unknown. To fill this gap, our study aims to answer three research questions: (1) Can ChatGPT debug DL programs effectively? (2) How can ChatGPT’s repair performance be improved by prompting? (3) In which way can dialogue help facilitate the repair? Our study analyzes the typical information that is useful for prompt design and suggests enhanced prompt templates that are more efficient for repairing DL programs. On top of them, we summarize the dual perspectives (i.e., advantages and disadvantages) of ChatGPT’s ability, such as its handling of API misuse and recommendation, and its shortcomings in identifying default parameters. Our findings indicate that ChatGPT has the potential to repair DL programs effectively and that prompt engineering and dialogue can further improve its performance by providing more code intention. We also identified the key intentions that can enhance ChatGPT’s program repairing capability.},
journal = {Automated Software Engg.},
month = mar,
numpages = {29},
keywords = {Large language model, LLM4SE, Automatic program repair, Deep learning program repair}
}

@proceedings{10.1145/3643788,
title = {APR '24: Proceedings of the 5th ACM/IEEE International Workshop on Automated Program Repair},
year = {2024},
isbn = {9798400705779},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Welcome to the fifth International Workshop on Automated Program Repair (APR 2024), hosted by International Conference on Software Engineering (ICSE) 2024. Since its inception in 2020, APR has become a central event of the program repair community, reflecting a growing interest in the field among the software engineering, programming language, machine learning and formal methods communities.APR 2024 continues the tradition of fostering interaction among researchers in program repair. As always, we are particularly focused on narrowing the divide between academic research and real-world industry applications.},
location = {Lisbon, Portugal}
}

@inproceedings{10.1007/978-3-032-09044-7_9,
author = {Pineda, Moises and Luna, Diego and Esquivel, Mariana and Bours, Jes\'{u}s and Salazar, Juan and Flores-Araiza, Dainel and Hinojosa, Salvador},
title = {Beyond SWE-Bench: A Compiler-Assisted Pipeline for&nbsp;Multi-language Automated Program Repair},
year = {2025},
isbn = {978-3-032-09043-0},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-032-09044-7_9},
doi = {10.1007/978-3-032-09044-7_9},
abstract = {Automated program repair (APR) research predominantly focuses on Python environments, creating significant infrastructure gaps for compiled languages like C, C++, and Java that dominate production systems. We present the first systematic pipeline addressing multi-language APR infrastructure limitations through compiler-assisted dataset curation and paradigm-aware evaluation frameworks. Our approach combines a DFA-based code classification system achieving 92.4\% accuracy in programming paradigm detection with systematic dataset filtering that processes over 3 million samples to extract 30,000 high-quality object-oriented examples. Initial evaluation on Qwen3-14B using LoRA fine-tuning reveals critical adaptation thresholds: effective multi-language adaptation requires modification of approximately 1.2\% or more model parameters, with lighter fine-tuning underperforming baseline models. Our open-source pipeline provides end-to-end infrastructure from compiler-assisted dataset curation to cloud deployment, enabling systematic research advancement in multi-language automated program repair and establishing methodological foundations for compiler-assisted machine learning across diverse programming environments.},
booktitle = {Advances in Soft Computing: 24th Mexican International Conference on Artificial Intelligence, MICAI 2025, Guanajuato, Mexico, November 3, 2025, Proceedings, Part II},
pages = {115–127},
numpages = {13},
keywords = {automated program repair, multi-language systems, dataset curation, compiler techniques, AI for Software Engineering},
location = {Guanajuato, Mexico}
}

@inbook{10.1109/ICSE55347.2025.00030,
author = {Huang, Kai and Zhang, Jian and Meng, Xiangxin and Liu, Yang},
title = {Template-Guided Program Repair in the Era of Large Language Models},
year = {2025},
isbn = {9798331505691},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE55347.2025.00030},
abstract = {Recent advancements in automated program repair (APR) have been significantly driven by the application of Large Language Models (LLMs). In particular, the integration of LLMs with traditional template-based repair methods has demonstrated effective outcomes. Despite this, the synergy between the strengths of traditional methods and LLMs remains underexploited. This oversight originates from the indiscriminate use of templates and their insufficient coverage. Also, using small-scale LLMs within the zero-shot learning context proves to be suboptimal.To alleviate the limitations, we propose NTR (Neural Template Repair), a two-stage repair framework including template selection and patch generation, both of which are under the fine-tuning paradigm. In the template selection phase, we formulate it as a multiclass classification problem and fine-tune million-level LLMs for better selecting possible templates. During the patch generation phase, we leverage the chosen templates as probable directions (e.g., 'Mutate Conditional Expression') to guide the fine-tuning process of LLMs at the billion-level scale for precise patch creation. Moreover, we incorporate a unique template to signify the absence of a suitable template and employ a probability-based prioritization of templates, thereby optimizing patch generation. This framework not only effectively addresses template mismatch issues, but also enables the billion-level LLMs to explore the patch space more efficiently, despite the GPU memory constraints.We evaluate NTR with different foundational models on Defects4J V1.2 and HumanEval-Java, the framework consistently demonstrates significant effectiveness. When utilizing StarCoder as the foundational model for patch generation, NTR fixes 128 and 129 bugs in Defects4J and HumanEval, outperforming the best baseline APR tool by 14 and 59 bugs. With the larger CodeLlama model, the fixed bugs rise to 139 and 136, respectively, exceeding the baseline by 25 and 66 bugs. Notably, the performance stems not only from the foundational models but also benefits greatly from our NTR framework. Specifically, NTR's implementation with StarCoder and CodeLlama leads to 22 and 23 additional fixes, which is beyond what the models achieve on their own. This emphasizes the success of our new perspective on utilizing templates to unlock the bug-fixing potential of LLMs.},
booktitle = {Proceedings of the IEEE/ACM 47th International Conference on Software Engineering},
pages = {1895–1907},
numpages = {13}
}

@inproceedings{10.1145/3568813.3600130,
author = {Koutcheme, Charles and Sarsa, Sami and Leinonen, Juho and Haaranen, Lassi and Hellas, Arto},
title = {Evaluating Distance Measures for Program Repair},
year = {2023},
isbn = {9781450399760},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3568813.3600130},
doi = {10.1145/3568813.3600130},
abstract = {Background and Context: Struggling with programming assignments while learning to program is a common phenomenon in programming courses around the world. Supporting struggling students is a common theme in Computing Education Research (CER), where a wide variety of support methods have been created and evaluated. An important stream of research here focuses on program repair, where methods for automatically fixing erroneous code are used for supporting students as they debug their code. Work in this area has so far assessed the performance of the methods by evaluating the closeness of the proposed fixes to the original erroneous code. The evaluations have mainly relied on the use of edit distance measures such as the sequence edit distance and there is a lack of research on which distance measure is the most appropriate. Objectives: Provide insight into measures for quantifying the distance between erroneous code written by a student and a proposed change. We conduct the evaluation in an introductory programming context, where insight into the distance measures can provide help in choosing a suitable metric that can inform which fixes should be suggested to novices. Method: A team of five experts annotated a subset of the Dublin dataset, creating solutions for over a thousand erroneous programs written by students. We evaluated how the prominent edit distance measures from the CER literature compare against measures used in Natural Language Processing (NLP) tasks for retrieving the experts’ solutions from a pool of proposed solutions. We also evaluated how the expert-generated solutions compare against the solutions proposed by common program repair algorithms. The annotated dataset and the evaluation code are published as part of the work. Findings: Our results highlight that the ROUGE score, classically used for evaluating the performance of machine summarization tasks, performs well as an evaluation and selection metric for program repair. We also highlight the practical utility of NLP metrics, which allow an easier interpretation and comparison of the performance of repair techniques when compared to the classic methods used in the CER literature. Implications: Our study highlights the variety of distance metrics used for comparing source codes. We find issues with the classically used distance measures that can be combated by using NLP metrics. Based on our findings, we recommend including NLP metrics, and in particular, the ROUGE metric, in evaluations when considering new program repair methodologies. We also suggest incorporating NLP metrics into other areas where source codes are compared, including plagiarism detection.},
booktitle = {Proceedings of the 2023 ACM Conference on International Computing Education Research - Volume 1},
pages = {495–507},
numpages = {13},
keywords = {BLEU, ROUGE, automated program repair, automatic program repair, bug fixing, computing education, dataset, distance measures, distance metrics, educational data mining, feedback, natural language processing, program repair},
location = {Chicago, IL, USA},
series = {ICER '23}
}

@inproceedings{10.1145/3524459.3527347,
author = {Ribeiro, Francisco and Abreu, Rui and Saraiva, Jo\~{a}o},
title = {Framing program repair as code completion},
year = {2022},
isbn = {9781450392853},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3524459.3527347},
doi = {10.1145/3524459.3527347},
abstract = {Many techniques have contributed to the advancement of automated program repair, such as: generate and validate approaches, constraint-based solvers and even neural machine translation. Simultaneously, artificial intelligence has allowed the creation of general-purpose pre-trained models that support several down-stream tasks. In this paper, we describe a technique that takes advantage of a generative model --- CodeGPT --- to automatically repair buggy programs by making use of its code completion capabilities. We also elaborate on where to perform code completion in a buggy line and how we circumvent the open-ended nature of code generation to appropriately fit the new code in the original program. Furthermore, we validate our approach on the ManySStuBs4J dataset containing real-world open-source projects and show that our tool is able to fix 1739 programs out of 6415 --- a 27\% repair rate. The repaired programs range from single-line changes to multiple line modifications. In fact, our technique is able to fix programs which were missing relatively complex expressions prior to being analyzed. In the end, we present case studies that showcase different scenarios our technique was able to handle.},
booktitle = {Proceedings of the Third International Workshop on Automated Program Repair},
pages = {38–45},
numpages = {8},
keywords = {program repair, code generation, code completion},
location = {Pittsburgh, Pennsylvania},
series = {APR '22}
}

@inproceedings{10.1145/3650212.3680323,
author = {Xia, Chunqiu Steven and Zhang, Lingming},
title = {Automated Program Repair via Conversation: Fixing 162 out of 337 Bugs for $0.42 Each using ChatGPT},
year = {2024},
isbn = {9798400706127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3650212.3680323},
doi = {10.1145/3650212.3680323},
abstract = {Automated Program Repair (APR) aims to automatically generate patches for buggy programs. Traditional APR techniques suffer from a lack of patch variety as they rely heavily on handcrafted or mined bug fixing patterns and cannot easily generalize to other bug/fix types. To address this limitation, recent APR work has been focused on leveraging modern Large Language Models (LLMs) to directly generate patches for APR. Such LLM-based APR tools work by first constructing an input prompt built using the original buggy code and then querying the LLM to either fill-in (cloze-style APR) the correct code at the bug location or to produce a completely new code snippet as the patch. While the LLM-based APR tools are able to achieve state-of-the-art results, they still follow the classic Generate and Validate (GV) repair paradigm of first generating lots of patches by sampling from the same initial prompt and then validating each one afterwards. This not only leads to many repeated patches that are incorrect, but also misses the crucial and yet previously ignored information in test failures as well as in plausible patches.        To address these aforementioned limitations, we propose ChatRepair, the first fully automated conversation-driven APR approach that interleaves patch generation with instant feedback to perform APR in a conversational style. ChatRepair first feeds the LLM with relevant test failure information to start with, and then learns from both failures and successes of earlier patching attempts of the same bug for more powerful APR. For earlier patches that failed to pass all tests, we combine the incorrect patches with their corresponding relevant test failure information to construct a new prompt for the LLM to generate the next patch. In this way, we can avoid making the same    mistakes. For earlier patches that passed all the tests (i.e., plausible patches), we further ask the LLM to generate alternative variations of the original plausible patches. In this way, we can further build on and learn from earlier successes to generate more plausible patches to increase the chance of having correct patches. While our approach is general, we implement ChatRepair using state-of-the-art dialogue-based LLM – ChatGPT. Our evaluation on the widely studied Defects4j dataset shows that ChatRepair is able to achieve the new state-of-the-art in repair performance, achieving 114 and 48 correct fixes on Defects4j 1.2 and 2.0 respectively. By calculating the cost    of accessing ChatGPT, we can fix 162 out of 337 bugs for $0.42 each!},
booktitle = {Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {819–831},
numpages = {13},
keywords = {Automated Program Repair, Large Language Model},
location = {Vienna, Austria},
series = {ISSTA 2024}
}

@article{10.1145/3704997,
author = {Renzullo, Joseph and Reiter, Pemma and Weimer, Westley and Forrest, Stephanie},
title = {Automated Program Repair: Emerging Trends Pose and Expose Problems for Benchmarks},
year = {2025},
issue_date = {August 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {57},
number = {8},
issn = {0360-0300},
url = {https://doi.org/10.1145/3704997},
doi = {10.1145/3704997},
abstract = {Machine learning (ML) pervades the field of Automated Program Repair (APR). Algorithms deploy neural machine translation and large language models (LLMs) to generate software patches, among other tasks. But, there are important differences between these applications of ML and earlier work, which complicates the task of ensuring that results are valid and likely to generalize. A challenge is that the most popular APR evaluation benchmarks were not designed with ML techniques in mind. This is especially true for LLMs, whose large and often poorly-disclosed training datasets may include problems on which they are evaluated.This article reviews work in APR published in the field’s top five venues since 2018, emphasizing emerging trends in the field, including the dramatic rise of ML models, including LLMs. ML-based articles are categorized along structural and functional dimensions, and a variety of issues are identified that these new methods raise. Importantly, data leakage and contamination concerns arise from the challenge of validating ML-based APR using existing benchmarks, which were designed before these techniques were popular. We discuss inconsistencies in evaluation design and performance reporting and offer pointers to solutions where they are available. Finally, we highlight promising new directions that the field is already taking.},
journal = {ACM Comput. Surv.},
month = mar,
articleno = {208},
numpages = {18},
keywords = {automated program repair, machine learning, benchmarks, patch quality}
}

@inproceedings{10.1145/3597926.3598101,
author = {Kim, YoungJae and Han, Seungheon and Khamit, Askar Yeltayuly and Yi, Jooyong},
title = {Automated Program Repair from Fuzzing Perspective},
year = {2023},
isbn = {9798400702211},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597926.3598101},
doi = {10.1145/3597926.3598101},
abstract = {In this work, we present a novel approach that connects two closely-related topics: fuzzing and automated program repair (APR). The paper is divided into two parts. In the first part, we describe the similarities between fuzzing and APR both of which can be viewed as a search problem. In the second part, we introduce a new patch-scheduling algorithm called Casino, which is designed from a fuzzing perspective to enhance search efficiency. Our experiments demonstrate that Casino outperforms existing algorithms. We also promote open science by sharing SimAPR, a simulation tool that can be used to evaluate new patch-scheduling algorithms.},
booktitle = {Proceedings of the 32nd ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {854–866},
numpages = {13},
keywords = {Patch Scheduling, Multi-Armed Bandit, Fuzzing, Automated Program Repair},
location = {Seattle, WA, USA},
series = {ISSTA 2023}
}

@inproceedings{10.1145/3618305.3623587,
author = {Ribeiro, Francisco},
title = {Large Language Models for Automated Program Repair},
year = {2023},
isbn = {9798400703843},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3618305.3623587},
doi = {10.1145/3618305.3623587},
abstract = {This paper introduces two methods for automated program repair (APR) utilizing pre-trained language models. The first method demonstrates program repair as a code completion task and is validated on a dataset of Java programs. The second method, Mentat, leverages OCaml’s parser and type system as fault localization techniques to generate prompts for GPT-3, producing candidate patches. Evaluation results show promising repair rates, with 27\% and 39.2\% effectiveness, respectively. For OCaml, a comparative study employing an automated validation strategy is presented in which the technique outperforms other tools. Language models are effective at APR, enhancing bug fixing and freeing developers to focus on other critical aspects of software engineering.},
booktitle = {Companion Proceedings of the 2023 ACM SIGPLAN International Conference on Systems, Programming, Languages, and Applications: Software for Humanity},
pages = {7–9},
numpages = {3},
keywords = {type systems, fault localization, code generation, automated program repair},
location = {Cascais, Portugal},
series = {SPLASH 2023}
}

@inproceedings{10.1145/3611643.3613892,
author = {Jin, Matthew and Shahriar, Syed and Tufano, Michele and Shi, Xin and Lu, Shuai and Sundaresan, Neel and Svyatkovskiy, Alexey},
title = {InferFix: End-to-End Program Repair with LLMs},
year = {2023},
isbn = {9798400703270},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3611643.3613892},
doi = {10.1145/3611643.3613892},
abstract = {Software development life cycle is profoundly influenced by bugs; their introduction, identification, and eventual resolution account for a significant portion of software development cost. This has motivated software engineering researchers and practitioners to propose different approaches for automating the identification and repair of software defects. Large Language Models (LLMs) have been adapted to the program repair task through few-shot demonstration learning and instruction prompting, treating this as an infilling task. However, these models have only focused on learning general bug-fixing patterns for uncategorized bugs mined from public repositories. In this paper, we propose : a transformer-based program repair framework paired with a state-of-the-art static analyzer to fix critical security and performance bugs.  combines a Retriever – transformer encoder model pretrained via contrastive learning objective, which aims at searching for semantically equivalent bugs and corresponding fixes; and a Generator – an LLM (12 billion parameter Codex Cushman model) finetuned on supervised bug-fix data with prompts augmented via adding bug type annotations and semantically similar fixes retrieved from an external non-parametric memory. To train and evaluate our approach, we curated , a novel, metadata-rich dataset of bugs extracted by executing the Infer static analyzer on the change histories of thousands of Java and C# repositories. Our evaluation demonstrates that  outperforms strong LLM baselines, with a top-1 accuracy of 65.6\% for generating fixes in C# and 76.8\% in Java. We discuss the deployment of alongside Infer at Microsoft which offers an end-to-end solution for detection, classification, and localization of bugs, as well as fixing and validation of candidate patches, integrated in the continuous integration (CI) pipeline to automate the software development workflow.},
booktitle = {Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {1646–1656},
numpages = {11},
keywords = {Program repair, finetuning, prompt augmentation, static analyses},
location = {San Francisco, CA, USA},
series = {ESEC/FSE 2023}
}

@inproceedings{10.1109/ICSE48619.2023.00126,
author = {Zhu, Qihao and Sun, Zeyu and Zhang, Wenjie and Xiong, Yingfei and Zhang, Lu},
title = {Tare: Type-Aware Neural Program Repair},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00126},
doi = {10.1109/ICSE48619.2023.00126},
abstract = {Automated program repair (APR) aims to reduce the effort of software development. With the development of deep learning, lots of DL-based APR approaches have been proposed using an encoder-decoder architecture. Despite the promising performance, these models share the same limitation: generating lots of untypable patches. The main reason for this phenomenon is that the existing models do not consider the constraints of code captured by a set of typing rules.In this paper, we propose, Tare, a type-aware model for neural program repair to learn the typing rules. To encode an individual typing rule, we introduce three novel components: (1) a novel type of grammars, T-Grammar, that integrates the type information into a standard grammar, (2) a novel representation of code, T-Graph, that integrates the key information needed for type checking an AST, and (3) a novel type-aware neural program repair approach, Tare, that encodes the T-Graph and generates the patches guided by T-Grammar.The experiment was conducted on three benchmarks, 393 bugs from Defects4J v1.2, 444 additional bugs from Defects4J v2.0, and 40 bugs from QuixBugs. Our results show that Tare repairs 62, 32, and 27 bugs on these benchmarks respectively, and outperforms the existing APR approaches on all benchmarks. Further analysis also shows that Tare tends to generate more compilable patches than the existing DL-based APR approaches with the typing rule information.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {1443–1445},
numpages = {3},
keywords = {neural networks, program repair},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@article{10.1145/3696450,
author = {Huang, Kai and Xu, Zhengzi and Yang, Su and Sun, Hongyu and Li, Xuejun and Yan, Zheng and Zhang, Yuqing},
title = {Evolving Paradigms in Automated Program Repair: Taxonomy, Challenges, and Opportunities},
year = {2024},
issue_date = {February 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {57},
number = {2},
issn = {0360-0300},
url = {https://doi.org/10.1145/3696450},
doi = {10.1145/3696450},
abstract = {With the rapid development and large-scale popularity of program software, modern society increasingly relies on software systems. However, the problems exposed by software have also come to the fore. The software bug has become an important factor troubling developers. In this context, Automated Program Repair (APR) techniques have emerged, aiming to automatically fix software bug problems and reduce manual debugging work. In particular, benefiting from the advances in deep learning, numerous learning-based APR techniques have emerged in recent years, which also bring new opportunities for APR research. To give researchers a quick overview of APR techniques’ complete development and future opportunities, we review the evolution of APR techniques and discuss in depth the latest advances in APR research. In this article, the development of APR techniques is introduced in terms of four different patch generation schemes: search-based, constraint-based, template-based, and learning-based. Moreover, we propose a uniform set of criteria to review and compare each APR tool and then discuss the current state of APR development. Finally, we analyze current challenges and future directions, especially highlighting the critical opportunities that large language models bring to APR research.},
journal = {ACM Comput. Surv.},
month = oct,
articleno = {36},
numpages = {43},
keywords = {Automated program repair}
}

@article{10.1145/3688834,
author = {Zhong, Wenkang and Li, Chuanyi and Liu, Kui and Ge, Jidong and Luo, Bin and Bissyand\'{e}, Tegawend\'{e} F. and Ng, Vincent},
title = {Benchmarking and Categorizing the Performance of Neural Program Repair Systems for Java},
year = {2025},
issue_date = {January 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {34},
number = {1},
issn = {1049-331X},
url = {https://doi.org/10.1145/3688834},
doi = {10.1145/3688834},
abstract = {Recent years have seen a rise in Neural Program Repair (NPR) systems in the software engineering community, which adopt advanced deep learning techniques to automatically fix bugs. Having a comprehensive understanding of existing systems can facilitate new improvements in this area and provide practical instructions for users. However, we observe two potential weaknesses in the current evaluation of NPR systems: ① published systems are trained with varying data, and ② NPR systems are roughly evaluated through the number of totally fixed bugs. Questions such as what types of bugs are repairable for current systems cannot be answered yet. Consequently, researchers cannot make target improvements in this area and users have no idea of the real affair of existing systems. In this article, we perform a systematic evaluation of the existing nine state-of-the-art NPR systems. To perform a fair and detailed comparison, we (1) build a new benchmark and framework that supports training and validating the nine systems with unified data and (2) evaluate re-trained systems with detailed performance analysis, especially on the effectiveness and the efficiency. We believe our benchmark tool and evaluation results could offer practitioners the real affairs of current NPR systems and the implications of further facilitating the improvements of NPR.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = dec,
articleno = {11},
numpages = {35},
keywords = {datasets, program repair, benchmark, empirical study}
}

@inproceedings{10.1145/3643795.3648390,
author = {Jiang, Shengbei and Zhang, Jiabao and Chen, Wei and Wang, Bo and Zhou, Jianyi and Zhang, Jie},
title = {Evaluating Fault Localization and Program Repair Capabilities of Existing Closed-Source General-Purpose LLMs},
year = {2024},
isbn = {9798400705793},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643795.3648390},
doi = {10.1145/3643795.3648390},
abstract = {Automated debugging is an emerging research field that aims to automatically find and repair bugs. In this field, Fault Localization (FL) and Automated Program Repair (APR) gain the most research efforts. Most recently, researchers have adopted pre-trained Large Language Models (LLMs) to facilitate FL and APR and their results are promising. However, the LLMs they used either vanished (such as Codex) or outdated (such as early versions of GPT). In this paper, we evaluate the performance of recent commercial closed-source general-purpose LLMs on FL and APR, i.e., ChatGPT 3.5, ERNIE Bot 3.5, and IFlytek Spark 2.0. We select three popular LLMs and evaluate them on 120 real-world Java bugs from the benchmark Defects4J. For FL and APR, we designed three kinds of prompts for each, considering different kinds of information. The results show that these LLMs could successfully locate 53.3\% and correctly fix 12.5\% of these bugs.},
booktitle = {Proceedings of the 1st International Workshop on Large Language Models for Code},
pages = {75–78},
numpages = {4},
keywords = {large language model, fault localization, program repair, software debugging},
location = {Lisbon, Portugal},
series = {LLM4Code '24}
}

@article{10.1145/3703408,
author = {Murali, Pavithra Sripathanallur},
title = {Leveraging Large Language Models for Automated Program Repair in Programming Education},
year = {2025},
issue_date = {Winter 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {31},
number = {2},
issn = {1528-4972},
url = {https://doi.org/10.1145/3703408},
doi = {10.1145/3703408},
journal = {XRDS},
month = jan,
pages = {58–60},
numpages = {3}
}

@inproceedings{10.1145/3691620.3695602,
author = {Kim, YoungJae and Park, Yechan and Han, Seungheon and Yi, Jooyong},
title = {Enhancing the Efficiency of Automated Program Repair via Greybox Analysis},
year = {2024},
isbn = {9798400712487},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3691620.3695602},
doi = {10.1145/3691620.3695602},
abstract = {In this paper, we pay attention to the efficiency of automated program repair (APR). Recently, an efficient patch scheduling algorithm, Casino, has been proposed to improve APR efficiency. Inspired by fuzzing, Casino adaptively chooses the next patch candidate to evaluate based on the results of previous evaluations. However, we observe that Casino utilizes only the test results, treating the patched program as a black box. Inspired by greybox fuzzing, we propose a novel patch-scheduling algorithm, Gresino, which leverages the internal state of the program to further enhance APR efficiency. Specifically, Gresino monitors the hit counts of branches observed during the execution of the program and uses them to guide the search for a valid patch. Our experimental evaluation on the Defects4J benchmark and eight APR tools demonstrates the efficacy of our approach.},
booktitle = {Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering},
pages = {1719–1731},
numpages = {13},
keywords = {automated program repair, patch scheduling, greybox analysis},
location = {Sacramento, CA, USA},
series = {ASE '24}
}


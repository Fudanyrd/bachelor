@comment{
benchmarks and datasets for Automated Program Repair
Added by snowballing: all (except gitbugactions, defects4ds, and megavul)
}

@inproceedings{defects4j,
  title={Defects4J: A database of existing faults to enable controlled testing studies for Java programs},
  author={Just, Ren{\'e} and Jalali, Darioush and Ernst, Michael D},
  booktitle={Proceedings of the 2014 International Symposium on Software Testing and Analysis},
  pages={437--440},
  url={https://dl.acm.org/doi/pdf/10.1145/2610384.2628055},
  keywords={Software Engineering, Testing and Debugging},
  year={2014}
}

@article{swebench,
  title={Swe-bench: Can language models resolve real-world github issues?},
  author={Jimenez, Carlos E and Yang, John and Wettig, Alexander and Yao, Shunyu and Pei, Kexin and Press, Ofir and Narasimhan, Karthik},
  journal={arXiv preprint arXiv:2310.06770},
  url={https://arxiv.org/abs/2310.06770},
  abstract={Language models have outpaced our ability to evaluate them effectively, but for their future development it is essential to study the frontier of their capabilities. We find real-world software engineering to be a rich, sustainable, and challenging testbed for evaluating the next generation of language models. To this end, we introduce SWE-bench, an evaluation framework consisting of 2,294 software engineering problems drawn from real GitHub issues and corresponding pull requests across 12 popular Python repositories. Given a codebase along with a description of an issue to be resolved, a language model is tasked with editing the codebase to address the issue. Resolving issues in SWE-bench frequently requires understanding and coordinating changes across multiple functions, classes, and even files simultaneously, calling for models to interact with execution environments, process extremely long contexts and perform complex reasoning that goes far beyond traditional code generation tasks. Our evaluations show that both state-of-the-art proprietary models and our fine-tuned model SWE-Llama can resolve only the simplest issues. The best-performing model, Claude 2, is able to solve a mere 1.96% of the issues. Advances on SWE-bench represent steps towards LMs that are more practical, intelligent, and autonomous.},
  year={2023}
}

@misc{codeagentbench,
  title={CodeAgent: Enhancing Code Generation with Tool-Integrated Agent Systems for Real-World Repo-level Coding Challenges}, 
  author={Kechi Zhang and Jia Li and Ge Li and Xianjie Shi and Zhi Jin},
  year={2024},
  eprint={2401.07339},
  archivePrefix={arXiv},
  primaryClass={cs.SE},
  url={https://arxiv.org/abs/2401.07339},
  abstract={Large Language Models (LLMs) have shown promise in automated code generation but typically excel only in simpler tasks such as generating standalone code units. Real-world software development, however, often involves complex code repositories (named repo) with complex dependencies and extensive documentation. To fill this gap, our research pivots towards evaluating LLMs in a more realistic setting -- real-world repo-level code generation. We introduce CodeAgentBench, a manually curated benchmark for repo-level code generation. This benchmark comprises five high-quality Python projects, encompassing a total of 101 samples. We assess nine leading LLMs on repo-level tasks and observe a decline in their performance. To tackle this, we present CodeAgent, a novel LLM-based agent framework that employs external tools for effective repo-level code generation. CodeAgent integrates five programming tools, enabling interaction with software artifacts for information retrieval, code symbol navigation, and code testing. We implement four agent strategies to optimize these tools' usage. Our experiments on CodeAgentBench show that CodeAgent enhances LLM performance significantly, with improvements ranging from 18.1\% to 250\%. Further tests on the HumanEval benchmark confirm CodeAgent's adaptability and efficacy across various code generation tasks. Notably, CodeAgent outperforms commercial products like Github Copilot, showcasing superior accuracy and efficiency. These results demonstrate CodeAgent's robust capabilities in code generation, highlighting its potential for real-world repo-level coding challenges.},
}

@article{manybugs,
  author={Le Goues, Claire and Holtschulte, Neal and Smith, Edward K. and Brun, Yuriy and Devanbu, Premkumar and Forrest, Stephanie and Weimer, Westley},
  journal={IEEE Transactions on Software Engineering}, 
  title={The ManyBugs and IntroClass Benchmarks for Automated Repair of C Programs}, 
  year={2015},
  abstract={The field of automated software repair lacks a set of common benchmark problems. Although benchmark sets are used widely throughout computer science, existing benchmarks are not easily adapted to the problem of automatic defect repair, which has several special requirements. Most important of these is the need for benchmark programs with reproducible, important defects and a deterministic method for assessing if those defects have been repaired. This article details the need for a new set of benchmarks, outlines requirements, and then presents two datasets, ManyBugs and IntroClass, consisting between them of 1,183 defects in 15 C programs. Each dataset is designed to support the comparative evaluation of automatic repair algorithms asking a variety of experimental questions. The datasets have empirically defined guarantees of reproducibility and benchmark quality, and each study object is categorized to facilitate qualitative evaluation and comparisons by category of bug or program. The article presents baseline experimental results on both datasets for three existing repair methods, GenProg, AE, and TrpAutoRepair, to reduce the burden on researchers who adopt these datasets for their own comparative evaluations.},
  volume={41},
  url={https://ieeexplore.ieee.org/ielaam/32/7349123/7153570-aam.pdf},
  number={12},
  pages={1236-1256}
}

@inproceedings{jsbugs,
  title={Discovering bug patterns in JavaScript},
  author={Hanam, Quinn and Brito, Fernando S de M and Mesbah, Ali},
  booktitle={Proceedings of the 2016 24th ACM SIGSOFT international symposium on foundations of software engineering},
  pages={144--156},
  abstract={JavaScript has become the most popular language used by
developers for client and server side programming. The lan guage, however, still lacks proper support in the form of warnings about potential bugs in the code. Most bug find ing tools in use today cover bug patterns that are discov ered by reading best practices or through developer intu ition and anecdotal observation. As such, it is still unclear which bugs happen frequently in practice and which are important for developers to be fixed. We propose a novel semi-automatic technique, called BugAID, for discovering the most prevalent and detectable bug patterns. BugAID is based on unsupervised machine learning using language construct-based changes distilled from AST differencing of bug fixes in the code. We present a large-scale study of common bug patterns by mining 105K commits from 134 server-side JavaScript projects. We discover 219 bug fixing change types and discuss 13 pervasive bug patterns that oc cur across multiple projects and can likely be prevented with better tool support. Our findings are useful for improving tools and techniques to prevent common bugs in JavaScript, guiding tool integration for IDEs, and making developers aware of common mistakes involved with programming in JavaScript.},
  url={https://dl.acm.org/doi/pdf/10.1145/2950290.2950308},
  year={2016}
}

@article{quixbugs,
  author = {Lin, Derrick and Koppel, James and Chen, Angela and Solar-Lezama, Armando},
  title = {QuixBugs: A Multi-Lingual Program Repair Benchmark Set Based on the Quixey Challenge},
  year = {2017},
  isbn = {9781450355148},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  pages = {55–56},
  numpages = {2},
  keywords = {benchmark, automated program repair},
  location = {Vancouver, BC, Canada},
  url={https://dl.acm.org/doi/pdf/10.1145/3135932.3135941},
  abstract={Recent years have seen an explosion of work in automated programrepair. While previous work has focusedexclusively on tools for single languages, recent work in multi-language transformation has opened the door for multi-language pro gram repair tools. Evaluating the performance of such a tool requires having a benchmark set of similar buggy programs in different languages. We present QuixBugs, consisting of 40 programs translated to both Python and Java, each with a bug on asingle line. The QuixBugs benchmark suite is based on problems from the Quixey Challenge, where program mers were given a short buggy program and 1 minute to fix the bug.},
  series = {SPLASH Companion 2017}
}

@inproceedings{cvedataset,
  title={A C/C++ code vulnerability dataset with code changes and CVE summaries},
  author={Fan, Jiahao and Li, Yi and Wang, Shaohua and Nguyen, Tien N},
  booktitle={Proceedings of the 17th international conference on mining software repositories},
  pages={508--512},
  url={https://dl.acm.org/doi/pdf/10.1145/3379597.3387501},
  abstract={We collected a large C/C++ code vulnerability dataset from open-source Github projects, namely Big-Vul. We crawled the public Common Vulnerabilities and Exposures (CVE) database and CVE-related source code repositories. Specifically, we collected the descriptive information of the vulnerabilities from the CVE database, e.g., CVE IDs, CVE severity scores, and CVE summaries. With the CVE information and its related published Github code repository links, we downloaded all of the code repositories and extracted vulnerability related code changes. In total, Big-Vul contains 3,754 code vulnerabilities spanning 91 different vulnerability types. All these code vulnerabilities are extracted from 348 Github projects. All information is stored in the CSV format. We linked the code changes with the CVE descriptive information. Thus, our Big-Vul can be used for various research topics, e.g., detecting and fixing vulnerabilities, analyzing the vulnerability related code changes. Big-Vul is publicly available on Github.},
  year={2020}
}

@inbook{bugsinpy,
  author = {Widyasari, Ratnadira and Sim, Sheng Qin and Lok, Camellia and Qi, Haodi and Phan, Jack and Tay, Qijin and Tan, Constance and Wee, Fiona and Tan, Jodie Ethelda and Yieh, Yuheng and Goh, Brian and Thung, Ferdian and Kang, Hong Jin and Hoang, Thong and Lo, David and Ouh, Eng Lieh},
  title = {BugsInPy: A Database of Existing Bugs in Python Programs to Enable Controlled Testing and Debugging Studies},
  year = {2020},
  isbn = {9781450370431},
  abstract={The 2019 edition of Stack Overflow developer survey highlights that, for the first time, Python outperformed Java in terms of popularity. The gap between Python and Java further widened in the 2020 edition of the survey. Unfortunately, despite the rapid increase in Python's popularity, there are not many testing and debugging tools that are designed for Python. This is in stark contrast with the abundance of testing and debugging tools for Java. Thus, there is a need to push research on tools that can help Python developers.
One factor that contributed to the rapid growth of Java testing and debugging tools is the availability of benchmarks. A popular benchmark is the Defects4J benchmark; its initial version contained 357 real bugs from 5 real-world Java programs. Each bug comes with a test suite that can expose the bug. Defects4J has been used by hundreds of testing and debugging studies and has helped to push the frontier of research in these directions.
In this project, inspired by Defects4J, we create another benchmark database and tool that contain 493 real bugs from 17 real-world Python programs. We hope our benchmark can help catalyze future work on testing and debugging tools that work on Python programs.},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  url={https://dl.acm.org/doi/pdf/10.1145/3368089.3417943},
  pages = {1556–1560},
  numpages = {5}
}

@inproceedings{ferenc2018unified,
  title={A public unified bug dataset for java},
  author={Ferenc, Rudolf and T{\'o}th, Zolt{\'a}n and Lad{\'a}nyi, Gergely and Siket, Istv{\'a}n and Gyim{\'o}thy, Tibor},
  booktitle={Proceedings of the 14th international conference on predictive models and data analytics in software engineering},
  pages={12--21},
  url={https://dl.acm.org/doi/pdf/10.1145/3273934.3273936},
  year={2018}
}

@inproceedings{manysstubs4j,
  title={How often do single-statement bugs occur? the manysstubs4j dataset},
  author={Karampatsis, Rafael-Michael and Sutton, Charles},
  booktitle={Proceedings of the 17th international conference on mining software repositories},
  url={https://dl.acm.org/doi/pdf/10.1145/3379597.3387491},
  pages={573--577},
  year={2020}
}

@inproceedings{cvefixes,
  title={CVEfixes: automated collection of vulnerabilities and their fixes from open-source software},
  author={Bhandari, Guru and Naseer, Amara and Moonen, Leon},
  booktitle={Proceedings of the 17th International Conference on Predictive Models and Data Analytics in Software Engineering},
  url={https://dl.acm.org/doi/pdf/10.1145/3475960.3475985},
  pages={30--39},
  year={2021}
}

@inproceedings{repobugs,
  author={Chen, Yuxiao and Wu, Jingzheng and Ling, Xiang and Li, Changjiang and Rui, Zhiqing and Luo, Tianyue and Wu, Yanjun},
  booktitle={2024 IEEE/ACM 46th International Conference on Software Engineering: Companion Proceedings (ICSE-Companion)}, 
  title={When Large Language Models Confront Repository-Level Automatic Program Repair: How Well They Done?}, 
  year={2024},
  url={https://ieeexplore.ieee.org/document/10554921},
  volume={},
  number={},
  pages={459-471},
  keywords={Analytical models;Codes;Accuracy;Computer bugs;Maintenance engineering;Benchmark testing;Task analysis;Large language models;automatic program repair;repository-level bugs;context;static analysis},
  doi={10.1145/3639478.3647633},
  abstract={In recent years, large language models (LLMs) have demonstrated substantial potential in addressing automatic program repair (APR) tasks. However, the current evaluation of these models for APR tasks focuses solely on the limited context of the single function or file where the bug is located, overlooking the valuable information in the repository-level context. This paper investigates the performance of popular LLMs in handling repository-level repair tasks. We introduce RepoBugs, a new benchmark comprising 124 typical repository-level bugs from open-source repositories. Preliminary experiments using GPT3.5 based on the function where the error is located, reveal that the repair rate on RepoBugs is only 22.58%, significantly diverging from the performance of GPT3.5 on function-level bugs in related studies. This underscores the importance of providing repository-level context when addressing bugs at this level. However, the repository-level context offered by the preliminary method often proves redundant and imprecise and easily exceeds the prompt length limit of LLMs. To solve the problem, we propose a simple and universal repository-level context extraction method (RLCE) designed to provide more precise context for repository-level code repair tasks. Evaluations of three mainstream LLMs show that RLCE significantly enhances the ability to repair repository-level bugs. The improvement reaches a maximum of 160% compared to the preliminary method. Additionally, we conduct a comprehensive analysis of the effectiveness and limitations of RLCE, along with the capacity of LLMs to address repository-level bugs, offering valuable insights for future research.},
}

@inproceedings{megavul,
author = {Ni, Chao and Shen, Liyu and Yang, Xiaohu and Zhu, Yan and Wang, Shaohua},
title = {MegaVul: A C/C++ Vulnerability Dataset with Comprehensive Code Representations},
year = {2024},
isbn = {9798400705878},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://dl.acm.org/doi/pdf/10.1145/3643991.3644886},
doi = {10.1145/3643991.3644886},
abstract = {We constructed a newly large-scale and comprehensive C/C++ vulnerability dataset named MegaVul by crawling the Common Vulnerabilities and Exposures (CVE) database and CVE-related open-source projects. Specifically, we collected all crawlable descriptive information of the vulnerabilities from the CVE database and extracted all vulnerability-related code changes from 28 Git-based websites. We adopt advanced tools to ensure the extracted code integrality and enrich the code with four different transformed representations. Totally, MegaVul contains 17,380 vulnerabilities collected from 992 open-source repositories spanning 169 different vulnerability types disclosed from January 2006 to October 2023. Thus, MegaVul can be used for a variety of software security-related tasks including detecting vulnerabilities and assessing vulnerability severity. All information is stored in the JSON format for easy usage. MegaVul is publicly available on GitHub and will be continuously updated. It can be easily extended to other programming languages.},
booktitle = {Proceedings of the 21st International Conference on Mining Software Repositories},
pages = {738–742},
numpages = {5},
keywords = {common vulnerabilities and exposures, C/C++ code, code representation},
location = {Lisbon, Portugal},
series = {MSR '24}
}

@inproceedings{vul4j,
author = {Bui, Quang-Cuong and Scandariato, Riccardo and Ferreyra, Nicol\'{a}s E. D\'{\i}az},
title = {Vul4J: a dataset of reproducible Java vulnerabilities geared towards the study of program repair techniques},
year = {2022},
isbn = {9781450393034},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3524842.3528482},
doi = {10.1145/3524842.3528482},
abstract = {In this work we present Vul4J, a Java vulnerability dataset where each vulnerability is associated to a patch and, most importantly, to a Proof of Vulnerability (PoV) test case. We analyzed 1803 fix commits from 912 real-world vulnerabilities in the Project KB knowledge base to extract the reproducible vulnerabilities, i.e., vulnerabilities that can be triggered by one or more PoV test cases. To this aim, we ran the test suite of the application in both, the vulnerable and secure versions, to identify the corresponding PoVs. Furthermore, if no PoV test case was spotted, then we wrote it ourselves. As a result, Vul4J includes 79 reproducible vulnerabilities from 51 open-source projects, spanning 25 different Common Weakness Enumeration (CWE) types. To the extent of our knowledge, this is the first dataset of its kind created for Java. Particularly, it targets the study of Automated Program Repair (APR) tools, where PoVs are often necessary in order to identify plausible patches. We made our dataset and related tools publically available on GitHub.},
booktitle = {Proceedings of the 19th International Conference on Mining Software Repositories},
pages = {464–468},
numpages = {5},
keywords = {vulnerability, program repair, java},
location = {Pittsburgh, Pennsylvania},
series = {MSR '22}
}

@inproceedings{gitbugactions,
author = {Saavedra, Nuno and Silva, Andr\'{e} and Monperrus, Martin},
title = {GitBug-Actions: Building Reproducible Bug-Fix Benchmarks with GitHub Actions},
year = {2024},
isbn = {9798400705021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639478.3640023},
doi = {10.1145/3639478.3640023},
abstract = {Bug-fix benchmarks are fundamental in advancing various sub-fields of software engineering such as automatic program repair (APR) and fault localization (FL). A good benchmark must include recent examples that accurately reflect technologies and development practices of today. To be executable in the long term, a benchmark must feature test suites that do not degrade overtime due to, for example, dependencies that are no longer available. Existing benchmarks fail in meeting both criteria. For instance, Defects4J, one of the foremost Java benchmarks, last received an update in 2020. Moreover, full-reproducibility has been neglected by the majority of existing benchmarks. In this paper, we present GitBug-Actions: a novel tool for building bug-fix benchmarks with modern and fully-reproducible bug-fixes. GitBug-Actions relies on the most popular CI platform, GitHub Actions, to detect bug-fixes and smartly locally execute the CI pipeline in a controlled and reproducible environment. To the best of our knowledge, we are the first to rely on GitHub Actions to collect bug-fixes. To demonstrate our toolchain, we deploy GitBug-Actions to build a proof-of-concept Go bug-fix benchmark containing executable, fully-reproducible bug-fixes from different repositories. A video demonstrating GitBug-Actions is available at: https://youtu.be/aBWwa1sJYBs.},
booktitle = {Proceedings of the 2024 IEEE/ACM 46th International Conference on Software Engineering: Companion Proceedings},
pages = {1–5},
numpages = {5},
keywords = {software bugs, bug benchmark, bug database, reproducibility, software testing, program analysis, github actions},
location = {Lisbon, Portugal},
series = {ICSE-Companion '24}
}

@article{defects4ds,
author = {Zhao, Qianhui and Zhang, Li and Liu, Fang and Liu, Yang and Yan, Zhen and Chen, Zhenghao and Zhou, Yufei and Jiang, Jing and Li, Ge and Sun, Zian and Li, Zhongqi and Ma, Yuchi},
title = {Peer-aided repairer: empowering large language models to repair advanced student assignments},
year = {2025},
issue_date = {Dec 2025},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {31},
number = {2},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-025-10716-z},
doi = {10.1007/s10664-025-10716-z},
abstract = {Automated generation of feedback on programming assignments holds significant benefits for programming education, especially when it comes to advanced assignments. Automated Program Repair techniques, especially Large Language Model-based approaches, have gained notable recognition for their potential in fixing introductory assignments. However, the programs used for evaluation are relatively simple. It remains unclear how existing approaches perform in repairing programs from higher-level programming courses. To address these limitations, we curate a new advanced student assignment dataset named Defects4DS from a higher-level programming course. Subsequently, we identify the challenges related to fixing bugs in advanced assignments. Based on the analysis, we develop a framework called PaR that is powered by the Large Language Models. PaR works in three phases: Peer Solution Selection, Multi-Source Prompt Generation, and Program Repair. Peer Solution Selection identifies the closely related peer programs based on lexical, semantic, and syntactic criteria. Then Multi-Source Prompt Generation adeptly combines multiple sources of information to create a comprehensive and informative prompt for the last Program Repair stage. Evaluation reveals that PaR achieves state-of-the-art performance on Defects4DS compared to baseline approaches, with the impressive improvement of 16.13\% in repair rate. And experimental results on several introductory programming assignment datasets further demonstrate the effectiveness of PaR, achieving state-of-the-art results on ITSP and IntroClass datasets.},
journal = {Empirical Softw. Engg.},
month = dec,
numpages = {49},
keywords = {Programming education, Program repair, Large language models, Software engineering}
}

@inproceedings{silva2024gitbug,
  title={Gitbug-java: A reproducible benchmark of recent java bugs},
  author={Silva, Andr{\'e} and Saavedra, Nuno and Monperrus, Martin},
  booktitle={Proceedings of the 21st International Conference on Mining Software Repositories},
  pages={118--122},
  url={https://dl.acm.org/doi/pdf/10.1145/3643991.3644884},
  year={2024}
}

@inproceedings{codeflaws,
  author={Shin Hwei Tan and Jooyong Yi and Yulis and Mechtaev, Sergey and Roychoudhury, Abhik},
  booktitle={2017 IEEE/ACM 39th International Conference on Software Engineering Companion (ICSE-C)}, 
  title={Codeflaws: a programming competition benchmark for evaluating automated program repair tools}, 
  year={2017},
  volume={},
  number={},
  pages={180-182},
  keywords={Maintenance engineering;Tools;Benchmark testing;Software engineering;Programming;Software;Conferences;automated program repair;defect classes;empirical evaluation;benchmark},
  url={https://ieeexplore.ieee.org/document/7965296},
  doi={10.1109/ICSE-C.2017.76}
}

@misc{xlcost,
  title={XLCoST: A Benchmark Dataset for Cross-lingual Code Intelligence}, 
  author={Ming Zhu and Aneesh Jain and Karthik Suresh and Roshan Ravindran and Sindhu Tipirneni and Chandan K. Reddy},
  year={2022},
  eprint={2206.08474},
  archivePrefix={arXiv},
  primaryClass={cs.SE},
  url={https://arxiv.org/abs/2206.08474}, 
}

@inproceedings{bugscpp,
author = {An, Gabin and Kwon, Minhyuk and Choi, Kyunghwa and Yi, Jooyong and Yoo, Shin},
title = {BugsC++: A Highly Usable Real World Defect Benchmark for C/C++},
year = {2024},
isbn = {9798350329964},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASE56229.2023.00208},
doi = {10.1109/ASE56229.2023.00208},
abstract = {As software systems grow larger and more complex, debugging takes up an increasingly significant portion of developers' time and efforts during software maintenance. To aid software engineers in debugging, many automated debugging and repair techniques have been proposed. Both the development and evaluation of these automated techniques depend on benchmarks of bugs. While many different defect benchmarks have been developed, only a few benchmarks are widely used due to the origin of the collected bugs as well as the usability of the benchmarks themselves, risking a biased research landscape. This paper presents BugsC++, a new benchmark that contains 209 real-world bugs collected from 22 open-source C/C++ projects. BugsC++ aims to provide high usability by providing a similar user interface to the widely used Defects4J. Further, BugsC++ ensures the replicability of the bugs in its collection by encapsulating each buggy program in a Docker container. By providing a highly usable real-world defect benchmark for C/C++, we hope to promote debugging research for C/C++.},
booktitle = {Proceedings of the 38th IEEE/ACM International Conference on Automated Software Engineering},
pages = {2034–2037},
numpages = {4},
keywords = {software testing, bug, fault, defect benchmark},
location = {Echternach, Luxembourg},
series = {ASE '23}
}

@inproceedings{androiddataset,
author = {Braconaro, Elisa and Losiouk, Eleonora},
title = {A Dataset for Evaluating LLMs Vulnerability Repair Performance in Android Applications: Data/Toolset paper},
year = {2025},
isbn = {9798400714764},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3714393.3726486},
doi = {10.1145/3714393.3726486},
abstract = {Automated Program Repair (APR) is a well-established research area that enhances software reliability and security by automatically fixing bugs, reducing manual effort, and accelerating debugging. Despite progress in publishing benchmarks to evaluate APR tools, datasets specifically targeting Android are lacking.To address this gap, we introduce a dataset of 272 real-world violations of Google's Android Security Best Practices, identified by statically analyzing 113 real-world Android apps. In addition to the faulty code, we manually crafted repairs based on Google's guidelines, covering 176 Java-based and 96 XML-based violations from Android Java classes and Manifest files, respectively. Additionally, we leveraged our novel dataset to evaluate Large Language Models (LLMs) as they are the latest promising APR tools. In particular, we evaluated GPT-4o, Gemini 1.5 Flash and Gemini in Android Studio and we found that GPT-4o outperforms Google's models, demonstrating higher accuracy and robustness across a range of violations types. Hence, with this dataset, we aim to provide valuable insights for advancing APR research and improving tools for Android security.},
booktitle = {Proceedings of the Fifteenth ACM Conference on Data and Application Security and Privacy},
pages = {353–358},
numpages = {6},
keywords = {android vulnerabilities, automated program repair, large language models},
location = {Pittsburgh, PA, USA},
series = {CODASPY '25}
}

@inproceedings{minecpp,
author = {Avula, Sai Krishna and Mondal, Shouvick},
title = {MineCPP: Mining Bug Fix Pairs and Their Structures},
year = {2024},
isbn = {9798400706585},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3663529.3663797},
doi = {10.1145/3663529.3663797},
abstract = {Modern software repositories serve as valuable sources of information for understanding and addressing software bugs. In this paper, we present MineCPP, a tool designed for large-scale bug fixing dataset generation, extending the capabilities of a recently proposed approach, namely Minecraft. MineCPP not only captures bug locations and types across multiple programming languages but introduces novel features like offset of a bug in a buggy source file, the sequence of syntactic constructs up to and including the location of the bug, etc. We discuss architectural and operational aspects of MineCPP, and show how it can be used to automatically mine GitHub repositories. A Graphical User Interface (GUI) further enhances user experience by providing interactive visualizations and quantitative analyses, facilitating fine-grained insights about the structure of bug fix pairs. MineCPP serves as a helpful solution for researchers, practitioners, and developers seeking comprehensive bug-fixing datasets and insights into coding practices. Tool demonstration is available at https://youtu.be/ln99irvbADE.},
booktitle = {Companion Proceedings of the 32nd ACM International Conference on the Foundations of Software Engineering},
pages = {552–556},
numpages = {5},
keywords = {Bug Fixes, Coding Effort, LLMs, Mining Software Repositories},
location = {Porto de Galinhas, Brazil},
series = {FSE 2024}
}

@inproceedings{precisebugcollector,
author = {He, Ye and Chen, Zimin and Goues, Claire Le},
title = {PreciseBugCollector: Extensible, Executable and Precise Bug-Fix Collection},
year = {2024},
isbn = {9798350329964},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASE56229.2023.00163},
doi = {10.1109/ASE56229.2023.00163},
abstract = {Bug datasets are vital for enabling deep learning techniques to address software maintenance tasks related to bugs. However, existing bug datasets suffer from precise and scale limitations: they are either small-scale but precise with manual validation or large-scale but imprecise with simple commit message processing. In this paper, we introduce Precise-BugCollector, a precise, multi-language bug collection approach that overcomes these two limitations. PreciseBugCollector is based on two novel components: a) A bug tracker to map the codebase repositories with external bug repositories to trace bug type information, and b) A bug injector to generate project-specific bugs by injecting noise into the correct codebases and then executing them against their test suites to obtain test failure messages.We implement PreciseBugCollector against three sources: 1) A bug tracker that links to the national vulnerability data set (NVD) to collect general-wise vulnerabilities, 2) A bug tracker that links to OSS-Fuzz to collect general-wise bugs, and 3) A bug injector based on 16 injection rules to generate project-wise bugs. To date, PreciseBugCollector comprises 1 057 818 bugs extracted from 2 968 open-source projects. Of these, 12 602 bugs are sourced from bug repositories (NVD and OSS-Fuzz), while the remaining 1 045 216 project-specific bugs are generated by the bug injector. Considering the challenge objectives, we argue that a bug injection approach is highly valuable for the industrial setting, since project-specific bugs align with domain knowledge, share the same codebase, and adhere to the coding style employed in industrial projects.},
booktitle = {Proceedings of the 38th IEEE/ACM International Conference on Automated Software Engineering},
pages = {1899–1910},
numpages = {12},
keywords = {bug datasets, program repair, software testing and debugging},
location = {Echternach, Luxembourg},
series = {ASE '23}
}

@inproceedings{condefects,
author = {Wu, Yonghao and Li, Zheng and Zhang, Jie M. and Liu, Yong},
title = {ConDefects: A Complementary Dataset to Address the Data Leakage Concern for LLM-Based Fault Localization and Program Repair},
year = {2024},
isbn = {9798400706585},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3663529.3663815},
doi = {10.1145/3663529.3663815},
abstract = {With the growing interest on Large Language Models (LLMs) for fault localization and program repair, ensuring the integrity and generalizability of the LLM-based methods becomes paramount. The code in existing widely-adopted benchmarks for these tasks was written before the bloom of LLMs and may be included in the training data of existing popular LLMs, thereby suffering from the threat of data leakage, leading to misleadingly optimistic performance metrics.  To address this issue, we introduce ConDefects, a dataset developed as a complement to existing datasets, meticulously curated with real faults to eliminate such overlap. ConDefects contains 1,254 Java faulty programs and 1,625 Python faulty programs. All these programs are sourced from the online competition platform AtCoder and were produced between October 2021 and September 2023. We pair each fault with fault locations and the corresponding repaired code versions, making it tailored for fault localization and program repair related research. We also provide interfaces for selecting subsets based on different time windows and coding task difficulties. While inspired by LLM-based tasks, ConDefects can be adopted for benchmarking ALL types of fault localization and program repair methods. The dataset is publicly available, and a demo video can be found at https://www.youtube.com/watch?v=22j15Hj5ONk.},
booktitle = {Companion Proceedings of the 32nd ACM International Conference on the Foundations of Software Engineering},
pages = {642--646},
numpages = {5},
keywords = {Dataset, Fault Localization, Large Language Model, Program Repair},
location = {Porto de Galinhas, Brazil},
series = {FSE 2024}
}

@inproceedings{kong2019mining,
  title={Mining android crash fixes in the absence of issue-and change-tracking systems},
  author={Kong, Pingfan and Li, Li and Gao, Jun and Bissyand{\'e}, Tegawend{\'e} F and Klein, Jacques},
  booktitle={Proceedings of the 28th ACM SIGSOFT International Symposium on Software Testing and Analysis},
  pages={78--89},
  url={https://dl.acm.org/doi/pdf/10.1145/3293882.3330572},
  year={2019}
}

@article{jiang2022bugbuilder,
  title={Bugbuilder: An automated approach to building bug repository},
  author={Jiang, Yanjie and Liu, Hui and Luo, Xiaoqing and Zhu, Zhihao and Chi, Xiaye and Niu, Nan and Zhang, Yuxia and Hu, Yamin and Bian, Pan and Zhang, Lu},
  journal={IEEE Transactions on Software Engineering},
  volume={49},
  number={4},
  pages={1443--1463},
  year={2022},
  url={https://homepages.uc.edu/~niunn/papers/TSE23.pdf},
  publisher={IEEE}
}

@inproceedings{tssb,
  title={TSSB-3M: Mining single statement bugs at massive scale},
  author={Richter, Cedric and Wehrheim, Heike},
  booktitle={Proceedings of the 19th International Conference on Mining Software Repositories},
  pages={418--422},
  url={https://dl.acm.org/doi/pdf/10.1145/3524842.3528505},
  year={2022}
}

@inproceedings{githubbugs,
  title={The github recent bugs dataset for evaluating llm-based debugging applications},
  author={Lee, Jae Yong and Kang, Sungmin and Yoon, Juyeon and Yoo, Shin},
  booktitle={2024 IEEE Conference on Software Testing, Verification and Validation (ICST)},
  pages={442--444},
  url={https://arxiv.org/pdf/2310.13229},
  year={2024},
  organization={IEEE}
}

@inproceedings{bears,
  title={Bears: An extensible java bug benchmark for automatic program repair studies},
  author={Madeiral, Fernanda and Urli, Simon and Maia, Marcelo and Monperrus, Martin},
  booktitle={2019 IEEE 26th international conference on software analysis, evolution and reengineering (SANER)},
  pages={468--478},
  year={2019},
  url={https://arxiv.org/pdf/1901.06024},
  organization={IEEE}
}

@inproceedings{bugsdotjar,
  title={Bugs. jar: A large-scale, diverse dataset of real-world java bugs},
  author={Saha, Ripon K and Lyu, Yingjun and Lam, Wing and Yoshida, Hiroaki and Prasad, Mukul R},
  booktitle={Proceedings of the 15th international conference on mining software repositories},
  pages={10--13},
  url={https://dl.acm.org/doi/pdf/10.1145/3196398.3196473},
  year={2018},
}

@article{swtbench,
  title={SWT-bench: Testing and validating real-world bug-fixes with code agents},
  author={M{\"u}ndler, Niels and M{\"u}ller, Mark and He, Jingxuan and Vechev, Martin},
  journal={Advances in Neural Information Processing Systems},
  volume={37},
  pages={81857--81887},
  url={https://proceedings.neurips.cc/paper_files/paper/2024/file/94f093b41fc2666376fb1f667fe282f3-Paper-Conference.pdf},
  year={2024}
}

@inproceedings{fixeval,
  title={Fixeval: Execution-based evaluation of program fixes for programming problems},
  author={Haque, Md Mahim Anjum and Ahmad, Wasi Uddin and Lourentzou, Ismini and Brown, Chris},
  booktitle={2023 IEEE/ACM International Workshop on Automated Program Repair (APR)},
  pages={11--18},
  year={2023},
  url={https://arxiv.org/pdf/2206.07796},
  organization={IEEE}
}

@comment{
SLRs, reviews, surveys, empirical studies, etc. on software engineering topics.
Added by snowballing: 15
}
@article{llm4se,
  title={Towards an understanding of large language models in software engineering tasks},
  author={Zheng, Zibin and Ning, Kaiwen and Zhong, Qingyuan and Chen, Jiachi and Chen, Wenqing and Guo, Lianghong and Wang, Weicheng and Wang, Yanlin},
  journal={Empirical Software Engineering},
  volume={30},
  number={2},
  pages={50},
  year={2025},
  url={https://arxiv.org/abs/2308.11396},
  abstract={Large Language Models (LLMs) have drawn widespread attention and research due to their astounding performance in text generation and reasoning tasks. Derivative products, like ChatGPT, have been extensively deployed and highly sought after. Meanwhile, the evaluation and optimization of LLMs in software engineering tasks, such as code generation, have become a research focus. However, there is still a lack of systematic research on applying and evaluating LLMs in software engineering. Therefore, this paper comprehensively investigate and collate the research and products combining LLMs with software engineering, aiming to answer two questions: (1) What are the current integrations of LLMs with software engineering? (2) Can LLMs effectively handle software engineering tasks? To find the answers, we have collected related literature as extensively as possible from seven mainstream databases and selected 123 timely papers published starting from 2022 for analysis. We have categorized these papers in detail and reviewed the current research status of LLMs from the perspective of seven major software engineering tasks, hoping this will help researchers better grasp the research trends and address the issues when applying LLMs. Meanwhile, we have also organized and presented papers with evaluation content to reveal the performance and effectiveness of LLMs in various software engineering tasks, guiding researchers and developers to optimize.},
  publisher={Springer}
}
@article{llm4se2,
  title={Large language models for software engineering: A systematic literature review},
  author={Hou, Xinyi and Zhao, Yanjie and Liu, Yue and Yang, Zhou and Wang, Kailong and Li, Li and Luo, Xiapu and Lo, David and Grundy, John and Wang, Haoyu},
  journal={ACM Transactions on Software Engineering and Methodology},
  volume={33},
  number={8},
  pages={1--79},
  url={https://dl.acm.org/doi/pdf/10.1145/3695988},
  year={2024},
  publisher={ACM New York, NY}
}
@misc{llm4se3,
  title={Large Language Models for Software Engineering: Survey and Open Problems}, 
  author={Angela Fan and Beliz Gokkaya and Mark Harman and Mitya Lyubarskiy and Shubho Sengupta and Shin Yoo and Jie M. Zhang},
  year={2023},
  eprint={2310.03533},
  archivePrefix={arXiv},
  primaryClass={cs.SE},
  url={https://arxiv.org/abs/2310.03533}, 
}

@article{llmcodequality,
title = {Using LLMs to enhance code quality: A systematic literature review},
journal = {Information and Software Technology},
volume = {190},
pages = {107960},
year = {2026},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2025.107960},
url = {https://www.sciencedirect.com/science/article/pii/S095058492500299X},
author = {Nawaf Alomari and Moussa Redah and Ahmad Ashraf and Mohammad Alshayeb},
keywords = {Code quality, LLM, systematic literature review},
abstract = {Context:
Large Language Models (LLMs) are increasingly used in software engineering to enhance code quality through tasks such as refactoring and code smell detection and many other tasks. Code smells are poor design decisions that can be resolved by changing the internal structure of the code without affecting its output, a process known as refactoring.
Objective:
This study systematically reviews the use of LLMs in code quality enhancement, focusing on techniques such as refactoring, smell detection, and other code improvement methods.
Method:
Using SLR techniques, we reviewed 49 studies up to September 2024, analyzing both qualitative and quantitative data to assess trends and effectiveness.
Results:
The field is active, with refactoring as the most common task, followed by smell detection. Refactored code by LLMs is not reliable. Prompting is used more frequently than fine-tuning, with few-shot learning as the leading prompting method. Java and Python are the most represented languages, while F1, Precision, Recall, and Accuracy are common evaluation metrics, along with BLEU and EM for generation tasks. Open-source and general language models are preferred, with validation datasets as the primary validation approach.
Conclusions:
LLMs show promise for code quality improvement, but challenges in optimization and reliability remain. Future research should prioritize fine-tuning for refactoring, linking LLMs to specific quality attributes, developing benchmark datasets, constructing datasets for diverse programming languages, and exploring a wider range of promoting techniques.}
}
@article{dl4defence,
  title={Deep learning for android malware defenses: a systematic literature review},
  author={Liu, Yue and Tantithamthavorn, Chakkrit and Li, Li and Liu, Yepang},
  journal={ACM Computing Surveys},
  volume={55},
  url={https://dl.acm.org/doi/pdf/10.1145/3544968},
  number={8},
  pages={1--36},
  year={2022},
  publisher={ACM New York, NY}
}
@article{ml4se,
  title={Machine/deep learning for software engineering: A systematic literature review},
  author={Wang, Simin and Huang, Liguo and Gao, Amiao and Ge, Jidong and Zhang, Tengfei and Feng, Haitao and Satyarth, Ishna and Li, Ming and Zhang, He and Ng, Vincent},
  journal={IEEE Transactions on Software Engineering},
  volume={49},
  url={https://ieeexplore.ieee.org/ielaam/32/10070338/9772253-aam.pdf},
  number={3},
  pages={1188--1231},
  year={2022},
  publisher={IEEE}
}

@misc{agent4bugfixempirical,
 abstract = {Large language models (LLMs) and LLM-based Agents have been applied to fix bugs automatically, demonstrating the capability in addressing software defects by engaging in development environment interaction, iterative validation and code modification. However, systematic analysis of these agent systems remain limited, particularly regarding performance variations among top-performing ones. In this paper, we examine six repair systems on the SWE-bench Verified benchmark for automated bug fixing. We first assess each system's overall performance, noting the instances solvable by all or none of these systems, and explore the capabilities of different systems. We also compare fault localization accuracy at file and code symbol levels and evaluate bug reproduction capabilities. Through analysis, we concluded that further optimization is needed in both the LLM capability itself and the design of Agentic flow to improve the effectiveness of the Agent in bug fixing.},
 archiveprefix = {arXiv},
 author = {Xiangxin Meng and Zexiong Ma and Pengfei Gao and Chao Peng},
 eprint = {2411.10213},
 primaryclass = {cs.SE},
 title = {An Empirical Study on LLM-based Agents for Automated Bug Fixing},
 url = {https://arxiv.org/abs/2411.10213},
 year = {2025}
}

@misc{agenticbugreproductioneffective,
 abstract = {Bug reports often lack sufficient detail for developers to reproduce and fix the underlying defects. Bug Reproduction Tests (BRTs), tests that fail when the bug is present and pass when it has been resolved, are crucial for debugging, but they are rarely included in bug reports, both in open-source and in industrial settings. Thus, automatically generating BRTs from bug reports has the potential to accelerate the debugging process and lower time to repair. This paper investigates automated BRT generation within an industry setting, specifically at Google, focusing on the challenges of a large-scale, proprietary codebase and considering real-world industry bugs extracted from Google's internal issue tracker. We adapt and evaluate a state-of-the-art BRT generation technique, LIBRO, and present our agent-based approach, BRT Agent, which makes use of a fine-tuned Large Language Model (LLM) for code editing. Our BRT Agent significantly outperforms LIBRO, achieving a 28% plausible BRT generation rate, compared to 10% by LIBRO, on 80 human-reported bugs from Google's internal issue tracker. We further investigate the practical value of generated BRTs by integrating them with an Automated Program Repair (APR) system at Google. Our results show that providing BRTs to the APR system results in 30% more bugs with plausible fixes. Additionally, we introduce Ensemble Pass Rate (EPR), a metric which leverages the generated BRTs to select the most promising fixes from all fixes generated by APR system. Our evaluation on EPR for Top-K and threshold-based fix selections demonstrates promising results and trade-offs. For example, EPR correctly selects a plausible fix from a pool of 20 candidates in 70% of cases, based on its top-1 ranking.},
 archiveprefix = {arXiv},
 author = {Runxiang Cheng and Michele Tufano and Jürgen Cito and José Cambronero and Pat Rondon and Renyao Wei and Aaron Sun and Satish Chandra},
 eprint = {2502.01821},
 primaryclass = {cs.SE},
 title = {Agentic Bug Reproduction for Effective Automated Program Repair at Google},
 url = {https://arxiv.org/abs/2502.01821},
 year = {2025}
}

@misc{langgraphbugfix,
 abstract = {This paper presents a novel framework for automated code generation and debugging, designed to improve accuracy, efficiency, and scalability in software development. The proposed system integrates three core components LangGraph, GLM4 Flash, and ChromaDB within a four step iterative workflow to deliver robust performance and seamless functionality.
LangGraph serves as a graph-based library for orchestrating tasks, providing precise control and execution while maintaining a unified state object for dynamic updates and consistency. It supports multi-agent, hierarchical, and sequential processes, making it highly adaptable to complex software engineering workflows. GLM4 Flash, a large language model, leverages its advanced capabilities in natural language understanding, contextual reasoning, and multilingual support to generate accurate code snippets based on user prompts. ChromaDB acts as a vector database for semantic search and contextual memory storage, enabling the identification of patterns and the generation of context-aware bug fixes based on historical data.
The system operates through a structured four-step process: (1) Code Generation, which translates natural language descriptions into executable code; (2) Code Execution, which validates the code by identifying runtime errors and inconsistencies; (3) Code Repair, which iteratively refines buggy code using ChromaDB's memory capabilities and LangGraph's state tracking; and (4) Code Update, which ensures the code meets functional and performance requirements through iterative modifications.},
 archiveprefix = {arXiv},
 author = {Jialin Wang and Zhihua Duan},
 eprint = {2502.18465},
 primaryclass = {cs.SE},
 title = {Empirical Research on Utilizing LLM-based Agents for Automated Bug Fixing via LangGraph},
 url = {https://arxiv.org/abs/2502.18465},
 year = {2025}
}

@misc{agents,
  title={Agent S: An Open Agentic Framework that Uses Computers Like a Human}, 
  author={Saaket Agashe and Jiuzhou Han and Shuyu Gan and Jiachen Yang and Ang Li and Xin Eric Wang},
  year={2024},
  eprint={2410.08164},
  archivePrefix={arXiv},
  primaryClass={cs.AI},
  url={https://arxiv.org/abs/2410.08164}, 
}

@inproceedings{incorrectfix,
  author = {Yin, Zuoning and Yuan, Ding and Zhou, Yuanyuan and Pasupathy, Shankar and Bairavasundaram, Lakshmi},
  title = {How do fixes become bugs?},
  year = {2011},
  isbn = {9781450304436},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  url = {https://doi.org/10.1145/2025113.2025121},
  doi = {10.1145/2025113.2025121},
  abstract = {Software bugs affect system reliability. When a bug is exposed in the field, developers need to fix them. Unfortunately, the bug-fixing process can also introduce errors, which leads to buggy patches that further aggravate the damage to end users and erode software vendors' reputation.This paper presents a comprehensive characteristic study on incorrect bug-fixes from large operating system code bases including Linux, OpenSolaris, FreeBSD and also a mature commercial OS developed and evolved over the last 12 years, investigating not only themistake patterns during bug-fixing but also the possible human reasons in the development process when these incorrect bug-fixes were introduced. Our major findings include: (1) at least 14.8\%--24.4\% of sampled fixes for post-release bugs in these large OSes are incorrect and have made impacts to end users. (2) Among several common bug types, concurrency bugs are the most difficult to fix correctly: 39\% of concurrency bug fixes are incorrect. (3) Developers and reviewers for incorrect fixes usually do not have enough knowledge about the involved code. For example, 27\% of the incorrect fixes are made by developers who have never touched the source code files associated with the fix. Our results provide useful guidelines to design new tools and also to improve the development process. Based on our findings, the commercial software vendor whose OS code we evaluated is building a tool to improve the bug fixing and code reviewing process.},
  booktitle = {Proceedings of the 19th ACM SIGSOFT Symposium and the 13th European Conference on Foundations of Software Engineering},
  pages = {26-36},
  numpages = {11},
  keywords = {bug fixing, human factor, incorrect fixes, software bugs, testing},
  location = {Szeged, Hungary},
  series = {ESEC/FSE '11}
}

@article{copilotsecurity,
  title={Asleep at the keyboard? assessing the security of github copilot's code contributions},
  author={Pearce, Hammond and Ahmad, Baleegh and Tan, Benjamin and Dolan-Gavitt, Brendan and Karri, Ramesh},
  journal={Communications of the ACM},
  volume={68},
  number={2},
  pages={96--105},
  year={2025},
  url={https://arxiv.org/abs/2108.09293},
  publisher={ACM New York, NY, USA}
}

@article{promptsurvey,
  title={Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing},
  author={Liu, Pengfei and Yuan, Weizhe and Fu, Jinlan and Jiang, Zhengbao and Hayashi, Hiroaki and Neubig, Graham},
  journal={ACM computing surveys},
  volume={55},
  number={9},
  pages={1--35},
  year={2023},
  url={https://dl.acm.org/doi/pdf/10.1145/3560815},
  publisher={ACM New York, NY}
}

@article{ragsurvey,
  title={Retrieval-augmented generation for ai-generated content: A survey},
  author={Zhao, Penghao and Zhang, Hailin and Yu, Qinhan and Wang, Zhengren and Geng, Yunteng and Fu, Fangcheng and Yang, Ling and Zhang, Wentao and Jiang, Jie and Cui, Bin},
  journal={Data Science and Engineering},
  pages={1--29},
  year={2026},
  url={https://link.springer.com/content/pdf/10.1007/s41019-025-00335-5.pdf},
  publisher={Springer}
}

@inproceedings{apranalysis,
  title={An analysis of the automatic bug fixing performance of chatgpt},
  author={Sobania, Dominik and Briesch, Martin and Hanna, Carol and Petke, Justyna},
  booktitle={2023 IEEE/ACM International Workshop on Automated Program Repair (APR)},
  pages={23--30},
  url={https://arxiv.org/abs/2301.08653},
  abstract={To support software developers in finding and fixing software bugs, several automated program repair techniques have been introduced. Given a test suite, standard methods usually either synthesize a repair, or navigate a search space of software edits to find test-suite passing variants. Recent program repair methods are based on deep learning approaches. One of these novel methods, which is not primarily intended for automated program repair, but is still suitable for it, is ChatGPT. The bug fixing performance of ChatGPT, however, is so far unclear. Therefore, in this paper we evaluate ChatGPT on the standard bug fixing benchmark set, QuixBugs, and compare the perfor mance with the results of several other approaches reported in the literature. We find that ChatGPT’s bug fixing performance is competitive to the common deep learning approaches CoCoNut and Codex and notably better than the results reported for the standard program repair approaches. In contrast to previous approaches, ChatGPT offers a dialogue system through which further information, e.g., the expected output for a certain input or an observed error message, can be entered. By providing such hints to ChatGPT, its success rate can be further increased, fixing 31 out of 40 bugs, outperforming state-of-the-art.},
  year={2023},
  organization={IEEE}
}

@comment{
Empirical studies on practitioner's debugging behavior, expectations, etc.
}

@inproceedings{expectationonfl,
  title={Practitioners' expectations on automated fault localization},
  author={Kochhar, Pavneet Singh and Xia, Xin and Lo, David and Li, Shanping},
  booktitle={Proceedings of the 25th international symposium on software testing and analysis},
  pages={165--176},
  url={https://dl.acm.org/doi/pdf/10.1145/2931037.2931051},
  abstract={Software engineering practitioners often spend significant amount of time and effort to debug. To help practitioners perform this crucial task, hundreds of papers have proposed various fault localization techniques. Fault localization helps practitioners to find the location of a defect given its symptoms (e.g., program failures). These localization techniques have pinpointed the locations of bugs of various systems of diverse sizes, with varying degrees of success, and for various usage scenarios. Unfortunately, it is unclear whether practitioners appreciate this line of research. To fill this gap, we performed an empirical study by surveying 386 practitioners from more than 30 countries across 5 continents about their expectations of research in fault localization. In particular, we investigated a number of factors that impact practitioners' willingness to adopt a fault localization technique. We then compared what practitioners need and the current state-of-research by performing a literature review of papers on fault localization techniques published in ICSE, FSE, ESEC-FSE, ISSTA, TSE, and TOSEM in the last 5 years (2011-2015). From this comparison, we highlight the directions where researchers need to put effort to develop fault localization techniques that matter to practitioners.},
  year={2016}
}
@inproceedings{autodebughelpful,
  title={Are automated debugging techniques actually helping programmers?},
  author={Parnin, Chris and Orso, Alessandro},
  booktitle={Proceedings of the 2011 international symposium on software testing and analysis},
  pages={199--209},
  url={https://dl.acm.org/doi/pdf/10.1145/2001420.2001445},
  abstract={Debugging is notoriously difficult and extremely time consuming. Researchers have therefore invested a considerable amount of effort in developing automated techniques and tools for supporting various debugging tasks. Although potentially useful, most of these techniques have yet to demonstrate their practical effectiveness. One common limitation of existing approaches, for instance, is their reliance on a set of strong assumptions on how developers behave when debugging (e.g., the fact that examining a faulty statement in isolation is enough for a developer to understand and fix the corresponding bug). In more general terms, most existing techniques just focus on selecting subsets of potentially faulty statements and ranking them according to some criterion. By doing so, they ignore the fact that understanding the root cause of a failure typically involves complex activities, such as navigating program dependencies and rerunning the program with different inputs. The overall goal of this research is to investigate how developers use and benefit from automated debugging tools through a set of human studies. As a first step in this direction, we perform a preliminary study on a set of developers by providing them with an automated debugging tool and two tasks to be performed with and without the tool. Our results provide initial evidence that several assumptions made by automated debugging techniques do not hold in practice. Through an analysis of the results, we also provide insights on potential directions for future work in the area of automated debugging.},
  year={2011}
}

@inproceedings{debugbench,
  title={Where is the bug and how is it fixed? an experiment with practitioners},
  author={B{\"o}hme, Marcel and Soremekun, Ezekiel O and Chattopadhyay, Sudipta and Ugherughe, Emamurho and Zeller, Andreas},
  booktitle={Proceedings of the 2017 11th joint meeting on foundations of software engineering},
  pages={117--128},
  url={https://dl.acm.org/doi/pdf/10.1145/3106237.3106255},
  abstract={Research has produced many approaches to automatically locate, explain, and repair software bugs. But do these approaches relate to the way practitioners actually locate, understand, and fix bugs? To help answer this question, we have collected a dataset named DBGBENCH --- the correct fault locations, bug diagnoses, and software patches of 27 real errors in open-source C projects that were consolidated from hundreds of debugging sessions of professional software engineers. Moreover, we shed light on the entire debugging process, from constructing a hypothesis to submitting a patch, and how debugging time, difficulty, and strategies vary across practitioners and types of errors. Most notably, DBGBENCH can serve as reality check for novel automated debugging and repair techniques.},
  keywords={software debugging, software testing},
  year={2017}
}

@misc{impactclm,
  title={Impact of Code Language Models on Automated Program Repair}, 
  author={Nan Jiang and Kevin Liu and Thibaud Lutellier and Lin Tan},
  year={2023},
  eprint={2302.05020},
  archivePrefix={arXiv},
  primaryClass={cs.SE},
  abstract={Automated program repair (APR) aims to help developers improve software reliability by generating patches for buggy programs. Although many code language models (CLM) are developed and effective in many software tasks such as code completion, there has been little comprehensive, in-depth work to evaluate CLMs' fixing capabilities and to fine-tune CLMs for the APR task. Firstly, this work is the first to evaluate ten CLMs on four APR benchmarks, which shows that surprisingly, the best CLM, as is, fixes 72% more bugs than the state-of-the-art deep-learning (DL)-based APR techniques. Secondly, one of the four APR benchmarks was created by us in this paper to avoid data leaking for a fair evaluation. Thirdly, it is the first work to fine-tune CLMs with APR training data, which shows that fine-tuning brings 31%-1,267% improvement to CLMs and enables them to fix 46%-164% more bugs than existing DL-based APR techniques. Fourthly, this work studies the impact of buggy lines, showing that CLMs, as is, cannot make good use of the buggy lines to fix bugs, yet fine-tuned CLMs could potentially over-rely on buggy lines. Lastly, this work analyzes the size, time, and memory efficiency of different CLMs. This work shows promising directions for the APR domain, such as fine-tuning CLMs with APR-specific designs, and also raises awareness of fair and comprehensive evaluations of CLMs and calls for more transparent reporting of open-source repositories used in the pre-training data to address the data leaking problem.},
  url={https://arxiv.org/abs/2302.05020}, 
}

@inproceedings{asrsurvey,
  title={Automatic software repair: A survey},
  author={Gazzola, Luca and Micucci, Daniela and Mariani, Leonardo},
  booktitle={Proceedings of the 40th International Conference on Software Engineering},
  url={https://dl.acm.org/doi/pdf/10.1145/3180155.3182526},
  abstract={Debugging software failures is still a painful, time consuming, and expensive process. For instance, recent studies showed that debugging activities often account for about 50% of the overall development cost of software products. There are many factors contributing to the cost of debugging, but the most impacting one is the extensive manual effort that is still required to identify and remove faults. So far, the automation of debugging activities essentially resulted in the development of techniques that provide useful insights about the possible locations of faults, the inputs and states of the application responsible for the failures, as well as the anomalous operations executed during failures. However, developers must still put a relevant effort on the analysis of the failed executions to exactly identify the faults that must be fixed. In addition, these techniques do not help the developers with the synthesis of an appropriate fix.},
  pages={1219--1219},
  year={2018}
}

@misc{llminfl,
  title={Large Language Models in Fault Localisation}, 
  author={Yonghao Wu and Zheng Li and Jie M. Zhang and Mike Papadakis and Mark Harman and Yong Liu},
  year={2023},
  eprint={2308.15276},
  archivePrefix={arXiv},
  primaryClass={cs.SE},
  keywords={Large Language Model, Fault Localisation, ChatGPT, Empirical Study},
  abstract={Large Language Models (LLMs) have shown promise in multiple software engineering tasks including code generation, program repair, code summarisation, and test generation. Fault localisation is instrumental in enabling automated debugging and repair of programs and was prominently featured as a highlight during the launch event of ChatGPT-4. Nevertheless, the performance of LLMs compared to state-of-the-art methods, as well as the impact of prompt design and context length on their efficacy, remains unclear. To fill this gap, this paper presents an in-depth investigation into the capability of ChatGPT-3.5 and ChatGPT-4, the two state-of-the-art LLMs, on fault localisation. Using the widely-adopted large-scale Defects4J dataset, we compare the two LLMs with the existing fault localisation techniques. We also investigate the consistency of LLMs in fault localisation, as well as how prompt engineering and the length of code context affect the fault localisation effectiveness. Our findings demonstrate that within function-level context, ChatGPT-4 outperforms all the existing fault localisation methods. Additional error logs can further improve ChatGPT models' localisation accuracy and consistency, with an average 46.9% higher accuracy over the state-of-the-art baseline SmartFL on the Defects4J dataset in terms of TOP-1 metric. However, when the code context of the Defects4J dataset expands to the class-level, ChatGPT-4's performance suffers a significant drop, with 49.9% lower accuracy than SmartFL under TOP-1 metric. These observations indicate that although ChatGPT can effectively localise faults under specific conditions, limitations are evident. Further research is needed to fully harness the potential of LLMs like ChatGPT for practical fault localisation applications.},
  url={https://arxiv.org/abs/2308.15276}, 
}

@article{apr4vul,
author={Bui Quang-Cuong and Paramitha Ranindya and Vu Duc-Ly and Massacci Fabio and Scandariato Riccardo},
title="APR4Vul: an empirical study of automatic program repair techniques on real-world Java vulnerabilities",
journal="Empirical Software Engineering",
year="2023",
month="Dec",
day="06",
volume="29",
number="1",
pages="18",
abstract="Security vulnerability fixes could be a promising research avenue for Automated Program Repair (APR) techniques. In recent years, APR tools have been thoroughly developed for fixing generic bugs. However, the area is still relatively unexplored when it comes to fixing security bugs or vulnerabilities. In this paper, we evaluate nine state-of-the-art APR tools and one vulnerability-specific repair tool. In particular, we investigate their ability to generate patches for 79 real-world Java vulnerabilities in the Vul4J dataset, as well as the level of trustworthiness of these patches. We evaluate the tools with respect to their ability to generate security patches that are (i) testable, (ii) having the positive effect of closing the vulnerability, and (iii) not having side effects from a functional point of view. Our results show that the evaluated APR tools were able to generate testable patches for around 20{\%} of the considered vulnerabilities. On average, nearly 73{\%} of the testable patches indeed eliminate the vulnerabilities, but only 44{\%} of them could actually fix security bugs while maintaining the functionalities. To understand the root cause of this phenomenon, we conduct a detailed comparative study of the general bug fix patterns in Defect4J and the vulnerability fix patterns in ExtraVul (which we extend from Vul4J). Our investigation shows that, although security patches are short in terms of lines of code, they contain unique characteristics in their fix patterns compared to general bugs. For example, many security fixes require adding method calls. These method calls contain specific input validation-related keywords, such as encode, normalize, and trim. In this regard, our study suggests that additional repair patterns should be implemented for existing APR tools to fix more types of security vulnerabilities.",
issn="1573-7616",
doi="10.1007/s10664-023-10415-7",
url="https://doi.org/10.1007/s10664-023-10415-7"
}

@inproceedings{evaluateavr,
author = {Khan, Zanis Ali and Garg, Aayush and Tang, Qiang},
title = {A Multi-dataset Evaluation of Models for Automated Vulnerability Repair},
year = {2025},
isbn = {978-3-032-00629-5},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-032-00630-1_5},
doi = {10.1007/978-3-032-00630-1_5},
abstract = {Software vulnerabilities pose significant security threats, requiring effective mitigation. While Automated Program Repair (APR) has advanced in fixing general bugs, vulnerability patching—a security-critical aspect of APR—remains underexplored. This study investigates pre-trained language models, CodeBERT and CodeT5, for automated vulnerability patching across six datasets and four languages. We evaluate their accuracy and generalization to unknown vulnerabilities. Results show that while both models face challenges with fragmented or sparse context, CodeBERT performs comparatively better in such scenarios, whereas CodeT5 excels in capturing complex vulnerability patterns. CodeT5 also demonstrates superior scalability. Furthermore, we test fine-tuned models on both in-distribution (trained) and out-of-distribution (unseen) datasets. While fine-tuning improves in-distribution performance, models struggle to generalize to unseen data, highlighting challenges in robust vulnerability detection. This study benchmarks model performance, identifies limitations in generalization, and provides actionable insights to advance automated vulnerability patching for real-world security applications.},
booktitle = {Availability, Reliability and Security: ARES 2025 International Workshops, Ghent, Belgium, August 11–14, 2025, Proceedings, Part I},
pages = {73--87},
numpages = {15},
keywords = {code patching, vulnerability patching, large language models, automated program repair},
location = {Ghent, Belgium}
}

@article{vulsecret,
  title={The secret life of software vulnerabilities: A large-scale empirical study},
  author={Iannone, Emanuele and Guadagni, Roberta and Ferrucci, Filomena and De Lucia, Andrea and Palomba, Fabio},
  journal={IEEE Transactions on Software Engineering},
  volume={49},
  number={1},
  pages={44--63},
  year={2022},
  url={https://emaiannone.github.io/assets/pdf/j2.pdf},
  abstract={Software vulnerabilities are weaknesses in source code that can be potentially exploited to cause loss or harm. While researchers have been devising a number of methods to deal with vulnerabilities, there is still a noticeable lack of knowledge on their software engineering life cycle, for example how vulnerabilities are introduced and removed by developers. This information can be exploited to design more effective methods for vulnerability prevention and detection, as well as to understand the granularity at which these methods should aim. To investigate the life cycle of known software vulnerabilities, we focus on how, when, and under which circumstances the contributions to the introduction of vulnerabilities in software projects are made, as well as how long, and how they are removed. We consider 3,663 vulnerabilities with public patches from the National Vulnerability Database—pertaining to 1,096 open-source software projects on GitHub—and define an eight-step process involving both automated parts (e.g., using a procedure based on the SZZ algorithm to find the vulnerability-contributing commits) and manual analyses (e.g., how vulnerabilities were fixed). The investigated vulnerabilities can be classified in 144 categories, take on average at least 4 contributing commits before being introduced, and half of them remain unfixed for at least more than one year. Most of the contributions are done by developers with high workload, often when doing maintenance activities, and removed mostly with the addition of new source code aiming at implementing further checks on inputs. We conclude by distilling practical implications on how vulnerability detectors should work to assist developers in timely identifying these issues.},
  publisher={IEEE}
}

@inproceedings{helpfulcontext,
author = {Antal, G\'{a}bor and Bogenf\"{u}rst, Bence and Ferenc, Rudolf and Hegedus, P\'{e}ter},
title = {Identifying Helpful Context for LLM-based Vulnerability Repair: A Preliminary Study},
year = {2025},
isbn = {9798400713859},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3756681.3757078},
doi = {10.1145/3756681.3757078},
abstract = {Recent advancements in large language models (LLMs) have shown promise for automated vulnerability detection and repair in software systems. This paper investigates the performance of GPT-4o in repairing Java vulnerabilities from a widely used dataset (Vul4J), exploring how different contextual information affects automated vulnerability repair (AVR) capabilities. We compare the latest GPT-4o’s performance against previous results with GPT-4 using identical prompts. We evaluated nine additional prompts crafted by us that contain various contextual information such as CWE or CVE information, and manually extracted code contexts. Each prompt was executed three times on 42 vulnerabilities, and the resulting fix candidates were validated using Vul4J’s automated testing framework. Our results show that GPT-4o performed 11.9\% worse on average than GPT-4 with the same prompt, but was able to fix 10.5\% more distinct vulnerabilities in the three runs together. CVE information significantly improved repair rates, while the length of the task description had minimal impact. Combining CVE guidance with manually extracted code context resulted in the best performance. Using our Top-3 prompts together, GPT-4o repaired 26 (62\%) vulnerabilities at least once, outperforming both the original baseline (40\%) and its reproduction (45\%), suggesting that ensemble prompt strategies could improve vulnerability repair in zero-shot settings.},
booktitle = {Proceedings of the 29th International Conference on Evaluation and Assessment in Software Engineering},
pages = {696–700},
numpages = {5},
keywords = {LLM, vulnerability repair, contextual information, prompt engineering, software security, automated program repair},
series = {EASE '25}
}

@inproceedings{jsvulrepairstudy,
author = {Le, Tan Khang and Alimadadi, Saba and Ko, Steven Y.},
title = {A Study of Vulnerability Repair in JavaScript Programs with Large Language Models},
year = {2024},
isbn = {9798400701726},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3589335.3651463},
doi = {10.1145/3589335.3651463},
abstract = {In recent years, JavaScript has become the most widely used programming language, especially in web development. However, writing secure JavaScript code is not trivial, and programmers often make mistakes that lead to security vulnerabilities in web applications. Large Language Models (LLMs) have demonstrated substantial advancements across multiple domains, and their evolving capabilities indicate their potential for automatic code generation based on a required specification, including automatic bug fixing. In this study, we explore the accuracy of LLMs, namely ChatGPT and Bard, in finding and fixing security vulnerabilities in JavaScript programs. We also investigate the impact of context in a prompt on directing LLMs to produce a correct patch of vulnerable JavaScript code. Our experiments on real-world software vulnerabilities show that while LLMs are promising in automatic program repair of JavaScript code, achieving a correct bug fix often requires an appropriate amount of context in the prompt.},
booktitle = {Companion Proceedings of the ACM Web Conference 2024},
pages = {666–669},
numpages = {4},
keywords = {automatic program repair, cwe, javascript, large language models, prompt engineering},
location = {Singapore, Singapore},
series = {WWW '24}
}

@article{asrbib,
author = {Monperrus, Martin},
title = {Automatic Software Repair: A Bibliography},
year = {2018},
issue_date = {January 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {51},
number = {1},
issn = {0360-0300},
url = {https://doi.org/10.1145/3105906},
doi = {10.1145/3105906},
journal = {ACM Comput. Surv.},
month = jan,
articleno = {17},
numpages = {24},
keywords = {Program repair, self-healing software}
}

@inproceedings{aprplm,
  title={Automated Program Repair in the Era of Large Pre-trained Language Models},
  url={http://dx.doi.org/10.1109/ICSE48619.2023.00129},
  DOI={10.1109/icse48619.2023.00129},
  booktitle={2023 IEEE/ACM 45th International Conference on Software Engineering (ICSE)},
  publisher={IEEE},
  author={Xia, Chunqiu Steven and Wei, Yuxiang and Zhang, Lingming},
  year={2023},
  month=may, 
  pages={1482–1494} 
}

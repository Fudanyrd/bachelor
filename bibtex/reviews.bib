@comment{
SLRs, reviews, surveys, empirical studies, etc. on software engineering topics.
Added by snowballing: 16
}
@article{llm4se,
  title={Towards an understanding of large language models in software engineering tasks},
  author={Zheng, Zibin and Ning, Kaiwen and Zhong, Qingyuan and Chen, Jiachi and Chen, Wenqing and Guo, Lianghong and Wang, Weicheng and Wang, Yanlin},
  journal={Empirical Software Engineering},
  volume={30},
  number={2},
  pages={50},
  year={2025},
  url={https://arxiv.org/abs/2308.11396},
  abstract={Large Language Models (LLMs) have drawn widespread attention and research due to their astounding performance in text generation and reasoning tasks. Derivative products, like ChatGPT, have been extensively deployed and highly sought after. Meanwhile, the evaluation and optimization of LLMs in software engineering tasks, such as code generation, have become a research focus. However, there is still a lack of systematic research on applying and evaluating LLMs in software engineering. Therefore, this paper comprehensively investigate and collate the research and products combining LLMs with software engineering, aiming to answer two questions: (1) What are the current integrations of LLMs with software engineering? (2) Can LLMs effectively handle software engineering tasks? To find the answers, we have collected related literature as extensively as possible from seven mainstream databases and selected 123 timely papers published starting from 2022 for analysis. We have categorized these papers in detail and reviewed the current research status of LLMs from the perspective of seven major software engineering tasks, hoping this will help researchers better grasp the research trends and address the issues when applying LLMs. Meanwhile, we have also organized and presented papers with evaluation content to reveal the performance and effectiveness of LLMs in various software engineering tasks, guiding researchers and developers to optimize.},
  publisher={Springer}
}
@article{llm4se2,
  title={Large language models for software engineering: A systematic literature review},
  author={Hou, Xinyi and Zhao, Yanjie and Liu, Yue and Yang, Zhou and Wang, Kailong and Li, Li and Luo, Xiapu and Lo, David and Grundy, John and Wang, Haoyu},
  journal={ACM Transactions on Software Engineering and Methodology},
  volume={33},
  number={8},
  pages={1--79},
  url={https://dl.acm.org/doi/pdf/10.1145/3695988},
  year={2024},
  publisher={ACM New York, NY}
}
@misc{llm4se3,
  title={Large Language Models for Software Engineering: Survey and Open Problems}, 
  author={Angela Fan and Beliz Gokkaya and Mark Harman and Mitya Lyubarskiy and Shubho Sengupta and Shin Yoo and Jie M. Zhang},
  year={2023},
  eprint={2310.03533},
  archivePrefix={arXiv},
  primaryClass={cs.SE},
  url={https://arxiv.org/abs/2310.03533}, 
}

@article{llmcodequality,
title = {Using LLMs to enhance code quality: A systematic literature review},
journal = {Information and Software Technology},
volume = {190},
pages = {107960},
year = {2026},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2025.107960},
url = {https://www.sciencedirect.com/science/article/pii/S095058492500299X},
author = {Nawaf Alomari and Moussa Redah and Ahmad Ashraf and Mohammad Alshayeb},
keywords = {Code quality, LLM, systematic literature review},
abstract = {Context:
Large Language Models (LLMs) are increasingly used in software engineering to enhance code quality through tasks such as refactoring and code smell detection and many other tasks. Code smells are poor design decisions that can be resolved by changing the internal structure of the code without affecting its output, a process known as refactoring.
Objective:
This study systematically reviews the use of LLMs in code quality enhancement, focusing on techniques such as refactoring, smell detection, and other code improvement methods.
Method:
Using SLR techniques, we reviewed 49 studies up to September 2024, analyzing both qualitative and quantitative data to assess trends and effectiveness.
Results:
The field is active, with refactoring as the most common task, followed by smell detection. Refactored code by LLMs is not reliable. Prompting is used more frequently than fine-tuning, with few-shot learning as the leading prompting method. Java and Python are the most represented languages, while F1, Precision, Recall, and Accuracy are common evaluation metrics, along with BLEU and EM for generation tasks. Open-source and general language models are preferred, with validation datasets as the primary validation approach.
Conclusions:
LLMs show promise for code quality improvement, but challenges in optimization and reliability remain. Future research should prioritize fine-tuning for refactoring, linking LLMs to specific quality attributes, developing benchmark datasets, constructing datasets for diverse programming languages, and exploring a wider range of promoting techniques.}
}
@article{dl4defence,
  title={Deep learning for android malware defenses: a systematic literature review},
  author={Liu, Yue and Tantithamthavorn, Chakkrit and Li, Li and Liu, Yepang},
  journal={ACM Computing Surveys},
  volume={55},
  url={https://dl.acm.org/doi/pdf/10.1145/3544968},
  number={8},
  pages={1--36},
  year={2022},
  publisher={ACM New York, NY}
}
@article{ml4se,
  title={Machine/deep learning for software engineering: A systematic literature review},
  author={Wang, Simin and Huang, Liguo and Gao, Amiao and Ge, Jidong and Zhang, Tengfei and Feng, Haitao and Satyarth, Ishna and Li, Ming and Zhang, He and Ng, Vincent},
  journal={IEEE Transactions on Software Engineering},
  volume={49},
  url={https://ieeexplore.ieee.org/ielaam/32/10070338/9772253-aam.pdf},
  number={3},
  pages={1188--1231},
  year={2022},
  publisher={IEEE}
}

@misc{agent4bugfixempirical,
 abstract = {Large language models (LLMs) and LLM-based Agents have been applied to fix bugs automatically, demonstrating the capability in addressing software defects by engaging in development environment interaction, iterative validation and code modification. However, systematic analysis of these agent systems remain limited, particularly regarding performance variations among top-performing ones. In this paper, we examine six repair systems on the SWE-bench Verified benchmark for automated bug fixing. We first assess each system's overall performance, noting the instances solvable by all or none of these systems, and explore the capabilities of different systems. We also compare fault localization accuracy at file and code symbol levels and evaluate bug reproduction capabilities. Through analysis, we concluded that further optimization is needed in both the LLM capability itself and the design of Agentic flow to improve the effectiveness of the Agent in bug fixing.},
 archiveprefix = {arXiv},
 author = {Xiangxin Meng and Zexiong Ma and Pengfei Gao and Chao Peng},
 eprint = {2411.10213},
 primaryclass = {cs.SE},
 title = {An Empirical Study on LLM-based Agents for Automated Bug Fixing},
 url = {https://arxiv.org/abs/2411.10213},
 year = {2025}
}

@misc{agenticbugreproductioneffective,
 abstract = {Bug reports often lack sufficient detail for developers to reproduce and fix the underlying defects. Bug Reproduction Tests (BRTs), tests that fail when the bug is present and pass when it has been resolved, are crucial for debugging, but they are rarely included in bug reports, both in open-source and in industrial settings. Thus, automatically generating BRTs from bug reports has the potential to accelerate the debugging process and lower time to repair. This paper investigates automated BRT generation within an industry setting, specifically at Google, focusing on the challenges of a large-scale, proprietary codebase and considering real-world industry bugs extracted from Google's internal issue tracker. We adapt and evaluate a state-of-the-art BRT generation technique, LIBRO, and present our agent-based approach, BRT Agent, which makes use of a fine-tuned Large Language Model (LLM) for code editing. Our BRT Agent significantly outperforms LIBRO, achieving a 28% plausible BRT generation rate, compared to 10% by LIBRO, on 80 human-reported bugs from Google's internal issue tracker. We further investigate the practical value of generated BRTs by integrating them with an Automated Program Repair (APR) system at Google. Our results show that providing BRTs to the APR system results in 30% more bugs with plausible fixes. Additionally, we introduce Ensemble Pass Rate (EPR), a metric which leverages the generated BRTs to select the most promising fixes from all fixes generated by APR system. Our evaluation on EPR for Top-K and threshold-based fix selections demonstrates promising results and trade-offs. For example, EPR correctly selects a plausible fix from a pool of 20 candidates in 70% of cases, based on its top-1 ranking.},
 archiveprefix = {arXiv},
 author = {Runxiang Cheng and Michele Tufano and Jürgen Cito and José Cambronero and Pat Rondon and Renyao Wei and Aaron Sun and Satish Chandra},
 eprint = {2502.01821},
 primaryclass = {cs.SE},
 title = {Agentic Bug Reproduction for Effective Automated Program Repair at Google},
 url = {https://arxiv.org/abs/2502.01821},
 year = {2025}
}

@misc{langgraphbugfix,
 abstract = {This paper presents a novel framework for automated code generation and debugging, designed to improve accuracy, efficiency, and scalability in software development. The proposed system integrates three core components LangGraph, GLM4 Flash, and ChromaDB within a four step iterative workflow to deliver robust performance and seamless functionality.
LangGraph serves as a graph-based library for orchestrating tasks, providing precise control and execution while maintaining a unified state object for dynamic updates and consistency. It supports multi-agent, hierarchical, and sequential processes, making it highly adaptable to complex software engineering workflows. GLM4 Flash, a large language model, leverages its advanced capabilities in natural language understanding, contextual reasoning, and multilingual support to generate accurate code snippets based on user prompts. ChromaDB acts as a vector database for semantic search and contextual memory storage, enabling the identification of patterns and the generation of context-aware bug fixes based on historical data.
The system operates through a structured four-step process: (1) Code Generation, which translates natural language descriptions into executable code; (2) Code Execution, which validates the code by identifying runtime errors and inconsistencies; (3) Code Repair, which iteratively refines buggy code using ChromaDB's memory capabilities and LangGraph's state tracking; and (4) Code Update, which ensures the code meets functional and performance requirements through iterative modifications.},
 archiveprefix = {arXiv},
 author = {Jialin Wang and Zhihua Duan},
 eprint = {2502.18465},
 primaryclass = {cs.SE},
 title = {Empirical Research on Utilizing LLM-based Agents for Automated Bug Fixing via LangGraph},
 url = {https://arxiv.org/abs/2502.18465},
 year = {2025}
}

@misc{agents,
  title={Agent S: An Open Agentic Framework that Uses Computers Like a Human}, 
  author={Saaket Agashe and Jiuzhou Han and Shuyu Gan and Jiachen Yang and Ang Li and Xin Eric Wang},
  year={2024},
  eprint={2410.08164},
  archivePrefix={arXiv},
  primaryClass={cs.AI},
  url={https://arxiv.org/abs/2410.08164}, 
}

@inproceedings{incorrectfix,
  author = {Yin, Zuoning and Yuan, Ding and Zhou, Yuanyuan and Pasupathy, Shankar and Bairavasundaram, Lakshmi},
  title = {How do fixes become bugs?},
  year = {2011},
  isbn = {9781450304436},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  url = {https://doi.org/10.1145/2025113.2025121},
  doi = {10.1145/2025113.2025121},
  abstract = {Software bugs affect system reliability. When a bug is exposed in the field, developers need to fix them. Unfortunately, the bug-fixing process can also introduce errors, which leads to buggy patches that further aggravate the damage to end users and erode software vendors' reputation.This paper presents a comprehensive characteristic study on incorrect bug-fixes from large operating system code bases including Linux, OpenSolaris, FreeBSD and also a mature commercial OS developed and evolved over the last 12 years, investigating not only themistake patterns during bug-fixing but also the possible human reasons in the development process when these incorrect bug-fixes were introduced. Our major findings include: (1) at least 14.8\%--24.4\% of sampled fixes for post-release bugs in these large OSes are incorrect and have made impacts to end users. (2) Among several common bug types, concurrency bugs are the most difficult to fix correctly: 39\% of concurrency bug fixes are incorrect. (3) Developers and reviewers for incorrect fixes usually do not have enough knowledge about the involved code. For example, 27\% of the incorrect fixes are made by developers who have never touched the source code files associated with the fix. Our results provide useful guidelines to design new tools and also to improve the development process. Based on our findings, the commercial software vendor whose OS code we evaluated is building a tool to improve the bug fixing and code reviewing process.},
  booktitle = {Proceedings of the 19th ACM SIGSOFT Symposium and the 13th European Conference on Foundations of Software Engineering},
  pages = {26-36},
  numpages = {11},
  keywords = {bug fixing, human factor, incorrect fixes, software bugs, testing},
  location = {Szeged, Hungary},
  series = {ESEC/FSE '11}
}

@article{copilotsecurity,
  title={Asleep at the keyboard? assessing the security of github copilot's code contributions},
  author={Pearce, Hammond and Ahmad, Baleegh and Tan, Benjamin and Dolan-Gavitt, Brendan and Karri, Ramesh},
  journal={Communications of the ACM},
  volume={68},
  number={2},
  pages={96--105},
  year={2025},
  url={https://arxiv.org/abs/2108.09293},
  publisher={ACM New York, NY, USA}
}

@article{promptsurvey,
  title={Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing},
  author={Liu, Pengfei and Yuan, Weizhe and Fu, Jinlan and Jiang, Zhengbao and Hayashi, Hiroaki and Neubig, Graham},
  journal={ACM computing surveys},
  volume={55},
  number={9},
  pages={1--35},
  year={2023},
  url={https://dl.acm.org/doi/pdf/10.1145/3560815},
  publisher={ACM New York, NY}
}

@article{ragsurvey,
  title={Retrieval-augmented generation for ai-generated content: A survey},
  author={Zhao, Penghao and Zhang, Hailin and Yu, Qinhan and Wang, Zhengren and Geng, Yunteng and Fu, Fangcheng and Yang, Ling and Zhang, Wentao and Jiang, Jie and Cui, Bin},
  journal={Data Science and Engineering},
  pages={1--29},
  year={2026},
  url={https://link.springer.com/content/pdf/10.1007/s41019-025-00335-5.pdf},
  publisher={Springer}
}

@inproceedings{apranalysis,
  title={An analysis of the automatic bug fixing performance of chatgpt},
  author={Sobania, Dominik and Briesch, Martin and Hanna, Carol and Petke, Justyna},
  booktitle={2023 IEEE/ACM International Workshop on Automated Program Repair (APR)},
  pages={23--30},
  url={https://arxiv.org/abs/2301.08653},
  abstract={To support software developers in finding and fixing software bugs, several automated program repair techniques have been introduced. Given a test suite, standard methods usually either synthesize a repair, or navigate a search space of software edits to find test-suite passing variants. Recent program repair methods are based on deep learning approaches. One of these novel methods, which is not primarily intended for automated program repair, but is still suitable for it, is ChatGPT. The bug fixing performance of ChatGPT, however, is so far unclear. Therefore, in this paper we evaluate ChatGPT on the standard bug fixing benchmark set, QuixBugs, and compare the perfor mance with the results of several other approaches reported in the literature. We find that ChatGPT’s bug fixing performance is competitive to the common deep learning approaches CoCoNut and Codex and notably better than the results reported for the standard program repair approaches. In contrast to previous approaches, ChatGPT offers a dialogue system through which further information, e.g., the expected output for a certain input or an observed error message, can be entered. By providing such hints to ChatGPT, its success rate can be further increased, fixing 31 out of 40 bugs, outperforming state-of-the-art.},
  year={2023},
  organization={IEEE}
}

@comment{
Empirical studies on practitioner's debugging behavior, expectations, etc.
}

@inproceedings{expectationonfl,
  title={Practitioners' expectations on automated fault localization},
  author={Kochhar, Pavneet Singh and Xia, Xin and Lo, David and Li, Shanping},
  booktitle={Proceedings of the 25th international symposium on software testing and analysis},
  pages={165--176},
  url={https://dl.acm.org/doi/pdf/10.1145/2931037.2931051},
  abstract={Software engineering practitioners often spend significant amount of time and effort to debug. To help practitioners perform this crucial task, hundreds of papers have proposed various fault localization techniques. Fault localization helps practitioners to find the location of a defect given its symptoms (e.g., program failures). These localization techniques have pinpointed the locations of bugs of various systems of diverse sizes, with varying degrees of success, and for various usage scenarios. Unfortunately, it is unclear whether practitioners appreciate this line of research. To fill this gap, we performed an empirical study by surveying 386 practitioners from more than 30 countries across 5 continents about their expectations of research in fault localization. In particular, we investigated a number of factors that impact practitioners' willingness to adopt a fault localization technique. We then compared what practitioners need and the current state-of-research by performing a literature review of papers on fault localization techniques published in ICSE, FSE, ESEC-FSE, ISSTA, TSE, and TOSEM in the last 5 years (2011-2015). From this comparison, we highlight the directions where researchers need to put effort to develop fault localization techniques that matter to practitioners.},
  year={2016}
}
@inproceedings{autodebughelpful,
  title={Are automated debugging techniques actually helping programmers?},
  author={Parnin, Chris and Orso, Alessandro},
  booktitle={Proceedings of the 2011 international symposium on software testing and analysis},
  pages={199--209},
  url={https://dl.acm.org/doi/pdf/10.1145/2001420.2001445},
  abstract={Debugging is notoriously difficult and extremely time consuming. Researchers have therefore invested a considerable amount of effort in developing automated techniques and tools for supporting various debugging tasks. Although potentially useful, most of these techniques have yet to demonstrate their practical effectiveness. One common limitation of existing approaches, for instance, is their reliance on a set of strong assumptions on how developers behave when debugging (e.g., the fact that examining a faulty statement in isolation is enough for a developer to understand and fix the corresponding bug). In more general terms, most existing techniques just focus on selecting subsets of potentially faulty statements and ranking them according to some criterion. By doing so, they ignore the fact that understanding the root cause of a failure typically involves complex activities, such as navigating program dependencies and rerunning the program with different inputs. The overall goal of this research is to investigate how developers use and benefit from automated debugging tools through a set of human studies. As a first step in this direction, we perform a preliminary study on a set of developers by providing them with an automated debugging tool and two tasks to be performed with and without the tool. Our results provide initial evidence that several assumptions made by automated debugging techniques do not hold in practice. Through an analysis of the results, we also provide insights on potential directions for future work in the area of automated debugging.},
  year={2011}
}

@inproceedings{debugbench,
  title={Where is the bug and how is it fixed? an experiment with practitioners},
  author={B{\"o}hme, Marcel and Soremekun, Ezekiel O and Chattopadhyay, Sudipta and Ugherughe, Emamurho and Zeller, Andreas},
  booktitle={Proceedings of the 2017 11th joint meeting on foundations of software engineering},
  pages={117--128},
  url={https://dl.acm.org/doi/pdf/10.1145/3106237.3106255},
  abstract={Research has produced many approaches to automatically locate, explain, and repair software bugs. But do these approaches relate to the way practitioners actually locate, understand, and fix bugs? To help answer this question, we have collected a dataset named DBGBENCH --- the correct fault locations, bug diagnoses, and software patches of 27 real errors in open-source C projects that were consolidated from hundreds of debugging sessions of professional software engineers. Moreover, we shed light on the entire debugging process, from constructing a hypothesis to submitting a patch, and how debugging time, difficulty, and strategies vary across practitioners and types of errors. Most notably, DBGBENCH can serve as reality check for novel automated debugging and repair techniques.},
  keywords={software debugging, software testing},
  year={2017}
}

@misc{impactclm,
  title={Impact of Code Language Models on Automated Program Repair}, 
  author={Nan Jiang and Kevin Liu and Thibaud Lutellier and Lin Tan},
  year={2023},
  eprint={2302.05020},
  archivePrefix={arXiv},
  primaryClass={cs.SE},
  abstract={Automated program repair (APR) aims to help developers improve software reliability by generating patches for buggy programs. Although many code language models (CLM) are developed and effective in many software tasks such as code completion, there has been little comprehensive, in-depth work to evaluate CLMs' fixing capabilities and to fine-tune CLMs for the APR task. Firstly, this work is the first to evaluate ten CLMs on four APR benchmarks, which shows that surprisingly, the best CLM, as is, fixes 72% more bugs than the state-of-the-art deep-learning (DL)-based APR techniques. Secondly, one of the four APR benchmarks was created by us in this paper to avoid data leaking for a fair evaluation. Thirdly, it is the first work to fine-tune CLMs with APR training data, which shows that fine-tuning brings 31%-1,267% improvement to CLMs and enables them to fix 46%-164% more bugs than existing DL-based APR techniques. Fourthly, this work studies the impact of buggy lines, showing that CLMs, as is, cannot make good use of the buggy lines to fix bugs, yet fine-tuned CLMs could potentially over-rely on buggy lines. Lastly, this work analyzes the size, time, and memory efficiency of different CLMs. This work shows promising directions for the APR domain, such as fine-tuning CLMs with APR-specific designs, and also raises awareness of fair and comprehensive evaluations of CLMs and calls for more transparent reporting of open-source repositories used in the pre-training data to address the data leaking problem.},
  url={https://arxiv.org/abs/2302.05020}, 
}

@inproceedings{asrsurvey,
  title={Automatic software repair: A survey},
  author={Gazzola, Luca and Micucci, Daniela and Mariani, Leonardo},
  booktitle={Proceedings of the 40th International Conference on Software Engineering},
  url={https://dl.acm.org/doi/pdf/10.1145/3180155.3182526},
  abstract={Debugging software failures is still a painful, time consuming, and expensive process. For instance, recent studies showed that debugging activities often account for about 50% of the overall development cost of software products. There are many factors contributing to the cost of debugging, but the most impacting one is the extensive manual effort that is still required to identify and remove faults. So far, the automation of debugging activities essentially resulted in the development of techniques that provide useful insights about the possible locations of faults, the inputs and states of the application responsible for the failures, as well as the anomalous operations executed during failures. However, developers must still put a relevant effort on the analysis of the failed executions to exactly identify the faults that must be fixed. In addition, these techniques do not help the developers with the synthesis of an appropriate fix.},
  pages={1219--1219},
  year={2018}
}

@article{reversibledebug,
  title={Reversible debugging software-quantify the time and cost saved using reversible debuggers},
  author={Britton, Tom and Jeng, Lisa and Carver, Graham and Cheak, Paul and Katzenellenbogen, Tomer},
  journal={University of Cambridge},
  year={2013}
}

@misc{llminfl,
  title={Large Language Models in Fault Localisation}, 
  author={Yonghao Wu and Zheng Li and Jie M. Zhang and Mike Papadakis and Mark Harman and Yong Liu},
  year={2023},
  eprint={2308.15276},
  archivePrefix={arXiv},
  primaryClass={cs.SE},
  keywords={Large Language Model, Fault Localisation, ChatGPT, Empirical Study},
  abstract={Large Language Models (LLMs) have shown promise in multiple software engineering tasks including code generation, program repair, code summarisation, and test generation. Fault localisation is instrumental in enabling automated debugging and repair of programs and was prominently featured as a highlight during the launch event of ChatGPT-4. Nevertheless, the performance of LLMs compared to state-of-the-art methods, as well as the impact of prompt design and context length on their efficacy, remains unclear. To fill this gap, this paper presents an in-depth investigation into the capability of ChatGPT-3.5 and ChatGPT-4, the two state-of-the-art LLMs, on fault localisation. Using the widely-adopted large-scale Defects4J dataset, we compare the two LLMs with the existing fault localisation techniques. We also investigate the consistency of LLMs in fault localisation, as well as how prompt engineering and the length of code context affect the fault localisation effectiveness. Our findings demonstrate that within function-level context, ChatGPT-4 outperforms all the existing fault localisation methods. Additional error logs can further improve ChatGPT models' localisation accuracy and consistency, with an average 46.9% higher accuracy over the state-of-the-art baseline SmartFL on the Defects4J dataset in terms of TOP-1 metric. However, when the code context of the Defects4J dataset expands to the class-level, ChatGPT-4's performance suffers a significant drop, with 49.9% lower accuracy than SmartFL under TOP-1 metric. These observations indicate that although ChatGPT can effectively localise faults under specific conditions, limitations are evident. Further research is needed to fully harness the potential of LLMs like ChatGPT for practical fault localisation applications.},
  url={https://arxiv.org/abs/2308.15276}, 
}

@article{apr4vul,
author={Bui Quang-Cuong and Paramitha Ranindya and Vu Duc-Ly and Massacci Fabio and Scandariato Riccardo},
title="APR4Vul: an empirical study of automatic program repair techniques on real-world Java vulnerabilities",
journal="Empirical Software Engineering",
year="2023",
month="Dec",
day="06",
volume="29",
number="1",
pages="18",
abstract="Security vulnerability fixes could be a promising research avenue for Automated Program Repair (APR) techniques. In recent years, APR tools have been thoroughly developed for fixing generic bugs. However, the area is still relatively unexplored when it comes to fixing security bugs or vulnerabilities. In this paper, we evaluate nine state-of-the-art APR tools and one vulnerability-specific repair tool. In particular, we investigate their ability to generate patches for 79 real-world Java vulnerabilities in the Vul4J dataset, as well as the level of trustworthiness of these patches. We evaluate the tools with respect to their ability to generate security patches that are (i) testable, (ii) having the positive effect of closing the vulnerability, and (iii) not having side effects from a functional point of view. Our results show that the evaluated APR tools were able to generate testable patches for around 20{\%} of the considered vulnerabilities. On average, nearly 73{\%} of the testable patches indeed eliminate the vulnerabilities, but only 44{\%} of them could actually fix security bugs while maintaining the functionalities. To understand the root cause of this phenomenon, we conduct a detailed comparative study of the general bug fix patterns in Defect4J and the vulnerability fix patterns in ExtraVul (which we extend from Vul4J). Our investigation shows that, although security patches are short in terms of lines of code, they contain unique characteristics in their fix patterns compared to general bugs. For example, many security fixes require adding method calls. These method calls contain specific input validation-related keywords, such as encode, normalize, and trim. In this regard, our study suggests that additional repair patterns should be implemented for existing APR tools to fix more types of security vulnerabilities.",
issn="1573-7616",
doi="10.1007/s10664-023-10415-7",
url="https://doi.org/10.1007/s10664-023-10415-7"
}

@inproceedings{evaluateavr,
author = {Khan, Zanis Ali and Garg, Aayush and Tang, Qiang},
title = {A Multi-dataset Evaluation of Models for Automated Vulnerability Repair},
year = {2025},
isbn = {978-3-032-00629-5},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-032-00630-1_5},
doi = {10.1007/978-3-032-00630-1_5},
abstract = {Software vulnerabilities pose significant security threats, requiring effective mitigation. While Automated Program Repair (APR) has advanced in fixing general bugs, vulnerability patching—a security-critical aspect of APR—remains underexplored. This study investigates pre-trained language models, CodeBERT and CodeT5, for automated vulnerability patching across six datasets and four languages. We evaluate their accuracy and generalization to unknown vulnerabilities. Results show that while both models face challenges with fragmented or sparse context, CodeBERT performs comparatively better in such scenarios, whereas CodeT5 excels in capturing complex vulnerability patterns. CodeT5 also demonstrates superior scalability. Furthermore, we test fine-tuned models on both in-distribution (trained) and out-of-distribution (unseen) datasets. While fine-tuning improves in-distribution performance, models struggle to generalize to unseen data, highlighting challenges in robust vulnerability detection. This study benchmarks model performance, identifies limitations in generalization, and provides actionable insights to advance automated vulnerability patching for real-world security applications.},
booktitle = {Availability, Reliability and Security: ARES 2025 International Workshops, Ghent, Belgium, August 11–14, 2025, Proceedings, Part I},
pages = {73--87},
numpages = {15},
keywords = {code patching, vulnerability patching, large language models, automated program repair},
location = {Ghent, Belgium}
}

@article{vulsecret,
  title={The secret life of software vulnerabilities: A large-scale empirical study},
  author={Iannone, Emanuele and Guadagni, Roberta and Ferrucci, Filomena and De Lucia, Andrea and Palomba, Fabio},
  journal={IEEE Transactions on Software Engineering},
  volume={49},
  number={1},
  pages={44--63},
  year={2022},
  url={https://emaiannone.github.io/assets/pdf/j2.pdf},
  abstract={Software vulnerabilities are weaknesses in source code that can be potentially exploited to cause loss or harm. While researchers have been devising a number of methods to deal with vulnerabilities, there is still a noticeable lack of knowledge on their software engineering life cycle, for example how vulnerabilities are introduced and removed by developers. This information can be exploited to design more effective methods for vulnerability prevention and detection, as well as to understand the granularity at which these methods should aim. To investigate the life cycle of known software vulnerabilities, we focus on how, when, and under which circumstances the contributions to the introduction of vulnerabilities in software projects are made, as well as how long, and how they are removed. We consider 3,663 vulnerabilities with public patches from the National Vulnerability Database—pertaining to 1,096 open-source software projects on GitHub—and define an eight-step process involving both automated parts (e.g., using a procedure based on the SZZ algorithm to find the vulnerability-contributing commits) and manual analyses (e.g., how vulnerabilities were fixed). The investigated vulnerabilities can be classified in 144 categories, take on average at least 4 contributing commits before being introduced, and half of them remain unfixed for at least more than one year. Most of the contributions are done by developers with high workload, often when doing maintenance activities, and removed mostly with the addition of new source code aiming at implementing further checks on inputs. We conclude by distilling practical implications on how vulnerability detectors should work to assist developers in timely identifying these issues.},
  publisher={IEEE}
}

@inproceedings{helpfulcontext,
author = {Antal, G\'{a}bor and Bogenf\"{u}rst, Bence and Ferenc, Rudolf and Hegedus, P\'{e}ter},
title = {Identifying Helpful Context for LLM-based Vulnerability Repair: A Preliminary Study},
year = {2025},
isbn = {9798400713859},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3756681.3757078},
doi = {10.1145/3756681.3757078},
abstract = {Recent advancements in large language models (LLMs) have shown promise for automated vulnerability detection and repair in software systems. This paper investigates the performance of GPT-4o in repairing Java vulnerabilities from a widely used dataset (Vul4J), exploring how different contextual information affects automated vulnerability repair (AVR) capabilities. We compare the latest GPT-4o’s performance against previous results with GPT-4 using identical prompts. We evaluated nine additional prompts crafted by us that contain various contextual information such as CWE or CVE information, and manually extracted code contexts. Each prompt was executed three times on 42 vulnerabilities, and the resulting fix candidates were validated using Vul4J’s automated testing framework. Our results show that GPT-4o performed 11.9\% worse on average than GPT-4 with the same prompt, but was able to fix 10.5\% more distinct vulnerabilities in the three runs together. CVE information significantly improved repair rates, while the length of the task description had minimal impact. Combining CVE guidance with manually extracted code context resulted in the best performance. Using our Top-3 prompts together, GPT-4o repaired 26 (62\%) vulnerabilities at least once, outperforming both the original baseline (40\%) and its reproduction (45\%), suggesting that ensemble prompt strategies could improve vulnerability repair in zero-shot settings.},
booktitle = {Proceedings of the 29th International Conference on Evaluation and Assessment in Software Engineering},
pages = {696–700},
numpages = {5},
keywords = {LLM, vulnerability repair, contextual information, prompt engineering, software security, automated program repair},
series = {EASE '25}
}

@inproceedings{jsvulrepairstudy,
author = {Le, Tan Khang and Alimadadi, Saba and Ko, Steven Y.},
title = {A Study of Vulnerability Repair in JavaScript Programs with Large Language Models},
year = {2024},
isbn = {9798400701726},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3589335.3651463},
doi = {10.1145/3589335.3651463},
abstract = {In recent years, JavaScript has become the most widely used programming language, especially in web development. However, writing secure JavaScript code is not trivial, and programmers often make mistakes that lead to security vulnerabilities in web applications. Large Language Models (LLMs) have demonstrated substantial advancements across multiple domains, and their evolving capabilities indicate their potential for automatic code generation based on a required specification, including automatic bug fixing. In this study, we explore the accuracy of LLMs, namely ChatGPT and Bard, in finding and fixing security vulnerabilities in JavaScript programs. We also investigate the impact of context in a prompt on directing LLMs to produce a correct patch of vulnerable JavaScript code. Our experiments on real-world software vulnerabilities show that while LLMs are promising in automatic program repair of JavaScript code, achieving a correct bug fix often requires an appropriate amount of context in the prompt.},
booktitle = {Companion Proceedings of the ACM Web Conference 2024},
pages = {666–669},
numpages = {4},
keywords = {automatic program repair, cwe, javascript, large language models, prompt engineering},
location = {Singapore, Singapore},
series = {WWW '24}
}

@article{asrbib,
author = {Monperrus, Martin},
title = {Automatic Software Repair: A Bibliography},
year = {2018},
issue_date = {January 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {51},
number = {1},
issn = {0360-0300},
url = {https://doi.org/10.1145/3105906},
doi = {10.1145/3105906},
journal = {ACM Comput. Surv.},
month = jan,
articleno = {17},
numpages = {24},
keywords = {Program repair, self-healing software}
}

@inproceedings{aprplm,
  title={Automated Program Repair in the Era of Large Pre-trained Language Models},
  url={http://dx.doi.org/10.1109/ICSE48619.2023.00129},
  DOI={10.1109/icse48619.2023.00129},
  booktitle={2023 IEEE/ACM 45th International Conference on Software Engineering (ICSE)},
  publisher={IEEE},
  author={Xia, Chunqiu Steven and Wei, Yuxiang and Zhang, Lingming},
  year={2023},
  month=may, 
  location = {Melbourne, Victoria, Australia},
  series = {ICSE '23},
  pages={1482–1494} 
}

@inproceedings{overfitinapr,
author = {Smith, Edward K. and Barr, Earl T. and Le Goues, Claire and Brun, Yuriy},
title = {Is the cure worse than the disease? overfitting in automated program repair},
year = {2015},
isbn = {9781450336758},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2786805.2786825},
doi = {10.1145/2786805.2786825},
abstract = {Automated program repair has shown promise for reducing the significant manual effort debugging requires. This paper addresses a deficit of earlier evaluations of automated repair techniques caused by repairing programs and evaluating generated patches' correctness using the same set of tests. Since tests are an imperfect metric of program correctness, evaluations of this type do not discriminate between correct patches and patches that overfit the available tests and break untested but desired functionality. This paper evaluates two well-studied repair tools, GenProg and TrpAutoRepair, on a publicly available benchmark of bugs, each with a human-written patch. By evaluating patches using tests independent from those used during repair, we find that the tools are unlikely to improve the proportion of independent tests passed, and that the quality of the patches is proportional to the coverage of the test suite used during repair. For programs that pass most tests, the tools are as likely to break tests as to fix them. However, novice developers also overfit, and automated repair performs no worse than these developers. In addition to overfitting, we measure the effects of test suite coverage, test suite provenance, and starting program quality, as well as the difference in quality between novice-developer-written and tool-generated patches when quality is assessed with a test suite independent from the one used for patch generation.},
booktitle = {Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering},
pages = {532--543},
numpages = {12},
keywords = {automated program repair, empirical evaluation, independent evaluation},
location = {Bergamo, Italy},
series = {ESEC/FSE 2015}
}

@article{demystifymemorization,
author = {Kong, Jiaolong and Xie, Xiaofei and Liu, Shangqing},
title = {Demystifying Memorization in LLM-Based Program Repair via a General Hypothesis Testing Framework},
year = {2025},
issue_date = {July 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {FSE},
url = {https://doi.org/10.1145/3729390},
doi = {10.1145/3729390},
abstract = {Large Language Models (LLMs) have achieved remarkable success in various applications, particularly in code-related tasks such as code generation and program repair, setting new performance benchmarks. However, the extensive use of large training corpora raises concerns about whether these achievements stem from genuine understanding or mere memorization of training data -- a question often overlooked in current research. This paper aims to study the memorization issue within LLM-based program repair by investigating whether the correct patches generated by LLMs are the result of memorization. The key challenge lies in the absence of ground truth for confirming memorization, leading to various ad-hoc methods designed for its detection. To address this challenge, we first propose a general framework that formalizes memorization detection as a general hypothesis testing problem, where existing approaches can be unified by defining a low-probability event under the null hypothesis that the data is not memorized. The occurrence of such an event leads to the rejection of the null hypothesis, indicating potential memorization. Based on this framework, we design two specific methods (i.e., low-probability events) to detect potential memorization: 1) basic ground-truth matching, and 2) reassessment after substantial code mutation. We investigate the memorization issue in LLM-based program repair using two datasets: Defects4J, a widely used benchmark that is likely included in the training data, and GitBug-Java, a new dataset that is unlikely to be part of the training data. Our findings reveal that a significant portion of correct patches exactly match the ground truths in Defects4J (e.g., 78.83\% and 87.42\% on GPT-3.5 and CodeLlama-7b, respectively). Moreover, even after significant modifications to the buggy code, where the original repairs should not be generated, a considerable percentage of bugs (e.g., 81.82\% on GPT-3.5 and 88.24\% on CodeLlama-7b) continue to be fixed exactly as in the original bug fixes, indicating a high likelihood of memorization. Furthermore, we evaluate existing memorization detection methods and demonstrate their ineffectiveness in this context (e.g., most AUROCs are below 0.5). The theoretical analysis under our hypothesis testing framework shows that their defined events may not meet the requirements for being low-probability. The study highlights the critical need for more robust and rigorous evaluations in LLM-based software engineering research, ensuring a clear distinction between true problem-solving capabilities and mere memorization.},
journal = {Proc. ACM Softw. Eng.},
month = jun,
articleno = {FSE120},
numpages = {23},
keywords = {Code Memorization, Program Repair}
}

@inproceedings{fusellm4apr,
author = {Wei, Yuxiang and Xia, Chunqiu Steven and Zhang, Lingming},
title = {Copiloting the Copilots: Fusing Large Language Models with Completion Engines for Automated Program Repair},
year = {2023},
isbn = {9798400703270},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3611643.3616271},
doi = {10.1145/3611643.3616271},
abstract = {During Automated Program Repair (APR), it can be challenging&nbsp;to synthesize correct patches for real-world systems in general-purpose programming languages. Recent Large Language Models&nbsp;(LLMs) have been shown to be helpful “copilots” in assisting developers with various coding tasks, and have also been directly&nbsp;applied for patch synthesis. However, most LLMs treat programs as&nbsp;sequences of tokens, meaning that they are ignorant of the underlying semantics constraints of the target programming language. This&nbsp;results in plenty of statically invalid generated patches, impeding&nbsp;the practicality of the technique. Therefore, we propose Repilot,&nbsp;a framework to further copilot the AI “copilots” (i.e., LLMs) by&nbsp;synthesizing more valid patches during the repair process. Our key&nbsp;insight is that many LLMs produce outputs autoregressively (i.e.,&nbsp;token by token), resembling human writing programs, which can&nbsp;be significantly boosted and guided through a Completion Engine.&nbsp;Repilot synergistically synthesizes a candidate patch through the&nbsp;interaction between an LLM and a Completion Engine, which 1)&nbsp;prunes away infeasible tokens suggested by the LLM and 2) proactively completes the token based on the suggestions provided by the&nbsp;Completion Engine. Our evaluation on a subset of the widely-used&nbsp;Defects4j 1.2 and 2.0 datasets shows that Repilot fixes 66 and 50&nbsp;bugs, respectively, surpassing the best-performing baseline by 14&nbsp;and 16 bugs fixed. More&nbsp;importantly, Repilot is capable of producing more valid and correct patches than the base LLM when given&nbsp;the same generation budget.},
booktitle = {Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {172–184},
numpages = {13},
keywords = {Completion Engine, Large Language Model, Program Repair},
location = {San Francisco, CA, USA},
series = {ESEC/FSE 2023}
}

@inbook{usefulaivuldet,
author = {Steenhoek, Benjamin and Sivaraman, Kalpathy and Gonzalez, Renata Saldivar and Mohylevskyy, Yevhen and Moghaddam, Roshanak Zilouchian and Le, Wei},
title = {Closing the Gap: A User Study on the Real-World Usefulness of AI-Powered Vulnerability Detection \&amp; Repair in the IDE},
year = {2025},
isbn = {9798331505691},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE55347.2025.00126},
abstract = {Security vulnerabilities impose significant costs on users and organizations. Detecting and addressing these vulnerabilities early is crucial to avoid exploits and reduce development costs. Recent studies have shown that deep learning models can effectively detect security vulnerabilities. Yet, little research explores how to adapt these models from benchmark tests to practical applications, and whether they can be useful in practice.This paper presents the first empirical study of a vulnerability detection and fix tool with professional software developers on real projects that they own. We implemented DeepVulGuard, an IDE-integrated tool based on state-of-the-art detection and fix models, and show that it has promising performance on benchmarks of historic vulnerability data. DeepVulGuard scans code for vulnerabilities (including identifying the vulnerability type and vulnerable region of code), suggests fixes, provides natural-language explanations for alerts and fixes, leveraging chat interfaces. We recruited 17 professional software developers at Microsoft, observed their usage of the tool on their code, and conducted interviews to assess the tool's usefulness, speed, trust, relevance, and workflow integration. We also gathered detailed qualitative feedback on users' perceptions and their desired features. Study participants scanned a total of 24 projects, 6.9k files, and over 1.7 million lines of source code, and generated 170 alerts and 50 fix suggestions. We find that although state-of-the-art AI-powered detection and fix tools show promise, they are not yet practical for real-world use due to a high rate of false positives and non-applicable fixes. User feedback reveals several actionable pain points, ranging from incomplete context to lack of customization for the user's codebase. Additionally, we explore how AI features, including confidence scores, explanations, and chat interaction, can apply to vulnerability detection and fixing. Based on these insights, we offer practical recommendations for evaluating and deploying AI detection and fix models. Our code and data are available at this link: https://doi.org/10.6084/m9.figshare.26367139.},
booktitle = {Proceedings of the IEEE/ACM 47th International Conference on Software Engineering},
pages = {2650--2662},
numpages = {13}
}

@misc{studypromptdesignadvantages,
author = {Cao, Jialun and Li, Meiziniu and Wen, Ming and Cheung, Shing-Chi},
title = {A study on prompt design, advantages and limitations of ChatGPT for deep learning program repair},
year = {2025},
issue_date = {May 2025},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {32},
number = {1},
issn = {0928-8910},
url = {https://doi.org/10.1007/s10515-025-00492-x},
doi = {10.1007/s10515-025-00492-x},
abstract = {The emergence of large language models (LLMs) such as ChatGPT has revolutionized many fields. In particular, recent advances in LLMs have triggered various studies examining the use of these models for software development tasks, such as program repair, code understanding, and code generation. Prior studies have shown the capability of ChatGPT in repairing conventional programs. However, debugging deep learning (DL) programs poses unique challenges since the decision logic is not directly encoded in the source code. This requires LLMs to not only parse the source code syntactically but also understand the intention of DL programs. Therefore, ChatGPT’s capability in repairing DL programs remains unknown. To fill this gap, our study aims to answer three research questions: (1) Can ChatGPT debug DL programs effectively? (2) How can ChatGPT’s repair performance be improved by prompting? (3) In which way can dialogue help facilitate the repair? Our study analyzes the typical information that is useful for prompt design and suggests enhanced prompt templates that are more efficient for repairing DL programs. On top of them, we summarize the dual perspectives (i.e., advantages and disadvantages) of ChatGPT’s ability, such as its handling of API misuse and recommendation, and its shortcomings in identifying default parameters. Our findings indicate that ChatGPT has the potential to repair DL programs effectively and that prompt engineering and dialogue can further improve its performance by providing more code intention. We also identified the key intentions that can enhance ChatGPT’s program repairing capability.},
journal = {Automated Software Engg.},
month = mar,
numpages = {29},
keywords = {Large language model, LLM4SE, Automatic program repair, Deep learning program repair}
}


@inproceedings{feasibleapr4programmingassignments,
author = {Yi, Jooyong and Ahmed, Umair Z. and Karkare, Amey and Tan, Shin Hwei and Roychoudhury, Abhik},
title = {A feasibility study of using automated program repair for introductory programming assignments},
year = {2017},
isbn = {9781450351058},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106237.3106262},
doi = {10.1145/3106237.3106262},
abstract = {Despite the fact an intelligent tutoring system for programming (ITSP) education has long attracted interest, its widespread use has been hindered by the difficulty of generating personalized feedback automatically. Meanwhile, automated program repair (APR) is an emerging new technology that automatically fixes software bugs, and it has been shown that APR can fix the bugs of large real-world software. In this paper, we study the feasibility of marrying intelligent programming tutoring and APR. We perform our feasibility study with four state-of-the-art APR tools (GenProg, AE, Angelix, and Prophet), and 661 programs written by the students taking an introductory programming course. We found that when APR tools are used out of the box, only about 30\% of the programs in our dataset are repaired. This low repair rate is largely due to the student programs often being significantly incorrect - in contrast, professional software for which APR was successfully applied typically fails only a small portion of tests. To bridge this gap, we adopt in APR a new repair policy akin to the hint generation policy employed in the existing ITSP. This new repair policy admits partial repairs that address part of failing tests, which results in 84\% improvement of repair rate. We also performed a user study with 263 novice students and 37 graders, and identified an understudied problem; while novice students do not seem to know how to effectively make use of generated repairs as hints, the graders do seem to gain benefits from repairs.},
booktitle = {Proceedings of the 2017 11th Joint Meeting on Foundations of Software Engineering},
pages = {740–751},
numpages = {12},
keywords = {Automated Program Repair, Intelligent Tutoring System},
location = {Paderborn, Germany},
series = {ESEC/FSE 2017}
}

@article{flsurvey,
author = {Wong, W. Eric and Gao, Ruizhi and Li, Yihao and Abreu, Rui and Wotawa, Franz},
title = {A Survey on Software Fault Localization},
year = {2016},
issue_date = {August 2016},
publisher = {IEEE Press},
volume = {42},
number = {8},
issn = {0098-5589},
url = {https://doi.org/10.1109/TSE.2016.2521368},
doi = {10.1109/TSE.2016.2521368},
abstract = {Software fault localization, the act of identifying the locations of faults in a program, is widely recognized to be one of the most tedious, time consuming, and expensive – yet equally critical – activities in program debugging. Due to the increasing scale and complexity of software today, manually locating faults when failures occur is rapidly becoming infeasible, and consequently, there is a strong demand for techniques that can guide software developers to the locations of faults in a program with minimal human intervention. This demand in turn has fueled the proposal and development of a broad spectrum of fault localization techniques, each of which aims to streamline the fault localization process and make it more effective by attacking the problem in a unique way. In this article, we catalog and provide a comprehensive overview of such techniques and discuss key issues and concerns that are pertinent to software fault localization as a whole.},
journal = {IEEE Trans. Softw. Eng.},
month = aug,
pages = {707--740},
numpages = {34}
}

@inproceedings{chatgpt4vul,
  title={Chatgpt for vulnerability detection, classification, and repair: How far are we?},
  author={Fu, Michael and Tantithamthavorn, Chakkrit Kla and Nguyen, Van and Le, Trung},
  booktitle={2023 30th Asia-Pacific Software Engineering Conference (APSEC)},
  pages={632--636},
  year={2023},
  abstract={Large language models (LLMs) like ChatGPT (i.e., gpt-3.5-turbo and gpt-4) exhibited remarkable advancement in a range of software engineering tasks associated with source code such as code review and code generation. In this paper, we undertake a comprehensive study by instructing ChatGPT for four prevalent vulnerability tasks: function and line-level vulnerability prediction, vulnerability classification, severity estimation, and vulnerability repair. We compare ChatGPT with state-of-the-art language models designed for software vulnerability purposes. Through an empirical assessment employing extensive real-world datasets featuring over 190,000 C/C++ functions, we found that ChatGPT achieves limited performance, trailing behind other language models in vulnerability contexts by a significant margin. The experimental outcomes highlight the challenging nature of vulnerability prediction tasks, requiring domain-specific expertise. Despite ChatGPT's substantial model scale, exceeding that of source code-pre-trained language models (e.g., CodeBERT) by a factor of 14,000, the process of fine-tuning remains imperative for ChatGPT to generalize for vulnerability prediction tasks. },
  comment={We publish the studied dataset, experimental prompts for ChatGPT, and experimental results at https://github.com/awsm-research/ChatGPT4Vul.},
  url={https://arxiv.org/pdf/2310.09810},
  organization={IEEE}
}

@inproceedings{autobugfixsurvey,
  author={Puvvadi, Meghana and Arava, Sai Kumar and Santoria, Adarsh and Chennupati, Sesha Sai Prasanna and Puvvadi, Harsha Vardhan},
  booktitle={2025 IEEE 14th International Conference on Communication Systems and Network Technologies (CSNT)},
  title={Coding Agents: A Comprehensive Survey of Automated Bug Fixing Systems and Benchmarks},
  year={2025},
  volume={},
  number={},
  url={https://ieeexplore.ieee.org/document/10968728},
  pages={680-686},
  keywords={Surveys;Large language models;Scalability;Semantics;Debugging;Maintenance engineering;Benchmark testing;Software;Software engineering;Context modeling;Agent-Based Models;Automated Program Repair;Large Language Models;Multi-Agent Systems;Context-Aware Debugging;Software Engineering Automation;Debugging Benchmarks;AI in Software Development},
  doi={10.1109/CSNT64827.2025.10968728}
}


@inproceedings{generalizabilityapr,
author = {Li, Fengjie and Jiang, Jiajun and Sun, Jiajun and Zhang, Hongyu},
title = {Evaluating the Generalizability of LLMs in Automated Program Repair},
year = {2025},
isbn = {9798331537111},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-NIER66352.2025.00024},
doi = {10.1109/ICSE-NIER66352.2025.00024},
abstract = {LLM-based automated program repair methods have attracted significant attention for their state-of-the-art performance. However, they were primarily evaluated on a few well-known datasets like Defects4J, raising questions about their effectiveness on new datasets. In this study, we evaluate 11 top-performing LLMs on Defects4J-Trans, a new dataset derived from transforming Defects4J while maintaining the original semantics. Results from experiments on both Defects4J and Defects4J-Trans show that all studied LLMs have limited generalizability in APR tasks, with the average number of correct and plausible patches decreasing by 49.48\% and 42.90\%, respectively, on Defects4J-Trans. Further investigation into incorporating additional repair-relevant information in repair prompts reveals that, although this information significantly enhances the LLMs' capabilities (increasing the number of correct and plausible patches by up to 136.67\% and 121.82\%, respectively), performance still falls short of their original results. This indicates that prompt engineering alone is insufficient to substantially enhance LLMs' repair capabilities. Based on our study, we also offer several recommendations for future research.},
booktitle = {Proceedings of the IEEE/ACM 47th International Conference on Software Engineering: New Ideas and Emerging Results},
pages = {91--95},
numpages = {5},
keywords = {program repair, LLM, generalizability of LLM},
location = {Ottawa, Ontario, Canada},
series = {ICSE-NIER '25}
}

@inproceedings{le2018overfitting,
  title={Overfitting in semantics-based automated program repair},
  author={Le, Xuan-Bach D and Thung, Ferdian and Lo, David and Le Goues, Claire},
  booktitle={Proceedings of the 40th international conference on software engineering},
  pages={163--163},
  url={https://dl.acm.org/doi/pdf/10.1145/3180155.3182536},
  year={2018},
}

@article{swebenchplus,
  title={Swe-bench+: Enhanced coding benchmark for llms},
  author={Aleithan, Reem and Xue, Haoran and Mohajer, Mohammad Mahdi and Nnorom, Elijah and Uddin, Gias and Wang, Song},
  url={https://arxiv.org/pdf/2410.06992},
  journal={arXiv preprint arXiv:2410.06992},
  year={2024}
}

@article{omariinvestigate,
author = {Omari, Safwan and Basnet, Kshitiz and Wardat, Mohammad},
title = {Investigating large language models capabilities for automatic code repair in Python},
year = {2024},
issue_date = {Nov 2024},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {27},
number = {8},
issn = {1386-7857},
url = {https://doi.org/10.1007/s10586-024-04490-8},
doi = {10.1007/s10586-024-04490-8},
abstract = {Developers often encounter challenges with their introductory programming tasks as part of the development process. Unfortunately, rectifying these mistakes manually can be time-consuming and demanding. Automated program repair (APR) techniques offer a potential solution by synthesizing fixes for such errors. Previous research has investigated the utilization of both symbolic and neural techniques within the APR domain. However, these approaches typically demand significant engineering efforts or extensive datasets and training. In this paper, we explore the potential of using a large language model trained on code, specifically, we assess ChatGPT’s capability to detect and repair bugs in simple Python programs. The experimental evaluation encompasses two benchmarks: QuixBugs and Textbook. Each benchmark consists of simple Python functions that implement well-known algorithms and each function contains a single bug. To gauge repair performance in various settings, several benchmark variations were introduced including addition of plain English documentation and code obfuscation. Based on thorough experiments, we found that ChatGPT was able to correctly detect and fix about 50\% of the methods, when code is documented. Repair performance drops to 25\% when code is obfuscated, and 15\% when documentation is removed and code is obfuscated. Furthermore, when compared to existing APR systems, ChatGPT considerably outperformed them.},
journal = {Cluster Computing},
month = may,
pages = {10717--10731},
numpages = {15},
keywords = {Automatic program repair, Large language models, Python, Bug detection}
}

@inproceedings{zhang2024evaluate,
author = {Zhang, Lan and Zou, Qingtian and Singhal, Anoop and Sun, Xiaoyan and Liu, Peng},
title = {Evaluating Large Language Models for Real-World Vulnerability Repair in C/C++ Code},
year = {2024},
isbn = {9798400705564},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643651.3659892},
doi = {10.1145/3643651.3659892},
abstract = {The advent of Large Language Models (LLMs) has enabled advancement in automated code generation, translation, and summarization. Despite their promise, evaluating the use of LLMs in repairing real-world code vulnerabilities remains underexplored. In this study, we address this gap by evaluating the capability of advanced LLMs, such as ChatGPT-4 and Claude, in fixing memory corruption vulnerabilities in real-world C/C++ code. We meticulously curated 223 real-world C/C++ code snippets encompassing a spectrum of memory corruption vulnerabilities, ranging from straightforward memory leaks to intricate buffer errors. Our findings demonstrate the proficiency of LLMs in rectifying simple memor errors like leaks, where fixes are confined to localized code segments. However, their effectiveness diminishes when addressing complicated vulnerabilities necessitating reasoning about cross-cutting concerns and deeper program semantics. Furthermore, we explore techniques for augmenting LLM performance by incorporating additional knowledge. Our results shed light on both the strengths and limitations of LLMs in automated program repair on genuine code, underscoring the need for advancements in reasoning abilities for handling complex code repair tasks.},
booktitle = {Proceedings of the 10th ACM International Workshop on Security and Privacy Analytics},
pages = {49–58},
numpages = {10},
keywords = {deep learning, large language models, program repair},
location = {Porto, Portugal},
series = {IWSPA '24}
}

@inproceedings{aprenergy,
author = {Martinez, Matias and Mart\'{\i}nez-Fern\'{a}ndez, Silverio and Franch, Xavier},
title = {Energy Consumption of Automated Program Repair},
year = {2024},
isbn = {9798400705021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639478.3643114},
doi = {10.1145/3639478.3643114},
abstract = {In the last decade, following current societal needs, software sustainability has emerged as research field [2]. In this paper, we particularly focus on environmental sustainability, defined as "how software product development, maintenance, and use affect energy consumption and the consumption of other natural resources. [...] This dimension is also known as Green Software" [2].},
booktitle = {Proceedings of the 2024 IEEE/ACM 46th International Conference on Software Engineering: Companion Proceedings},
pages = {358–359},
numpages = {2},
location = {Lisbon, Portugal},
series = {ICSE-Companion '24}
}

@article{huang2025comprehensive,
author = {Huang, Kai and Zhang, Jian and Bao, Xinlei and Wang, Xu and Liu, Yang},
title = {Comprehensive Fine-Tuning Large Language Models of Code for Automated Program Repair},
year = {2025},
issue_date = {April 2025},
publisher = {IEEE Press},
volume = {51},
number = {4},
issn = {0098-5589},
url = {https://doi.org/10.1109/TSE.2025.3532759},
doi = {10.1109/TSE.2025.3532759},
abstract = {Automated program repair (APR) research has entered the era of large language models (LLM), and researchers have conducted several empirical studies to explore the repair capabilities of LLMs for APR. Many studies adopt the zero/few-shot learning paradigm for APR, which directly use LLMs to generate the possibly correct code given its surrounding context. Though effective, the repair capabilities of LLMs based on the fine-tuning paradigm have yet to be extensively explored. Also, it remains unknown whether LLMs have the potential to repair more complicated bugs (e.g., multi-hunk bugs). To fill the gap, in the conference version of this work, we conduct an initial study on the program repair capability of million-level LLMs in the fine-tuning paradigm. We&nbsp;select 5 popular million-level LLMs with representative pre-training architectures, including CodeBERT, GraphCodeBERT, PLBART, CodeT5, and UniXcoder. We&nbsp;consider 3 typical program repair scenarios (i.e., bugs, vulnerabilities, and errors) involving 3 programming languages (i.e., Java, C/C++, and JavaScript). Our experimental results show that fine-tuning these LLMs can significantly outperform previous state-of-the-art APR tools. However, the repair capabilities of billion-level LLMs for APR remain largely unexplored. Moreover, their substantial model sizes significantly increase the computational cost of fine-tuning. While parameter-efficient fine-tuning (PEFT) techniques offer a promising solution, their effectiveness in repair tasks and the selection of appropriate PEFT strategies remain unclear. Similarly, many novel APR strategies have been developed for non-pre-trained models, yet their applicability and effectiveness on LLMs are still unexamined. To address these gaps, we extend our prior study through three key dimensions: 1) LLM4APR, which evaluates the repair capabilities of five billion-level LLM families (InCoder, CodeGeeX, CodeGen, StarCoder, and CodeLlama) under the fine-tuning paradigm; 2) PEFT4LLM, which compares full-parameter fine-tuning (FPFT) with three PEFT techniques (LoRA, AdaLoRA, and IA3) to determine optimal strategies that balance repair cost and performance of LLMs; and 3) APR4LLM, which investigates the potential of a basic neural machine translation (NMT) approach alongside three advanced repair strategies (TENURE, ITER, and KATANA) to enhance the repair capabilities of LLMs. Overall, our extensive results suggest that larger scale models typically have better repair capabilities. The&nbsp;LoRA technique is still the best choice for LLM4APR studies. Different repair strategies result in different repair capabilities for the foundation models, but some of the strategies that performed well on the non-pre-trained model did not show an advantage on LLMs. Besides, we released all LLMs fine-tuned with repair tasks to facilitate LLM4APR research, and we encourage researchers to develop more powerful APR tools on the basis of these repair LLMs.},
journal = {IEEE Trans. Softw. Eng.},
month = apr,
pages = {904–928},
numpages = {25}
}

@article{learningaprsurvey,
author = {Zhang, Quanjun and Fang, Chunrong and Ma, Yuxiang and Sun, Weisong and Chen, Zhenyu},
title = {A Survey of Learning-based Automated Program Repair},
year = {2023},
issue_date = {February 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {33},
number = {2},
issn = {1049-331X},
url = {https://doi.org/10.1145/3631974},
doi = {10.1145/3631974},
abstract = {Automated program repair (APR) aims to fix software bugs automatically and plays a crucial role in software development and maintenance. With the recent advances in deep learning (DL), an increasing number of APR techniques have been proposed to leverage neural networks to learn bug-fixing patterns from massive open-source code repositories. Such learning-based techniques usually treat APR as a neural machine translation (NMT) task, where buggy code snippets (i.e., source language) are translated into fixed code snippets (i.e., target language) automatically. Benefiting from the powerful capability of DL to learn hidden relationships from previous bug-fixing datasets, learning-based APR techniques have achieved remarkable performance.In this article, we provide a systematic survey to summarize the current state-of-the-art research in the learning-based APR community. We illustrate the general workflow of learning-based APR techniques and detail the crucial components, including fault localization, patch generation, patch ranking, patch validation, and patch correctness phases. We then discuss the widely adopted datasets and evaluation metrics and outline existing empirical studies. We discuss several critical aspects of learning-based APR techniques, such as repair domains, industrial deployment, and the open science issue. We highlight several practical guidelines on applying DL techniques for future APR studies, such as exploring explainable patch generation and utilizing code features. Overall, our article can help researchers gain a comprehensive understanding about the achievements of the existing learning-based APR techniques and promote the practical application of these techniques. Our artifacts are publicly available at the repository: .},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = dec,
articleno = {55},
numpages = {69},
keywords = {AI and software engineering, neural machine translation, deep learning, Automatic program repair}
}

@article{wang2025assessing,
author = {Wang, Bo and Deng, Ming and Chen, Mingda and Lin, Youfang and Zhou, Jianyi and Zhang, Jie M.},
title = {Assessing the effectiveness of recent closed-source large language models in fault localization and automated program repair},
year = {2025},
issue_date = {Dec 2025},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {33},
number = {1},
issn = {0928-8910},
url = {https://doi.org/10.1007/s10515-025-00549-x},
doi = {10.1007/s10515-025-00549-x},
abstract = {Large Language Models (LLMs) have made significant advancements in code-related tasks. In the field of automated debugging, fault localization (FL) and automated program repair (APR) are two prevalent topics attracting significant research effort. Recently, in the field of FL and APR, many novel LLM-based approaches have emerged. However, most existing LLM-based studies primarily focus on the GPT models from OpenAI or open-source LLMs. With the rapid development of LLMs, various internet giants have introduced new closed-source models. In addition, due to policy restrictions, some regions can only access the commercial LLMs provided by specified companies. Despite the LLMs of OpenAI, the effectiveness of the other closed-source LLMs in FL and APR remains unknown. To better understand the effectiveness of contemporary closed-source models, we conduct a large-scale empirical study on their performance with respect to FL and APR. Specifically, our study involves 4 recent commercial closed-source LLMs (i.e., GPT-4o-Mini, Ernie-3.5, Qwen-turbo, and Doubao-pro) and 1 open-source LLM (i.e., DeepSeek-V3-chat). Note that only the GPT models have region restrictions among all LLMs we studied. We designed a total of 12 distinct prompt templates, 6 each for FL and APR, incorporating various formats and information sources. We conducted experiments to evaluate the effectiveness of FL and APR on 1036 real Java bugs from two datasets, Defects4J 2.0 and ConDefects. The key findings of the experiments indicate that (1) different LLMs tend to succeed on different sets of bugs in both FL and APR, with relatively little overlap among successful cases, implying the models possess distinct strengths in handling specific kinds of bugs, (2) the effectiveness of prompt templates varies across different models, and (3) the effectiveness of FL and APR capabilities of the studied models is significantly correlated with the bug type. We summarized all 14 findings obtained into 3 implications, which could help researchers further improve the performance of LLMs on FL and APR.},
journal = {Automated Software Engg.},
month = oct,
numpages = {42},
keywords = {Large language models, Software debugging, Fault localization, Automated program repair, Empirical study}
}

@inproceedings{ouyang2024benchmarking,
author = {Ouyang, Yicheng and Yang, Jun and Zhang, Lingming},
title = {Benchmarking Automated Program Repair: An Extensive Study on Both Real-World and Artificial Bugs},
year = {2024},
isbn = {9798400706127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3650212.3652140},
doi = {10.1145/3650212.3652140},
abstract = {As bugs are inevitable and prevalent in real-world programs, many Automated Program Repair (APR) techniques have been proposed to generate patches for them. However, due to the lack of a standard for evaluating APR techniques, prior works tend to use different settings and benchmarks in evaluation, threatening the trustworthiness of the evaluation results. Additionally, they typically only adopt plausibility and genuineness as evaluation metrics, which may potentially mask some underlying issues in APR techniques. To overcome these issues, in this paper, we conduct an extensive and multi-dimensional evaluation of nine learning-based and three traditional state-of-the-art APR techniques under the same environment and settings. We employ the widely studied Defects4J V2.0.0 benchmark and a newly constructed large-scale mutation-based benchmark named MuBench, derived from Defects4J and including 1,700 artificial bugs generated by various mutators, to uncover potential limitations in these APR techniques. We also apply multi-dimensional metrics, including compilability/plausibility/genuineness metrics, as well as SYE (SYntactic Equivalence) and TCE (Trivial Compiler Equivalence) metrics, to thoroughly analyze the 1,814,652 generated patches. This paper presents noteworthy findings from the extensive evaluation: Firstly, Large Language Model (LLM) based APR demonstrates less susceptibility to overfitting on the Defects4J V1.2.0 dataset and fixes the most number of bugs. Secondly, the study suggests a promising future for combining traditional and learning-based APR techniques, as they exhibit complementary advantages in fixing different types of bugs. Additionally, this work highlights the necessity for further enhancing patch compilability of learning-based APR techniques, despite the presence of various existing strategies attempting to improve it. The study also reveals other guidelines for enhancing APR techniques, including the need for handling unresolvable symbol compilability issues and reducing duplicate/no-op patch generation. Finally, our study uncovers seven implementation issues in the studied techniques, with five of them confirmed and fixed by the corresponding authors.},
booktitle = {Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {440–452},
numpages = {13},
keywords = {Empirical assessment, Mutation testing, Program repair},
location = {Vienna, Austria},
series = {ISSTA 2024}
}

@inproceedings{hemati2025comparative,
author = {Hemati Moghadam, Iman and Lijzenga, Oebele and Zaytsev, Vadim},
title = {Comparative Analysis of Pre-trained Code Language Models for Automated Program Repair via Code Infill Generation},
year = {2025},
isbn = {9798400719950},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3742876.3742881},
doi = {10.1145/3742876.3742881},
abstract = {Automated Program Repair (APR) has advanced significantly with the emergence of pre-trained Code Language Models (CLMs), enabling the generation of high-quality patches. However, selecting the most suitable CLM for APR remains challenging due to a range of factors, including accuracy, efficiency, and scalability, among others. These factors are interdependent and interact in complex ways, making the selection of a CLM for APR a multifaceted problem.    This study systematically evaluates 20 pre-trained CLMs, ranging from 60M to 16B parameters, on the HumanEval-Java benchmark (163 buggy Java methods). The evaluation examines bug-fixing accuracy, resource consumption, compilability, patch diversity, and sampling strategies (beam search vs. nucleus sampling).    Results indicate that larger models such as CodeLLaMA-13B and StarCoder generally perform better in bug fixing and compiler error handling, but scale alone does not guarantee effectiveness, as some (e.g., CodeGen2) perform poorly despite their size. Notably, memory usage increases with model size, but time consumption does not exhibit a clear correlation, suggesting that efficiency is influenced by architecture rather than scale alone. Additionally, nucleus sampling slightly outperforms beam search, though the difference is not statistically significant. Since no single CLM fixes all bugs, these findings highlight the potential of hybrid or ensemble-based CLM-driven APR approaches for more robust bug-fixing.},
booktitle = {Proceedings of the 24th ACM SIGPLAN International Conference on Generative Programming: Concepts and Experiences},
pages = {13--26},
numpages = {14},
keywords = {Automated program repair, Java, empirical study, pre-trained code language model, zero-shot learning},
location = {Bergen, Norway},
series = {GPCE '25}
}

@inproceedings{jiang2023impact,
author = {Jiang, Nan and Liu, Kevin and Lutellier, Thibaud and Tan, Lin},
title = {Impact of Code Language Models on Automated Program Repair},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00125},
doi = {10.1109/ICSE48619.2023.00125},
abstract = {Automated program repair (APR) aims to help developers improve software reliability by generating patches for buggy programs. Although many code language models (CLM) are developed and effective in many software tasks such as code completion, there has been little comprehensive, in-depth work to evaluate CLMs' fixing capabilities and to fine-tune CLMs for the APR task.Firstly, this work is the first to evaluate ten CLMs on four APR benchmarks, which shows that surprisingly, the best CLM, as is, fixes 72\% more bugs than the state-of-the-art deep-learning (DL)-based APR techniques. Secondly, one of the four APR benchmarks was created by us in this paper to avoid data leaking for a fair evaluation. Thirdly, it is the first work to fine-tune CLMs with APR training data, which shows that fine-tuning brings 31\%--1,267\% improvement to CLMs and enables them to fix 46\%--164\% more bugs than existing DL-based APR techniques. Fourthly, this work studies the impact of buggy lines, showing that CLMs, as is, cannot make good use of the buggy lines to fix bugs, yet fine-tuned CLMs could potentially over-rely on buggy lines. Lastly, this work analyzes the size, time, and memory efficiency of different CLMs.This work shows promising directions for the APR domain, such as fine-tuning CLMs with APR-specific designs, and also raises awareness of fair and comprehensive evaluations of CLMs and calls for more transparent reporting of open-source repositories used in the pre-training data to address the data leaking problem.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {1430--1442},
numpages = {13},
keywords = {deep learning, fine-tuning, code language model, automated program repair},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@article{federatedllmprogramrepair,
author = {Luo, Wenqiang and Keung, Jacky and Yang, Boyang and Ye, He and Le Goues, Claire and Bissyand\'{e}, Tegawend\'{e} F. and Tian, Haoye and Le, Xuan Bach D.},
title = {When Fine-Tuning LLMs Meets Data Privacy: An Empirical Study of Federated Learning in LLM-Based Program Repair},
year = {2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1049-331X},
url = {https://doi.org/10.1145/3733599},
doi = {10.1145/3733599},
abstract = {Software systems have been evolving rapidly and inevitably introducing bugs at an increasing rate, leading to significant maintenance costs. While large language models (LLMs) have demonstrated remarkable potential in enhancing software development and maintenance practices, particularly in automated program repair (APR), they rely heavily on high-quality code repositories. Most code repositories are proprietary assets that capture the diversity and nuances of real-world industry software practices, which public datasets cannot fully represent. However, obtaining such data from various industries is hindered by data privacy concerns, as companies are reluctant to share their proprietary codebases. There has also been no in-depth investigation of collaborative software development by learning from private and decentralized data while preserving data privacy for program repair.To address the gap, we investigate federated learning as a privacy-preserving method for fine-tuning LLMs on proprietary and decentralized data to boost collaborative software development and maintenance. We use the private industrial dataset TutorCode for fine-tuning and the EvalRepair-Java benchmark for evaluation, and assess whether federated fine-tuning enhances program repair. We then further explore how code heterogeneity (i.e., variations in coding style, complexity, and embedding) and different federated learning algorithms affect bug fixing to provide practical implications for real-world software development collaboration. Our evaluation reveals that federated fine-tuning can significantly enhance program repair, achieving increases of up to 16.67\% for Top@10 and 18.44\% for Pass@10, even comparable to the bug-fixing capabilities of centralized learning. Moreover, the negligible impact of code heterogeneity implies that industries can effectively collaborate despite diverse data distributions. Different federated algorithms also demonstrate unique strengths across LLMs, suggesting that tailoring the optimization process to specific LLM characteristics can further improve program repair.},
note = {Just Accepted},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = may,
keywords = {Program Repair, Federated Learning, Large Language Models}
}

@inproceedings{zhong2022review,
author = {Zhong, Wenkang and Li, Chuanyi and Ge, Jidong and Luo, Bin},
title = {Neural Program Repair : Systems, Challenges and Solutions},
year = {2022},
isbn = {9781450397803},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3545258.3545268},
doi = {10.1145/3545258.3545268},
abstract = {Automated Program Repair (APR) aims to automatically fix bugs in the source code. Recently, with advances in Deep Learning (DL) field, there has been an increase of Neural Program Repair (NPR) studies that use neural networks to model the patch-generation process. NPR approaches have a significant benefit in applicability over prior APR techniques because they do not require any specifications (e.g., a test suite) when generating patches. For this reason, NPR has recently become a popular research topic. In this paper, We undertake a literature review of latest NPR systems to help interested readers understand advancements in this emerging field. We begin by introducing background information of NPR. Next, to make the various NPR systems more understandable, we split them into a four-phase pipeline and discuss various design choices for each phase. To investigate the motivations of different design choices, We further highlight a number of challenges and summarize corresponding solutions adopted by existing NPR systems. Finally, we suggest some intriguing directions for the future research.},
booktitle = {Proceedings of the 13th Asia-Pacific Symposium on Internetware},
pages = {96–106},
numpages = {11},
keywords = {Software reliability, Neural networks, Automatic program repair},
location = {Hohhot, China},
series = {Internetware '22}
}

@article{lou2024study,
author = {Lou, Yiling and Yang, Jun and Benton, Samuel and Hao, Dan and Tan, Lin and Chen, Zhenpeng and Zhang, Lu and Zhang, Lingming},
title = {When Automated Program Repair Meets Regression Testing--An Extensive Study on Two Million Patches},
year = {2024},
issue_date = {September 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {33},
number = {7},
issn = {1049-331X},
url = {https://doi.org/10.1145/3672450},
doi = {10.1145/3672450},
abstract = {In recent years, Automated Program Repair (APR) has been extensively studied in academia and even drawn wide attention from the industry. However, APR techniques can be extremely time consuming since (1) a large number of patches can be generated for a given bug, and (2) each patch needs to be executed on the original tests to ensure its correctness. In the literature, various techniques (e.g., based on learning, mining, and constraint solving) have been proposed/studied to reduce the number of patches. Intuitively, every patch can be treated as a software revision during regression testing; thus, traditional Regression Test Selection (RTS) techniques can be leveraged to only execute the tests affected by each patch (as the other tests would keep the same outcomes) to further reduce patch execution time. However, few APR systems actually adopt RTS and there is still a lack of systematic studies demonstrating the benefits of RTS and the impact of different RTS strategies on APR. To this end, this article presents the first extensive study of widely used RTS techniques at different levels (i.e., class/method/statement levels) for 12 state-of-the-art APR systems on over 2M patches. Our study reveals various practical guidelines for bridging the gap between APR and regression testing, including: (1) the number of patches widely used for measuring APR efficiency can incur skewed conclusions, and the use of inconsistent RTS configurations can further skew the conclusions; (2) all studied RTS techniques can substantially improve APR efficiency and should be considered in future APR work; (3) method- and statement-level RTS outperform class-level RTS substantially and should be preferred; (4) RTS techniques can substantially outperform state-of-the-art test prioritization techniques for APR, and combining them can further improve APR efficiency; and (5) traditional Regression Test Prioritization (RTP) widely studied in regression testing performs even better than APR-specific test prioritization when combined with most RTS techniques. Furthermore, we also present the detailed impact of different patch categories and patch validation strategies on our findings.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = sep,
articleno = {180},
numpages = {23},
keywords = {Test selection, program repair, patch validation}
}

@inproceedings{developercenteredapr,
author = {Winter, Emily Rowan and Nowack, Vesna and Bowes, David and Counsell, Steve and Hall, Tracy and Haraldsson, S\ae{}mundur and Woodward, John and Kirbas, Serkan and Windels, Etienne and McBello, Olayori and Atakishiyev, Abdurahman and Kells, Kevin and Pagano, Matthew},
title = {Towards developer-centered automatic program repair: findings from Bloomberg},
year = {2022},
isbn = {9781450394130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3540250.3558953},
doi = {10.1145/3540250.3558953},
abstract = {This paper reports on qualitative research into automatic program repair (APR) at Bloomberg. Six focus groups were conducted with a total of seventeen participants (including both developers of the APR tool and developers using the tool) to consider: the development at Bloomberg of a prototype APR tool (Fixie); developers’ early experiences using the tool; and developers’ perspectives on   how they would like to interact with the tool in future. APR is developing rapidly and it is important to understand in greater detail developers' experiences using this emerging technology. In this paper, we provide in-depth, qualitative data from an industrial setting. We found that the development of APR at Bloomberg had become increasingly user-centered, emphasising how fixes were presented to developers, as well as particular features, such as customisability. From the focus groups with developers who had used Fixie, we found particular concern with the pragmatic aspects of APR, such as how and when fixes were presented to them. Based on our findings, we make a series of recommendations to inform future APR development, highlighting how APR tools should 'start small', be customisable, and fit with developers' workflows. We also suggest that APR tools should capitalise on the promise of repair bots and draw on advances in explainable AI.},
booktitle = {Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {1578–1588},
numpages = {11},
keywords = {qualitative methods, human factors, automatic program repair},
location = {Singapore, Singapore},
series = {ESEC/FSE 2022}
}

@article{llm4prslr,
author = {Zubair, Fida and Al-Hitmi, Maryam and Catal, Cagatay},
title = {The use of large language models for program repair},
year = {2025},
issue_date = {Apr 2025},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {93},
number = {C},
issn = {0920-5489},
url = {https://doi.org/10.1016/j.csi.2024.103951},
doi = {10.1016/j.csi.2024.103951},
journal = {Comput. Stand. Interfaces},
month = apr,
numpages = {12},
abstract={Large Language Models (LLMs) have emerged as a promising approach for automated program repair, offering code comprehension and generation capabilities that can address software bugs. Several program repair models based on LLMs have been developed recently. However, findings and insights from these efforts are scattered across various studies, lacking a systematic overview of LLMs' utilization in program repair. Therefore, this Systematic Literature Review (SLR) was conducted to investigate the current landscape of LLM utilization in program repair. This study defined seven research questions and thoroughly selected 41 relevant studies from scientific databases to explore these questions. The results showed the diverse capabilities of LLMs for program repair. The findings revealed that Encoder-Decoder architectures emerged as the most common LLM design for program repair tasks and that mostly open-access datasets were used. Several evaluation metrics were applied, primarily consisting of accuracy, exact match, and BLEU scores. Additionally, the review investigated several LLM fine-tuning methods, including fine-tuning on specialized datasets, curriculum learning, iterative approaches, and knowledge-intensified techniques. These findings pave the way for further research on utilizing the full potential of LLMs to revolutionize automated program repair.},
keywords = {Large language model, Program repair, Software engineering, Automated program repair, SLR, LLM, AI, GPT, BERT, APR, F1, BLEU, GLUE, SQuAD, RQ, C1}
}

@inproceedings{hori2024effects,
author = {Hori, Shota and Matsumoto, Shinsuke and Higo, Yoshiki and Kusumoto, Shinji and Yasuda, Kazuya and Ito, Shinji and Huyen, Phan Thi Thanh},
title = {The Effects of Semantic Information on LLM-Based Program Repair},
year = {2024},
isbn = {978-3-031-78385-2},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-78386-9_28},
doi = {10.1007/978-3-031-78386-9_28},
abstract = {Large Language Model-based Automated Program Repair (LLM-APR) has recently received significant attention as a debugging assistance. Our objective is to improve the performance of LLM-APR.&nbsp;In this study, we focus on semantic information contained in the source code. Semantic information refers to elements used by the programmer to understand the source code, which does not contribute&nbsp;to compilation or execution. We picked out specification, method&nbsp;names and variable names as semantic information. In the investigation,&nbsp;we prepared eight prompts, each consisting of all combinations of&nbsp;three types of semantic information. The experimental results showed&nbsp;that all semantic information improves the performance of LLM-APR,&nbsp;and variable names are particularly significant.},
booktitle = {Product-Focused Software Process Improvement: 25th International Conference, PROFES 2024, Tartu, Estonia, December 2–4, 2024, Proceedings},
pages = {377--385},
numpages = {9},
keywords = {Large language model (LLM), automated program repair (APR), semantic information, prompt engineering, ChatGPT},
location = {Tartu, Estonia}
}

@article{martinez2025sustainability,
author = {Martinez, Matias and Mart\'{\i}nez-Fern\'{a}ndez, Silverio and Franch, Xavier},
title = {The Sustainability Face of Automated Program Repair Tools},
year = {2025},
issue_date = {November 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {34},
number = {8},
issn = {1049-331X},
url = {https://doi.org/10.1145/3744900},
doi = {10.1145/3744900},
abstract = {Automated program repair (APR) aims to automatize the process of repairing software bugs in order to reduce the cost of maintaining software programs. While APR accuracy has significantly improved in recent years, its energy impact remains unstudied. The field of green software research aims to measure the energy consumption required to develop, maintain, and use software products. Our main goal is to define the foundation for measuring the energy consumption of the APR activity. We state that an environmentally sustainable (or green) APR tool achieves a good balance between the ability to correctly repair bugs and the amount of energy consumed during such process. We measure the energy consumption of 10 traditional APR tools for Java and 11 fine-tuned large-language models (LLM) trying to repair real bugs from Defects4J. The results of this study show the existing tradeoff between energy consumption and repairability. In particular, APR tools such as TBar and RepairLlama repair more bugs than other approaches at the expense of a higher energy consumption. Other tools, such as SimFix and the LLM CodeT5-large, provide a good tradeoff between energy consumption and repairability. We also present guidelines consisting of a set of recommendations for developing greener APR.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = oct,
articleno = {237},
numpages = {37},
keywords = {Automated Program Repair, Software Sustainability, Energy Consumption of Software Tools, Green Computing}
}

@article{apradvancementsreview,
author = {Dikici, Sena and Bilgin, Turgay Tugay},
title = {Advancements in automated program repair: a comprehensive review},
year = {2025},
issue_date = {Jun 2025},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {67},
number = {6},
issn = {0219-1377},
url = {https://doi.org/10.1007/s10115-025-02383-9},
doi = {10.1007/s10115-025-02383-9},
abstract = {This review paper presents a comprehensive examination of automated program repair (APR) and its significant contribution to the field of modern software engineering. It elucidates how APR methodologies markedly mitigate manual debugging needs by automating the detection and resolution of software glitches. The study encompasses an in-depth exploration of three primary categories of APR techniques: template-based, machine learning, and deep learning approaches, drawing from an exhaustive evaluation of 41 APR tools. Each category showcases distinct strategies for managing diverse software errors, underscoring the breadth and effectiveness of current APR methodologies. Template-based APR solutions utilize pre-established patterns to efficiently tackle common coding issues, while machine learning-driven approaches dynamically devise repair strategies from historical bug-fix datasets. Deep learning methods extend error rectification boundaries by delving into the semantic context of code, yielding more precise adjustments. The ongoing advancement of APR technologies necessitates researchers to address critical challenges, including the integration of semantic-syntactic analyses, mitigation of data scarcity, optimization of cross-platform tools, development of context-aware approaches, enhancement of fault localization and patch validation processes, and establishment of standardized performance evaluation metrics. This comprehensive analysis underscores the pivotal role of APR in enhancing software efficiency and reliability, representing significant progress in software development and maintenance practices.},
journal = {Knowl. Inf. Syst.},
month = mar,
pages = {4737–4783},
numpages = {47},
keywords = {Automated program repair, Software bugs, Machine learning, Deep learning, Code patterns}
}

@inbook{programrepairreview,
author = {Rinard, Martin C.},
title = {Research in Program Repair and Approximate Computing: A Retrospective},
year = {2025},
isbn = {9798331505691},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE55347.2025.00257},
abstract = {This paper and accompanying talk trace the trajectory of my research in program repair and approximate computing. The prevailing value system in the field at the time focused on program correctness as a fundamental goal. This research, in contrast, was driven by a new perspective that emphasized acceptable (but not necessarily fully correct) survival through errors and the automatic identification and exploitation of performance versus accuracy tradeoff spaces implicitly present in computations coded to operate at only a single point in this space.Because the research challenged the prevailing value system at the time, it met with some skepticism despite empirical results highlighting its effectiveness. The following quote from an anonymous reviewer may give some idea of the reaction:"The basic idea—to assist incorrect programs in their efforts to emit incorrect output—is an abomination and if adopted would likely usher in a new dark age."As the research progressed, we gained a deeper understanding of the reasons behind the surprising — at least to us — phenomena we observed. We were able to formalize this understanding to generate source code patches and obtain performance, accuracy, and acceptability guarantees for computations that leveraged our techniques, bringing the research full circle to once again focus on reasoning statically about program behavior but with different reasoning techniques and guarantees.Finally, I discuss lessons learned and future relevance of the principles, perspectives, and concepts that this research pioneered.},
booktitle = {Proceedings of the IEEE/ACM 47th International Conference on Software Engineering},
pages = {1–15},
numpages = {15}
}

@article{renzullo2025trends,
author = {Renzullo, Joseph and Reiter, Pemma and Weimer, Westley and Forrest, Stephanie},
title = {Automated Program Repair: Emerging Trends Pose and Expose Problems for Benchmarks},
year = {2025},
issue_date = {August 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {57},
number = {8},
issn = {0360-0300},
url = {https://doi.org/10.1145/3704997},
doi = {10.1145/3704997},
abstract = {Machine learning (ML) pervades the field of Automated Program Repair (APR). Algorithms deploy neural machine translation and large language models (LLMs) to generate software patches, among other tasks. But, there are important differences between these applications of ML and earlier work, which complicates the task of ensuring that results are valid and likely to generalize. A challenge is that the most popular APR evaluation benchmarks were not designed with ML techniques in mind. This is especially true for LLMs, whose large and often poorly-disclosed training datasets may include problems on which they are evaluated.This article reviews work in APR published in the field’s top five venues since 2018, emphasizing emerging trends in the field, including the dramatic rise of ML models, including LLMs. ML-based articles are categorized along structural and functional dimensions, and a variety of issues are identified that these new methods raise. Importantly, data leakage and contamination concerns arise from the challenge of validating ML-based APR using existing benchmarks, which were designed before these techniques were popular. We discuss inconsistencies in evaluation design and performance reporting and offer pointers to solutions where they are available. Finally, we highlight promising new directions that the field is already taking.},
journal = {ACM Comput. Surv.},
month = mar,
articleno = {208},
numpages = {18},
keywords = {automated program repair, machine learning, benchmarks, patch quality}
}

@article{huang2024evolving,
author = {Huang, Kai and Xu, Zhengzi and Yang, Su and Sun, Hongyu and Li, Xuejun and Yan, Zheng and Zhang, Yuqing},
title = {Evolving Paradigms in Automated Program Repair: Taxonomy, Challenges, and Opportunities},
year = {2024},
issue_date = {February 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {57},
number = {2},
issn = {0360-0300},
url = {https://doi.org/10.1145/3696450},
doi = {10.1145/3696450},
abstract = {With the rapid development and large-scale popularity of program software, modern society increasingly relies on software systems. However, the problems exposed by software have also come to the fore. The software bug has become an important factor troubling developers. In this context, Automated Program Repair (APR) techniques have emerged, aiming to automatically fix software bug problems and reduce manual debugging work. In particular, benefiting from the advances in deep learning, numerous learning-based APR techniques have emerged in recent years, which also bring new opportunities for APR research. To give researchers a quick overview of APR techniques’ complete development and future opportunities, we review the evolution of APR techniques and discuss in depth the latest advances in APR research. In this article, the development of APR techniques is introduced in terms of four different patch generation schemes: search-based, constraint-based, template-based, and learning-based. Moreover, we propose a uniform set of criteria to review and compare each APR tool and then discuss the current state of APR development. Finally, we analyze current challenges and future directions, especially highlighting the critical opportunities that large language models bring to APR research.},
journal = {ACM Comput. Surv.},
month = oct,
articleno = {36},
numpages = {43},
keywords = {Automated program repair}
}

@inproceedings{jiang2024evaluating,
author = {Jiang, Shengbei and Zhang, Jiabao and Chen, Wei and Wang, Bo and Zhou, Jianyi and Zhang, Jie},
title = {Evaluating Fault Localization and Program Repair Capabilities of Existing Closed-Source General-Purpose LLMs},
year = {2024},
isbn = {9798400705793},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643795.3648390},
doi = {10.1145/3643795.3648390},
abstract = {Automated debugging is an emerging research field that aims to automatically find and repair bugs. In this field, Fault Localization (FL) and Automated Program Repair (APR) gain the most research efforts. Most recently, researchers have adopted pre-trained Large Language Models (LLMs) to facilitate FL and APR and their results are promising. However, the LLMs they used either vanished (such as Codex) or outdated (such as early versions of GPT). In this paper, we evaluate the performance of recent commercial closed-source general-purpose LLMs on FL and APR, i.e., ChatGPT 3.5, ERNIE Bot 3.5, and IFlytek Spark 2.0. We select three popular LLMs and evaluate them on 120 real-world Java bugs from the benchmark Defects4J. For FL and APR, we designed three kinds of prompts for each, considering different kinds of information. The results show that these LLMs could successfully locate 53.3\% and correctly fix 12.5\% of these bugs.},
booktitle = {Proceedings of the 1st International Workshop on Large Language Models for Code},
pages = {75–78},
numpages = {4},
keywords = {large language model, fault localization, program repair, software debugging},
location = {Lisbon, Portugal},
series = {LLM4Code '24}
}

@inproceedings{sun2022importance,
  title={On the importance of building high-quality training datasets for neural code search},
  author={Sun, Zhensu and Li, Li and Liu, Yan and Du, Xiaoning and Li, Li},
  booktitle={Proceedings of the 44th International Conference on Software Engineering},
  pages={1609--1620},
  url={https://dl.acm.org/doi/pdf/10.1145/3510003.3510160},
  year={2022}
}

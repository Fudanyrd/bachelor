@comment{
SLRs, reviews, surveys, empirical studies, etc. on software engineering topics.
}
@article{llm4se2,
  title={Large language models for software engineering: A systematic literature review},
  author={Hou, Xinyi and Zhao, Yanjie and Liu, Yue and Yang, Zhou and Wang, Kailong and Li, Li and Luo, Xiapu and Lo, David and Grundy, John and Wang, Haoyu},
  journal={ACM Transactions on Software Engineering and Methodology},
  volume={33},
  number={8},
  pages={1--79},
  url={https://dl.acm.org/doi/pdf/10.1145/3695988},
  year={2024},
  publisher={ACM New York, NY}
}
@article{dl4defence,
  title={Deep learning for android malware defenses: a systematic literature review},
  author={Liu, Yue and Tantithamthavorn, Chakkrit and Li, Li and Liu, Yepang},
  journal={ACM Computing Surveys},
  volume={55},
  url={https://dl.acm.org/doi/pdf/10.1145/3544968},
  number={8},
  pages={1--36},
  year={2022},
  publisher={ACM New York, NY}
}
@article{ml4se,
  title={Machine/deep learning for software engineering: A systematic literature review},
  author={Wang, Simin and Huang, Liguo and Gao, Amiao and Ge, Jidong and Zhang, Tengfei and Feng, Haitao and Satyarth, Ishna and Li, Ming and Zhang, He and Ng, Vincent},
  journal={IEEE Transactions on Software Engineering},
  volume={49},
  url={https://ieeexplore.ieee.org/ielaam/32/10070338/9772253-aam.pdf},
  number={3},
  pages={1188--1231},
  year={2022},
  publisher={IEEE}
}

@misc{agent4bugfixempirical,
 abstract = {Large language models (LLMs) and LLM-based Agents have been applied to fix bugs automatically, demonstrating the capability in addressing software defects by engaging in development environment interaction, iterative validation and code modification. However, systematic analysis of these agent systems remain limited, particularly regarding performance variations among top-performing ones. In this paper, we examine six repair systems on the SWE-bench Verified benchmark for automated bug fixing. We first assess each system's overall performance, noting the instances solvable by all or none of these systems, and explore the capabilities of different systems. We also compare fault localization accuracy at file and code symbol levels and evaluate bug reproduction capabilities. Through analysis, we concluded that further optimization is needed in both the LLM capability itself and the design of Agentic flow to improve the effectiveness of the Agent in bug fixing.},
 archiveprefix = {arXiv},
 author = {Xiangxin Meng and Zexiong Ma and Pengfei Gao and Chao Peng},
 eprint = {2411.10213},
 primaryclass = {cs.SE},
 title = {An Empirical Study on LLM-based Agents for Automated Bug Fixing},
 url = {https://arxiv.org/abs/2411.10213},
 year = {2025}
}

@misc{agenticbugreproductioneffective,
 abstract = {Bug reports often lack sufficient detail for developers to reproduce and fix the underlying defects. Bug Reproduction Tests (BRTs), tests that fail when the bug is present and pass when it has been resolved, are crucial for debugging, but they are rarely included in bug reports, both in open-source and in industrial settings. Thus, automatically generating BRTs from bug reports has the potential to accelerate the debugging process and lower time to repair. This paper investigates automated BRT generation within an industry setting, specifically at Google, focusing on the challenges of a large-scale, proprietary codebase and considering real-world industry bugs extracted from Google's internal issue tracker. We adapt and evaluate a state-of-the-art BRT generation technique, LIBRO, and present our agent-based approach, BRT Agent, which makes use of a fine-tuned Large Language Model (LLM) for code editing. Our BRT Agent significantly outperforms LIBRO, achieving a 28% plausible BRT generation rate, compared to 10% by LIBRO, on 80 human-reported bugs from Google's internal issue tracker. We further investigate the practical value of generated BRTs by integrating them with an Automated Program Repair (APR) system at Google. Our results show that providing BRTs to the APR system results in 30% more bugs with plausible fixes. Additionally, we introduce Ensemble Pass Rate (EPR), a metric which leverages the generated BRTs to select the most promising fixes from all fixes generated by APR system. Our evaluation on EPR for Top-K and threshold-based fix selections demonstrates promising results and trade-offs. For example, EPR correctly selects a plausible fix from a pool of 20 candidates in 70% of cases, based on its top-1 ranking.},
 archiveprefix = {arXiv},
 author = {Runxiang Cheng and Michele Tufano and Jürgen Cito and José Cambronero and Pat Rondon and Renyao Wei and Aaron Sun and Satish Chandra},
 eprint = {2502.01821},
 primaryclass = {cs.SE},
 title = {Agentic Bug Reproduction for Effective Automated Program Repair at Google},
 url = {https://arxiv.org/abs/2502.01821},
 year = {2025}
}

@misc{langgraphbugfix,
 abstract = {This paper presents a novel framework for automated code generation and debugging, designed to improve accuracy, efficiency, and scalability in software development. The proposed system integrates three core components LangGraph, GLM4 Flash, and ChromaDB within a four step iterative workflow to deliver robust performance and seamless functionality.
LangGraph serves as a graph-based library for orchestrating tasks, providing precise control and execution while maintaining a unified state object for dynamic updates and consistency. It supports multi-agent, hierarchical, and sequential processes, making it highly adaptable to complex software engineering workflows. GLM4 Flash, a large language model, leverages its advanced capabilities in natural language understanding, contextual reasoning, and multilingual support to generate accurate code snippets based on user prompts. ChromaDB acts as a vector database for semantic search and contextual memory storage, enabling the identification of patterns and the generation of context-aware bug fixes based on historical data.
The system operates through a structured four-step process: (1) Code Generation, which translates natural language descriptions into executable code; (2) Code Execution, which validates the code by identifying runtime errors and inconsistencies; (3) Code Repair, which iteratively refines buggy code using ChromaDB's memory capabilities and LangGraph's state tracking; and (4) Code Update, which ensures the code meets functional and performance requirements through iterative modifications.},
 archiveprefix = {arXiv},
 author = {Jialin Wang and Zhihua Duan},
 eprint = {2502.18465},
 primaryclass = {cs.SE},
 title = {Empirical Research on Utilizing LLM-based Agents for Automated Bug Fixing via LangGraph},
 url = {https://arxiv.org/abs/2502.18465},
 year = {2025}
}

@misc{agents,
  title={Agent S: An Open Agentic Framework that Uses Computers Like a Human}, 
  author={Saaket Agashe and Jiuzhou Han and Shuyu Gan and Jiachen Yang and Ang Li and Xin Eric Wang},
  year={2024},
  eprint={2410.08164},
  archivePrefix={arXiv},
  primaryClass={cs.AI},
  url={https://arxiv.org/abs/2410.08164}, 
}
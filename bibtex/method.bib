@comment{
Technical papers that propose novel methodologies.
Added by snowballing: 4
}

@article{genprog,
  title={Genprog: A generic method for automatic software repair},
  author={Le Goues, Claire and Nguyen, ThanhVu and Forrest, Stephanie and Weimer, Westley},
  journal={Ieee transactions on software engineering},
  volume={38},
  number={1},
  pages={54--72},
  year={2011},
  url={https://web.eecs.umich.edu/~weimerw/p/weimer-tse2011-genprog-preprint.pdf},
  keywords={automatic programming, testing and debugging},
  abstract={This paper describes GenProg, an automated method for repairing defects in off-the-shelf, legacy programs without formal specifications, program annotations, or special coding practices. GenProg uses an extended form of genetic programming to evolve a program variant that retains required functionality but is not susceptible to a given defect, using existing test suites to encode both the defect and required functionality. Structural differencing algorithms and delta debugging reduce the difference between this variant and the original program to a minimal repair. We describe the algorithm and report experimental results of its success on 16 programs totaling 1.25 M lines of C code and 120K lines of module code, spanning eight classes of defects, in 357 seconds, on average. We analyze the generated repairs qualitatively and quantitatively to demonstrate that the process efficiently produces evolved programs that repair the defect, are not fragile input memorizations, and do not lead to serious degradation in functionality.},
  publisher={IEEE}
}

@misc{du2025exploringlargelanguagemodels,
  title={Exploring Large Language Models in Resolving Environment-Related Crash Bugs: Localizing and Repairing}, 
  author={Xueying Du and Mingwei Liu and Hanlin Wang and Juntao Li and Xin Peng and Yiling Lou},
  year={2025},
  eprint={2312.10448},
  archivePrefix={arXiv},
  primaryClass={cs.SE},
  abstract={Software crash bugs cause unexpected program behaviors or even abrupt termination, thus demanding immediate resolution. However, resolving crash bugs can be challenging due to their complex root causes, which can originate from issues in the source code or external factors like third-party library dependencies. Large language models (LLMs) have shown promise in software engineering tasks. However, existing research predominantly focuses on the capability of LLMs to localize and repair code-related crash bugs, leaving their effectiveness in resolving environment-related crash bugs in real-world software unexplored. To fill this gap, we conducted the first comprehensive study to assess the capability of LLMs in resolving real-world environment-related crash bugs. We first systematically compare LLMs' performance in resolving code-related and environment-related crash bugs with varying levels of crash contextual information. Our findings reveal that localization is the primary challenge for resolving code-related crashes, while repair poses a greater challenge for environment-related crashes. Furthermore, we investigate the impact of different prompt strategies on improving the resolution of environment-related crash bugs, incorporating different prompt templates and multi-round interactions. Building on this, we further explore an advanced active inquiry prompting strategy leveraging the self-planning capabilities of LLMs. Based on these explorations, we propose IntDiagSolver, an interactive methodology designed to enable precise crash bug resolution through ongoing engagement with LLMs. Extensive evaluations of IntDiagSolver across multiple LLMs (including GPT-3.5, GPT-4, Claude, CodeLlama, DeepSeek-R1, and Qwen-3-Coder) demonstrate consistent improvements in resolution accuracy, with substantial enhancements ranging from 9.1% to 43.3% in localization and 9.1% to 53.3% in repair.},
  url={https://arxiv.org/abs/2312.10448}, 
}

@inproceedings{securemind,
author = {Wang, Huanting and Jacob, Dejice and Kelly, David and Elkhatib, Yehia and Singer, Jeremy and Wang, Zheng},
title = {SecureMind: A Framework for Benchmarking Large Language Models in Memory Bug Detection and Repair},
year = {2025},
isbn = {9798400716102},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3735950.3735954},
doi = {10.1145/3735950.3735954},
abstract = {Large language models (LLMs) hold great promise for automating software vulnerability detection and repair, but ensuring their correctness remains a challenge. While recent work has developed benchmarks for evaluating LLMs in bug detection and repair, existing studies rely on hand-crafted datasets that quickly become outdated. Moreover, systematic evaluation of advanced reasoning-based LLMs using chain-of-thought prompting for software security is lacking.   We introduce SecureMind, an open-source framework for evaluating LLMs in vulnerability detection and repair, focusing on memory-related vulnerabilities. SecureMind provides a user-friendly Python interface for defining test plans, which automates data retrieval, preparation, and benchmarking across a wide range of metrics.   Using SecureMind, we assess 10 representative LLMs, including 7 state-of-the-art reasoning models, on 16K test samples spanning 8 Common Weakness Enumeration (CWE) types related to memory safety violations. Our findings highlight the strengths and limitations of current LLMs in handling memory-related vulnerabilities.},
booktitle = {Proceedings of the 2025 ACM SIGPLAN International Symposium on Memory Management},
pages = {27–40},
numpages = {14},
keywords = {Bug repair, Large language models, Software bug detection},
location = {Seoul, Republic of Korea},
series = {ISMM '25}
}

@inproceedings{llm4testfreefl,
author = {Yang, Aidan Z. H. and Le Goues, Claire and Martins, Ruben and Hellendoorn, Vincent},
title = {Large Language Models for Test-Free Fault Localization},
year = {2024},
isbn = {9798400702174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597503.3623342},
doi = {10.1145/3597503.3623342},
abstract = {Fault Localization (FL) aims to automatically localize buggy lines of code, a key first step in many manual and automatic debugging tasks. Previous FL techniques assume the provision of input tests, and often require extensive program analysis, program instrumentation, or data preprocessing. Prior work on deep learning for APR struggles to learn from small datasets and produces limited results on real-world programs. Inspired by the ability of large language models (LLMs) of code to adapt to new tasks based on very few examples, we investigate the applicability of LLMs to line level fault localization. Specifically, we propose to overcome the left-to-right nature of LLMs by fine-tuning a small set of bidirectional adapter layers on top of the representations learned by LLMs to produce LLMAO, the first language model based fault localization approach that locates buggy lines of code without any test coverage information. We fine-tune LLMs with 350 million, 6 billion, and 16 billion parameters on small, manually curated corpora of buggy programs such as the Defects4J corpus. We observe that our technique achieves substantially more confidence in fault localization when built on the larger models, with bug localization performance scaling consistently with the LLM size. Our empirical evaluation shows that LLMAO improves the Top-1 results over the state-of-the-art machine learning fault localization (MLFL) baselines by 2.3\%--54.4\%, and Top-5 results by 14.4\%-35.6\%. LLMAO is also the first FL technique trained using a language model architecture that can detect security vulnerabilities down to the code line level.},
booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
articleno = {17},
numpages = {12},
location = {Lisbon, Portugal},
series = {ICSE '24}
}

@article{vulrepairusingllm,
author = {de-Fitero-Dominguez, David and Garcia-Lopez, Eva and Garcia-Cabot, Antonio and Martinez-Herraiz, Jose-Javier},
title = {Enhanced automated code vulnerability repair using large language models},
year = {2024},
issue_date = {Dec 2024},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {138},
number = {PA},
issn = {0952-1976},
url = {https://doi.org/10.1016/j.engappai.2024.109291},
doi = {10.1016/j.engappai.2024.109291},
journal = {Eng. Appl. Artif. Intell.},
month = dec,
abstract={This research addresses the complex challenge of automated repair of code vulnerabilities, vital for enhancing digital security in an increasingly technology-driven world. The study introduces a novel and efficient format for the representation of code modification, using advanced Large Language Models (LLMs) such as Code Llama and Mistral. These models, fine-tuned on datasets featuring C/C++ code vulnerabilities, significantly improve the accuracy and adaptability of automated code repair techniques. A key finding is the enhanced repair accuracy of these models when compared to previous methods such as VulRepair, which underscores their practical utility and efficiency. The research also offers a critical assessment of current evaluation metrics, such as “Perfect Predictions”, and their limitations in reflecting the true capabilities of automated repair models in real-world scenarios. Following this, it underscores the importance of using test datasets devoid of train samples, emphasizing the need for dataset integrity to enhance the effectiveness of LLMs in code repair tasks. The significance of this work is its contribution to digital security, setting new standards for automated code vulnerability repair and paving the way for future advancements in the fields of cybersecurity and artificial intelligence. The study does not only highlight the potential of LLMs in enhancing code security but also fosters further exploration and research in these crucial areas.},
numpages = {13},
keywords = {Automated code repair, Deep learning, Large language models, Vulnerability repair, Mistral, Code llama}
}

@misc{vulmaster,
 abstract = {The advances of deep learning (DL) have paved the way for automatic software vulnerability repair approaches, which effectively learn the mapping from the vulnerable code to the fixed code. Nevertheless, existing DL-based vulnerability repair methods face notable limitations: 1) they struggle to handle lengthy vulnerable code, 2) they treat code as natural language texts, neglecting its inherent structure, and 3) they do not tap into the valuable expert knowledge present in the expert system.
To address this, we propose VulMaster, a Transformer-based neural network model that excels at generating vulnerability repairs through data-centric innovation. Specifically, VulMaster introduces the utilization and combination of various types of input data, including complete vulnerable code of any size, vulnerable code structures, and expert knowledge from the CWE system. Additionally, VulMaster leverages the collaboration between two Large Language Models (LLMs), CodeT5 and ChatGPT: CodeT5 acts as the customizable backbone LLM, fine-tuned with the training data, while ChatGPT supplements by providing missing relevant inputs to CodeT5. We evaluated VulMaster on a real-world C/C++ vulnerability repair dataset comprising 1,754 projects with 5,800 vulnerable functions. The experimental results demonstrated that VulMaster exhibits substantial improvements compared to the learning-based state-of-the-art vulnerability repair approach. Specifically, VulMaster improves the EM, BLEU, and CodeBLEU scores from 10.2\% to 20.0\%, 21.3\% to 29.3\%, and 32.5\% to 40.9\%, respectively.},
 archiveprefix = {arXiv},
 author = {Xin Zhou and Kisub Kim and Bowen Xu and DongGyun Han and David Lo},
 eprint = {2401.15459},
 primaryclass = {cs.SE},
 title = {Multi-LLM Collaboration + Data-Centric Innovation = 2x Better Vulnerability Repair},
 url = {https://arxiv.org/abs/2401.15459},
 year = {2024}
}


@inproceedings{factselection,
author = {Parasaram, Nikhil and Yan, Huijie and Yang, Boyu and Flahy, Zineb and Qudsi, Abriele and Ziaber, Damian and Barr, Earl T. and Mechtaev, Sergey},
title = {The Fact Selection Problem in LLM-Based Program Repair},
year = {2025},
isbn = {9798331505691},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE55347.2025.00162},
doi = {10.1109/ICSE55347.2025.00162},
abstract = {Recent research has shown that incorporating bug-related facts, such as stack traces and GitHub issues, into prompts enhances the bug-fixing capabilities of large language models (LLMs). Considering the ever-increasing context window of these models, a critical question arises: what and how many facts should be included in prompts to maximise the chance of correctly fixing bugs? To answer this question, we conducted a large-scale study, employing over 19K prompts featuring various combinations of seven diverse facts to rectify 314 bugs from open-source Python projects within the BugsInPy benchmark. Our findings revealed that each fact, ranging from simple syntactic details like code context to semantic information previously unexplored in the context of LLMs such as angelic values, is beneficial. Specifically, each fact aids in fixing some bugs that would remain unresolved or only be fixed with a low success rate without it. Importantly, we discovered that the effectiveness of program repair prompts is non-monotonic over the number of used facts; using too many facts leads to subpar outcomes. These insights led us to define the fact selection problem: determining the optimal set of facts for inclusion in a prompt to maximise LLM's performance on a given task instance. We found that there is no one-size-fits-all set of facts for bug repair. Therefore, we developed a basic statistical model, named Maniple, which selects facts specific to a given bug to include in the prompt. This model significantly surpasses the performance of the best generic fact set. To underscore the significance of the fact selection problem, we benchmarked MANIPLE against the state-of-the-art zero-shot, non-conversational LLM-based bug repair methods. On our testing dataset of 157 bugs, Maniple repairs 88 bugs, 17\% above the best configuration.},
booktitle = {Proceedings of the IEEE/ACM 47th International Conference on Software Engineering},
pages = {2574–2586},
numpages = {13},
keywords = {automated program repair, large language models, prompt engineering},
location = {Ottawa, Ontario, Canada},
series = {ICSE '25}
}

@inproceedings{tbar,
  title={TBar: Revisiting template-based automated program repair},
  author={Liu, Kui and Koyuncu, Anil and Kim, Dongsun and Bissyand{\'e}, Tegawend{\'e} F},
  booktitle={Proceedings of the 28th ACM SIGSOFT International Symposium on Software Testing and Analysis},
  pages={31--42},
  url={https://dl.acm.org/doi/pdf/10.1145/3293882.3330577},
  abstract={We revisit the performance of template-based APR to build comprehensive knowledge about the effectiveness of fix patterns, and to highlight the importance of complementary steps such as fault localization or donor code retrieval. To that end, we first investigate the literature to collect, summarize and label recurrently-used fix patterns. Based on the investigation, we build TBar, a straightforward APR tool that systematically attempts to apply these fix patterns to program bugs. We thoroughly evaluate TBar on the Defects4J benchmark. In particular, we assess the actual qualitative and quantitative diversity of fix patterns, as well as their effectiveness in yielding plausible or correct patches. Eventually, we find that, assuming a perfect fault localization, TBar correctly/plausibly fixes 74/101 bugs. Replicating a standard and practical pipeline of APR assessment, we demonstrate that TBar correctly fixes 43 bugs from Defects4J, an unprecedented performance in the literature},
  year={2019}
}

@inproceedings{randomsearch,
  title={The strength of random search on automated program repair},
  author={Qi, Yuhua and Mao, Xiaoguang and Lei, Yan and Dai, Ziying and Wang, Chengsong},
  booktitle={Proceedings of the 36th international conference on software engineering},
  url={https://dl.acm.org/doi/pdf/10.1145/2568225.2568254},
  pages={254--265},
  year={2014}
}

@inproceedings{parsystem,
  title={Automatic patch generation learned from human-written patches},
  author={Kim, Dongsun and Nam, Jaechang and Song, Jaewoo and Kim, Sunghun},
  booktitle={2013 35th international conference on software engineering (ICSE)},
  pages={802--811},
  url={https://staff.fmi.uvt.ro/~daniela.zaharie/ma2018/projects/biblio/applications/AutomatedProgramRepair/PatternBasedPatchGeneration.pdf},
  year={2013},
  abstract={Patch generation is an essential software maintenance task because most software systems inevitably have bugs that need to be fixed. Unfortunately, human resources are often insufficient to fix all reported and known bugs. To address this issue, several automated patch generation techniques have been proposed. In particular, a genetic-programming-based patch generation technique, GenProg, proposed by Weimer et al., has shown promising results. However, these techniques can generate nonsensical patches due to the randomness of their mutation operations. To address this limitation, we propose a novel patch generation approach, Pattern-based Automatic program Repair (Par), using fix patterns learned from existing human-written patches. We manually inspected more than 60,000 human-written patches and found there are several common fix patterns. Our approach leverages these fix patterns to generate program patches automatically. We experimentally evaluated Par on 119 real bugs. In addition, a user study involving 89 students and 164 developers confirmed that patches generated by our approach are more acceptable than those generated by GenProg. Par successfully generated patches for 27 out of 119 bugs, while GenProg was successful for only 16 bugs.},
  organization={IEEE}
}

@inproceedings{semiautofl,
author = {Bin Murtaza, Sardar and Mccoy, Aidan and Ren, Zhiyuan and Murphy, Aidan and Banzhaf, Wolfgang},
title = {LLM Fault Localisation within Evolutionary Computation Based Automated Program Repair},
year = {2024},
isbn = {9798400704956},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3638530.3664174},
doi = {10.1145/3638530.3664174},
abstract = {Repairing bugs can be a daunting task for even a human experienced in debugging, so naturally, attempting to automatically repair programs with a computer system is quite challenging. The existing methods of automated program repair leave a lot of room for improvement. Fault localization, which aims to find lines of code that are potentially buggy, minimises the search space of an automated program repair system. Recent work has shown improvement in these fault localization methods, with the use of Large Language Models. Here, we propose a system where a LLM-based fault localization tool, which we call SemiAutoFL, is used within a fully automatic program repair program, ARJA-e. We show that utilising LLM-based fault localization with ARJA-e can significantly improve its performance on real world bugs. ARJA-e with SemiAutoFL can repair 10 bugs that ARJA-e was previously unable to so do. This finding adds to our understanding of how to improve fault localization and automated program repair, highlighting the potential for more efficient and accurate fault localisation methods being applied to automated program repair.},
booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference Companion},
pages = {1824--1829},
numpages = {6},
keywords = {genetic improvement, fault localisation, large language models},
location = {Melbourne, VIC, Australia},
series = {GECCO '24 Companion}
}

@inproceedings{vuladvisor,
author = {Zhang, Jian and Wang, Chong and Li, Anran and Wang, Wenhan and Li, Tianlin and Liu, Yang},
title = {VulAdvisor: Natural Language Suggestion Generation for Software Vulnerability Repair},
year = {2024},
isbn = {9798400712487},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3691620.3695555},
doi = {10.1145/3691620.3695555},
abstract = {Software vulnerabilities pose serious threats to the security of modern software systems. Deep Learning-based Automated Vulnerability Repair (AVR) has gained attention as a potential solution to accelerate the remediation of vulnerabilities. However, recent studies indicate that existing AVR approaches often only generate patches, which may not align with developers' current repair practices or expectations. In this paper, we introduce VulAdvisor, an automated approach that generates natural language suggestions to guide developers or AVR tools in repairing vulnerabilities. VulAdvisor comprises two main components: oracle extraction and suggestion learning. To address the challenge of limited historical data, we propose an oracle extraction method facilitating ChatGPT to construct a comprehensive and high-quality dataset. For suggestion learning, we take the supervised fine-tuning CodeT5 model as the basis, integrating local context into Multi-Head Attention and introducing a repair action loss, to improve the relevance and meaningfulness of the generated suggestions. Extensive experiments on a large-scale dataset from real-world C/C++ projects demonstrate the effectiveness of VulAdvisor, surpassing several alternatives in terms of both lexical and semantic metrics. Moreover, we show that the generated suggestions enhance the patch generation capabilities of existing AVR tools. Human evaluations further validate the quality and utility of VulAdvisor's suggestions, confirming their potential to improve software vulnerability repair practices.},
booktitle = {Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering},
pages = {1932--1944},
numpages = {13},
keywords = {vulnerability repair, large language models, suggestion generation, program repair},
location = {Sacramento, CA, USA},
series = {ASE '24}
}

@inproceedings{vulrepair,
author = {Fu, Michael and Tantithamthavorn, Chakkrit and Le, Trung and Nguyen, Van and Phung, Dinh},
title = {VulRepair: a T5-based automated software vulnerability repair},
year = {2022},
isbn = {9781450394130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3540250.3549098},
doi = {10.1145/3540250.3549098},
abstract = {As software vulnerabilities grow in volume and complexity, researchers proposed various Artificial Intelligence (AI)-based approaches to help under-resourced security analysts to find, detect, and localize vulnerabilities. However, security analysts still have to spend a huge amount of effort to manually fix or repair such vulnerable functions. Recent work proposed an NMT-based Automated Vulnerability Repair, but it is still far from perfect due to various limitations. In this paper, we propose VulRepair, a T5-based automated software vulnerability repair approach that leverages the pre-training and BPE components to address various technical limitations of prior work. Through an extensive experiment with over 8,482 vulnerability fixes from 1,754 real-world software projects, we find that our VulRepair achieves a Perfect Prediction of 44\%, which is 13\%-21\% more accurate than competitive baseline approaches. These results lead us to conclude that our VulRepair is considerably more accurate than two baseline approaches, highlighting the substantial advancement of NMT-based Automated Vulnerability Repairs. Our additional investigation also shows that our VulRepair can accurately repair as many as 745 out of 1,706 real-world well-known vulnerabilities (e.g., Use After Free, Improper Input Validation, OS Command Injection), demonstrating the practicality and significance of our VulRepair for generating vulnerability repairs, helping under-resourced security analysts on fixing vulnerabilities.},
booktitle = {Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {935--947},
numpages = {13},
keywords = {Software Vulnerability Repair},
location = {Singapore, Singapore},
series = {ESEC/FSE 2022}
}

@inproceedings{coconut,
author = {Lutellier, Thibaud and Pham, Hung Viet and Pang, Lawrence and Li, Yitong and Wei, Moshi and Tan, Lin},
title = {CoCoNuT: combining context-aware neural translation models using ensemble for program repair},
year = {2020},
isbn = {9781450380089},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3395363.3397369},
doi = {10.1145/3395363.3397369},
abstract = {Automated generate-and-validate (GV) program repair techniques (APR) typically rely on hard-coded rules, thus only fixing bugs following specific fix patterns. These rules require a significant amount of manual effort to discover and it is hard to adapt these rules to different programming languages. To address these challenges, we propose a new G&V technique—CoCoNuT, which uses ensemble learning on the combination of convolutional neural networks (CNNs) and a new context-aware neural machine translation (NMT) architecture to automatically fix bugs in multiple programming languages. To better represent the context of a bug, we introduce a new context-aware NMT architecture that represents the buggy source code and its surrounding context separately. CoCoNuT uses CNNs instead of recurrent neural networks (RNNs), since CNN layers can be stacked to extract hierarchical features and better model source code at different granularity levels (e.g., statements and functions). In addition, CoCoNuT takes advantage of the randomness in hyperparameter tuning to build multiple models that fix different bugs and combines these models using ensemble learning to fix more bugs. Our evaluation on six popular benchmarks for four programming languages (Java, C, Python, and JavaScript) shows that CoCoNuT correctly fixes (i.e., the first generated patch is semantically equivalent to the developer’s patch) 509 bugs, including 309 bugs that are fixed by none of the 27 techniques with which we compare.},
booktitle = {Proceedings of the 29th ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {101--114},
numpages = {14},
keywords = {Neural Machine Translation, Deep Learning, Automated program repair, AI and Software Engineering},
location = {Virtual Event, USA},
series = {ISSTA 2020}
}

@misc{vulrag,
  title={Vul-RAG: Enhancing LLM-based Vulnerability Detection via Knowledge-level RAG}, 
  author={Xueying Du and Geng Zheng and Kaixin Wang and Yi Zou and Yujia Wang and Wentai Deng and Jiayi Feng and Mingwei Liu and Bihuan Chen and Xin Peng and Tao Ma and Yiling Lou},
  year={2025},
  eprint={2406.11147},
  archivePrefix={arXiv},
  primaryClass={cs.SE},
  url={https://arxiv.org/abs/2406.11147}, 
}

@inproceedings{trap,
author = {Liu, Pei and Lin, Bo and Qin, Yihao and Weng, Cheng and Chen, Liqian},
title = {T-RAP: A Template-guided Retrieval-Augmented Vulnerability Patch Generation Approach},
year = {2024},
isbn = {9798400707056},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3671016.3672506},
doi = {10.1145/3671016.3672506},
abstract = {Vulnerabilities exert great burden on developers in terms of debugging and maintenance. Automated Vulnerability Repair(AVR) is considered as a promising approach to alleviate the burden of developers. Template-based automated program repair techniques have shown their effectiveness in fixing general bugs. However, due to the diverse root causes of vulnerabilities, it is challenging to construct sufficient repair templates to cover various vulnerabilities. In this paper, we introduce a Template-guided Retrieval-Augmented Patch generation approach, named T-RAP. Inspired by retrieval-augmented techniques that effectively utilize historical data, our approach leverages repair templates to extract similar vulnerability repair patches from the codebase. These patches then guide the process of generating vulnerability patches. To extract similar patches, we also propose a matching algorithm specifically designed for the retrieval-augmented vulnerability repair. This involves identifying similarities between numerous templates and vulnerabilities during the template-guided stage. Experimental results demonstrate that T-RAP outperforms all the studied AVR approaches, repairing 56.8\% more vulnerabilities than VulRepair and 30.24\% more than VulMaster. It can also accurately repair more types of real-world vulnerabilities than VulMaster. Additionally, we evaluated the effectiveness of our patch retriever. The results indicate that our template-guided retriever, which is based on our matching algorithm, outperforms the retrieval algorithm proposed in the recent retrieval-augmented patch generation approach RAP-Gen.},
booktitle = {Proceedings of the 15th Asia-Pacific Symposium on Internetware},
pages = {105--114},
numpages = {10},
keywords = {Automated Vulnerability Repair, Deep Learning, Repair Template, Software Vulnerability},
location = {Macau, China},
series = {Internetware '24}
}

@inproceedings{promptvuldet,
author = {Zhang, Chenyuan and Liu, Hao and Zeng, Jiutian and Yang, Kejing and Li, Yuhong and Li, Hui},
title = {Prompt-Enhanced Software Vulnerability Detection Using ChatGPT},
year = {2024},
isbn = {9798400705021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639478.3643065},
doi = {10.1145/3639478.3643065},
abstract = {With the increase in software vulnerabilities that cause significant economic and social losses, automatic vulnerability detection has become essential in software development and maintenance. Recently, large language models (LLMs) have received considerable attention due to their stunning intelligence, and some studies consider using ChatGPT for vulnerability detection. However, they do not fully consider the characteristics of LLMs, since their designed questions to ChatGPT are simple without a prompt design tailored for vulnerability detection. This paper launches a study on the performance of software vulnerability detection using ChatGPT with different prompt designs. Firstly, we complement previous work by applying various improvements to the basic prompt. Moreover, we incorporate structural and sequential auxiliary information to improve the prompt design. Moreover, we leverage ChatGPT's ability of memorizing multi-round dialogue to design suitable prompts for vulnerability detection. We conduct extensive experiments on two vulnerability datasets to demonstrate the effectiveness of prompt-enhanced vulnerability detection using ChatGPT.},
booktitle = {Proceedings of the 2024 IEEE/ACM 46th International Conference on Software Engineering: Companion Proceedings},
pages = {276--277},
numpages = {2},
keywords = {software vulnerability detection, prompt engineering, large language model, chatgpt},
location = {Lisbon, Portugal},
series = {ICSE-Companion '24}
}

@article{ragnlp,
  title={Retrieval-augmented generation for knowledge-intensive nlp tasks},
  author={Lewis, Patrick and Perez, Ethan and Piktus, Aleksandra and Petroni, Fabio and Karpukhin, Vladimir and Goyal, Naman and K{\"u}ttler, Heinrich and Lewis, Mike and Yih, Wen-tau and Rockt{\"a}schel, Tim and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={9459--9474},
  url={https://proceedings.neurips.cc/paper/2020/file/6b493230205f780e1bc26945df7481e5-Paper.pdf},
  year={2020}
}

@inbook{san2patch,
author = {Kim, Youngjoon and Shin, Sunguk and Kim, Hyoungshick and Yoon, Jiwon},
title = {Logs in, patches out: automated vulnerability repair via tree-of-thought LLM analysis},
year = {2025},
isbn = {978-1-939133-52-6},
publisher = {USENIX Association},
address = {USA},
abstract = {Research on automated vulnerability repair often requires extensive program analysis and expert input, making it challenging to deploy in practice. We propose SAN2PATCH, a system that generates patches using only sanitizer logs and source code, eliminating the need for costly program analysis or manual intervention. SAN2PATCH employs multi-stage reasoning with Large Language Models (LLMs) to decompose the patching process into four distinct tasks: vulnerability comprehension, fault localization, fix strategy formulation, and patch generation. Through tree-structured prompting and rigorous validation, SAN2PATCH can generate diverse, functionally-correct patches. Evaluations on the VulnLoc dataset show that SAN2PATCH successfully patches 79.5\% of vulnerabilities, surpassing state-of-the-art tools like ExtractFix (43\%) and VulnFix (51\%) by significant margins. On our newly curated SAN2VULN dataset of 27 new vulnerabilities from various open-source projects, SAN2PATCH achieves a 63\% success rate, demonstrating its effectiveness on modern security flaws. Notably, SAN2PATCH excels at patching complex memory-related vulnerabilities, successfully fixing 81.8\% of buffer overflows while preserving program functionality. This high performance, combined with minimal deployment requirements and elimination of manual steps, makes SAN2PATCH a practical solution for real-world vulnerability remediation.},
booktitle = {Proceedings of the 34th USENIX Conference on Security Symposium},
articleno = {227},
url={https://www.usenix.org/system/files/usenixsecurity25-kim-youngjoon.pdf},
numpages = {19}
}

@article{adverintentagent,
author = {Ye, He and Yang, Aidan Z.H. and Hu, Chang and Wang, Yanlin and Zhang, Tao and Le Goues, Claire},
title = {AdverIntent-Agent: Adversarial Reasoning for Repair Based on Inferred Program Intent},
year = {2025},
issue_date = {July 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {ISSTA},
url = {https://doi.org/10.1145/3728939},
doi = {10.1145/3728939},
abstract = {Automated program repair (APR) has shown promising results, particularly with the use of neural networks. Currently, most APR tools focus on code transformations specified by test suites, rather than reasoning about the program’s intent and the high-level bug specification. Without a proper understanding of program intent, these tools tend to generate patches that overfit incomplete test suites and fail to reflect the developer’s intentions. However, reasoning about program intent is challenging. In our work, we propose an approach called AdverIntent-Agent, based on critique and adversarial reasoning. Our approach is novel to shift the focus from generating multiple APR patches to inferring multiple potential program intents. Ideally, we aim to infer intents that are, to some extent, adversarial to each other, maximizing the probability that at least one aligns closely with the developer’s original intent. AdverIntent-Agent is a multi-agent approach consisting of three agents: a reasoning agent, a test agent, and a repair agent. First, the reasoning agent generates adversarial program intents along with the corresponding faulty statements. Next, the test agent produces adversarial test cases that align with each inferred intent, constructing oracles that use the same inputs but have different expected outputs. Finally, the repair agent uses dynamic and precise LLM prompts to generate patches that satisfy both the inferred program intent and the generated tests. AdverIntent-Agent was evaluated on two benchmarks: Defects4J 2.0 and HumanEval-Java. AdverIntentAgent correctly repaired 77 and 105 bugs in both benchmarks, respectively. Our work helps reduce the effort required to review patches by enabling developers to assess program intent in natural language, rather than reviewing code patches.},
journal = {Proc. ACM Softw. Eng.},
month = jun,
articleno = {ISSTA062},
numpages = {23},
keywords = {Large Language Models, Program Repair}
}

@misc{repairagent,
 abstract = {Automated program repair has emerged as a powerful technique to mitigate the impact of software bugs on system reliability and user experience. This paper introduces RepairAgent, the first work to address the program repair challenge through an autonomous agent based on a large language model (LLM). Unlike existing deep learning-based approaches, which prompt a model with a fixed prompt or in a fixed feedback loop, our work treats the LLM as an agent capable of autonomously planning and executing actions to fix bugs by invoking suitable tools. RepairAgent freely interleaves gathering information about the bug, gathering repair ingredients, and validating fixes, while deciding which tools to invoke based on the gathered information and feedback from previous fix attempts. Key contributions that enable RepairAgent include a set of tools that are useful for program repair, a dynamically updated prompt format that allows the LLM to interact with these tools, and a finite state machine that guides the agent in invoking the tools. Our evaluation on the popular Defects4J dataset demonstrates RepairAgent's effectiveness in autonomously repairing 164 bugs, including 39 bugs not fixed by prior techniques. Interacting with the LLM imposes an average cost of 270,000 tokens per bug, which, under the current pricing of OpenAI's GPT-3.5 model, translates to 14 cents of USD per bug. To the best of our knowledge, this work is the first to present an autonomous, LLM-based agent for program repair, paving the way for future agent-based techniques in software engineering.},
 archiveprefix = {arXiv},
 author = {Islem Bouzenia and Premkumar Devanbu and Michael Pradel},
 eprint = {2403.17134},
 primaryclass = {cs.SE},
 title = {RepairAgent: An Autonomous, LLM-Based Agent for Program Repair},
 url = {https://arxiv.org/abs/2403.17134},
 year = {2024}
}

@misc{llmselfdebug,
  title={Teaching Large Language Models to Self-Debug}, 
  author={Xinyun Chen and Maxwell Lin and Nathanael Scharli and Denny Zhou},
  year={2023},
  eprint={2304.05128},
  archivePrefix={arXiv},
  primaryClass={cs.CL},
  abstract={Large language models (LLMs) have achieved impressive performance on code generation. However, for complex programming tasks, generating the correct solution in one go becomes challenging, thus some prior works have designed program repair approaches to improve code generation performance. In this work, we propose Self-Debugging, which teaches a large language model to debug its predicted program via few-shot demonstrations. In particular, we demonstrate that Self-Debugging can teach the large language model to perform rubber duck debugging; i.e., without any human feedback on the code correctness or error messages, the model is able to identify its mistakes by investigating the execution results and explaining the generated code in natural language. Self-Debugging achieves the state-of-the-art performance on several code generation benchmarks, including the Spider dataset for text-to-SQL generation, TransCoder for C++-to-Python translation, and MBPP for text-to-Python generation. On the Spider benchmark where there are no unit tests to verify the correctness of predictions, Self-Debugging with code explanation consistently improves the baseline by 2-3%, and improves the prediction accuracy on problems of the hardest level by 9%. On TransCoder and MBPP where unit tests are available, Self-Debugging improves the baseline accuracy by up to 12%. Meanwhile, by leveraging feedback messages and reusing failed predictions, Self-Debugging notably improves sample efficiency, and can match or outperform baseline models that generate more than 10x candidate programs.},
  url={https://arxiv.org/abs/2304.05128}, 
}

@inproceedings{aprfromllm,
  title={Automated repair of programs from large language models},
  author={Fan, Zhiyu and Gao, Xiang and Mirchev, Martin and Roychoudhury, Abhik and Tan, Shin Hwei},
  booktitle={2023 IEEE/ACM 45th International Conference on Software Engineering (ICSE)},
  pages={1469--1481},
  url={https://arxiv.org/abs/2205.10583},
  abstract={Large language models such as Codex, have shown the capability to produce code for many programming tasks. However, the success rate of existing models is low, especially for complex programming tasks. One of the reasons is that language models lack awareness of program semantics, resulting in incorrect programs, or even programs which do not compile. In this paper, we systematically study whether automated program repair (APR) techniques can fix the incorrect solutions produced by language models in LeetCode contests. The goal is to study whether APR techniques can enhance reliability in the code produced by large language models. Our study revealed that: (1) automatically generated code shares common programming mistakes with human-crafted solutions, indicating APR techniques may have potential to fix auto-generated code; (2) given bug location information provided by a statistical fault localization approach, the newly released Codex edit mode, which supports editing code, is similar to or better than existing Java repair tools TBar and Recoder in fixing incorrect solutions. By analyzing the experimental results generated by these tools, we provide several suggestions: (1) enhancing APR tools to surpass limitations in patch space (e.g., introducing more flexible fault localization) is desirable; (2) as large language models can derive more fix patterns by training on more data, future APR tools could shift focus from adding more fix patterns to synthesis/semantics based approaches, (3) combination of language models with APR to curate patch ingredients, is worth studying.},
  year={2023},
  organization={IEEE}
}

@article{contrastrepair,
  title={ContrastRepair: Enhancing Conversation-Based Automated Program Repair via Contrastive Test Case Pairs},
  volume={34},
  ISSN={1557-7392},
  url={http://dx.doi.org/10.1145/3719345},
  DOI={10.1145/3719345},
  number={8},
  journal={ACM Transactions on Software Engineering and Methodology},
  publisher={Association for Computing Machinery (ACM)},
  author={Kong, Jiaolong and Xie, Xiaofei and Cheng, Mingfei and Liu, Shangqing and Du, Xiaoning and Guo, Qi},
  year={2025},
  month=oct, 
  pages={1--31}
}

@article{invalidator,
  author={Le-Cong, Thanh and Luong, Duc-Minh and Le, Xuan Bach D. and Lo, David and Tran, Nhat-Hoa and Quang-Huy, Bui and Huynh, Quyet-Thang},
  journal={IEEE Transactions on Software Engineering}, 
  title={Invalidator: Automated Patch Correctness Assessment Via Semantic and Syntactic Reasoning}, 
  year={2023},
  volume={49},
  url={https://ieeexplore.ieee.org/document/10066209},
  number={6},
  pages={3411-3429},
  keywords={Syntactics;Semantics;Maintenance engineering;Cognition;Manuals;Codes;Source coding;Automated patch correctness assessment;automated program repair;code representations;overfitting problem;program invariants},
  doi={10.1109/TSE.2023.3255177}
}

@misc{unidebugger,
  title={UniDebugger: Hierarchical Multi-Agent Framework for Unified Software Debugging}, 
  author={Cheryl Lee and Chunqiu Steven Xia and Longji Yang and Jen-tse Huang and Zhouruixin Zhu and Lingming Zhang and Michael R. Lyu},
  year={2025},
  eprint={2404.17153},
  archivePrefix={arXiv},
  primaryClass={cs.SE},
  url={https://arxiv.org/abs/2404.17153}, 
}

@inproceedings{recode,
author = {Zhao, Yicong and Chen, Shisong and Zhang, Jiacheng and Li, Zhixu},
title = {ReCode: Improving LLM-based Code Repair with Fine-Grained Retrieval-Augmented Generation},
year = {2025},
isbn = {9798400720406},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3746252.3761035},
doi = {10.1145/3746252.3761035},
abstract = {Recent advances in large language models (LLMs) have demonstrated impressive capabilities in code-related tasks such as code generation and automated program repair. Despite their promising performance, most existing approaches for code repair suffer from high training costs or computationally expensive inference. Retrieval-augmented generation (RAG), with its efficient in-context learning paradigm, offers a more scalable alternative. However, conventional retrieval strategies, which are often based on holistic code-text embeddings, fail to capture the structural intricacies of code, resulting in suboptimal retrieval quality. To address the above limitations, we propose ReCode, a fine-grained retrieval-augmented in-context learning framework designed for accurate and efficient code repair. Specifically, ReCode introduces two key innovations: (1) an algorithm-aware retrieval strategy that narrows the search space using preliminary algorithm type predictions; and (2) a modular dual-encoder architecture that separately processes code and textual inputs, enabling fine-grained semantic matching between input and retrieved contexts. Furthermore, we propose RACodeBench, a new benchmark constructed from real-world user-submitted buggy code, which addresses the limitations of synthetic benchmarks and supports realistic evaluation. Experimental results on RACodeBench and competitive programming datasets demonstrate that ReCode achieves higher repair accuracy with significantly reduced inference cost, highlighting its practical value for real-world code repair scenarios.},
booktitle = {Proceedings of the 34th ACM International Conference on Information and Knowledge Management},
pages = {4368–4378},
numpages = {11},
keywords = {benchmark, code repair, in-context learning, retrieval augmented},
location = {Seoul, Republic of Korea},
series = {CIKM '25}
}

@article{divellm4bugrepair,
author = {Hossain, Soneya Binta and Jiang, Nan and Zhou, Qiang and Li, Xiaopeng and Chiang, Wen-Hao and Lyu, Yingjun and Nguyen, Hoan and Tripp, Omer},
title = {A Deep Dive into Large Language Models for Automated Bug Localization and Repair},
year = {2024},
issue_date = {July 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {FSE},
url = {https://doi.org/10.1145/3660773},
doi = {10.1145/3660773},
abstract = {Large language models (LLMs) have shown impressive effectiveness in various software engineering tasks, including automated program repair (APR). In this study, we take a deep dive into automated bug localization and repair utilizing LLMs. In contrast to many deep learning-based APR methods that assume known bug locations, rely on line-level localization tools, or address bug prediction and fixing in one step, our approach uniquely employs LLMs to predict bug location at the token level and subsequently utilizes them for bug fixing. This methodological separation of bug localization and fixing using different LLMs enables effective integration of diverse contextual information and improved incorporation of inductive biases. We introduce Toggle: Token-Granulated Bug Localization and Repair, a comprehensive program repair framework that integrates a bug localization model, an adjustment model to address tokenizer inconsistencies, and a bug-fixing model. Toggle takes a buggy function as input and generates a complete corrected function. We investigate various styles of prompting to the bug fixing model to identify the most effective prompts that better utilize the inductive bias and significantly outperform others. Toggle achieves the new state-of-the-art performance on the CodeXGLUE code refinement benchmark, and exhibits better and comparable performance on several other widely-used APR datasets, including Defects4J. In the Defects4J benchmark, our approach consistently ranks above other methods, achieving superior results in the Top-10, Top-30, Top-50, and Top-100 metrics. Besides examining Toggle’s generalizability to unseen data, evaluating the effectiveness of various prompts, we also investigate the impact of additional contextual information such as buggy lines and code comments on bug localization, and explore the importance of the adjustment model. Our extensive experiments offer valuable insights and answers to critical research questions.},
journal = {Proc. ACM Softw. Eng.},
month = jul,
articleno = {66},
numpages = {23},
keywords = {Software and its engineering, Software testing and debugging, Automated Bug Localization and Fix, Large Language Models}
}

@inproceedings{patchcorrectness,
author = {Xiong, Yingfei and Liu, Xinyuan and Zeng, Muhan and Zhang, Lu and Huang, Gang},
title = {Identifying patch correctness in test-based program repair},
year = {2018},
isbn = {9781450356381},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3180155.3180182},
doi = {10.1145/3180155.3180182},
abstract = {Test-based automatic program repair has attracted a lot of attention in recent years. However, the test suites in practice are often too weak to guarantee correctness and existing approaches often generate a large number of incorrect patches.To reduce the number of incorrect patches generated, we propose a novel approach that heuristically determines the correctness of the generated patches. The core idea is to exploit the behavior similarity of test case executions. The passing tests on original and patched programs are likely to behave similarly while the failing tests on original and patched programs are likely to behave differently. Also, if two tests exhibit similar runtime behavior, the two tests are likely to have the same test results. Based on these observations, we generate new test inputs to enhance the test suites and use their behavior similarity to determine patch correctness.Our approach is evaluated on a dataset consisting of 139 patches generated from existing program repair systems including jGen-Prog, Nopol, jKali, ACS and HDRepair. Our approach successfully prevented 56.3\% of the incorrect patches to be generated, without blocking any correct patches.},
booktitle = {Proceedings of the 40th International Conference on Software Engineering},
pages = {789–799},
numpages = {11},
location = {Gothenburg, Sweden},
series = {ICSE '18}
}

@article{repairllama,
  title={RepairLLaMA: Efficient Representations and Fine-Tuned Adapters for Program Repair},
  volume={51},
  ISSN={2326-3881},
  url={http://dx.doi.org/10.1109/TSE.2025.3581062},
  DOI={10.1109/tse.2025.3581062},
  number={8},
  journal={IEEE Transactions on Software Engineering},
  publisher={Institute of Electrical and Electronics Engineers (IEEE)},
  author={Silva, André and Fang, Sen and Monperrus, Martin},
  year={2025},
  month=aug, pages={2366–2380}
}

@inproceedings{llm4apr,
author = {Ribeiro, Francisco},
title = {Large Language Models for Automated Program Repair},
year = {2023},
isbn = {9798400703843},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3618305.3623587},
doi = {10.1145/3618305.3623587},
abstract = {This paper introduces two methods for automated program repair (APR) utilizing pre-trained language models. The first method demonstrates program repair as a code completion task and is validated on a dataset of Java programs. The second method, Mentat, leverages OCaml’s parser and type system as fault localization techniques to generate prompts for GPT-3, producing candidate patches. Evaluation results show promising repair rates, with 27\% and 39.2\% effectiveness, respectively. For OCaml, a comparative study employing an automated validation strategy is presented in which the technique outperforms other tools. Language models are effective at APR, enhancing bug fixing and freeing developers to focus on other critical aspects of software engineering.},
booktitle = {Companion Proceedings of the 2023 ACM SIGPLAN International Conference on Systems, Programming, Languages, and Applications: Software for Humanity},
pages = {7–9},
numpages = {3},
keywords = {type systems, fault localization, code generation, automated program repair},
location = {Cascais, Portugal},
series = {SPLASH 2023}
}

@inproceedings{rapgen,
author = {Wang, Weishi and Wang, Yue and Joty, Shafiq and Hoi, Steven C.H.},
title = {RAP-Gen: Retrieval-Augmented Patch Generation with CodeT5 for Automatic Program Repair},
year = {2023},
isbn = {9798400703270},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3611643.3616256},
doi = {10.1145/3611643.3616256},
abstract = {Automatic program repair (APR) is crucial to reduce manual debugging efforts for developers and improve software reliability. While conventional search-based techniques typically rely on heuristic rules or a redundancy assumption to mine fix patterns, recent years have witnessed the surge of deep learning (DL) based approaches to automate the program repair process in a data-driven manner. However, their performance is often limited by a fixed set of parameters to model the highly complex search space of APR. To ease such burden on the parametric models, in this work, we propose a novel Retrieval-Augmented Patch Generation framework (RAP-Gen) by explicitly leveraging relevant fix patterns retrieved from a codebase of previous bug-fix pairs. Specifically, we build a hybrid patch retriever to account for both lexical and semantic matching based on the raw source code in a language-agnostic manner, which does not rely on any code-specific features. In addition, we adapt a code-aware language model CodeT5 as our foundation model to facilitate both patch retrieval and generation tasks in a unified manner. We adopt a stage-wise approach where the patch retriever first retrieves a relevant external bug-fix pair to augment the buggy input for the CodeT5 patch generator, which synthesizes a ranked list of repair patch candidates. Notably, RAP-Gen is a generic APR framework that can flexibly integrate different patch retrievers and generators to repair various types of bugs. We thoroughly evaluate RAP-Gen on three benchmarks in two programming languages, including the TFix benchmark in JavaScript, and Code Refinement and Defects4J benchmarks in Java, where the bug localization information may or may not be provided. Experimental results show that RAP-Gen significantly outperforms previous state-of-the-art (SoTA) approaches on all benchmarks, e.g., boosting the accuracy of T5-large on TFix from 49.70\% to 54.15\% (repairing 478 more bugs) and repairing 15 more bugs on 818 Defects4J bugs. Further analysis reveals that our patch retriever can search for relevant fix patterns to guide the APR systems.},
booktitle = {Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {146–158},
numpages = {13},
keywords = {Automated program repair, Neural networks, Pretrained language models, Retrieval-augmented generation},
location = {San Francisco, CA, USA},
series = {ESEC/FSE 2023}
}

@inproceedings{codet5,
  title = "CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation",
  author = {Wang, Yue  and     Wang Weishi  and     Joty Shafiq  and     Hoi Steven C.H.},
  editor = {Moens Marie-Francine  and     Huang Xuanjing  and     Specia Lucia  and     Yih Scott Wen-tau},
  booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
  month = nov,
  year = "2021",
  address = "Online and Punta Cana, Dominican Republic",
  publisher = "Association for Computational Linguistics",
  url = "https://aclanthology.org/2021.emnlp-main.685/",
  doi = "10.18653/v1/2021.emnlp-main.685",
  pages = "8696--8708",
  abstract = "Pre-trained models for Natural Languages (NL) like BERT and GPT have been recently shown to transfer well to Programming Languages (PL) and largely benefit a broad set of code-related tasks. Despite their success, most current methods either rely on an encoder-only (or decoder-only) pre-training that is suboptimal for generation (resp. understanding) tasks or process the code snippet in the same way as NL, neglecting the special characteristics of PL such as token types. We present CodeT5, a unified pre-trained encoder-decoder Transformer model that better leverages the code semantics conveyed from the developer-assigned identifiers. Our model employs a unified framework to seamlessly support both code understanding and generation tasks and allows for multi-task learning. Besides, we propose a novel identifier-aware pre-training task that enables the model to distinguish which code tokens are identifiers and to recover them when they are masked. Furthermore, we propose to exploit the user-written code comments with a bimodal dual generation task for better NL-PL alignment. Comprehensive experiments show that CodeT5 significantly outperforms prior methods on understanding tasks such as code defect detection and clone detection, and generation tasks across various directions including PL-NL, NL-PL, and PL-PL. Further analysis reveals that our model can better capture semantic information from code. Our code and pre-trained models are released at \url{https://github.com/salesforce/CodeT5}."
}

@inproceedings{inferfix,
author = {Jin, Matthew and Shahriar, Syed and Tufano, Michele and Shi, Xin and Lu, Shuai and Sundaresan, Neel and Svyatkovskiy, Alexey},
title = {InferFix: End-to-End Program Repair with LLMs},
year = {2023},
isbn = {9798400703270},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3611643.3613892},
doi = {10.1145/3611643.3613892},
abstract = {Software development life cycle is profoundly influenced by bugs; their introduction, identification, and eventual resolution account for a significant portion of software development cost. This has motivated software engineering researchers and practitioners to propose different approaches for automating the identification and repair of software defects. Large Language Models (LLMs) have been adapted to the program repair task through few-shot demonstration learning and instruction prompting, treating this as an infilling task. However, these models have only focused on learning general bug-fixing patterns for uncategorized bugs mined from public repositories. In this paper, we propose : a transformer-based program repair framework paired with a state-of-the-art static analyzer to fix critical security and performance bugs.  combines a Retriever – transformer encoder model pretrained via contrastive learning objective, which aims at searching for semantically equivalent bugs and corresponding fixes; and a Generator – an LLM (12 billion parameter Codex Cushman model) finetuned on supervised bug-fix data with prompts augmented via adding bug type annotations and semantically similar fixes retrieved from an external non-parametric memory. To train and evaluate our approach, we curated , a novel, metadata-rich dataset of bugs extracted by executing the Infer static analyzer on the change histories of thousands of Java and C# repositories. Our evaluation demonstrates that  outperforms strong LLM baselines, with a top-1 accuracy of 65.6\% for generating fixes in C# and 76.8\% in Java. We discuss the deployment of alongside Infer at Microsoft which offers an end-to-end solution for detection, classification, and localization of bugs, as well as fixing and validation of candidate patches, integrated in the continuous integration (CI) pipeline to automate the software development workflow.},
booktitle = {Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {1646–1656},
numpages = {11},
keywords = {Program repair, finetuning, prompt augmentation, static analyses},
location = {San Francisco, CA, USA},
series = {ESEC/FSE 2023}
}

@inproceedings{prompt4vulrepair,
author = {Luo, Yining and Li, Baobao and Singhal, Anoop and Tseng, Peiyu and Zhang, Lan and Zou, Qingtian and Sun, Xiaoyan and Liu, Peng},
title = {Exploring Prompt Patterns for Effective Vulnerability Repair in Real-World Code by Large Language Models},
year = {2025},
isbn = {9798400715013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3716815.3729010},
doi = {10.1145/3716815.3729010},
abstract = {Large Language Models (LLMs) have shown promise in automating code vulnerability repair, but their effectiveness in handling real-world code remains limited. This paper investigates the capability of LLMs,  in repairing vulnerabilities and proposes a systematic approach to enhance their performance through specialized prompt engineering. Through extensive evaluation of 5,826 code samples, we found that while LLMs successfully repair vulnerabilities in simple cases, they struggle with complex real-world code that involves intricate dependencies, contextual requirements, and multi-file interactions. To address these limitations, we first incorporated Control Flow Graphs (CFGs) as supplementary prompts, achieving a 14.4\% success rate in fixing previously unresolvable vulnerabilities. Through analysis of repair failures, we identified three primary challenge categories and developed corresponding prompt patterns incorporating techniques such as granular contextual information provision and progressive code simplification. Evaluation on real-world projects demonstrated that our approach significantly improved LLMs' repair capabilities, achieving over 85\% success rates across all identified challenge categories. Our findings suggest that while LLMs have inherent limitations in handling complex vulnerabilities independently, they can become effective tools for automated vulnerability repair when guided by carefully crafted prompts.},
booktitle = {Proceedings of the 10th ACM International Workshop on Security and Privacy Analytics},
pages = {23--33},
numpages = {11},
keywords = {large language models, program repair, deep learning},
location = {Pittsburgh, PA, USA},
series = {IWSPA '25}
}

@article{sft,
  title={Transfer learning for sentiment analysis using BERT based supervised fine-tuning},
  author={Prottasha, Nusrat Jahan and Sami, Abdullah As and Kowsher, Md and Murad, Saydul Akbar and Bairagi, Anupam Kumar and Masud, Mehedi and Baz, Mohammed},
  journal={Sensors},
  volume={22},
  number={11},
  pages={4157},
  url={https://www.mdpi.com/1424-8220/22/11/4157/pdf},
  year={2022},
  publisher={MDPI}
}

@inproceedings{asan,
  author = {Serebryany, Konstantin and Bruening, Derek and Potapenko, Alexander and Vyukov, Dmitry},
  title = {AddressSanitizer: a fast address sanity checker},
  year = {2012},
  publisher = {USENIX Association},
  address = {USA},
  abstract = {Memory access bugs, including buffer overflows and uses of freed heap memory, remain a serious problem for programming languages like C and C++. Many memory error detectors exist, but most of them are either slow or detect a limited set of bugs, or both.This paper presents AddressSanitizer, a new memory error detector. Our tool finds out-of-bounds accesses to heap, stack, and global objects, as well as use-after-free bugs. It employs a specialized memory allocator and code instrumentation that is simple enough to be implemented in any compiler, binary translation system, or even in hardware.AddressSanitizer achieves efficiency without sacrificing comprehensiveness. Its average slowdown is just 73\% yet it accurately detects bugs at the point of occurrence. It has found over 300 previously unknown bugs in the Chromium browser and many bugs in other software.},
  booktitle = {Proceedings of the 2012 USENIX Conference on Annual Technical Conference},
  pages = {28},
  url={https://dl.acm.org/doi/10.5555/2342821.2342849},
  numpages = {1},
  location = {Boston, MA},
  series = {USENIX ATC'12}
}
@inproceedings{valgrind,
author = {Nethercote, Nicholas and Seward, Julian},
title = {Valgrind: a framework for heavyweight dynamic binary instrumentation},
year = {2007},
isbn = {9781595936332},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1250734.1250746},
doi = {10.1145/1250734.1250746},
abstract = {Dynamic binary instrumentation (DBI) frameworks make it easy to build dynamic binary analysis (DBA) tools such as checkers and profilers. Much of the focus on DBI frameworks has been on performance; little attention has been paid to their capabilities. As a result, we believe the potential of DBI has not been fully exploited.In this paper we describe Valgrind, a DBI framework designed for building heavyweight DBA tools. We focus on its unique support for shadow values-a powerful but previously little-studied and difficult-to-implement DBA technique, which requires a tool to shadow every register and memory value with another value that describes it. This support accounts for several crucial design features that distinguish Valgrind from other DBI frameworks. Because of these features, lightweight tools built with Valgrind run comparatively slowly, but Valgrind can be used to build more interesting, heavyweight tools that are difficult or impossible to build with other DBI frameworks such as Pin and DynamoRIO.},
booktitle = {Proceedings of the 28th ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {89–100},
numpages = {12},
keywords = {Memcheck, Valgrind, dynamic binary analysis, dynamic binary instrumentation, shadow values},
location = {San Diego, California, USA},
series = {PLDI '07}
}

@inproceedings {omnitabledebug,
author = {Andrew Quinn and Jason Flinn and Michael Cafarella and Baris Kasikci},
title = {Debugging the {OmniTable} Way},
booktitle = {16th USENIX Symposium on Operating Systems Design and Implementation (OSDI 22)},
year = {2022},
isbn = {978-1-939133-28-1},
address = {Carlsbad, CA},
pages = {357--373},
url = {https://www.usenix.org/conference/osdi22/presentation/quinn},
publisher = {USENIX Association},
month = jul
}

@inbook{sok,
author = {Li, Ying and Shezan, Faysal Hossain and Wei, Bomin and Wang, Gang and Tian, Yuan},
title = {SoK: towards effective automated vulnerability repair},
year = {2025},
isbn = {978-1-939133-52-6},
publisher = {USENIX Association},
address = {USA},
abstract = {The increasing prevalence of software vulnerabilities necessitates automated vulnerability repair (AVR) techniques. This Systematization of Knowledge (SoK) provides a comprehensive overview of the AVR landscape, encompassing both synthetic and real-world vulnerabilities. Through a systematic literature review and quantitative benchmarking across diverse datasets, methods, and strategies, we establish a taxonomy of existing AVR methodologies, categorizing them into template- guided, search-based, constraint-based, and learning-driven approaches. We evaluate the strengths and limitations of these approaches, highlighting common challenges and practical implications. Our comprehensive analysis of existing AVR methods reveals a diverse landscape with no single "best" approach. Learning-based methods excel in specific scenarios but lack complete program understanding, and both learning and non-learning methods face challenges with complex vulnerabilities. Additionally, we identify emerging trends and propose future research directions to advance the field of AVR. This SoK serves as a valuable resource for researchers and practitioners, offering a structured understanding of the current state-of-the-art and guiding future research and development in this critical domain.},
booktitle = {Proceedings of the 34th USENIX Conference on Security Symposium},
articleno = {229},
numpages = {22}
}

@inproceedings{artofrepair,
author = {Vallecillos Ruiz, Fernando and Hort, Max and Moonen, Leon},
title = {The Art of Repair: Optimizing Iterative Program Repair with Instruction-Tuned Models},
year = {2025},
isbn = {9798400713859},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3756681.3756966},
doi = {10.1145/3756681.3756966},
abstract = {Automatic program repair (APR) aims at reducing the manual efforts required to identify and fix errors in source code. Before the rise of Large Language Model (LLM)-based agents, a common strategy was simply to increase the number of generated patches, sometimes to the thousands, which usually yielded better repair results on benchmarks. More recently, self-iterative capabilities enabled LLMs to refine patches over multiple rounds guided by feedback. However, literature often focuses on many iterations and disregards different numbers of outputs.We investigate an APR pipeline that balances these two approaches, the generation of multiple outputs and multiple rounds of iteration, while imposing a limit of 10 total patches per bug. We apply three SOTA instruction-tuned LLMs – DeepSeekCoder-Instruct, Codellama-Instruct, Llama3.1-Instruct – to the APR task. We further fine-tune each model on an APR dataset with three sizes (1K, 30K, 65K) and two techniques (Full Fine-Tuning and LoRA), allowing us to assess their repair capabilities on two APR benchmarks: HumanEval-Java and Defects4J.Our results show that by using only a fraction (&lt;1\%) of the fine-tuning dataset, we can achieve improvements of up to 78\% in the number of plausible patches generated, challenging prior studies that reported limited gains using Full Fine-Tuning. However, we find that exceeding certain thresholds leads to diminishing outcomes, likely due to overfitting. Moreover, we show that base models greatly benefit from creating patches in an iterative fashion rather than generating them all at once. In addition, the benefit of iterative strategies becomes more pronounced in complex benchmarks. Even fine-tuned models, while benefiting less from iterations, still gain advantages, particularly on complex benchmarks. The research underscores the need for balanced APR strategies that combine multi-output generation and iterative refinement.},
booktitle = {Proceedings of the 29th International Conference on Evaluation and Assessment in Software Engineering},
pages = {500--511},
numpages = {12},
keywords = {Automated Program Repair, Software Testing, Software Maintenance, Large Language Models},
location = {
},
series = {EASE '25}
}

@article{linejlocrepair,
author = {Hou, Jing and Han, Jiaxuan and Huang, Cheng and Wang, Nannan and Li, Lerong},
title = {         LineJLocRepair: A line-level method for Automated Vulnerability Repair based on joint training},
year = {2025},
issue_date = {May 2025},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {166},
number = {C},
issn = {0167-739X},
url = {https://doi.org/10.1016/j.future.2024.107671},
doi = {10.1016/j.future.2024.107671},
journal = {Future Gener. Comput. Syst.},
month = may,
numpages = {14},
keywords = {Vulnerability localization, Vulnerability repair, Code LLM}
}

@article{reapr,
author = {Liu, Zixin and Du, Xiaozhi and Liu, Hairui},
title = {ReAPR: Automatic program repair via retrieval-augmented large language models},
year = {2025},
issue_date = {Jul 2025},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {33},
number = {3},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-025-09728-1},
doi = {10.1007/s11219-025-09728-1},
abstract = {Automatic Program Repair (APR) aims to automatically fix software defects, significantly reducing the efforts of manual debugging. Recent studies have demonstrated impressive results in utilizing Large Language Models (LLMs) for software bug fixing. Current LLM-based approaches depend solely on the pre-trained knowledge of LLMs, overlooking the prior knowledge contained in historical bug repair records, which increases the likelihood of hallucinations. To address this challenge, this paper proposes ReAPR, a retrieval-augmented framework for APR. We first curate a high-quality retrieval database by carefully compiling and filtering the existing datasets for APR. Subsequently, ReAPR leverages a retriever to fetch bug-fix pairs similar to the target bug from a retrieval database, providing contextual hints to guide the LLMs in the repair process. We then investigate two techniques to retrieve bug-fix pairs associated with the function to be fixed: BM25 and Dense Passage Retrieval (DPR). After retrieving the relevant bug-fix pair, we construct a prompt and integrate the retrieved pair into it. Besides, we also compare the proposed RAG-based approach with the parameter-efficient fine-tuning (PEFT) approaches on repair performance. To validate the effectiveness of ReAPR, we conduct extensive experiments based on the widely-used benchmark dataset Defects4j 2.0 as well as the latest benchmark GitBug-Java. The results show that ReAPR, based on the CodeLlama(7B) backbone, successfully fixes 68 and 59 bugs in the DPR and BM25 settings, respectively, in Defects4j 2.0, outperforming the best baseline approach by 18 and 9 bugs under the same repair settings.},
journal = {Software Quality Journal},
month = jul,
numpages = {31},
keywords = {Automated Program Repair, Retrieval-Augmented Generation, Large Language Models, Prompt Learning}
}

@inproceedings{xia2024automated,
  title={Automated program repair via conversation: Fixing 162 out of 337 bugs for $0.42 each using chatgpt},
  author={Xia, Chunqiu Steven and Zhang, Lingming},
  booktitle={Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis},
  pages={819--831},
  url={https://arxiv.org/pdf/2304.00385},
  year={2024}
}

@article{contextpromptrepair,
author = {Li, Yingling and Cai, Muxin and Chen, Junjie and Xu, Yang and Huang, Lei and Li, Jianping},
title = {Context-aware prompting for LLM-based program repair},
year = {2025},
issue_date = {Aug 2025},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {32},
number = {2},
issn = {0928-8910},
url = {https://doi.org/10.1007/s10515-025-00512-w},
doi = {10.1007/s10515-025-00512-w},
abstract = {Automated program repair (APR) plays a crucial role in ensuring the quality of software code, as manual bug-fixing is extremely time-consuming and labor-intensive. Traditional APR tools (e.g., template-based approaches) face the challenge of generalizing to different bug patterns, while deep learning (DL)-based methods heavily rely on training datasets and struggle to fix unseen bugs. Recently, large language models (LLMs) have shown great potential in APR due to their ability to generate patches, having achieved promising results. However, their effectiveness is still constrained by the casually-determined context (e.g., being unable to adaptively select the specific context according to the situation of each defect). Therefore, a more effective APR approach is highly needed, which provides more precise and comprehensive context for the given defect to enhance the robustness of LLM-based APRs. In this paper, we propose a context-aware APR approach named CodeCorrector, which designs a Chain-of-Thought (CoT) approach to follow developers’ program repair behaviors. Given a failing test and its buggy file, CodeCorrector first analyzes why the test fails based on the failure message to infer repair direction; then selects the relevant context information to this repair direction; finally builds the context-aware repair prompt to guide LLMs for patch generation. Our motivation is to offer a novel perspective for enhancing LLM-based program repair through context-aware prompting, which adaptively selects specific context for a given defect. The evaluation on the widely-used Defects4J (i.e., v1.2 and v2.0) benchmark shows that overall, by executing a small number of repairs (i.e., as few as ten rounds), CodeCorrector outperforms all the state-of-the-art baselines on the more complex defects in Defects4J v2.0 and the defects without fine-grained defect localization information in Defects4J v1.2. Specifically, a total of 38 defects are fixed by only CodeCorrector. We further analyze the contributions of two core components (i.e., repair directions, global context selection) to the performance of CodeCorrector, especially repair directions, which improve CodeCorrector by 112\% in correct patches and 78\% in plausible patches on Defects4J v1.2. Moreover, CodeCorrector generates more valid and correct patches, achieving a 377\% improvement over the base LLM GPT-3.5 and a 268\% improvement over GPT-4.},
journal = {Automated Software Engg.},
month = apr,
numpages = {34},
keywords = {APR, LLMs, Repair directions, Adaptive context selection}
}

@article{premm,
author = {Xie, Linna and Li, Zhong and Pei, Yu and Wen, Zhongzhen and Liu, Kui and Zhang, Tian and Li, Xuandong},
title = {PReMM: LLM-Based Program Repair for Multi-method Bugs via Divide and Conquer},
year = {2025},
issue_date = {October 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {OOPSLA2},
url = {https://doi.org/10.1145/3763097},
doi = {10.1145/3763097},
abstract = {Large-language models (LLMs) have been leveraged to enhance the capability of automated program repair techniques in recent research.   While existing LLM-based program repair techniques compared favorably to other techniques based on heuristics, constraint-solving, and learning in producing high-quality patches,  they mainly target bugs that can be corrected by changing a single faulty method,  which greatly limits the effectiveness of such techniques in repairing bugs that demand patches spanning across multiple methods.   In this work, we propose the PReMM technique to effectively propose patches changing multiple methods.   PReMM builds on three core component techniques:   the faulty method clustering technique to partition the faulty methods into clusters based on the dependence relationship among them,   enabling a divide-and-conquer strategy for the repairing task;   the fault context extraction technique to gather extra information about the fault context which can be utilized to better guide the diagnosis of the fault and the generation of correct patches;   the dual-agent-based patch generation technique that employs two LLM-based agents with different roles to analyze the fault more precisely and generate patches of higher-quality.  We have implemented the PReMM technique into a tool with the same name and applied the tool to repair real-world bugs from datasets Defects4J V1.2 and V2.0.   PReMM produced correct patches for 307 bugs in total.   Compared with ThinkRepair, the state-of-the-art LLM-based program repair technique,  PReMM correctly repaired 102 more bugs, achieving an improvement of 49.8\%.},
journal = {Proc. ACM Program. Lang.},
month = oct,
articleno = {319},
numpages = {29},
keywords = {Automated Program Repair, Context-Aware Repair, Divide and Conquer, Large Language Models, Multi-method Bugs}
}

@article{spvr,
author = {Wang, Ruoke and Li, Zongjie and Gao, Cuiyun and Wang, Chaozheng and Xiao, Yang and Wang, Xuan},
title = {SPVR: syntax-to-prompt vulnerability repair based on large language models},
year = {2025},
issue_date = {Dec 2025},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {33},
number = {1},
issn = {0928-8910},
url = {https://doi.org/10.1007/s10515-025-00579-5},
doi = {10.1007/s10515-025-00579-5},
abstract = {Purpose: In the field of vulnerability repair, previous research has leveraged pre-trained models and LLM-based prompt engineering, among which LLM-based approaches show better generalizability and achieve the best performance. However, the LLM-based approaches generally regard vulnerability repair as a sequence-to-sequence task, and do not explicitly capture the syntax patterns for different vulnerability types, leading to limited accuracy. We aim to create a method that ensures the specificity of prompts targeting vulnerable code while also leveraging the generative capabilities of Large Language Models. Methods: We propose SPVR (Syntax-to-Prompt Vulnerability Repair), a novel framework that collects information from syntax trees, and generates corresponding prompts. Our method consists of three steps: rule design, prompt generation, and patch generation. In the rule design step, our method parses code patches and designs rules to extract relevant contextual information. These rules aid in identifying vulnerability-related issues. In the prompt generation step, our method extracts information from vulnerable code with pre-defined rules, automatically converting them into prompts. We also incorporate the description of CWE (Common Weakness Enumeration) as known information into the prompts. Finally, in the patch generation step, this prompt will serve as input to any conversational LLM to obtain code patches. Results: Extensive experiments validate that our method achieves excellent results in assisting LLMs to fix vulnerabilities accurately. We utilize multiple Large Language Models to validate the effectiveness of our work, repairing 143 of 547 vulnerable code using ChatGPT-4. We conducted a comparison of our approach against several existing vulnerability repair approaches (including fine-tuning-based and prompt-based), across multiple metrics. Conclusion: Our method is a novel framework that combines the Abstract Syntax Tree structure of code, providing targeted prompts of repair code for vulnerabilities. Our method demonstrates promising potential for real-world code vulnerability repair.},
journal = {Automated Software Engg.},
month = dec,
numpages = {33},
keywords = {Deep learning, Automated program repair, Common weakness enumeration, Generative AI, Automated code analysis}
}

@inproceedings{elmo,
author = {Castle-Green, Teresa and Castle-Green, Simon and Lindley, Joseph and Sailaja, Neelima and Lechelt, Susan},
title = {Elmo: An Embodied Conversational Assistant For Community Repair Caf\'{e}s},
year = {2025},
isbn = {9798400715273},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3719160.3737627},
doi = {10.1145/3719160.3737627},
abstract = {Repair caf\'{e}s provide a community service that supports and empowers item ‘Bringers’ to reinstate traditional values of repair in order to reduce climate impacts of waste. In this paper, we report on a pilot study exploring the use of Elmo (an embodied conversational assistant) by volunteer Repairers within repair caf\'{e} settings. Our findings show the different ways Repairers incorporated this technology probe into their work and the challenges they faced in doing so. Through this, we contribute three areas of consideration for designers looking to implement conversational assistants into community repair settings. These include, contextual awareness of time to undertake suggestions and of the related repair caf\'{e} processes, motivation and customer relations requirements of volunteer Repairers, and enhancing empowerment work through supporting current practices and demonstrations of navigating trouble in repair-resource interactions.},
booktitle = {Proceedings of the 7th ACM Conference on Conversational User Interfaces},
articleno = {7},
numpages = {6},
keywords = {Repair Assistant, Voice Interface, Embodied, Large Language Model (LLM), ChatGPT, Repair, Community, Robot, UBTech Alpha Mini},
location = {
},
series = {CUI '25}
}

@article{vulrepairconcolic,
author = {Shariffdeen, Ridwan and Timperley, Christopher S. and Noller, Yannic and Le Goues, Claire and Roychoudhury, Abhik},
title = {Vulnerability Repair via Concolic Execution and Code Mutations},
year = {2025},
issue_date = {May 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {34},
number = {4},
issn = {1049-331X},
url = {https://doi.org/10.1145/3707454},
doi = {10.1145/3707454},
abstract = {Security vulnerabilities detected via techniques like greybox fuzzing are often fixed with a significant time lag. This increases the exposure of the software to vulnerabilities. Automated fixing of vulnerabilities where a tool can generate fix suggestions is thus of value. In this work, we present such a tool, called CrashRepair, to automatically generate fix suggestions using concolic execution, specification inference, and search techniques. Our approach avoids generating fix suggestions merely at the crash location because such fixes often disable the manifestation of the error instead of fixing the error. Instead, based on sanitizer-guided concolic execution, we infer desired constraints at specific program locations and then opportunistically search for code mutations that help respect those constraints. Our technique only requires a single detected vulnerability or exploit as input; it does not require any user-provided properties. Evaluation results on a wide variety of CVEs in the VulnLoc benchmark, show CrashRepair achieves greater efficacy than state-of-the-art vulnerability repair tools like Senx. The repairs suggested come in the form of a ranked set of patches at different locations, and we show that on most occasions, the desired fix is among the top-3 fixes reported by CrashRepair.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = apr,
articleno = {105},
numpages = {27},
keywords = {Automated Program Repair, Vulnerability Repair, Semantic Program Analysis, Concolic Execution}
}

@inproceedings{humaninsightrepair,
author = {Zhang, Yifan and Leach, Kevin},
title = {Leveraging Human Insights for Enhanced LLM-based Code Repair},
year = {2025},
isbn = {9798400712760},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3696630.3731672},
doi = {10.1145/3696630.3731672},
abstract = {Large Language Models (LLMs) show promise for automating code repair but often lack the nuanced, iterative reasoning and effective use of historical context employed by human developers. We propose a framework to enhance LLM-based repair by incorporating human-inspired mechanisms: mining commit histories for recurring patterns, employing dynamic feedback loops, and facilitating reasoning over historical repair experiences. By extracting abstract fix patterns, iteratively refining patches using automated feedback (from tests and static analysis) alongside optional human guidance, and leveraging a vectorized repository of past experiences for context-aware reasoning, our approach seeks to improve automated patch generation. This framework aims to increase the accuracy and efficiency of repairs by guiding LLMs with more human-like, iterative, and context-grounded problem-solving strategies.},
booktitle = {Proceedings of the 33rd ACM International Conference on the Foundations of Software Engineering},
pages = {1536--1537},
numpages = {2},
keywords = {automated code repair, large language models, software vulnerabilities},
location = {Clarion Hotel Trondheim, Trondheim, Norway},
series = {FSE Companion '25}
}

@inproceedings{vrpilot,
author = {Kulsum, Ummay and Zhu, Haotian and Xu, Bowen and d'Amorim, Marcelo},
title = {A Case Study of LLM for Automated Vulnerability Repair: Assessing Impact of Reasoning and Patch Validation Feedback},
year = {2024},
isbn = {9798400706851},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3664646.3664770},
doi = {10.1145/3664646.3664770},
abstract = {Recent work in automated program repair (APR) proposes the use of reasoning and patch validation feedback to reduce the semantic gap between the LLMs and the code under analysis. The idea has been shown to perform well for general APR, but its effectiveness in other particular contexts remains underexplored.                In this work, we assess the impact of reasoning and patch validation feedback to LLMs in the context of vulnerability repair, an important and challenging task in security. To support the evaluation, we present VRpilot, an LLM-based vulnerability repair technique based on reasoning and patch validation feedback. VRpilot (1) uses a chain-of-thought prompt to reason about a vulnerability prior to generating patch candidates and (2) iteratively refines prompts according to the output of external tools (e.g., compiler, code sanitizers, test suite, etc.) on previously generated patches.                 To evaluate performance, we compare VRpilot against the state-of-the-art vulnerability repair techniques for C and Java using public datasets from the literature. Our results show that VRpilot generates, on average, 14\% and 7.6\% more correct patches than the baseline techniques on C and Java, respectively. We show, through an ablation study, that reasoning and patch validation feedback are critical. We report several lessons from this study and potential directions for advancing LLM-empowered vulnerability repair.},
booktitle = {Proceedings of the 1st ACM International Conference on AI-Powered Software},
pages = {103–111},
numpages = {9},
keywords = {Automated Vulnerability Repair, Large Language Models},
location = {Porto de Galinhas, Brazil},
series = {AIware 2024}
}

@inproceedings{niodebugger,
author = {Ke, Kaiyao},
title = {NIODebugger: A Novel Approach to Repair Non-Idempotent-Outcome Tests with LLM-Based Agent},
year = {2025},
isbn = {9798331505691},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE55347.2025.00226},
doi = {10.1109/ICSE55347.2025.00226},
abstract = {Flaky tests, characterized by inconsistent results across repeated executions, present significant challenges in software testing, especially during regression testing. Recently, there has been emerging research interest in non-idempotent-outcome (NIO) flaky tests—tests that pass on the initial run but fail on subsequent executions within the same environment. Despite progress in utilizing Large Language Models (LLMs) to address flaky tests, existing methods have not tackled NIO flaky tests. The limited context window of LLMs restricts their ability to incorporate relevant source code beyond the test method itself, often overlooking crucial information needed to address state pollution, which is the root cause of NIO flakiness.This paper introduces NIODebugger, the first framework to utilize an LLM-based agent to repair flaky tests. NIODebugger features a three-phase design: detection, exploration, and fixing. In the detection phase, dynamic analysis collects stack traces and custom test execution logs from multiple test runs, which helps in understanding accumulative state pollution. During the exploration phase, the LLM-based agent provides instructions for extracting relevant source code associated with test flakiness. In the fixing phase, NIODebugger repairs the tests using the information gathered from the previous phases. NIODebugger can be integrated with multiple LLMs, achieving patching success rates ranging from 11.63\% to 58.72\%. Its best-performing variant, NIODebugger-GPT-4, successfully generated correct patches for 101 out of 172 previously unknown NIO tests across 20 large-scale open-source projects. We submitted pull requests for all generated patches; 58 have been merged, only 1 was rejected, and the remaining 42 are pending. The Java implementation of NIODebugger is provided as a Maven plugin accessible at https://github.com/kaiyaok2/NIOInspector.},
booktitle = {Proceedings of the IEEE/ACM 47th International Conference on Software Engineering},
pages = {1014–1025},
numpages = {12},
location = {Ottawa, Ontario, Canada},
series = {ICSE '25}
}

@article{fu2024vision,
author = {Fu, Michael and Nguyen, Van and Tantithamthavorn, Chakkrit and Phung, Dinh and Le, Trung},
title = {Vision Transformer Inspired Automated Vulnerability Repair},
year = {2024},
issue_date = {March 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {33},
number = {3},
issn = {1049-331X},
url = {https://doi.org/10.1145/3632746},
doi = {10.1145/3632746},
abstract = {Recently, automated vulnerability repair approaches have been widely adopted to combat increasing software security issues. In particular, transformer-based encoder-decoder models achieve competitive results. Whereas vulnerable programs may only consist of a few vulnerable code areas that need repair, existing AVR approaches lack a mechanism guiding their model to pay more attention to vulnerable code areas during repair generation. In this article, we propose a novel vulnerability repair framework inspired by the Vision Transformer based approaches for object detection in the computer vision domain. Similar to the object queries used to locate objects in object detection in computer vision, we introduce and leverage vulnerability queries (VQs) to locate vulnerable code areas and then suggest their repairs. In particular, we leverage the cross-attention mechanism to achieve the cross-match between VQs and their corresponding vulnerable code areas. To strengthen our cross-match and generate more accurate vulnerability repairs, we propose to learn a novel vulnerability mask (VM) and integrate it into decoders’ cross-attention, which makes our VQs pay more attention to vulnerable code areas during repair generation. In addition, we incorporate our VM into encoders’ self-attention to learn embeddings that emphasize the vulnerable areas of a program. Through an extensive evaluation using the real-world 5,417 vulnerabilities, our approach outperforms all of the automated vulnerability repair baseline methods by 2.68\% to 32.33\%. Additionally, our analysis of the cross-attention map of our approach confirms the design rationale of our VM and its effectiveness. Finally, our survey study with 71 software practitioners highlights the significance and usefulness of AI-generated vulnerability repairs in the realm of software security. The training code and pre-trained models are available at https://github.com/awsm-research/VQM.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = mar,
articleno = {78},
numpages = {29},
keywords = {Software security, automated vulnerability repair}
}

@article{codehealer,
author = {Zhang, Lehuan and Guo, Shikai and Guo, Yi and Li, Hui and Chai, Yu and Chen, Rong and Li, Xiaochen and Jiang, He},
title = {Context-based Transfer Learning for Structuring Fault Localization and Program Repair Automation},
year = {2025},
issue_date = {May 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {34},
number = {4},
issn = {1049-331X},
url = {https://doi.org/10.1145/3705302},
doi = {10.1145/3705302},
abstract = {Automated software debugging plays a crucial role in aiding software developers to swiftly identify and attempt to rectify faults, thereby significantly reducing developers’ workload. Previous researches have predominantly relied on simplistic semantic deep learning or statistical analysis methods to locate faulty statements in diverse projects. However, code repositories often consist of lengthy sequences with long-distance dependencies, posing challenges for accurately modeling fault localization using these methods. In addition, the lack of joint reasoning among various faults prevents existing models from deeply capturing fault information. To address these challenges, we propose a method named CodeHealer to achieve accurate fault localization and program repair. CodeHealer comprises three components: a Deep Semantic Information Extraction Component that effectively extracts deep semantic features from suspicious code statements using classifiers based on Joint-attention mechanisms; a Suspicious Statement Ranking Component that combines various fault localization features and employs multilayer perceptrons to derive multidimensional vectors of suspicion values; and a Fault Repair Component that, based on ranked suspicious statements generated by fault localization, adopts a top-down approach using multiple classifiers based on Co-teaching mechanisms to select repair templates and generate patches. The experimental results indicate that when applied to fault localization, CodeHealer outperforms the best baseline method with improvements of 11.4\%, 2.7\%, and 1.6\% on Top-1/3/5 metrics, respectively. It also reduces the MFR and MAR by 9.8\% and 2.1\%, where lower values denote better fault localization effectiveness. Additionally, in automated software debugging, CodeHealer fixes an additional 6 faults compared to the current best method, totaling 53 faults repaired.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = apr,
articleno = {96},
numpages = {32},
keywords = {Software debugging, Fault Localization, Transfer learning}
}

@inproceedings{zhong2024pepr,
author = {Zhong, Wenkang and Li, Chuanyi and Liu, Kui and Xu, Tongtong and Ge, Jidong and Bissyande, Tegawende F. and Luo, Bin and Ng, Vincent},
title = {Practical Program Repair via Preference-based Ensemble Strategy},
year = {2024},
isbn = {9798400702174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597503.3623310},
doi = {10.1145/3597503.3623310},
abstract = {To date, over 40 Automated Program Repair (APR) tools have been designed with varying bug-fixing strategies, which have been demonstrated to have complementary performance in terms of being effective for different bug classes. Intuitively, it should be feasible to improve the overall bug-fixing performance of APR via assembling existing tools. Unfortunately, simply invoking all available APR tools for a given bug can result in unacceptable costs on APR execution as well as on patch validation (via expensive testing). Therefore, while assembling existing tools is appealing, it requires an efficient strategy to reconcile the need to fix more bugs and the requirements for practicality. In light of this problem, we propose a Preference-based Ensemble Program Repair framework (P-EPR), which seeks to effectively rank APR tools for repairing different bugs. P-EPR is the first non-learning-based APR ensemble method that is novel in its exploitation of repair patterns as a major source of knowledge for ranking APR tools and its reliance on a dynamic update strategy that enables it to immediately exploit and benefit from newly derived repair results. Experimental results show that P-EPR outperforms existing strategies significantly both in flexibility and effectiveness.},
booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
articleno = {5},
numpages = {13},
keywords = {program repair, ensemble strategy},
location = {Lisbon, Portugal},
series = {ICSE '24}
}

@inproceedings{arjaclm,
author = {Lijzenga, Oebele and Hemati Moghadam, Iman and Zaytsev, Vadim},
title = {Leveraging Search-Based and Pre-Trained Code Language Models for Automated Program Repair},
year = {2025},
isbn = {9798400706295},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3672608.3707774},
doi = {10.1145/3672608.3707774},
abstract = {Background. Automated Program Repair (APR) techniques often face challenges in navigating vast search space of possible patches and often rely on redundancy-based assumptions, which can restrict the diversity of generated patches. Recently, Code Language Models (CLMs) have emerged as a method for dynamically generating patch ingredients, potentially enhancing patch quality.Aim. This study aims to enhance APR by integrating search-based methods with CLMs to improve both the quality of generated patch ingredients and the efficiency of the search process.Method. We propose ARJACLM, a novel APR technique that uses a genetic algorithm for search space navigation and dynamically generates patch ingredients with the CodeLLaMA-13B model, combining redundancy-based and CLM-derived patch ingredients.Results. Testing on 176 bugs across 9 Java projects from Defect4J shows that CLM-generated patch ingredients significantly boost ARJACLM's performance, though at the cost of increased computation time. ARJACLM outperforms ARJA and GenProg, and CLM-generated patch ingredients are of higher quality than their redundancy-based counterparts. Additionally, ARJACLM performs best when redundancy-based patch ingredients are ignored.},
booktitle = {Proceedings of the 40th ACM/SIGAPP Symposium on Applied Computing},
pages = {1627--1636},
numpages = {10},
keywords = {program repair, search-based algorithm, code language model},
location = {Catania International Airport, Catania, Italy},
series = {SAC '25}
}
@inproceedings{rete,
author = {Parasaram, Nikhil and Barr, Earl T. and Mechtaev, Sergey},
title = {Rete: Learning Namespace Representation for Program Repair},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00112},
doi = {10.1109/ICSE48619.2023.00112},
abstract = {A key challenge of automated program repair is finding correct patches in the vast search space of candidate patches. Real-world programs define large namespaces of variables that considerably contributes to the search space explosion. Existing program repair approaches neglect information about the program namespace, which makes them inefficient and increases the chance of test-overfitting. We propose Rete, a new program repair technique, that learns project-independent information about program namespace and uses it to navigate the search space of patches. Rete uses a neural network to extract project-independent information about variable CDU chains, defuse chains augmented with control flow. Then, it ranks patches by jointly ranking variables and the patch templates into which the variables are inserted. We evaluated Rete on 142 bugs extracted from two datasets, ManyBugs and BugsInPy. Our experiments demonstrate that Rete generates six new correct patches that fix bugs that previous tools did not repair, an improvement of 31\% and 59\% over the existing state of the art.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {1264–1276},
numpages = {13},
keywords = {variable representation, patch prioritisation, deep learning, program repair},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{yang2025revisiting,
author = {Yang, Aidan Z. H. and Kolak, Sophia and Hellendoorn, Vincent and Martins, Ruben and Goues, Claire Le},
title = {Revisiting Unnaturalness for Automated Program Repair in the Era of Large Language Models},
year = {2025},
isbn = {9798331505691},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE55347.2025.00089},
doi = {10.1109/ICSE55347.2025.00089},
abstract = {The problem of software quality has motivated the development of a variety of techniques for Automatic Program Repair (APR). Meanwhile, recent advances in AI and Large Language Models (LLMs) have produced orders of magnitude performance improvements over previous code generation techniques, affording promising opportunities for program repair and its constituent subproblems (e.g., fault localization, patch generation). Because models are trained on large volumes of code in which defects are relatively rare, they tend to both simultaneously perceive faulty code as unlikely (or "unnatural") and to produce generally correct code (which is more "natural"). This paper comprehensively revisits the idea of (un)naturalness for program repair. We argue that, fundamentally, LLMs can only go so far on their own in reasoning about and fixing buggy code. This motivates the incorporation of traditional tools, which compress useful contextual and analysis information, as a complement to LLMs for repair. We interrogate the role of entropy at every stage of traditional repair, and show that it is indeed usefully complementary to classic techniques. We show that combining measures of naturalness with class Spectrum-Based Fault Localization (SBFL) approaches improves Top-5 scoring by 50\% over SBFL alone. We show that entropy delta, or change in entropy induced by a candidate patch, can improve patch generation efficiency by 24 test suite executions per repair, on average, on our dataset. Finally, we show compelling results that entropy delta for patch classification is highly effective at distinguishing correct from overfitting patches. Overall, our results suggest that LLMs can effectively complement classic techniques for analysis and transformation, producing more efficient and effective automated repair techniques overall.},
booktitle = {Proceedings of the IEEE/ACM 47th International Conference on Software Engineering},
pages = {2561–2573},
numpages = {13},
keywords = {program repair, deep learning, large language models},
location = {Ottawa, Ontario, Canada},
series = {ICSE '25}
}

@inproceedings{orvalho2025counterexample,
author = {Orvalho, Pedro and Janota, Mikol\'{a}\v{s} and Manquinho, Vasco M.},
title = {Counterexample guided program repair using zero-shot learning and MaxSAT-based fault localization},
year = {2025},
isbn = {978-1-57735-897-8},
publisher = {AAAI Press},
url = {https://doi.org/10.1609/aaai.v39i1.32046},
doi = {10.1609/aaai.v39i1.32046},
abstract = {Automated Program Repair (APR) for introductory programming assignments (IPAS) is motivated by the large number of student enrollments in programming courses each year. Since providing feedback on programming assignments requires substantial time and effort from faculty, personalized automated feedback often involves suggesting repairs to students' programs. Symbolic semantic repair approaches, which rely on Formal Methods (FM), check a program's execution against a test suite or reference solution, are effective but limited. These tools excel at identifying buggy parts but can only fix programs if the correct implementation and the faulty one share the same control flow graph. Conversely, Large Language Models (LLMS) are used for program repair but often make extensive rewrites instead of minimal adjustments. This tends to lead to more invasive fixes, making it harder for students to learn from their mistakes. In summary, LLMS excel at completing strings, while FM-based fault localization excel at identifying buggy parts of a program.In this paper, we propose a novel approach that combines the strengths of both FM-based fault localization and LLMS, via zero-shot learning, to enhance APR for IPAS. Our method uses MaxSAT-based fault localization to identify buggy parts of a program, then presents the LLM with a program sketch devoid of these buggy statements. This hybrid approach follows a Counterexample Guided Inductive Synthesis (CEGIS) loop to iteratively refine the program. We ask the LLM to synthesize the missing parts, which are then checked against a test suite. If the suggested program is incorrect, a counterexample from the test suite is fed back to the LLM for revised synthesis. Our experiments on 1,431 incorrect student programs show that our counterexample guided approach, using MaxSAT-based bug-free program sketches, significantly improves the repair capabilities of all six evaluated LLMS. This method allows LLMS to repair more programs and produce smaller fixes, outperforming other configurations and state-of-the-art symbolic program repair tools. Code — https://doi.org/10.5281/zenodo.14517771},
booktitle = {Proceedings of the Thirty-Ninth AAAI Conference on Artificial Intelligence and Thirty-Seventh Conference on Innovative Applications of Artificial Intelligence and Fifteenth Symposium on Educational Advances in Artificial Intelligence},
articleno = {73},
numpages = {9},
series = {AAAI'25/IAAI'25/EAAI'25}
}

@inproceedings{gamma,
author = {Zhang, Quanjun and Fang, Chunrong and Zhang, Tongke and Yu, Bowen and Sun, Weisong and Chen, Zhenyu},
title = {Gamma: Revisiting Template-based Automated Program Repair via Mask Prediction},
year = {2024},
isbn = {9798350329964},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASE56229.2023.00063},
doi = {10.1109/ASE56229.2023.00063},
abstract = {Automated program repair (APR) aims to fix software bugs without manual debugging efforts and plays a crucial role in software development and maintenance. Template-based APR has been widely investigated and shown promising results. However, it is challenging for template-based APR to select the appropriate donor code, which is an important repair ingredient for generating candidate patches. Inappropriate donor code may cause plausible but incorrect patch generation even with correct fix patterns, limiting the repair performance.In this paper, we aim to revisit template-based APR, and propose Gamma, to directly leverage large pre-trained language models for donor code generation. Our main insight is that instead of retrieving donor code in the local buggy file, we can directly predict the correct code tokens based on the context code snippets and repair patterns by a cloze task. Specifically, (1) Gamma revises a variety of fix templates from state-of-the-art template-based APR techniques (i.e., TBar) and transforms them into mask patterns. (2) Gamma adopts a pre-trained language model to predict the correct code for masked code as a fill-in-the-blank task. Although our idea is general and can be built on various existing pre-trained language models, we have implemented Gamma as a practical APR tool based on the recent UniXcoder model. The experimental results demonstrate that Gamma correctly repairs 82 bugs on Defects4J-v1.2, which achieves 20.59\% (14 bugs) and 26.15\% (17 bugs) improvement over the previous state-of-the-art template-based approach TBar and learning-based one Recoder. Furthermore, Gamma repairs 45 bugs and 22 bugs from the additional Defects4J-v2.0 and QuixBugs, indicating the generalizability of Gamma in addressing the dataset overfitting issue. We also prove that adopting other pre-trained language models can provide substantial advancement, e.g., CodeBERT-based and ChatGPT-based Gamma is able to fix 80 and 67 bugs on Defects4J-v1.2, indicating the scalability of Gamma. Overall, our study highlights the promising future of adopting pre-trained models to generate correct patches on top of fix patterns in practice.},
booktitle = {Proceedings of the 38th IEEE/ACM International Conference on Automated Software Engineering},
pages = {535–547},
numpages = {13},
keywords = {automated program repair, fix pattern, pre-trained model, LLM4SE},
location = {Echternach, Luxembourg},
series = {ASE '23}
}

@inproceedings{transrepair,
author = {Li, Xueyang and Liu, Shangqing and Feng, Ruitao and Meng, Guozhu and Xie, Xiaofei and Chen, Kai and Liu, Yang},
title = {TransRepair: Context-aware Program Repair for Compilation Errors},
year = {2023},
isbn = {9781450394758},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3551349.3560422},
doi = {10.1145/3551349.3560422},
abstract = {Automatically fixing compilation errors can greatly raise the productivity of software development, by guiding the novice or AI programmers to write and debug code. Recently, learning-based program repair has gained extensive attention and became the state-of-the-art in practice. But it still leaves plenty of space for improvement. In this paper, we propose an end-to-end solution&nbsp;TransRepair to locate the error lines and create the correct substitute for a C program simultaneously. Superior to the counterpart, our approach takes into account the context of erroneous code and diagnostic compilation feedback. Then we devise a Transformer-based neural network to learn the ways of repair from the erroneous code as well as its context and the diagnostic feedback. To increase the effectiveness of TransRepair, we summarize 5 types and 74 fine-grained sub-types of compilations errors from two real-world program datasets and the Internet. Then a program corruption technique is developed to synthesize a large dataset with 1,821,275 erroneous C programs. Through the extensive experiments, we demonstrate that TransRepair outperforms the state-of-the-art in both single repair accuracy and full repair accuracy. Further analysis sheds light on the strengths and weaknesses in the contemporary solutions for future improvement.},
booktitle = {Proceedings of the 37th IEEE/ACM International Conference on Automated Software Engineering},
articleno = {108},
numpages = {13},
keywords = {Program repair, compilation error, context-aware, deep learning},
location = {Rochester, MI, USA},
series = {ASE '22}
}

@inproceedings{knod,
author = {Jiang, Nan and Lutellier, Thibaud and Lou, Yiling and Tan, Lin and Goldwasser, Dan and Zhang, Xiangyu},
title = {KNOD: Domain Knowledge Distilled Tree Decoder for Automated Program Repair},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00111},
doi = {10.1109/ICSE48619.2023.00111},
abstract = {Automated Program Repair (APR) improves software reliability by generating patches for a buggy program automatically. Recent APR techniques leverage deep learning (DL) to build models to learn to generate patches from existing patches and code corpora. While promising, DL-based APR techniques suffer from the abundant syntactically or semantically incorrect patches in the patch space. These patches often disobey the syntactic and semantic domain knowledge of source code and thus cannot be the correct patches to fix a bug.We propose a DL-based APR approach KNOD, which incorporates domain knowledge to guide patch generation in a direct and comprehensive way. KNOD has two major novelties, including (1) a novel three-stage tree decoder, which directly generates Abstract Syntax Trees of patched code according to the inherent tree structure, and (2) a novel domain-rule distillation, which leverages syntactic and semantic rules and teacher-student distributions to explicitly inject the domain knowledge into the decoding procedure during both the training and inference phases.We evaluate KNOD on three widely-used benchmarks. KNOD fixes 72 bugs on the Defects4J v1.2, 25 bugs on the QuixBugs, and 50 bugs on the additional Defects4J v2.0 benchmarks, outperforming all existing APR tools.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {1251–1263},
numpages = {13},
keywords = {deep learning, abstract syntax tree, automated program repair},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{shibboleth,
author = {Ghanbari, Ali and Marcus, Andrian (Andi)},
title = {Shibboleth: Hybrid Patch Correctness Assessment in Automated Program Repair},
year = {2023},
isbn = {9781450394758},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3551349.3559519},
doi = {10.1145/3551349.3559519},
abstract = {Test-based generate-and-validate automated program repair (APR) systems generate many patches that pass the test suite without fixing the bug. The generated patches must be manually inspected by the developers, a task that tends to be time-consuming, thereby diminishing the role of APR in reducing debugging costs. We present the design and implementation of a novel tool, named Shibboleth, for automatic assessment of the patches generated by test-based generate-and-validate APR systems. Shibboleth leverages lightweight static and dynamic heuristics from both test and production code to rank and classify the patches. Shibboleth is based on the idea that the buggy program is almost correct and the bugs are small mistakes that require small changes to fix and specifically the fix does not remove the code implementing correct functionality of the program. Thus, the tool measures the impact of patches on both production code (via syntactic and semantic similarity) and test code (via code coverage) to separate the patches that result in similar programs and that do not remove desired program elements. We have evaluated Shibboleth on 1,871 patches, generated by 29 Java-based APR systems for Defects4J programs. The technique outperforms state-of-the-art raking and classification techniques. Specifically, in our ranking data set, in 66\% of the cases, Shibboleth ranks the correct patch in top-1 or top-2 positions and, in our classification data set, it achieves an accuracy and F1-score of 0.887 and 0.852, respectively, in classification mode. A demo video of the tool is available at https://bit.ly/3NvYJN8.},
booktitle = {Proceedings of the 37th IEEE/ACM International Conference on Automated Software Engineering},
articleno = {166},
numpages = {4},
keywords = {Automated Program Repair, Branch Coverage, Patch Correctness Assessment, Similarity},
location = {Rochester, MI, USA},
series = {ASE '22}
}

@article{ifix,
author = {Wang, Ruixin and Zhao, Zhongkai and Fang, Le and Jiang, Nan and Lou, Yiling and Tan, Lin and Zhang, Tianyi},
title = {Show Me Why It’s Correct: Saving 1/3 of Debugging Time in Program Repair with Interactive Runtime Comparison},
year = {2025},
issue_date = {April 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {OOPSLA1},
url = {https://doi.org/10.1145/3720510},
doi = {10.1145/3720510},
abstract = {Automated Program Repair (APR) holds the promise of alleviating the burden of debugging and fixing software bugs. Despite this, developers still need to manually inspect each patch to confirm its correctness, which is tedious and time-consuming. This challenge is exacerbated in the presence of plausible patches, which accidentally pass test cases but may not correctly fix the bug. To address this challenge, we propose an interactive approach called iFix to facilitate patch understanding and comparison based on their runtime difference. iFix performs static analysis to identify runtime variables related to the buggy statement and captures their runtime values during execution for each patch. These values are then aligned across different patch candidates, allowing users to compare and contrast their runtime behavior. To evaluate iFix, we conducted a within-subjects user study with 28 participants. Compared with manual inspection and a state-of-the-art interactive patch filtering technique, iFix reduced participants’ task completion time by 36\% and 33\% while also improving their confidence by 50\% and 20\%, respectively. Besides, quantitative experiments demonstrate that iFix improves the ranking of correct patches by at least 39\% compared with other patch ranking methods and is generalizable to different APR tools.},
journal = {Proc. ACM Program. Lang.},
month = apr,
articleno = {145},
numpages = {27},
keywords = {Automatic Program Repair, Interaction Support, User Trust}
}

@inbook{patchagent,
author = {Yu, Zheng and Guo, Ziyi and Wu, Yuhang and Yu, Jiahao and Xu, Meng and Mu, Dongliang and Chen, Yan and Xing, Xinyu},
title = {PATCHAGENT: a practical program repair agent mimicking human expertise},
year = {2025},
isbn = {978-1-939133-52-6},
publisher = {USENIX Association},
address = {USA},
abstract = {Automated program repair (APR) techniques, which aim to triage and fix software bugs autonomously, have emerged as powerful tools against vulnerable code. Recent advancements in large language models (LLMs) have further shown promising results when applied to APR, especially on patch generation. However, without effective fault localization and patch validation, APR tools specialized in patching alone cannot handle a more practical and end-to-end setting—given a concrete input that triggers a vulnerability, how to patch the program without breaking existing tests?In this paper, we introduce PATCHAGENT, a novel LLM-based APR tool that seamlessly integrates fault localization, patch generation, and validation within a single autonomous agent. PATCHAGENT employs a language server, a patch verifier, and interaction optimization techniques to mimic human-like reasoning during vulnerability repair. Evaluated on a dataset of 178 real-world vulnerabilities, PATCHAGENT successfully repairs over 90\% of the cases, outperforming state-of-the-art APR tools where applicable. Our ablation study further offer insights into the how various interaction optimizations contribute to PATCHAGENT'S effectiveness.},
booktitle = {Proceedings of the 34th USENIX Conference on Security Symposium},
articleno = {226},
numpages = {20}
}

@inproceedings{adapr,
author = {Dai, Zhenlong and Chen, Bingrui and Zhao, Zhuoluo and Tang, Xiu and Wu, Sai and Yao, Chang and Gao, Zhipeng and Chen, Jingyuan},
title = {Less is more: adaptive program repair with bug localization and preference learning},
year = {2025},
isbn = {978-1-57735-897-8},
publisher = {AAAI Press},
url = {https://doi.org/10.1609/aaai.v39i1.31988},
doi = {10.1609/aaai.v39i1.31988},
abstract = {Automated Program Repair (APR) is a task to automatically generate patches for the buggy code. However, most research focuses on generating correct patches while ignoring the consistency between the fixed code and the original buggy code. How to conduct adaptive bug fixing and generate patches with minimal modifications have seldom been investigated. To bridge this gap, we first introduce a novel task, namely AdaPR (Adaptive Program Repair). We then propose a two-stage approach AdaPatcher (Adaptive Patch Generator) to enhance program repair while maintaining the consistency. In the first stage, we utilize a Bug Locator with self-debug learning to accurately pinpoint bug locations. In the second stage, we train a Program Modifier to ensure consistency between the post-modified fixed code and the pre-modified buggy code. The Program Modifier is enhanced with a location-aware repair learning strategy to generate patches based on identified buggy lines, a hybrid training strategy for selective reference and an adaptive preference learning to prioritize fewer changes. The experimental results show that our approach outperforms a set of baselines by a large margin, validating the effectiveness of our two-stage framework for the newly proposed AdaPR task. Code — https://github.com/zhenlongDai/AdaPatcher},
booktitle = {Proceedings of the Thirty-Ninth AAAI Conference on Artificial Intelligence and Thirty-Seventh Conference on Innovative Applications of Artificial Intelligence and Fifteenth Symposium on Educational Advances in Artificial Intelligence},
articleno = {15},
numpages = {9},
series = {AAAI'25/IAAI'25/EAAI'25}
}

@inproceedings{cure,
author = {Jiang, Nan and Lutellier, Thibaud and Tan, Lin},
title = {CURE: Code-Aware Neural Machine Translation for Automatic Program Repair},
year = {2021},
isbn = {9781450390859},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE43902.2021.00107},
doi = {10.1109/ICSE43902.2021.00107},
abstract = {Automatic program repair (APR) is crucial to improve software reliability. Recently, neural machine translation (NMT) techniques have been used to fix software bugs automatically. While promising, these approaches have two major limitations. Their search space often does not contain the correct fix, and their search strategy ignores software knowledge such as strict code syntax. Due to these limitations, existing NMT-based techniques underperform the best template-based approaches.We propose CURE, a new NMT-based APR technique with three major novelties. First, CURE pre-trains a programming language (PL) model on a large software codebase to learn developer-like source code before the APR task. Second, CURE designs a new code-aware search strategy that finds more correct fixes by focusing on compilable patches and patches that are close in length to the buggy code. Finally, CURE uses a subword tokenization technique to generate a smaller search space that contains more correct fixes.Our evaluation on two widely-used benchmarks shows that CURE correctly fixes 57 Defects4J bugs and 26 QuixBugs bugs, outperforming all existing APR techniques on both benchmarks.},
booktitle = {Proceedings of the 43rd International Conference on Software Engineering},
pages = {1161–1173},
numpages = {13},
keywords = {software reliability, automatic program repair},
location = {Madrid, Spain},
series = {ICSE '21}
}

@inproceedings{ledex,
author = {Jiang, Nan and Li, Xiaopeng and Wang, Shiqi and Zhou, Qiang and Hossain, Soneya Binta and Ray, Baishakhi and Kumar, Varun and Ma, Xiaofei and Deoras, Anoop},
title = {LEDEX: training LLMs to better self-debug and explain code},
year = {2024},
isbn = {9798331314385},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {In the domain of code generation, self-debugging is crucial. It allows LLMs to refine their generated code based on execution feedback. This is particularly important because generating correct solutions in one attempt proves challenging for complex tasks. Prior works on self-debugging mostly focus on prompting methods by providing LLMs with few-shot examples, which work poorly on small open-sourced LLMs. In this work, we propose LEDEX, a training framework that significantly improves the self-debugging capability of LLMs. Intuitively, we observe that a chain of explanations on the wrong code followed by code refinement helps LLMs better analyze the wrong code and do refinement. We thus propose an automated pipeline to collect a high-quality dataset for code explanation and refinement by generating a number of explanations and refinement trajectories from the LLM itself or a larger teacher model and filtering via execution verification. We perform supervised fine-tuning (SFT) and further reinforcement learning (RL) on both success and failure trajectories with a novel reward design considering code explanation and refinement quality. SFT improves the pass@1 by up to 15.92\% and pass@10 by 9.30\% over four benchmarks. RL training brings additional up to 3.54\% improvement on pass@1 and 2.55\% improvement on pass@10. The trained LLMs show iterative refinement ability and can keep refining code continuously. Lastly, our human evaluation shows that the LLMs trained with our framework generate more useful code explanations and help developers better understand bugs in source code.},
booktitle = {Proceedings of the 38th International Conference on Neural Information Processing Systems},
articleno = {1120},
numpages = {27},
location = {Vancouver, BC, Canada},
series = {NIPS '24}
}

@article{hierarchydebug,
author = {Parsa, Saeed and Vahidi-Asl, Mojtaba and Asadi-Aghbolaghi, Maryam},
title = {Hierarchy-Debug: a scalable statistical technique for fault localization},
year = {2014},
issue_date = {September 2014},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {22},
number = {3},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-013-9199-x},
doi = {10.1007/s11219-013-9199-x},
abstract = {Considering the fact that faults may be revealed as undesired mutual effect of program predicates on each other, a new approach for localizing latent bugs, namely Hierarchy-Debug, is presented in this paper. To analyze the vertical effect of predicates on each other and on program termination status, the predicates are fitted into a logistic lasso model. To support scalability, a hierarchical clustering algorithm is applied to cluster the predicates according to their presence in different executions. Considering each cluster as a pseudo-predicate, a distinct lasso model is built for intermediate levels of the hierarchy. Then, we apply a majority voting technique to score the predicates according to their lasso coefficients at different levels of the hierarchy. The predicates with relatively higher scores are ranked as fault relevant predicates. To provide the context of failure, faulty sub-paths are identified as sequences of fault relevant predicates. The grouping effect of Hierarchy-Debug helps programmers to detect multiple bugs. Four case studies have been designed to evaluate the proposed approach on three well-known test suites, Space, Siemens, and Bash. The evaluations show that Hierarchy-Debug produces more precise results compared with prior fault localization techniques on the subject programs.},
journal = {Software Quality Journal},
month = sep,
pages = {427–466},
numpages = {40},
keywords = {Statistical bug localization, Multiple bugs, Majority voting, Lasso method, Hierarchical clustering, Faulty sub-paths, Fault relevant predicates}
}

@article{lmfuzz,
author = {Lin, Renze and Wang, Ran and Hu, Guanghuan and Xu, Xianghua},
title = {LMFuzz: Program repair fuzzing based on large language models},
year = {2025},
issue_date = {Dec 2025},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {33},
number = {1},
issn = {0928-8910},
url = {https://doi.org/10.1007/s10515-025-00568-8},
doi = {10.1007/s10515-025-00568-8},
abstract = {Generating programs using large language models (LLMs) for fuzz testing has emerged as a significant testing methodology. While traditional fuzzers can produce correct programs, their effectiveness is limited by excessive constraints and restricted API combinations, resulting in insufficient coverage of the target system’s code and impacting testing efficiency. Unlike traditional methods, large language model based fuzzers can generate more diverse code, effectively addressing key issues of conventional fuzzers. However, the lack of constraints on API combinations during the generation process often leads to reduced program validity. Therefore, a crucial challenge is to enhance the validity of generated code while maintaining its diversity. To address this issue, we propose a novel and universal fuzzer, LMFuzz. To ensure the fuzzer’s generation capability, we utilize a large language model as the primary generator and model the operator selection problem within the fuzzing loop as a multi-armed bandit problem. We introduce the Thompson Sampling algorithm to enhance both the diversity and validity of program generation. To improve the validity of the generated code, we incorporate a program repair loop that iteratively corrects the generated programs, thereby reducing errors caused by the lack of API combination constraints. Experimental results demonstrate that LMFuzz significantly surpasses existing state-of-the-art large language model based fuzzers in terms of coverage and validity, and also exhibits notable advantages in generating diverse programs. Furthermore, LMFuzz has identified 24 bugs across five popular programming languages and their corresponding systems.},
journal = {Automated Software Engg.},
month = oct,
numpages = {27},
keywords = {Fuzzing, Large language model, Code generation, Thompson sampling algorithm, Program Repair, Software security}
}

@inproceedings{xu2025aligning,
author = {Xu, Junjielong and Fu, Ying and Tan, Shin Hwei and He, Pinjia},
title = {Aligning the Objective of LLM-Based Program Repair},
year = {2025},
isbn = {9798331505691},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE55347.2025.00169},
doi = {10.1109/ICSE55347.2025.00169},
abstract = {Large language models (LLMs) have achieved decent results on automated program repair (APR). However, the next token prediction training objective of decoder-only LLMs (e.g., GPT-4) is misaligned with the masked span prediction objective of current infilling-style methods, which impedes LLMs from fully leveraging pre-trained knowledge for program repair. In addition, while some LLMs can locate and repair bugs in certain functions using the related artifacts (e.g., test cases), existing methods still depend on statement-level fault localization methods to provide a list of buggy hunks for repair. This restriction hinders LLMs from exploring potential patches beyond the given locations.In this paper, we investigate a new approach to adapt LLMs to program repair. Our core insight is that LLM's APR capability can be greatly improved by simply aligning the output to their training objective and allowing them to refine the whole program without first identifying faulty statements. Based on this insight, we designed D4C, a straightforward prompting framework for APR. D4C can repair 180 bugs correctly in Defects4J, with each patch being sampled only 10 times. This surpasses the SOTA APR methods with perfect fault localization by 10\% and reduces the patch sampling number by 90\%. Our findings reveal that (1) objective alignment is crucial for fully exploiting LLM's pre-trained capability, and (2) replacing the traditional localize-buggy-hunks-then-repair workflow with direct debugging is more effective for LLM-based APR methods. Thus, we believe this paper introduces a new mindset for harnessing LLMs in APR.},
booktitle = {Proceedings of the IEEE/ACM 47th International Conference on Software Engineering},
pages = {2548–2560},
numpages = {13},
keywords = {automated program repair, large language model, objective alignment},
location = {Ottawa, Ontario, Canada},
series = {ICSE '25}
}

@inproceedings{thinkrepair,
author = {Yin, Xin and Ni, Chao and Wang, Shaohua and Li, Zhenhao and Zeng, Limin and Yang, Xiaohu},
title = {ThinkRepair: Self-Directed Automated Program Repair},
year = {2024},
isbn = {9798400706127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3650212.3680359},
doi = {10.1145/3650212.3680359},
abstract = {Though many approaches have been proposed for Automated Program Repair (APR) and indeed achieved remarkable performance, they still have limitations in fixing bugs that require analyzing and reasoning about the logic of the buggy program. Recently, large language models (LLMs) instructed by prompt engineering have attracted much attention for their powerful ability to address many kinds of tasks including bug-fixing. However, the quality of the prompt will highly affect the ability of LLMs and manually constructing high-quality prompts is a costly endeavor.    To address this limitation, we propose a self-directed LLM-based automated program repair, ThinkRepair, with two main phases: collection phase and fixing phase. The former phase automatically collects various chains of thoughts that constitute pre-fixed knowledge by instructing LLMs with the Chain-of-Thought (CoT) prompt. The latter phase targets fixing a bug by first selecting examples for few-shot learning and second automatically interacting with LLMs, optionally appending with feedback of testing information.    Evaluations on two widely studied datasets (Defects4J and QuixBugs) by comparing ThinkRepair with 12 SOTA APRs indicate the priority of ThinkRepair in fixing bugs. Notably, ThinkRepair fixes 98 bugs and improves baselines by 27\%∼344.4\% on Defects4J V1.2. On Defects4J V2.0, ThinkRepair fixes 12∼65 more bugs than the SOTA APRs. Additionally, ThinkRepair also makes a considerable improvement on QuixBugs (31 for Java and 21 for Python at most).},
booktitle = {Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {1274–1286},
numpages = {13},
keywords = {Automated Program Repair, Large Language Model, Prompt Engineering},
location = {Vienna, Austria},
series = {ISSTA 2024}
}

@article{feng2025integrating,
author = {Feng, Qiong and Ma, Xiaotian and Sheng, Jiayi and Feng, Ziyuan and Song, Wei and Liang, Peng},
title = {Integrating Various Software Artifacts for Better LLM-based Bug Localization and Program Repair},
year = {2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1049-331X},
url = {https://doi.org/10.1145/3770581},
doi = {10.1145/3770581},
abstract = {LLMs have garnered considerable attention for their potential to streamline Automated Program Repair (APR). LLM-based approaches can either insert the correct code using an infilling-style technique or directly generate patches when provided with buggy methods, aiming for plausible patches to pass all tests. However, most of LLM-based APR methods rely on a single type of software information, such as issue descriptions or error stack traces, without fully leveraging a combination of diverse software artifacts. Human developers, in contrast, often use a range of information — such as debugging data, issue discussions, and error stack traces — to diagnose and fix bugs. Despite this, many LLM-based approaches do not explore which specific types of software information best assist in localizing and repairing software bugs. Addressing this gap is crucial for advancing LLM-based APR techniques.To investigate this and mimic the way human developers fix bugs, we propose DEVLoRe (short for DEVeloper Localization and Repair). In this framework, LLMs first use issue content (description and discussion) and stack error traces to localize buggy methods, then rely on debug information in buggy methods and issue content and stack error to localize buggy lines and generate valid patches. We evaluated the effectiveness of issue content, error stack traces, and debugging information in bug localization and automatic program repair. Our results show that while issue content and error stack is particularly effective in assisting LLMs with fault localization and program repair respectively, different types of software artifacts complement each other in addressing various bugs. By incorporating these three types of artifacts and using the Defects4J v2.0 dataset for evaluation, DEVLoRe successfully localizes 49.3\% of single-method bugs and generates 56.0\% plausible patches. Additionally, DEVLoRe can localize 47.6\% of non-single-method bugs and generates 14.5\% plausible patches. Moreover, our framework streamlines the end-to-end process from buggy source code to a complete repair, and achieves a 39.7\% and 17.1\% of single-method and non-single-method bug repair rate, outperforming current state-of-the-art APR methods. Furthermore, we re-implemented and evaluated our framework, demonstrating its effectiveness in resolving 9 unique issues compared to other state-of-the-art frameworks using the same or more advanced models on SWE-bench Lite. We also discussed whether a leading framework for Python code can be directly applied to Java code, or vice versa. The source code and experimental results of this work for replication are available at .},
note = {Just Accepted},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = oct,
keywords = {Large Language Model, Automatic Program Repair, Fault Localization}
}

@article{li2025hybrid,
author = {Li, Fengjie and Jiang, Jiajun and Sun, Jiajun and Zhang, Hongyu},
title = {Hybrid Automated Program Repair by Combining Large Language Models and Program Analysis},
year = {2025},
issue_date = {September 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {34},
number = {7},
issn = {1049-331X},
url = {https://doi.org/10.1145/3715004},
doi = {10.1145/3715004},
abstract = {Automated Program Repair (APR) has garnered significant attention due to its potential to streamline the bug repair process for human developers. Recently, LLM-based APR methods have shown promise in repairing real-world bugs. However, existing APR methods often utilize patches generated by LLMs without further optimization, resulting in reduced effectiveness due to the lack of program-specific knowledge. Furthermore, the evaluations of these APR methods have typically been conducted under the assumption of perfect fault localization, which may not accurately reflect their real-world effectiveness. To address these limitations, this article introduces an innovative APR approach called GiantRepair. Our approach leverages the insight that LLM-generated patches, although not necessarily correct, offer valuable guidance for the patch generation process. Based on this insight, GiantRepair first constructs patch skeletons from LLM-generated patches to confine the patch space, and then generates high-quality patches tailored to specific programs through context-aware patch generation by instantiating the skeletons. To evaluate the performance of our approach, we conduct two large-scale experiments. The results demonstrate that GiantRepair not only effectively repairs more bugs (an average of 27.78\% on Defects4J v1.2 and 23.40\% on Defects4J v2.0) than using LLM-generated patches directly, but also outperforms state-of-the-art APR methods by repairing at least 42 and 7 more bugs under perfect and automated fault localization scenarios, respectively.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = aug,
articleno = {202},
numpages = {28},
keywords = {Program Repair, Large Language Model, Program Synthesis}
}

@inproceedings{repaircat,
author = {Jiang, Nan and Wu, Yi},
title = {RepairCAT: Applying Large Language Model to Fix Bugs in AI-Generated Programs},
year = {2024},
isbn = {9798400705779},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643788.3648020},
doi = {10.1145/3643788.3648020},
abstract = {Automated program repair has been a crucial and popular domain for years, and with the development of large language models (LLMs) and the trend of using LLMs for code generation, there comes the new challenge of fixing bugs in LLM-generated (AI-generated) programs. In this work, we introduce RepairCAT, a simple and neat framework for fine-tuning large language models for automated repairing Python programs. Our experiments built on StarCoder-1B successfully generated patches fixing the failed test cases for 14 out of 100 bugs in the Python programs, 2 of which passed all the public test cases and were considered plausible.},
booktitle = {Proceedings of the 5th ACM/IEEE International Workshop on Automated Program Repair},
pages = {58–60},
numpages = {3},
keywords = {automated program repair, large language model},
location = {Lisbon, Portugal},
series = {APR '24}
}

@inproceedings{aprsolutiondesign,
author = {Zhao, Jiuang and Yang, Donghao and Zhang, Li and Lian, Xiaoli and Yang, Zitian and Liu, Fang},
title = {Enhancing Automated Program Repair with Solution Design},
year = {2024},
isbn = {9798400712487},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3691620.3695537},
doi = {10.1145/3691620.3695537},
abstract = {Automatic Program Repair (APR) endeavors to autonomously rectify issues within specific projects, which generally encompasses three categories of tasks: bug resolution, new feature development, and feature enhancement. Despite extensive research proposing various methodologies, their efficacy in addressing real issues remains unsatisfactory. It's worth noting that, typically, engineers have design rationales (DR) on solution--- planed solutions and a set of underlying reasons---before they start patching code. In open-source projects, these DRs are frequently captured in issue logs through project management tools like Jira. This raises a compelling question: How can we leverage DR scattered across the issue logs to efficiently enhance APR?To investigate this premise, we introduce DRCodePilot, an approach designed to augment GPT-4-Turbo's APR capabilities by incorporating DR into the prompt instruction. Furthermore, given GPT-4's constraints in fully grasping the broader project context and occasional shortcomings in generating precise identifiers, we have devised a feedback-based self-reflective framework, in which we prompt GPT-4 to reconsider and refine its outputs by referencing a provided patch and suggested identifiers. We have established a benchmark comprising 938 issue-patch pairs sourced from two open-source repositories hosted on GitHub and Jira. Our experimental results are impressive: DRCodePilot achieves a full-match ratio that is a remarkable 4.7x higher than when GPT-4 is utilized directly. Additionally, the CodeBLEU scores also exhibit promising enhancements. Moreover, our findings reveal that the standalone application of DR can yield promising increase in the full-match ratio across CodeLlama, GPT-3.5, and GPT-4 within our benchmark suite. We believe that our DRCodePilot initiative heralds a novel human-in-the-loop avenue for advancing the field of APR.},
booktitle = {Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering},
pages = {1706–1718},
numpages = {13},
keywords = {design rationale, issue logs, developer discussion, automated program repair},
location = {Sacramento, CA, USA},
series = {ASE '24}
}

@article{programrepaireval,
author = {Le-Cong, Thanh and Nguyen, Dat and Le, Bach and Murray, Toby},
title = {Towards Reliable Evaluation of Neural Program Repair with Natural Robustness Testing},
year = {2025},
issue_date = {September 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {34},
number = {7},
issn = {1049-331X},
url = {https://doi.org/10.1145/3716167},
doi = {10.1145/3716167},
abstract = {Automated program repair (APR) has recently gained ground, with numerous research efforts being conducted in the area that have been adopted in the industry. One notable class of APR is neural program repair (NPR), which typically employs deep learning techniques that are trained on vast amounts of historical data to fix bugs that have not been seen in the past. To study the true effectiveness of NPR on existing limited datasets, recent work augments the evaluation data by employing semantics-preserving transformations to convert original buggy programs to semantically equivalent ones. Experiments show that NPR techniques are not robust; e.g., NPR cannot repair semantically equivalent counterparts of 20\%–35\% of bugs that they can repair in the original dataset. However, we found that many of these transformations are unnatural, that are unlikely to occur in real-world scenarios, leading to misleading conclusions about NPR effectiveness and misguide the improvement on unrobust behaviors, which have minimal real-world impact.In this article, we propose shifting the focus of robustness evaluation for NPR techniques towards naturally occurring data transformations. To accomplish this, we first examine the naturalness of semantic-preserving transformations through a two-stage human study. This study includes: (i) interviews with senior software developers to establish concrete criteria for evaluating the naturalness of these transformations and (ii) a survey involving 10 developers to assess the naturalness of 1,178 transformations, i.e., pairs of original and transformed programs, applied to 225 real-world bugs. Our findings show that only 60\% of these transformations are considered natural, while 20\% are considered unnatural, with strong agreement among the annotators. Moreover, the unnaturalness of these transformations significantly impacts both their applicability to benchmarks and the conclusions drawn from robustness testing.Next, we conduct natural robustness tests on NPR techniques to assess their true effectiveness against real-world data variations. Our experimental results reveal a substantial number of prediction changes in NPR techniques, leading to significant reductions in both plausible and correct patch rates when comparing performance on the original and transformed datasets. Furthermore, we observe notable differences in performance improvements between NPR techniques, suggesting potential biases in the evaluation of NPR introduced by limited datasets. Finally, we explore automating the assessment of transformation naturalness by developing a new naturalness metric, namely RNC, using large language models. This metric effectively evaluates naturalness with an AUC of 0.7, offering a promising direction for automating the naturalness assessment of code transformations.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = aug,
articleno = {213},
numpages = {44},
keywords = {Automated Program Repair, Natural Robustness, Code Naturalness, Code Transformations}
}

@inproceedings{ishizue2024improved,
author = {Ishizue, Ryosuke and Sakamoto, Kazunori and Washizaki, Hironori and Fukazawa, Yoshiaki},
title = {Improved Program Repair Methods using Refactoring with GPT Models},
year = {2024},
isbn = {9798400704239},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3626252.3630875},
doi = {10.1145/3626252.3630875},
abstract = {Teachers often utilize automatic program repair methods to provide feedback on submitted student code using model answer code. A state-of-the-art tool is Refactory, which achieves a high repair success rate and small patch size (less code repair) by refactoring code to expand the variety of correct code samples that can be referenced. However, Refactory has two major limitations. First, it cannot fix code with syntax errors. Second, it has difficulty fixing code when there are few correct submissions. Herein we propose a new method that combines Refactory and OpenAI's GPT models to address these issues and conduct a performance measurement experiment. The experiment uses a dataset consisting of 5 programming assignment problems and almost 1,800 real-life incorrect Python program submissions from 361 students for an introductory programming course at a large public university. The proposed method improves the repair success rate by 1-21\% when the set of correct code samples is sufficient and the patch size is smaller than Refactory alone in 16-45\% of the cases. When there was no set of correct code samples at all (only the model answer code was used as a reference for repair), method improves the repair success rate by 1-43\% and the patch size is smaller than Refactory alone in 42-68\% of the cases.},
booktitle = {Proceedings of the 55th ACM Technical Symposium on Computer Science Education V. 1},
pages = {569–575},
numpages = {7},
keywords = {generative ai, program repair, programming assignment},
location = {Portland, OR, USA},
series = {SIGCSE 2024}
}

@inproceedings{pineda2025beyond,
author = {Pineda, Moises and Luna, Diego and Esquivel, Mariana and Bours, Jes\'{u}s and Salazar, Juan and Flores-Araiza, Dainel and Hinojosa, Salvador},
title = {Beyond SWE-Bench: A Compiler-Assisted Pipeline for&nbsp;Multi-language Automated Program Repair},
year = {2025},
isbn = {978-3-032-09043-0},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-032-09044-7_9},
doi = {10.1007/978-3-032-09044-7_9},
abstract = {Automated program repair (APR) research predominantly focuses on Python environments, creating significant infrastructure gaps for compiled languages like C, C++, and Java that dominate production systems. We present the first systematic pipeline addressing multi-language APR infrastructure limitations through compiler-assisted dataset curation and paradigm-aware evaluation frameworks. Our approach combines a DFA-based code classification system achieving 92.4\% accuracy in programming paradigm detection with systematic dataset filtering that processes over 3 million samples to extract 30,000 high-quality object-oriented examples. Initial evaluation on Qwen3-14B using LoRA fine-tuning reveals critical adaptation thresholds: effective multi-language adaptation requires modification of approximately 1.2\% or more model parameters, with lighter fine-tuning underperforming baseline models. Our open-source pipeline provides end-to-end infrastructure from compiler-assisted dataset curation to cloud deployment, enabling systematic research advancement in multi-language automated program repair and establishing methodological foundations for compiler-assisted machine learning across diverse programming environments.},
booktitle = {Advances in Soft Computing: 24th Mexican International Conference on Artificial Intelligence, MICAI 2025, Guanajuato, Mexico, November 3, 2025, Proceedings, Part II},
pages = {115--127},
numpages = {13},
keywords = {automated program repair, multi-language systems, dataset curation, compiler techniques, AI for Software Engineering},
location = {Guanajuato, Mexico}
}

@inbook{huang2025templateguided,
author = {Huang, Kai and Zhang, Jian and Meng, Xiangxin and Liu, Yang},
title = {Template-Guided Program Repair in the Era of Large Language Models},
year = {2025},
isbn = {9798331505691},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE55347.2025.00030},
abstract = {Recent advancements in automated program repair (APR) have been significantly driven by the application of Large Language Models (LLMs). In particular, the integration of LLMs with traditional template-based repair methods has demonstrated effective outcomes. Despite this, the synergy between the strengths of traditional methods and LLMs remains underexploited. This oversight originates from the indiscriminate use of templates and their insufficient coverage. Also, using small-scale LLMs within the zero-shot learning context proves to be suboptimal.To alleviate the limitations, we propose NTR (Neural Template Repair), a two-stage repair framework including template selection and patch generation, both of which are under the fine-tuning paradigm. In the template selection phase, we formulate it as a multiclass classification problem and fine-tune million-level LLMs for better selecting possible templates. During the patch generation phase, we leverage the chosen templates as probable directions (e.g., 'Mutate Conditional Expression') to guide the fine-tuning process of LLMs at the billion-level scale for precise patch creation. Moreover, we incorporate a unique template to signify the absence of a suitable template and employ a probability-based prioritization of templates, thereby optimizing patch generation. This framework not only effectively addresses template mismatch issues, but also enables the billion-level LLMs to explore the patch space more efficiently, despite the GPU memory constraints.We evaluate NTR with different foundational models on Defects4J V1.2 and HumanEval-Java, the framework consistently demonstrates significant effectiveness. When utilizing StarCoder as the foundational model for patch generation, NTR fixes 128 and 129 bugs in Defects4J and HumanEval, outperforming the best baseline APR tool by 14 and 59 bugs. With the larger CodeLlama model, the fixed bugs rise to 139 and 136, respectively, exceeding the baseline by 25 and 66 bugs. Notably, the performance stems not only from the foundational models but also benefits greatly from our NTR framework. Specifically, NTR's implementation with StarCoder and CodeLlama leads to 22 and 23 additional fixes, which is beyond what the models achieve on their own. This emphasizes the success of our new perspective on utilizing templates to unlock the bug-fixing potential of LLMs.},
booktitle = {Proceedings of the IEEE/ACM 47th International Conference on Software Engineering},
pages = {1895–1907},
numpages = {13}
}

@inproceedings{tare,
author = {Zhu, Qihao and Sun, Zeyu and Zhang, Wenjie and Xiong, Yingfei and Zhang, Lu},
title = {Tare: Type-Aware Neural Program Repair},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00126},
doi = {10.1109/ICSE48619.2023.00126},
abstract = {Automated program repair (APR) aims to reduce the effort of software development. With the development of deep learning, lots of DL-based APR approaches have been proposed using an encoder-decoder architecture. Despite the promising performance, these models share the same limitation: generating lots of untypable patches. The main reason for this phenomenon is that the existing models do not consider the constraints of code captured by a set of typing rules.In this paper, we propose, Tare, a type-aware model for neural program repair to learn the typing rules. To encode an individual typing rule, we introduce three novel components: (1) a novel type of grammars, T-Grammar, that integrates the type information into a standard grammar, (2) a novel representation of code, T-Graph, that integrates the key information needed for type checking an AST, and (3) a novel type-aware neural program repair approach, Tare, that encodes the T-Graph and generates the patches guided by T-Grammar.The experiment was conducted on three benchmarks, 393 bugs from Defects4J v1.2, 444 additional bugs from Defects4J v2.0, and 40 bugs from QuixBugs. Our results show that Tare repairs 62, 32, and 27 bugs on these benchmarks respectively, and outperforms the existing APR approaches on all benchmarks. Further analysis also shows that Tare tends to generate more compilable patches than the existing DL-based APR approaches with the typing rule information.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {1443–1445},
numpages = {3},
keywords = {neural networks, program repair},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{gresino,
author = {Kim, YoungJae and Park, Yechan and Han, Seungheon and Yi, Jooyong},
title = {Enhancing the Efficiency of Automated Program Repair via Greybox Analysis},
year = {2024},
isbn = {9798400712487},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3691620.3695602},
doi = {10.1145/3691620.3695602},
abstract = {In this paper, we pay attention to the efficiency of automated program repair (APR). Recently, an efficient patch scheduling algorithm, Casino, has been proposed to improve APR efficiency. Inspired by fuzzing, Casino adaptively chooses the next patch candidate to evaluate based on the results of previous evaluations. However, we observe that Casino utilizes only the test results, treating the patched program as a black box. Inspired by greybox fuzzing, we propose a novel patch-scheduling algorithm, Gresino, which leverages the internal state of the program to further enhance APR efficiency. Specifically, Gresino monitors the hit counts of branches observed during the execution of the program and uses them to guide the search for a valid patch. Our experimental evaluation on the Defects4J benchmark and eight APR tools demonstrates the efficacy of our approach.},
booktitle = {Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering},
pages = {1719–1731},
numpages = {13},
keywords = {automated program repair, patch scheduling, greybox analysis},
location = {Sacramento, CA, USA},
series = {ASE '24}
}
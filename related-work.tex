%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Please Keep each line less than 80 characters.
%
% requires: data/bibtex/slr.bib
% requires packge: natbib,
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{相 关 工 作}

% \par We present a Systematic Literature Review (SLR) including articles
% published in January 2021 and Feburary 2026 that focuses on the use of
% LLM \citep{llmsurvey, csllm} based solutions to Automated Program Repairs 
% (APR).
% The SLR follows the methdology proposed by Kitchenham et al.
% \citep{slrguidese, segress}, used in many SE-related SLRs
% \citep{dl4defence, ml4se, llm4se, llm4se2}. Following the guidelines provided by
% Kitchenham et al., our methodology include three main steps:
% planning the review (i.e. Section~\ref{sec:search}), conducting the review
% (i.e. Section~\ref{sec:selection}), and analyzing the basic review results 
% (i.e. Section~\ref{sec:analysis}).
\par 我们进行了一项系统文献综述（SLR），涵盖了2021年1月至2026年2月期间发表的文章，
重点关注基于大型语言模型（LLM）\citep{llmsurvey, csllm}
的自动程序修复（APR）\citep{asrsurvey, asrbib, aprplm} 解决方案的使用情况。
这项SLR遵循了Kitchenham等人提出的方法论\citep{slrguidese, segress}，该方法已被许
多与软件工程相关的SLR使用\citep{dl4defence, ml4se, llm4se, llm4se2}。
根据Kitchenham等人提供的指南，我们的方法论包括三个主要步骤：
规划审查（即第\ref{sec:search}节），进行审查（即第\ref{sec:selection}节），以及
分析基本审查结果（即第\ref{sec:analysis}节）。

% \section{Search Strategy}\label{sec:search}
\section{检索方法}\label{sec:search}

% \par We employed the \textbf{Precise Search Strategy} \citep{optimalsearch}
% for article search, i.e., we crafted the search string to maximize the relevance 
% of resulting studies. More specifically, we followed the following three steps
% in order to establish a set of relevant studies:
%
% \begin{enumerate}
%  \item Conduct an automated search based on our crafted search string;
%  \item Screen the title and abstract of all articles and filter by
%        inclusion/exclusion criteria;
%  \item Conduct snowballing search on the result of previous step.
% \end {enumerate}

\par 我们采用了\textbf{精确搜索策略} \citep{optimalsearch}进行文章检索，
即我们精心设计了搜索字符串，以最大化结果研究的相关性。更具体地说，
我们遵循以下三个步骤来建立一组相关研究：

\begin{enumerate}
    \item 基于我们设计的搜索字符串进行自动化搜索；
    \item 筛选所有文章的标题和摘要，并根据包含/排除标准进行过滤 (见表\ref{tab:criteria})；
    \item 对上一步的结果进行滚雪球式搜索 (Snowballing Search)。
\end{enumerate}

% \par Our search string needs to combine three set of keywords: 
% one pertaining to software defects, one related to large language models,
% last about the task definition. The complete set of search keywords is
% as follows:
\par 我们的搜索字符串需要结合三组关键词：一组与软件缺陷相关，一组与大型语言模型相关，
最后一组与任务定义相关。完整的搜索关键词如下：
\begin{itemize}
  % \item \textit{Keywords related to LLMs}:
  \item \textit{与大语言模型相关的关键词}:
  large language model, LLM, GPT, CodeX, agent
  % \item \textit{Keywords related to Task Definition}:
  \item \textit{与任务定义相关的关键词}:
  repair, resolve, reproduce, fix, localize
  % \item \textit{Keywords related to software defects}:
  \item \textit{与软件缺陷相关的关键词}:
  bug, defect, vulnerability, crash
\end{itemize}

% \par It is important to note that the list of keywords related to 
% LLMS we setup includes \textit{agent}, that does not seem to be
% necessarily related to LLMs. The reason for this is that we observe 
% recent advancement in agentic technologies \citep{agents}
% and their applications in Software Engineering
% \citep{agent4bugfixempirical,agenticbugreproductioneffective,langgraphbugfix},
%  and that we want to avoid omitting
% articles related to our research as much as possible.
\par 需要注意的是，我们设置的与LLMs相关的关键词列表中包括\textit{agent}，
这似乎并不一定与LLMs相关。
原因是我们观察到最近在代理技术方面的进展\citep{agents}以及它们在软件
工程中的应用
\citep{agent4bugfixempirical,agenticbugreproductioneffective,langgraphbugfix}，
我们希望尽可能避免遗漏与我们的研究相关的文章。

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \paragraph{Search Databases}. After determining the search strings, we
% employed the SLR Tool\citep{slrtool} and conducted
% an automated search across four widely used databases
% \footnote{ACM Digital Library, arXiv, IEEE Xplore, Springer}, 
% which cover most published
% articles and preprints of under-review articles. Given that the first
% LLM trained on code corpus was published in 2021\citep{codex}, we focus our search on articles
% published from that year onward. The search results from each database were
% merged and deduplicated with SLR Tool\citep{slrtool}. Specifically, we obtained
% 373 articles from ACM Digital Library, 213 articles from arXiv, 9 articles
% from IEEE Xplore, and 1187 articles from Springer.
\paragraph{搜索数据库}. 确定搜索字符串后，我们使用SLR工具\citep{slrtool}在四个广泛
使用的数据库
\footnote{ACM Digital Library, arXiv, IEEE Xplore, Springer}上进行了自动化搜索，
这些数据库涵盖了大多数已发表的文章和正在审核的预印本文章。
由于第一个在代码语料库上训练的LLM是在2021年发布的\citep{codex}
，我们将搜索重点放在从那年开始发表的文章上。
每个数据库的搜索结果都通过SLR工具\citep{slrtool}进行了合并和去重。
具体来说，我们从ACM Digital Library获得了373篇文章，从arXiv获得了213篇文章，
从IEEE Xplore获得了9篇文章，从Springer获得了1187篇文章。

% \section{Study Selection}\label{sec:selection}
\section{研究选择}\label{sec:selection}

\begin{table}[ht]
    % \caption{Inclusion and Exclusion Criteria}
    \centering
    \caption{包含和排除标准}
    % Inclusion: The article claims that an LLM is used;
    % The article claims that the study involves an debugging task;
    % The article is full-text accessible.
    % Exclusion:
    % Short articles with less than 8 pages;
    % Tool demos and editorials;
    % Duplicate articles or similar studies with different versions from the same author.
    \label{tab:criteria}
\begin{tabular}{ll}
\hline
\multicolumn{2}{l}{Inclusion Criteria} \\ \hline
(1)                 & 文章声称大语言模型被应用 \\
(2)                 & 文章声称研究解决的是代码调试问题 \\
(3)                 & 文章全文可以访问 \\ \hline
\multicolumn{2}{l}{Exclusion Criteria} \\ \hline
(1)                 & 重复或来自同一作者不同版本的文章 \\
(2)                 & 工具展示,新闻报道或教程  \\
(3)                 & 非英语写作的文章 \\ \hline
\end{tabular}
\end{table}

% \par Using our search strategy, we initially obtained 1782 articles
% that potentially relate to our research. Next, we further 
% evaluate the relevance of these articles based on our inclusion
% and exclusion criteria, as shown in Table~\ref{tab:criteria}.
% In this step, we conducted a manual selection by examining the 
% title, abstract, and keywords of each article, resulting
% in a total of \TODO{} papers.
\par 使用我们的搜索策略，我们最初获得了1782篇可能与我们的研究相关的文章。
接下来，我们根据表\ref{tab:criteria}中所示的包含和排除标准进一步评估这些
文章的相关性。
在这一步中，我们通过检查每篇文章的标题、摘要和关键词进行手动选择，
最终得到了TODO篇论文。

% \paragraph{Snowballing Search. } To identify any additional 
% possibly relevant primary studies, we conducted a snowballing 
% search. Snowballing search refers to using the reference list
% of an article or the citations to the article to identify additional
% articles. As a result, we obtained an additional TODO papers.
\paragraph{滚雪球式搜索\citep{llm4se2}} 为了识别任何可能相关的主要研究，
我们进行了滚雪球式搜索。
滚雪球式搜索是指使用文章的参考文献列表或对该文章的引用来识别其他文章。结果，
我们获得了额外的TODO篇论文。

% \section{Data Extraction and Analysis}\label{sec:analysis}
\section{数据提取与分析}\label{sec:analysis}

% \par To provide a comprehensive overview of the LLM for automatic debugging
% task, it is important to fully comprehend how these models are currently
% being applied to bug localization, reproduction, and repairment; as well
% as the challenges they are facing.
% In this section, we try to answer the following research questions:

\par 为了全面概述LLM在自动调试任务中的应用，重要的是要充分理解这些模型目前是如何被应用于
错误定位、重现和修复的，以及它们面临的挑战。
在本节中，我们尝试回答以下研究问题：

% \paragraph{RQ1}: How are debugging-related datasets collected, preprocessed,
% and used in LLMs? \textit{RQ1} aims to discover the strategies of
% dataset collection, the criteria for dataset selection and the 
% pre-processing steps necessary for boosting the performances of coding 
% LLMs. We also provide a list of up-to-date benchmarks,
% which are based on real-world scenarios, for evaluating
% LLM-based program repair techniques.
\paragraph{RQ1:} 调试相关的数据集是如何被收集、预处理和应用于LLMs的？
\textit{RQ1}旨在发现数据集收集的策略、
数据集选择的标准以及提升编码LLMs性能所需的预处理步骤。我们还提供了
一个基于真实场景的最新基准列表，用于评估基于LLM的程序修复技术。

% \paragraph{RQ2}: what techniques are applied to improve the performance
% of large language models\footnote{and agentic systems} for program 
% repairment? \textit{RQ2} explores the effectiveness 
% of different optimization techniques in the context of LLM for 
% automatic program repairment. This includes an investigation into 
% Retrieval Augmentation \citep{ragnlp, ragsurvey}, 
% Prompt Engineering \citep{promptsurvey}, as well as Agentic 
% Corportation \citep{agents}, which are designed to enhance the performance of
% LLMs on various NLP tasks.
%^C Nevertheless, Li et al. discovered that main-stream LLMs have already
%^C overfit existing APR datasets (e.g. Defects4j)\citep{generalizabilityapr}.

\paragraph{RQ2:} 目前有哪些技术被应用于提升大语言模型\footnote{以及多智能体系统}
在程序修复方面的性能？\textit{RQ2}探讨了不同优化技术在LLM自动程序修复背景下
的有效性。这包括对检索增强\citep{ragnlp, ragsurvey}
、提示词工程\citep{promptsurvey}、和多智能体协作\citep{agents}等
用于增强大语言模型表现技术的总结。

% \paragraph{RQ3: } how do researchers verify the correctness of patches
% generated by code LLMs? Fixing bugs may introduce new bugs in a subtle
% way and potentially affect the end-use, 
% especially when the bugs are fixed by developers who does not have 
% enough knowledge of the repository \citep{incorrectfix}.
% Moreover, the code generated LLM may contain bugs or vulnerabilities.
% For instance, Pearce et al. prompt Github Copilot\citep{copilot} to 
% generate code in scenarios relevant to high-risk cybersecurity 
% weaknesses, and found that 40\% of 1689 programs are vulnerable
% \citep{copilotsecurity}.
% This RQ hence aims to uncover common patch verification techniques, 
% including formal verification and enhanced unit testing, and their 
% effectiveness.

\paragraph{RQ3:} 研究人员如何验证代码LLMs生成的补丁的正确性\citep{patchcorrectness}？
修复错误可能以微妙的方式引入新的错误，并可能影响最终用户，尤
其是当错误由对代码库没有足够了解的开发人员修复时\citep{incorrectfix}。
另外, 大语言模型生成的代码可能有缺陷和安全漏洞; 例如, Pearce et al.
提示Copilot\citep{copilot}在网络安全非常敏感的场景下生成代码, 发现
1689个生成的程序中有40\%有安全漏洞\citep{copilotsecurity}。 
因此，这个研究问题旨在揭示常见的补丁验证技术，包括形式化
验证和增强单元测试，以及它们的有效性。

%^C Experimental. Depends on literature review result.
%^C Me: it is likely that several papers have addressed this...
% \paragraph{RQ4:} How do researchers measure the generalizability
% of their program repair methodologies? The extensive use of 
% code-related corpus in training Code Language Models (CLMs)
% raises concerns about whether Large Language Models' achievements
% in automated program repair stem from genuine code understanding,
% or mere memorization of training data -- particularly the patch
% to software bugs. Kong et al. proposed a general hypothesis
% testing framework to study such memorization issue and
% found a high likelihood of memorization of Defects4j\citep{defects4j}
% -- a common benchmark for automated program repair 
% \citep{demystifymemorization}. Li et al. evaluated 11 top-performing
% LLMs on the Defects4j-Trans dataset (derived from transforming 
% the Defects4j dataset) and concluded that the 
% generalizability of LLMs on APR is limited \citep{generalizabilityapr}.
% These evidences highlight the importance of evaluating
% the generalizability of LLM-driven program repair techniques.

\paragraph{RQ4:} 研究人员如何衡量其程序修复方法的泛化能力？
在训练代码相关语料库的大语言模型中广泛使用，引发了人们对大语言模型在自
动程序修复方面的成就是否源于真正的代码理解，还是仅仅是对训练数据的
记忆——特别是对软件错误的补丁的记忆的担忧。
Kong等人提出了一个通用假设测试框架来研究这种记忆问题，并发现
Defects4j\citep{defects4j}——一个常见的自动程序修复基准--有很高
的记忆可能性\citep{demystifymemorization}。
Li等人在Defects4j-Trans数据集（源自Defects4j数据集的转换）上
评估了11个表现最好的大语言模型，并得出结论认为大语言模型在自动程序修复方
面的泛化能力有限\citep{generalizabilityapr}。
这些证据显示评价LLM驱动的程序修复的泛化能力的重要性。

% \subsection{RQ1: Dataset \& Benchmark}
\subsection{RQ1: 数据集与基准测试}

%^C WARN: this is not your note. Instead of simply stacking the collected
%^C papers, try to organize your content
% and make the review readable -- like Hou et al. did
%^C \citep{llm4se2}.

%^C Begins with a short introduction: importance of data; 
%^C how (you plan) to analyze collected papers.
%^C I'm interested in addressing:
%^C * How are these datasets collected?
%^C     sources, pre-processing steps, etc.
%^C * What types of dataset have been used in APR research?
%^C    by programming languages, or scenarios ("web app", "mobile app", "snippet", etc.)
%^C 
\par Data plays a crutial role in both training and fine-tuning 
large language models \citep{sun2022importance}. Benchmarks,
on the other hand, not only aid evaluations of proposed techniques,
they also help researchers conduct empirical studies on bug
characteristics \citep{androiddataset}.
In this RQ, we analyze the datasets and benchmarks collected via
snowballing search, 29 in total (including several bug builders
that automatically mine buggy and patch commits from repositories
\citep{fixeval, jiang2022bugbuilder, minecpp, precisebugcollector}).

\paragraph{Where are APR datasets collected?}
We first investigate the methods used to obtain the 
repositories or programs where bugs are digged later.
Following the terminologies of Hou et al. \citep{llm4se2},
we found that most program repair datasets are \textit{collected datasets}
-- those that researchers compile directly from a multitude of sources,
except the public unified bug dataset for java built by Ferenc
et al.\citep{ferenc2018unified}, which is (by definition) a
\textit{constructed dataset}.
These \textit{collected dataset} are usually collected from 
code repositories  (e.g. GitHub, F-Droid) \citep{androiddataset, bears, bugscpp, bugsdotjar}, 
CVE lists \citep{cvedataset, cvefixes}, 
and student solutions from programming courses or competitions
\citep{codeflaws, condefects, defects4ds, fixeval}, to meet
specific research goals.

\par We also provide a quantitative analysis of where the
dataset is collected.   
The sources of the bug datasets collected in this study are 
summarized in Figure~\ref{fig:data}. Among all datasets, 
the largest proportion (56\%) is derived from public 
repositories. Datasets extracted from programming problems 
and CVE lists each account for 20\% of the total. 
The smallest portion (4\%) consists of composed datasets, which 
are constructed by integrating or modifying multiple existing 
data sources. In particular, only one unified 
public Java dataset~\citep{ferenc2018unified} is categorized as composed. 
This distribution reflects the diversity of data origins 
and highlights the reliance on openly accessible resources, 
complemented by targeted data from programming tasks and vulnerability records.
We assume that the reasons why open-source repositories are popular
are their open-access and their close resemblance of real-world
scenarios, making them suitable for building realistic program repair
tools or conducting empirical studies.

\begin{figure}[ht]
	\centering
	\caption{Source of Studied Datasets. \\
	"Programming Problems" includes both solution to problems of both programming courses
	and contests; "Composed" means the dataset is made by incorporating/modifying
	multiple existing datasets. }
	\label{fig:data}

	\input{figures/data.tex}
\end{figure}

\begin{table}[ht]
	\centering
	\caption{Source of Studied Datasets. \\
	"Programming Problems" includes both solution to problems of both programming courses
	and contests; "Composed" means the dataset is made by incorporating/modifying
	multiple existing datasets. }
	\label{tab:data}

	\input{figures/data-tab.tex}
\end{table}

\paragraph{How are APR datasets pre-processed?}
After collecting these initial buggy code
repositories or snippets, depending on the dataset, there are 
one or more pre-processing (filtering) steps, including identifying bug-fixing
patches \citep{jiang2022bugbuilder}, reproducing bugs or 
manual patch review. After this pre-processing
step, the ground-truth bug location and correct patch is obtained,
setting up the basis of fault localization evaluation and patch correctness
verification.
For instance, the widely 
used Defects4j dataset \citep{defects4j} is collected by 
first mining the commit history of 5 open-source Java projects, 
then filtering commits that are likely to be bug-fixing patches 
based on keywords in commit messages, reproducing bugs
to filter out flaky tests, isolating bugs to filter out irrelevant commits
(e.g. those adding new features), and finally obtain the source code diffs
of reproducible bugs. In reviewing dataset/benchmark construction
methodologies, we have found two cases which obviously deviate from 
the common practice of pre-processing steps.
The BEARS (BEnchmark for Automated Repair Studies)
benchmark employ a novel strategy in finding potential buggy and patched
commit of program -- by checking the Continuous Integration (CI)
building states \citep{bears}. Elisa et al. utilized static analysis to find real-world
violations of Google's Android Security Best Practices
\footnote{\url{https://developer.android.com/privacy-and-security/security-tips}}, 
which is more
efficient than checking commit messages in mining software bugs, and then
manually verify the exploitability of detected vulnerabilities and
manually crafted patches \citep{androiddataset}.
It is worth noting that \textbf{single statement bugs} are one of the most
important ingredient of evaluating modern automated bug detection and program
fix techniques \citep{tssb}, several works provide insight into discovering
single-statement bug fix \citep{manysstubs4j, tssb}.



\paragraph{What are the programming language(s) of each dataset?}

%^C possible future work: not enough support for newer languages,
%^C Golang, Rust, Typescript, etc.

\begin{figure}[ht]
	\centering
	\caption{Language Distribution of Collected Datasets.}
	\label{fig:lang}

	\input{figures/lang.tex}
\end{figure}

\paragraph{RQ1 -- Summary}

\begin{figure}[ht]
    \centering
    \caption{The most common steps to build a bug dataset.}
    \label{fig:rq1}
\resizebox{0.5\textwidth}{!}{
	\begin{tikzpicture}[
        scale=0.5,
		function/.style={diamond, draw=blue!60, fill=blue!5, very thick, minimum size=5mm},
        artifact/.style={rectangle, draw=red!60, fill=red!5, very thick, minimum size=5mm},
	]
        \node[function,align=left] (F0) {
            Project Selection
        };

        \node[artifact,align=left] (A1) [below=of F0] {
            Open-source Repositories
        };

        \node[function,align=left] (F1) [below=of A1] {
            Pre-processing
        };

        \node[artifact,align=left] (A2) [below=of F1] {
            Bug Locations \\
            Tests exposing the bug \\
            Correct Patches
        };

        \node[artifact,align=left] (A3) [right=of F1] {
            Bug Datasets/Benchmarks
        };

        \draw[thick, ->] (F0) -- (A1);
        \draw[thick, ->] (A1) -- (F1);
        \draw[thick, ->] (F1) -- (A2);
        \draw[thick, ->] (A1) -| (A3);
        \draw[thick, ->] (A2) -| (A3);
	\end{tikzpicture}
}
\end{figure}

\begin{itemize}
	\item The most commonly adopted process of building a bug dataset 
	is shown in Figure~\ref{fig:rq1}. It begins with project selection
	-- selecting a set of representative code repositories or programs.
	 The collected code then undergo a pre-processing step, which 
	includes identifying bug locations, tests that expose the bugs, 
	and correct patches. Finally, the processed data is compiled 
	into bug datasets or benchmarks for further research and evaluation.
\end{itemize}


% \subsection{RQ2: LLM Enhancement Techniques for APR}
\subsection{RQ2: }

% \subsection{RQ3: Patch Verification}
\subsection{RQ3: }

% \subsection{RQ4: Generalizability Concerns}

